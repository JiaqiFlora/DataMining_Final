question,answer
data request - Is there a list of all US Government agencies and sub agencies and is it available via API?,"
Check out:

An official source for agency information: https://www.federalregister.gov/api/v1/agencies
GSA API: http://www.usa.gov/About/developer-resources/federal-agency-directory/index.shtml

Source: https://github.com/unitedstates/orgchart/issues/1
"
usa - Is there a list of all municipal government forms in a machine readable format?,"
Probably not.
There is definitely no such list for Washington, DC (Government of the District of Columbia).
There was forms automation project back in 2006/2007 where some forms where converted to database backend using LiquidOffice tool. You can see list of all currently published forms for DC.gov here http://forms.dc.gov/index.html (click Forms on the left). But beware, there are plenty of forms that are just stan alone PDFs or custom webpages.
"
usa - Is there a list of all utilities that offer the Green Button Download and Green Button Connect?,"
The Department of Energy's Green Button website offers a list of companies who have implemented it:

The Green Button initiative was officially launched in January 2012.
  To date, a total of 35 utilities and electricity suppliers have signed
  on to the initiative. In total, these commitments ensure that 36
  million homes and businesses will be able to securely access their own
  energy information in a standard format. This number will continue to
  grow as utilities nation-wide voluntarily make energy data more
  available to their customers in this common, machine-readable format.
The following utilities have already committed to Green Button:
  American Electric Power, Austin Energy, Baltimore Gas & Electric,
  CenterPoint Energy, Chattanooga EPB, Commonwealth Edison, Glendale
  Water and Power, National Grid, NSTAR, Oncor, Pacific Power, Pepco
  Holdings, PG&E, PECO, Portland General Electric, PPL Electric
  Utilities, Reliant, Rocky Mountain Power, SDG&E, Southern California
  Edison, TXU Energy, and Virginia Dominion Power.

... there is also a much more substantial list of companies who have have expressed interest in supporting it.
"
usa - U.S. City and County Web Data (API),"
Using the Census Bureau's API, you can retrieve quite a bit of information about counties, but you need to learn where it's stored. You can request up to 50 different variables in a single call, and you can ask for all counties in the US with one call.
To the census, ""cities"" are ""places,"" and that includes things that you and I might think of as a city but which are not exactly cities legally.
For example, this URL would return the total population from the 2010 Decennial Census for all US counties, as a list of lists (in JSON format). The first row is a header, followed by 3221 rows, one per county. The county name and state and county FIPS codes are included in addition to the variables requested.
http://api.census.gov/data/2010/sf1?key=[your API key here]&get=P0010001,NAME&for=county:*
And this is for all the ""places"" in the US (almost 30K):
http://api.census.gov/data/2010/sf1?key=[your API key here]&get=P0010001,NAME&for=place:*
There's more info about the API at http://www.census.gov/developers/ and you can request a key here.
Update:
The Census Bureau is developing a CitySDK project which is specifically intended to make it easier for developers to pull together city-level data, including, eventually, data from sources other than the US Census. The project is also an effort to experiment with agile development methodologies and community involvement, so if you're at all interested, they invite suggestions of user stories and other participation. See also the Github project and Waffle.io project board.
"
usa - Does any City have POIA (Public Online Information act) laws in place?,"
The closest law I've seen to being as comprehensive as POIA is NYC's Local Law 11, but many other cities have an open data law with looser language about whether ""all"" public data has to be put online.
More broadly speaking there's a fairly comprehensive list of city open data laws at:
http://wiki.civiccommons.org/Open_Data_Policy It's also worth noting that most cities are covered by the FOIA laws in place by the state that they're in. You can find a list of those laws at http://www.rcfp.org/open-government-guide
"
medical - De-identified patient data,"
CDC's National Health Interview Survey (HIS) provides public use files containing person-level data including demographic data and physical parameters such as weight, and disability. Dating back to 1957, it is an annual survey of a nationally representative sample dates.
CDC's National Health And Nutritional Examination Survey (NHANES) provides public use files containing person-level data including demographic, dietary, examination, and laboratory data.
The Health and Retirement Study (sponsored by the National Institute on Aging, and Social Security Administration) is longitudinal panel cohort study that started in 1992, and covers a representative sample of Americans over the age of 50, as they transition into retirement. Only certain person-level data are public use, because of the sensitive data collected including genetic information, prescription drug use, cognitive function, biomarkers, etc. The ""core"" survey is supplemented by focused add-on modules.
The 3 surveys above are used for econometric microsimulation models; in case you wanted another way to search for these datasets that would be one way to back into it. 
Note, de-identified patient data will not include names. 
Concerning EAV models that use large datasets where information in certain categories is relatively sparse, ""sparse"" suggests that an individual could be re-identified. At least with US government release of data, any such sparse data would be suppressed if an individual could be re-identified. Protection of personally identifiable information (PII) and maintaining patient confidentiality are considered fundamental to ensuring public trust and participation in these surveys.
Disclosure: I work for the Asst Secretary for Planning and Evaluation (ASPE) in the Dept of Health and Human Services (HHS) on the open data initiative. Surveys mentioned above are all HHS surveys, though I have no direct connection to any of them.
"
usa - Is the data used to calculate national CPI public?,"
The CPI is based on surveys conducted by BLS (Bureau of Labor Statistics). Those surveys collect price information which is reviewed by commodity experts. Those experts review (and possible adjust) the data based on their knowledge of the particular commodity. That adjusted data is then averaged to calculate the CPI. You can definitely get the values that are averaged in the final calculation. I'm less clear on how to find the actual survey responses.

Various options for consuming the data: http://www.bls.gov/cpi/data.htm
The average price data: ftp site, documentation

"
data request - Is there a complete list of all US tax-exempt nonprofits in machine-readable format?,"
The IRS website has an Exempt Organizations Select Check that allows the user to search for organizations that:

Are eligible to receive tax-deductible contributions
Were automatically revoked
Have filed Form 990-N (e-Postcard)

After selecting one of these options, a link will appear to download the entire database of organizations. The databases (plain-text files) are delimited by a vertical bar and should be easily imported into any RDBMS or spreadsheet application.

Database of organizations eligible to receive tax-deductible contributions (Pub. 78 data).
Download page
Database file (16 MB compressed, 59 MB raw)
Database of organizations whose federal tax exemption was automatically revoked for not filing a Form 990-series return or notice for three consecutive years (Automatic Revocation of Exemption List). 
Download page
Database file (16 MB compressed, 55 MB raw)
Database of e-Postcard filings.
Download page
Database file (39 MB compressed, 130 MB raw)

Sample of the database
000344394|Pandas Foundation Inc.|Salem|MA|United States|PC
000587764|Iglesia Bethesda Inc.|Lowell|MA|United States|PC
000635913|Ministerio Apostolico Jesucristo Es El Senor Inc.|Lawrence|MA|United States|PC
The columns (best guess) are:
EIN|Name|City|State|Country|Deductibility Status
The Deductibility Status is a code that:

describes the basis for the organization's or organizations' ability to accept tax-deductible, charitable contributions. 

Deductibility Status Codes
"
"usa - Given a federal agency, what can I expect to find on the regulations.gov API?","
Regulations.gov offers a list of participating agencies in PDF Format on its About Page:

Participating Agencies: http://www.regulations.gov/docs/Participating_Agencies.pdf
Non-Participating Agencies: http://www.regulations.gov/docs/Non_Participating_Agencies.pdf

On the Regulations.gov FAQ, they describe what a non-participating agency does and does not provide:

A Non Participating agency is a Federal agency that publishes Federal
  Register documents on Regulations.gov but does not participate in the
  eRulemaking program; therefore, public comments and additional
  supporting documentation are not posted on Regulations.gov. In order
  to view these comments, users should contact the agency directly. In
  order to find the contact, reference the section in the Federal
  Register entitled ""For further information contact.""

"
usa - How can I get a list of all non-geodata datasets on Data.gov?,"
Currently Data.gov is run on bunch of technologies. Specifically one of the primary data platforms is Socrata - explore.data.gov.
There is API for it defined here http://dev.socrata.com/, but essentially you can call this endpoint and iterate through all pages of master list of datasets
https://explore.data.gov/api/dcat.json?page=1

I have sample scrapers for list of datasets and associated metadata here:
https://github.com/kachok/data-json/tree/master/data-gov
You then can filter spatial vs tabular data based on metadata (I believe all of the datasets in explore.data.gov are tabular)
With imminent migration of data.gov to CKAN, all of the above will be obsolete (but new API will allow to do similar things as well)
"
How can I trust the authenticity of an open data source?,"
There are attempts at creating a registry of open data sites, see ckan's datahub http://datahub.io/. I'm not aware of any effort to list bad sites, but I've not come across any instances of deliberately false open data.
"
licensing - Benefits of using CC0 over CC-BY for data,"
I have worked for numerous companies in the past who have had a policy of not using any software, libraries, or datasets that would impose requirements on their product, this would include the attribution clause in the CC-BY licence.
Placing a dataset in the public domain will maximise its potential audience; whether or not this is more desirable than attribution is going to be a matter of opinion for those involved.
"
analysis - What's an easy-to-use tool to manage datasets?,"
I found OpenRefine really useful for people new to Open Data. It offer a nice and clean spreadsheet like interface with the power of querying by field (like SQL).
"
"data request - Is there an official/complete list of all US government agency ""developers' pages""?","
Here is most comprehensive list of all .gov /Developer pages compiled by Gray Brooks at GSA
http://gsa.github.io/slash-developer-pages/
You can suggest new additions if some info is missing. It is currently well maintained.
"
data request - Is there a free downloadable administrative division database of Germany?,"
Here is German Open Data portal for geodata - http://www.geodatenzentrum.de/geodaten/gdz_rahmen.gdz_div?gdz_spr=eng&gdz_akt_zeile=5&gdz_anz_zeile=0&gdz_user_id=0
It has administrative areas, zipcodes and geo names (cities, points of interests), etc available for download (Shapefiles) and as webservices (WMS).
Specifically German addresses data is not available, though.
Also, you can find zipcodes/admin areas on Wikipedia or DBpedia 
http://en.wikipedia.org/wiki/List_of_postal_codes_in_Germany
http://dbpedia.org/page/List_of_postal_codes_in_Germany
"
usa - Should I approach an agency unofficially before FOIAing them?,"
It is very recommended and would very likely smooth the process by at least an order of magnitude.
"
language - Database with English words with grammatical classification?,"
There are plenty of open corpora (databases) of English words available.
Specifically, take a look at Brown and WordNet.
Check out Natural Language Toolkit it is written in Python and has those corpora available for download. It is one of the most popular packages to work with human languages data.
If you prefer to use web based API, take a look at Wordnik API 
"
data request - Multinational list of popular first names and surnames?,"
There is a pretty massive list of given (first) names (~50,000), and it's carefully curated (not machine generated).
More details are available on another answer:

The best source of international human given (first) names comes from a German computer magazine. The text file has nearly 50k names that are classified by likely gender, and how popular in each country. It's carefully curated and has a friendly license (GNU Free Documentation License).
The file can be downloaded here : ftp://ftp.heise.de/pub/ct/listings/0717-182.zip (name_dict.txt contains the data).

(archive link: https://web.archive.org/web/20200414235453/ftp://ftp.heise.de/pub/ct/listings/0717-182.zip )
"
data request - Database with the historical prices of most popular products in U.S.?,"
The CPI indices from the Bureau of Labor and Statistics is probably what you're looking for. While it doesn't tell you what the price of 12 ounces of Coca Cola is, it does tell you what the average price of, say, ""carbonated beverages"" is over time. It will take some munging, but the raw data is located here
"
"tool request - What server and technologies can I use to extract data out of Wikipedia's infoboxes (e.g., ATC code for drugs)","
Project DBpedia is a crowd-sourced community effort to extract structured information from Wikipedia.
Drugs and Chemicals infoboxes are available in structured form, already.
Check out 
http://dbpedia.org/page/Rosiglitazone
"
government - Open Data Scorecards?,"
The Open Data Institute are building Open Data Certificates ( https://certificates.theodi.org/ ) as a mechanism for describing how a particular dataset does in terms of legal, practical, technical and social information.
It's still being built, but you can see an example at: https://certificates.theodi.org/datasets/347/certificates/90 . Although it does require human input it seems like it might possibly be an effective way of 'grading' open data.
"
geospatial - Where can I get digital cartographic maps of Brazil?,"
Open Street Maps and GEOTIFF Files from MODIS, wold do the job.
"
computing - GPU hardware specifications data?,"
Wikipedia has comparisons of Nvidia, AMD, and Intel GPUs, providing tabular information on different tech specs of the graphical processing units.
"
parsing - Good tools to parse repetitive unstructured data,"
OpenRefine can be used to parse semi-structured data into a table like structure, where it can be operated on in a manner similar to a spread sheet and exported.
The site features a tutorial on converting a list on wikipedia into a table which may be a good starting point.
The operations involved can also be exported incase you need to perform the same cleanup operation again.
"
Graph visualization/analysis tool?,"
There are actually quite a few applications for visualizing and analyzing graphs:

Gephi and Cytoscape are two well-known open source
applications that support large and complex graphs.
If you're mainly interested in visualizing graphs, have a look at
Graphviz, which is an absolute classic.
You can also use R or commercial tools like Mathematica if you're 
more interested in the statistical and analytical aspects (see also this 
question over on Stats SE).

"
data request - What is the most comprehensive resource for querying french and english books having an ISBN code?,"
A quick Google search turns up several APIs.

Google Books
Amazon
LibraryThing
half.com
isbndb.com
Goodreads
Bookfinder
Open Library
WorldCat API

You can limit your queries to these resources by knowing that French ISBNs start with a 2 and English ISBN's start with a 0 or 1.
Edit
As far as getting ISBNs for older books

Reprints of older books will be assigned ISBNs by the publisher. That
  is why books in the public domain (like most 'classics': Shakespeare,
  Chaucer, etc.) are issued by different publishers and have different
  ISBNs. - Source

I would assume this goes for any books that existed before ISBN was in wide use. ISBN became a standard in 1970 but in France, ISBN codes have been mandatory only since the 3rd December of 1981 (link is in French).
"
finance - Where can I find open data on historical forex rates for financial reporting purposes?,"
I've used the Yahoo Finance API in the past, but apparently it doesn't exist (although it works fine). There is no documentation.
For a site with documentation, I'd suggest Open Exchange Rates, which contains forex values that you are looking for, via an API.
There are many API methods and in particular for your case, you can request a time series.
From their site:

You can access historical data snapshots, where available, in the format ‘/api/historical/yyyy-mm-dd.json’ (for example: /api/historical/2011-11-21.json.) There will soon be an 'available dates' endpoint available too.

You'll need a code to cycle over dates, here is an example of my python 2.7 code that collects 2013 precipitation and weather in Zurich.
import requests
def get_precip(gooddate):
    urlstart = 'http://api.wunderground.com/api/REDACTED_KEY/history_'
    urlend = '/q/Switzerland/Zurich.json'

    url = urlstart + str(gooddate) + urlend
    data = requests.get(url).json()
    for summary in data['history']['dailysummary']:
        print ','.join((gooddate,summary['date']['year'],summary['date']['mon'],summary['date']['mday'],summary['precipm'], summary['maxtempm'], summary['meantempm'],summary['mintempm']))

if __name__ == ""__main__"":
    from datetime import date
    from dateutil.rrule import rrule, DAILY

    a = date(2013, 1, 1)
    b = date(2013, 12, 31)

    for dt in rrule(DAILY, dtstart=a, until=b):
        get_precip(dt.strftime(""%Y%m%d""))

update I noticed for the free account you can only make 1000 API calls per month (link). Not so open after all.
"
weather - Data download for Chinese meteorological satellites,"
Try http://satellite.cma.gov.cn/portalsite/Data/Satellite.aspx?currentculture=en-US

You can then download a ""quickview"" by clicking the little ""picture"" icon:

The full resolution data seemingly require payment.
---Information below is OBSOLETE. ---
Try FENGYUN Satellite Data Center. Getting the data from it can be a little tricky.
First click one of the icons:

In the next page, remember to select one of the ""products"" before clicking ""next"":

I haven't really used the system. It seems you need to pay for the data. However, ""quickviews"" are free:

"
data request - How can I download the complete Wikidata database?,"
The wikidata dump is already available. As of now, the last (mostly) complete dump is from 5 May 2013 and it includes the dump of pages in the important namespaces (pages-articles.xml).
"
tool request - Is there any world-wide real time catastrophe information system?,"
As far as I know there is no system in use that would alert people before the disaster.
To all the other questions, quite a lot of countries have adopted Sahana Eden, that does all the tasks related to the Management of the Disaster, including all the information you have putted there and a lot more...
It has been deployed in large disaster scenarios and it is ""as good as it gets"", also it complies with UN Standards!
"
"economics - How can I access open data on the Indian rupee value, inflation and sensex index over time?","
Trading Economic provides information about India inflation over the years (apparently registered users can export this data).
Google finance provides the Rupee value over th years and you can download this as a csv file.
Google finance also provides SENSEX data.
"
tool request - Are there any good libraries available for doing normalization of company names?,"
[Note: I'm the co-founder of OpenCorporates, which along with our reconciliation service for OpenRefine, has been kindly mentioned by several of the answers, but I've tried to cover some of the general issues here, using our experience, rather than suggesting we've got all the answers]
This is a really difficult problem because in general it requires more than just string algorithms, and also because of the underlying question of what you are matching.
So first, what will you be matching the normalised strings to? If you're matching them to companies (legal entities), OpenRefine is really great for that as long as it's for jurisdictions that we have in OpenCorporates (we have about 30 of the US states, for example), or it's matching the names against entries in Wikipedia, using Freebase. I'm not sure of the matching algorithms that Freebase use, but we do all the usual things such as you've got with your ACME examples.
With the Walmart examples, it's rather more difficult. In part this is because sometimes these normalisations would lose information that is helpful in matching to entities. Take the hyphenation issue for example, and say you were searching for Wal-Mart Limited. If you search OpenCorporates on the web interface (which is pretty liberal in what it returns, and for the sake of this example, let's say we're limiting to companies in the UK, and this is what you get:

CREDITS INVESTMENTS LTD (United Kingdom, 16 Feb 2007- )
DOS-MART (UK) LIMITED (United Kingdom, 18 Feb 1999- )
GEORGE SOURCING SERVICES UK LIMITED (United Kingdom, 2 Jun 2000- )
WAL MART LIMITED (United Kingdom, 5 Jan 1999-17 Sep 2002)
WAL-MART LIMITED (United Kingdom, 12 Apr 2006-31 Mar 2009)
WAL-MART LN (UK) LIMITED (United Kingdom, 3 Feb 2000- )
WAL-MART STORES (UK) LIMITED (United Kingdom, 26 Apr 1999- )
WAL-MART STORES LIMITED (United Kingdom, 18 Mar 1994-10 Jul 2001)

Look at the fourth and fifth entries and you can see the problem. One has a hyphen, one doesn't. It's also worth looking at the first three entries, and these were returned because their previous names matched Wal-mart. There are also lots of other cases where normalisation brings false positives.
So although we do some normalisation, we also score against the non-normalised search term in our reconciliation API. We also are increasingly using other attributes of the companies, scoring current companies higher than dissolved companies (company names are often used by unrelated companies over time), unless a date is supplied in which case we return the company that was called that name at the given date.
Finally there's the question of what do you want to match? With Walmart, maybe it's 'obvious', but in general it isn't, even with something like ""Tesco"", the world's second biggest retailer (after Walmart), which could match several unrelated entities around the world, including the US.
That's why when we're matching a dataset using OpenRefine and OpenCorporates we do a first pass, limiting to a jurisdiction, passing dates in where we can, and then automatically reconciling to those entities which have a high score and where there's just one high scoring result returned by the API (there's some cool filtering in Google Refine to do this), and then progressively go down the scores, sometimes using the ability to do live reconciliations, to match the ones with no match, or with ambiguous results.
Because we use the service internally we're constantly finding 'edge cases' (that aren't so edgy), and improving the matching. People who've used it and the proprietary DBs reckon OC is very good. However, we can see there's massive of room for improvement. We're now doing more normalisation for non-US/UK company forms (e.g. GmbH and SA/SARL), and playing around with transliteration, and when we launch company hierarchies, there's obviously potential for assuming that by Walmart/Wal-mart you mean the company at the 'top' of the Walmart tree.
Hope this helps.
Chris  
"
geospatial - Where can I find high-precision cartographic data of French rural areas?,"
Data.gov provides geospatial data for areas around the world through NASA satellite imagery and other shared services.  You can search for mapping data in France or anywhere else at http://geo.data.gov/geoportal/.  You can also see a view of a new geospatial catalog on Data.gov below.  
I've already entered the parameters for France, so you can see the data available.  Just draw the bounding box where you need it to find any geospatial data that is available in that area.
"
tool request - Extracting tables from multiple PDFs,"
I have had great luck with https://github.com/jazzido/tabula
Once the PDF is loaded into the system, it takes manual selection of the table to get the data, but I really prefer it over rolling my own computer vision system, as I've found tabula to be highly accurate, and I can't say the same of a 100% automated system.
"
Tools for merging similar datasets continuously,"
OpenRefine (formerly Google Refine) offers nice tools for data cleansing, e.g. correcting slight spelling variations. You can also script all transformations on the data and re-apply them later for updated datasets.
Is this what you're looking for?
"
data request - What is the best source for finding what businesses have opened/exist/closed in a given geographical area?,"
There are several pay services, including ReferenceUSA and Nexis.
If you're interested specific geographic areas (as opposed to everywhere in the US), you can check with local governments (usually county) for FOIA-able occupational license data and/or data about property records, usually available via tax assessors office, which will often contain ownership information and flags for commercial properties.
A service at Florida International University called TerraFly has tons of geocoded business records. I'm not sure what all their sources are, but it might be worth your while to reach out to the researchers there. I think they have some API access.
"
crowdsourcing - What are the available tools for managing crowdsourced data-cleaning tasks?,"
PyBossa is an open platform for crowd-sourcing. You write a bit of HTML/JS that is the microtask. They have examples including PDF transcribing. Features include user registration, user credits, statistics.
http://crowdcrafting.org/about
"
trust - Open Data Conflict Resolution,"
I will answer this in the more general term (leaving the consumer out). Invariably when data is aggregated and coalesced from plural data sources you will have conflicts in records. Just think of all the wacky occupants you receive in your mailbox or the wacky places you supposedly lived when you do a FREE background search.
These data sources need to periodically perform a validation/reconciliation process on the aggregated/coalesced dataset. Typically, the process involves:

A primary table which holds the 'best values' for a record.
A secondary table which uses the primary key in the first table as a foreign key, which holds the conflicting values of the first table.
A process which consists of a:
A. Validation - eliminates secondary records deemed to be invalid.
B. Reconciliation - chooses the values that are 'best' at the moment.
The process is repeated periodically (or on demand) on records that are updated or new records are added.

Note: The not-best-value records in the secondary table, until deemed invalid, are retained, so that they can be reapplied when other records are updated or added.
"
census - API for information about Brazilian cities/states,"
Both the Brazilian Community CKAN instance http://br.ckan.net/dataset and the Brazilian Government's official Data Portal (CKAN-powered) which you link to, have APIs.
There's also the World Bank API: http://data.worldbank.org/
"
Metadata standards and best practices for data dictionaries for CSV files/data,"
I'd suggest using JSON Table Schema: https://specs.frictionlessdata.io/table-schema
It's:

JSON-based
Super simple
Extendable

Here's a rough outline:

  # fields is an ordered list of field descriptors
  # one for each field (column) in the data
  ""fields"": [
    # a field-descriptor
    {
      ""id"": ""field unique name / id"",
      ""label"": ""A nicer human readable label for the field"",
      ""type"": ""A string specifying the type"",
      ""format"": ""A string specifying a format"",
      ""description"": ""A description for the field""
      ...
    },
    ... more field descriptors
  ]

"
tool request - Cost of ownership - CKAN for city/local government or small federal agency,"
Figuring out the cost for running CKAN boils down to two different categories: the cost of running the software itself, as well as the cost of building and maintaining your open data catalog going forward.
The cost of the software itself is simpler to answer: CKAN itself is free/open-source, and available as a hosted solution. 

If you go the open-source route, you'll need to work out the cost of your employee's time, cost of server resources, bandwidth, storage, etc. Your employee time may be minimized by using CKAN's deployment services to get started. As for hardware, the system requirements for CKAN are minimal (it can run on a single server) and probably won't be a significant factor in your costs. This can grow significantly as your data catalog grows, your catalog usage grows, or as the service becomes critical enough to warrant making your CKAN highly-available. You'll need to know your own data size and usage patterns to accurately predict this.
If you go the hosted route, this is simpler to figure out. CKAN publishes their hosted pricing online - it runs from from between $400 - $3500 at the time of this writing.

The cost to run an effective open data implementation within your government can vary widely. It could be near-zero for a site that appear to be ""abandonware"" but if you want a continually updated site with a growing repository of useful content, you'll need to spend time building and curating it. This goes far beyond the scope of IT; having a champion within the organization with a knowledge of the data produced will almost certainly help, rather than making your CKAN instance ""yet another tool being forced on me.""
"
language - American English SMS Text Message Corpora,"
The largest English corpus I've found (over 10,000 messages) is the National University of Singapore's SMS corpus -- select the corpus with ""all"" messages -- however, closer examination reveals that relatively few of the messages originate from US participants.
A corpus of SMS spam messages has been created which are written in English. There are over 1,000 legitimate messages and only a few hundred are spam-related.
Dr. Caroline Tagg created a corpus of SMS messages (although I believe they are primarily in British English), but I cannot find the corpus online. However, her paper contains hundreds of messages from the corpus.
I created a text message corpus of SMS messages in American English that contains over 4,900 messages, a few hundred of which are related to illegal drug use. Now available only on archive.org.
"
usa - What is the appropriate way to timestamp/determine recency of given data set?,"
The way I read the question it seems like you're asking whether ""old"" data can still be considered recent. 
Larger data sets or data that is published at some frequency may not always be current. It takes time to scrub data for publication. Technically, the data is considered current if you're using the most recent published data. In other words, if a data set is published end of year, you will have ""current"" data up until the new data is published. 
However, if the data is used by an application, typically the users/shareholders of that application understand the publication cycle of data being used.
"
usa - What is the best way to request machine readable data from a FOIA request?,"
A common strategy used by journalists I've worked with is to first FOIA a data schema or other explanation of how the data is managed by the government body. This allows you to be much more explicit in constructing your actual request.
And as rcackerman noted, you may not actually have to start with a FOIA -- but ask them what they have and how they have it, so that when you do file a FOIA for actual data, you can be precise.
"
companies - Non US-centric databases on boards of directors and government agency memberships?,"
From corporate boards of directors, OpenCorporates is a fantastic resource for this kind of thing, if a bit intimidating to wade through.  They do have lots of US data, but also UK, and many other jurisdictions.  It's all scraper-assembled, so not quite as clean as you get from LittleSis, but you can definitely find corporate officers.
I'm a little fuzzy on what you're asking for with respect to governments.  Do you want a list of government employees?  Or a list of corporate actors who serve in government capacities (on advisory committees and the like?)
"
data request - Where can I find open listings of zipcodes in Indonesia?,"
This website claims to contain a complete [list of?] postal code[s] in Indonesia
It is not in a good machine readable format, but the html uses <pre> tags with makes much more easier script the data off there.
"
usa - Is there a centralized schedule of data release dates for U.S. federal agencies?,"
On Data.gov we haven't looked at trying to aggregate a forward-looking schedule. In general we encourage agencies to release data as quickly as possible. Some data releases occur on a very regular basis, but there is often some fluctuation based on the internal approval processes.  I expect as agencies respond to the actions to comply with the new Open Data Executive Order http://www.whitehouse.gov/the-press-office/2013/05/09/executive-order-making-open-and-machine-readable-new-default-government- it will be easier to create such a list.
"
geospatial - Is there an open API for world terrain data?,"
The Google Elevation API should allow you to access elevation data world-wide and allows you to give location as latitude/longitude. 

What Can You Do With the Elevation API?
The Elevation API provides elevation data for all locations on the surface of the earth, including depth locations on the ocean floor (which return negative values). In those cases where Google does not possess exact elevation measurements at the precise location you request, the service will interpolate and return an averaged value using the four nearest locations.

It does have restrictions on its usage unfortunately - ""the Elevation API may only be used in conjunction with displaying results on a Google map; using elevation data without displaying a map for which elevation data was requested is prohibited."" - https://developers.google.com/maps/documentation/elevation/#Limits
"
data request - Are there open complete usenet archives?,"
You should check out Exploring the USENET Archive: Early Thoughts and here is archive.org's archive.
"
data request - Database of ships?,"
From this answer on Get the Data: http://getthedata.org/questions/262/list-of-ocean-going-oil-tankers-and-owner/ (provided by Kit Wallace). Note none of this detail seems to be open data (as per Open Definition).

There are a number of commercial sources such as http://www.ship-info.com/ or restricted sites http://www.equasis.org/ sites with copyright data http://www.digital-seas.com/ but also some amateur sites (which I cant find now).
http://www.shipais.com includes data about the ships it plots - eg. http://www.shipais.com/showship.php?mmsi=256933000 but I'm not sure where that comes from now. Worth investigating.
The ITU holds public details about MMSI numbers in their MARS database which does hold some category data , but its limited - here is the data for tanker with the above MMSI and the database is only searchable by MMSI, name or callsign.
The best source though you need to register and search by individual boat is equasis which contains full ownership details. According to the website

France and the European Commission shared the cost of developing and running Equasis until 31 December 2001 when the maritime authorities of the United Kingdom, Spain, Singapore and Japan also agreed to support Equasis financially. The budget of Equasis is agreed and provided by the Equasis MoU members. It is anticipated that the use of this website will remain free for the foreseeable future


Also:
Marinemapper exports KML files, and seems to have links to a lot of information about the vessels themselves.
Vesseltracker also provides data about a variety of ship types, although it's commercial and non-open.
"
data request - Database of fictional characters?,"
A past alternative to using DBpedia as suggested by Nicolas Raoul was Freebase before it was shut down.
"
creative commons - CC-BY vs MIT or BSD licenses regarding re-use?,"
A good reason to not use MIT and BSD licenses for data is that they were written for software, so they're not a great fit. And, CC-BY was written for creative works, not data.
I don't know where you got the impression that BSD doesn't allow sublicensing, but it certainly does.  It also allows adding additional licenses, as long as they are license compatible.
CC-BY says that attribution needs to be given in the form specified by the licensor.  If you want lighterweight attribution requirements, just say so in your CC-BY license.
Rather than choosing A, B, or C, you might want to start by writing down the goals that you're trying to achieve with your license.  Given a set of goals, people might be able to suggest a license or family of licenses which is a good fit.
"
usa - Is there an API or global source for US ballot information?,"
Several sources of this kind of information:

Ballotpedia - lots of information, but in wiki pages, so not well-structured/API-able.
The Ballot Information Project - an NOI project; a fair bit of data, structured (with an API, I think?)
The Voting Information Project - this one has big sponsors (Google, etc.), and has an API as well.
Project Votesmart - Used to be the place to go for this kind of thing, but now charges for their data; still, they have a lot of stuff and could be a fallback, depending on whether the other sources have what you're looking for.

"
usa - Is there a list of Chemical Weapon Industry Facilities/Funders?,"
According to wikipedia:
""the United States had destroyed 89.75% of the original stockpile of nearly 31,100 metric tons (30,609 long tons) of nerve and mustard agents declared in 1997.""
""The primary remaining chemical weapon storage facilities in the U.S. are Pueblo Chemical Depot in Colorado and Blue Grass Army Depot in Kentucky.[28] These two facilities hold 10.25% of the U.S. 1997 declared stockpile.""
Another good place to start would be the EPA: http://www.epa.gov/envirofw/geo_data.html
You can sort out the type of regulated sites you are looking for and extrapolate which are likely candidates.
Here is a list of Biosafety Level 4 Facilities which are those facilities designed for work with extremely dangerous pathogens.
"
Are there any international non-governmental data aggregators?,"
The Data Hub, powered by CKAN, currently lists more than 6000 datasets, though not all of them are Open Data. These datasets come from all kinds of sources, not just governments and statistics institutes.
There is also the Linked Data community which collects datasets in the Linking Open Data Cloud group on the Data Hub. Most of these datasets are related to research, but there is also geographic data, media-related data, as well as other user-generated data available.
You can also have a look at the related discussion at A database of open databases?.
"
data format - Standards for self-documenting text files?,"
YAML frontmatter tends to be the generic standard for documenting text files. Parseable/compatible with JSON, easily human readable, easy to type in. It's used in content management systems, in combination with Markdown, to append metadata to blog posts in text files without the need for fully blown content management systems. 
http://www.yaml.org/
"
licensing - Requirements of the Open Data Commons Attribution License,"
I am going to give a somewhat different answer than Gisle does above: Regarding legal requirements ask a lawyer.   However otherwise, this largely supplements Gisle's post above.
The reason I say to contact a lawyer if in doubt is that copyright law varies significantly from one jurisdiction to another and in fruits-of-labor jurisdictions it isn't clear to me where the natural clear line where copyright protections end would be in a mashup.  What holds in one jurisdiction might not in another, but additionally to the extent that there is a balance between users and copyright holders, this may be different from one jurisdiction to another and from one application to another within a given jurisdiction.  So definitely discuss the matter with a lawyer.
The second point Gisle makes though is a good one and that is about norms.  It is important to consider that coordination and cooperation can be helpful, and so doing what the community expects in a way that works with what you are doing is a good way to build some bridges which can be quite useful later on.  I would urge people regardless of the law to engage in the community, and try to work out a mutually acceptable solution regardless of what the law requires.  Different communities have different cultures and to the extent you can work with a community's existing culture, you may get more support, both moral and material.  To the extent the community feels there is an obligation there is often a lot to be gained through fulfilling it.
A third point I would make though is that as a businessman it is not the worst idea to figure that norms are the essence of licenses.  If the norms are to not worry about parts of licenses, those will be very hard to enforce (politically and possibly legally too depending on jurisdiction), and if norms are not followed even if the legal requirements are, this can result in painful consequences.  So I would say if in doubt, the norms are what you follow.
"
releasing data - A database of open databases?,"
The short answer
DataPortals.org (previously DataCatalogs.org) provides a comprehensive list of open data portals from around the world. Their (meta-)data is in the public domain and available for download as CSV and JSON.
The longer answer
Data that is somehow related is usually grouped in datasets or databases, contained in files (e.g. CSV or spreadsheets) or some kind of database management system, which might be accessible via an API.
In the context of Open Data, data portals, data catalogs, or data hubs make it easier to find these datasets or databases.
A great example of such a data portal is the Datahub, which currently lists more than 4,500 open datasets.
However, there are already hundreds of data portals. A few prominent examples are the official data portals of the US (data.gov), the UK (data.gov.uk), or the European Union (open-data.europa.eu).
This is where DataPortal.org comes in: It is a data portal of data portals.
To sum it up:
    DataPortals.org --> Specific Data Portal --> Specific Data Set --> Open Data

"
data request - Open API for nutritional information and/or food barcodes?,"
The complete USDA National Nutrient Database for Standard Reference can be downloaded as ASCII text files from here — no PDF scraping necessary. 🙂
Regarding product barcodes, have a look at Open Product Data, a new project by the Open Knowledge Foundation.
"
Restrict search to open datasets on CKAN's Data Hub?,"
There is a simple answer thanks to the isopen parameter. Here's the query:
http://datahub.io/api/3/action/package_search?q=isopen:true
Note that q is basically the classic solr query paramemter so if you want to combine this with normal queries you do stuff like:
http://datahub.io/api/3/action/package_search?q=gold%20isopen:true
Or, slightly more elegantly:
http://datahub.io/api/3/action/package_search?fq=isopen:true&q=gold
Note that this just checks whether the license is open (as in conforms to the Open Definition).
Strictly, for full openness as per the open definition the data should be accessible (meaning available (in bulk) and machine-readable). As this is difficult to check automatedly these requirements are not part of the isopen computation in CKAN at the moment.
"
linked data - Any uses of JSON-LD?,"
I'm maintaining a list of early adopter in the JSON-LD Wiki
If you want a more visual representation, you might wanna look at http://slideshare.net/lanthaler/building-next-generation-web-ap-is-with-jsonld-and-hydra/34
"
What are the practical limits of releasing open data via bit torrent?,"
BitTorrent is not a great solution for this. Because each file distributed would need its own network of seeds and peers, you'd effectively dilute the network pool with each file you release, leaving you where you started: you being the one doing most of the distribution for most of the files in the first place. 
It's probably better to emulate govtrack.us' rsync strategy. Perhaps also with terms that say that anybody rsyncing must run an open rsync themselves (or voluntarily, depending on the size and happiness of your community) This will cut down on redundant downloading, and give you some bandwidth advantages.
Here's how govtrack does it: http://www.govtrack.us/developers/rsync
"
documentation - Standards for documenting use caveats?,"
I don't know that there can be a good standard other than what I was taught when studying environmental computer models, which is to be clear about everything up-front. Obviously this has to be documented somewhere in the expected place (like a README or the like) but the following things should probably be addressed on some way:

Methodology of how the data was collected.
Intended use, selection methodology
Any normalization which occurred prior to publication
Any known assumptions underlying the above three.
Any other notes that the collectors think might be useful.

This way people can read and check against their assumptions before they use the data in various ways.
"
data request - Is there a source for various Scrabble dictionaries?,"
The Wordnik API will tell you whether any single word is a valid scrabble word (among other information).
http://developer.wordnik.com/docs.html#!/word/getWord_get_1
I am not sure that's exactly what you're looking for, but it's the best I've got.
"
computing - What quantified self products have open data behind them?,"
There are some companies that are sharing aggregate data like the Green Button companies in energy: http://www.greenbuttondata.org/greenadopt.html but there are many of them, and it's hard to be specific without further description of the type of data you are looking for.
Some companies offer such information at an aggregate level, and others, depending on terms of service, obviously offer individualized data for a fee.
"
usa - How do United States federal agencies release data?,"
The processes vary from agency to agency. In general, data is gathered as part of a regular task for a project or program within an agency (this could be the Mars Program or the 2010 census). The data is structured, validated, and organized, and then that data is used for a purpose for the agency (anything from scientific analysis of the surface of Mars to a count of people in a specific city).  The data is generally approved for release outside of the agency through a secondary process that ensures it is valid, understandable, and does not violate either citizen privacy or national security.  It is then posted on an agency site.  
Most agencies also release their data to Data.gov, which allows people to find the data independent of knowing the specific program, project, or web site to which the agency has released the data.  The existing policies to which U.S. federal data must comply to be released are posted at: http://www.data.gov/data-policy
The new Executive Order and Open Data Policy http://www.whitehouse.gov/the-press-office/2013/05/09/executive-order-making-open-and-machine-readable-new-default-government- requires that agencies default to openly posting data gathered in the course of doing their work.  This still allows an agency to not post data that would violate a citizen's privacy or national security, and it would also allow an agency the opportunity to post the data once it is structured, valid, organized, and understandable (pursuant to the Information Quality Act, P.L. 106-554).
"
How does one parse weather data?,"
NOAA provides weather data. You can see the general information and visualization at http://www.weather.gov/  Specific data products are found at http://www.ncdc.noaa.gov/most-popular-data  When you click on a dataset you are interested, there is technical documentation and material to guide you in the use of the data. For example, local climatological data can be found at http://cdo.ncdc.noaa.gov/qclcd/QCLCD?prior=N with detailed notes at http://cdo.ncdc.noaa.gov/qclcd/qclcdfaq.htm
"
data request - Is there an exoplanet API or dataset?,"
With a little searching, I found what I was looking for. NASA has an archive of Exoplanets, as well as an API for it. The data are updated weekly.

Archive Home
API

"
data request - Are there datasets prepared for machine learning?,"
The University of California, Irvine provides a dataset repository specifically for machine learning purposes. There are currently 239 datasets in the repository.
These datasets come in many different formats and topics. The oldest datasets in the repository date back to the late 80s, and there are some datasets that are from 2013.
"
best practice - Releasing old historical/genealogical datasets as open data,"
Publishing the Data
If you don't want to go down the full CKAN-style route a really simple option if you've got a bunch of CSV files is just to put them online and turn them into ""simple data format"" data packages by adding a tiny bit of metadata in the form of a datapackage.json. Building a catalog out of this is really easy (and can be done with just a bit of JS+HTML!).
Simple Data Format
For details you can see this simple introduction to simple data format for more information. Here's a simple example:
Here's an example of a minimal simple data format dataset:
There are 2 just files, the data file data.csv and the datapackage.json:
data.csv
datapackage.json

data.csv looks like:
var1,var2,var3
A,1,2.5
B,3,4.3

That is there are 3 fields (columns) and 2 rows of data.
A simple datapackage.json for this data would be:
{
  ""name"": ""my-dataset"",
  # here we list the data files in this dataset
  ""resources"": [
    {
      ""path"": ""data.csv"",
      ""schema"": {
        ""fields"": [
          {
            ""id"": ""var1"",
            ""type"": ""string""
          },
          {
            ""id"": ""var2"",
            ""type"": ""integer""
          },
          {
            ""id"": ""var3"",
            ""type"": ""number""
          }
        ]
      }
    }
  ]
}

People to contact
Suggest these type of folks (you may already know them): 

Folks at the Open Knowledge Foundation - e.g. worth pinging the okfn-discuss mailing list (there have been several discussions about genealogical data here over the years)
http://www.opengenalliance.org/

(Disclosure: I helped write the simple data format spec and am a member of the Open Knowledge Foundation!)
"
usa - What new open data do you need from the U.S. government?,"
Some of the information you are looking for can be found by looking at the tags portion of the web site you can see how people are tagging their questions. Of the More popular tags the one I think you would most interested in is API (at the time of this writing approximately 1 in 7 questions are tagged with API). I think the most important thing in an API is accessibility, and from my experience accessibility can be broken down into a couple different areas. 

How much effort does it take to get the data into a native data structure within my program?

Are there third party libraries like python-twitter for Twitter?
Is the data in a standard format like JSON or XML? (coming from a python background JSON is preferable to XML)

How well documented is the API and how easy is it to find and navigate the documentation?

Does the documentation have broken links, was it's last update in usenet post from 1995?
Are there code examples in the documentation that I can simply copy and paste into my editor and get instant results? (The less time I have to spend figuring out how something works the faster I can develop... Duh :-)
Is deprecated functionality annotated? Are alternative methods provided for deprecated functionality? 

Is the data provided by the API complete and/or meaningful?

Are there specific error messages? (i.e. You entered an incorrect date format is a much better error message than Your input is invalid) 
I run into a lot of situations where an API has a list function of some kind and the list function will only return a portion of the expected results. (i.e. if query example.com/api.php?list=letters and get back anything other than ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'] there's a problem. If there is some reason for not sending an API user all 10 million baby names from your baby name database explain that to them and offer an alternative way to get the data.
Are there explanations for all the properties found in the data? (i.e. an API response gives me a something like {'license':None, 'data':[1,2,3]} does that mean that there is no license associated with the data or does it mean that the API providers don't have a license on record but one may exist. Ambiguity is bad.)


To sum this up I don't think there is a problem with the amount of data or variety of data but rather with accessibility.
"
What are the most useful formats in which to release geospatial data?,"
You can find a list of GIS file formats on Wikipedia. Here is a decent overview of open source GIS servers from the gis.se site, these are the servers people who use open data will most likely be using, so target the formats that those servers use. I would consider some kind of open vector/raster format (I like geoJSON for personal projects because it works well with openlayers, I also know people who use netCDF and JPEG2000). As a side note I've used this tool before to convert between different formats.
"
usa - What is the governance of the US Government's Project Open Data?,"
From the FAQ (emphasis is mine):

Who can participate in Project Open Data?
Anyone – Federal employees, contractors, developers, the general
  public – can view and contribute to Project Open Data.

It is possible to modify the content of the website as well as to contribute to the various tools offered on github by forking the projects. If your pushed changes are approved, they are integrated in the ""official"" app.
As for the governance, later on in the FAQ:

Who is in charge of Project Open Data? 
Ultimately? You. While the
  White House founded and continues to oversee the project, Project Open
  Data is a collaborative work — commonly known as “open source” — and
  is supported by the efforts of an entire community. See the “how to
  contribute” section above to learn more.
At the onset, the General Services Administration is here to provide
  daily oversight and support, but over time, it is our vision that
  contributors both inside and outside of > government can be empowered
  to take on additional leadership roles.

"
usa - What is the plan for the US Government's Data.gov?,"
On the Data.gov team, we'll be blogging and posting here as well about some of the upcoming changes.  The intent is to have all of you give us feedback as we start to evolve ideas, rather than just seeing what happens at the end of the design process.
My blog above shows the catalog upgrade using CKAN 2.0 (it's the demonstration site, so don't worry if you see only some of the data there): http://geo.gov.ckan.org/dataset  We'll show you a demonstration shortly of the harvesting of agency JSON files as a new way to federate data to the catalog, and later redesigns of some of home page, dataset pages, and other Drupal components on the site.
We are rebuilding Data.gov from the inside out, and have spent time this last year working with the government of India, and now Canada and Ghana as well to open source the Open Government Platform as the underlying code under Data.gov.  You can find it on Github: https://github.com/opengovplatform/opengovplatform-beta/wiki
"
legal - How should I categorize municipal legislation?,"
I may have a bad case of hammer-seeks-nail, as I'm currently working fairly full time on a federal legislative markup system with CATO called Deepbills that tags federal legislation in a way that tracks what code it modifies. But here's my thought, based on my experience with how that works combined with my years of poring through the DC code:
Rather than a system that applies a (possibly) subjective classification to the proposed legislation based on reading, what about instead using a system of classification/tagging against the existing code and then tracking what the proposed legislation modifies? The organization of the Chicago municipal code already provides a very loose taxonomy of sorts:

TITLE 1 GENERAL PROVISIONS
TITLE 2 CITY GOVERNMENT AND ADMINISTRATION
TITLE 3 REVENUE AND FINANCE
TITLE 4 BUSINESSES, OCCUPATIONS AND CONSUMER PROTECTION
TITLE 5 HOUSING AND ECONOMIC DEVELOPMENT
TITLE 6 RESERVED
TITLE 7 HEALTH AND SAFETY
TITLE 8 OFFENSES AFFECTING PUBLIC PEACE, MORALS AND WELFARE
TITLE 9 VEHICLES, TRAFFIC AND RAIL TRANSPORTATION
TITLE 10 STREETS, PUBLIC WAYS, PARKS, AIRPORTS AND HARBORS
TITLE 11 UTILITIES AND ENVIRONMENTAL PROTECTION
TITLE 12 RESERVED
TITLE 13 BUILDINGS AND CONSTRUCTION
TITLE 14 RESERVED*
TITLE 15 FIRE PREVENTION
TITLE 16 LAND USE
TITLE 17 CHICAGO ZONING ORDINANCE
TITLE 18 BUILDING INFRASTRUCTURE

Certain sections are related to certain issues and may be very specific if you drill down. Titles 16 & 17 relate specifically to land use and zoning. If you look at 16-18 it regards open space, 16-18-40 is the related fee schedule and 16-18-120 specifies the Paulina Street Corridor and outlines where exactly it is.
So any proposed legislation which referenced 16-18 could be land-use related and classified as of interest to anyone doing new development or renovation (but not replacement); referencing 16-18-40 indicates that it's cost relevant. Similar logic works for many other sections -- 9-52 is bicycles.
The advantages of this as a taxonomic basis is that you'll make your classification something that could more easily be done via machine. It's somewhat limited, but a second level of taxonomy or tagging done against the code could provide a more flexible alert system. Anything modifying title 9 is transportation, 9-52 is bicycles, 9-68-020 is residential parking permits. 9-68-022 is one of several Wrigley areas which is firmly defined.
By using a looser initial taxonomy and then applying attributes to code sections you could even use a geographic area definition in an automated way. Given an address XYZ you can then see when it falls within areas impacted by legislation: if it falls within the area described by 9-68-022 then you know any modifications to that section would impact that person.
That example is probably more specific than you care about, but the same would apply to any broader neighborhood definitions.
"
Categories and varieties of Open Data licensing?,"
Here are the three options listed on opendatacommons.org, the licenses FAQ also has a bunch of really good information.

Public Domain Dedication and License (PDDL) — “The PDDL places the data(base) in the public domain (waiving all rights)”
Attribution License (ODC-By) — “Attribution for data/databases”
Open Database License (ODC-ODbL) — “Attribution Share-Alike for data/databases” - The community norms for Attribution-Sharealike are share your work, give credit where credit is due, let others know, use open data formats and don't use DRM.

There is an here is an interview of Steve Coast (OpenStreetMap's founder)

Steve Coast: Licensing is incredibly important for the community to
  trust that the data won’t be closed off. So we need to make sure that
  data from OpenStreetMap will always be free and open. It’s also
  important that we are able to stop anyone from trying to close it off
  or derive from it without giving back to the community. We have a
  multi-year process to re-license based on advice from multiple sources
  that Creative Commons is not applicable to data. We wish it were, and
  it probably will be in the future but it wasn’t clear when we began.
  Until that happens we have a process to move to the Open Database
  License, which explicitly covers data and not just creative works like
  photographs or text. The ODbL was in fact started as a result of
  investigations around the needs of Science Commons and we just helped
  it to its conclusion.

Here is a guide (incomplete) from opendatacommons.ors on how licensing applies to data from different fields and different countries.
"
data request - Open dataset on manned space missions,"
The Johnson Space Center has a website with biographies of Astronauts and Cosmonauts that you should be able to extract much of your requested information from:

http://www.jsc.nasa.gov/Bios/

I find it easier to extract from HTML, but the 'time in space' text might be more difficult to extract from the free text than the PDF that Jeanne linked to.
update : oops ... you asked for time in space of the missions, not the astronauts ... Johnson also has a general Manned Space Flight website, which has information about each of the missions (shuttle, ISS, skylab, etc.), but they're each formatted differently enough that it's a bit cumbersome.
If I were you, I'd probably e-mail the contact for those two websites, and ask if they had the information available in more easily parseable form.
Also, you might be able to get some information from Wikipedia, such as their list of shuttle missions and list of human spaceflights.
"
data.gov - How should governments build community around their datasets?,"
Some general advice:

Invest in community managers and evangelists. The worst thing you can do is create a forum or invite feedback and then not have anyone with a mandate to respond to it in an official capacity; I'm seeing this happen right now with the newly launched project-open-data in which they've invited contributions and have no one to actually respond or accept those contributions
Create time-boxed campaigns. Governments and policy mandates come and go, so rather than creating an open-ended ""chat with us about anything"" that may trail off, create 3 or 6 month challenges that you can budget for a high degree of feedback and engagement. This is also helpful for testing your own ability to engage long-term because you can explain your outcomes to superiors/oversight in the short-term. A platform like Challenge.gov can be helpful for this... assuming you actively engage with participants during the challenge periods (many agencies on Challenge.gov are really bad at this and it turns into a black box of disappointment when the winners are announced).
Write publicly about your experience in managing the platform and performing community engagement. Have a blog that you post to (at least) weekly about how you are helping actual people do stuff. Use first-person pronouns and first names; don't make it a series of press-releases. The Github blog is a good example of this kind of engagement. If you can't regularly write about your own experiences running the platform (whether because of interest, mandate or oversight), you probably won't be able to relate to the experiences of people using it.
Create in-person ""networking"" events. Embrace opportunities to meet with the public, on their level. Which means either attending the meetings of existing community/developer groups, or hosting your own (though I'd recommend starting with the former). Chicago's OpenGov Meetup group often has government employees attending (even when they aren't making a presentation!) who can put a public face on things and help further relationships that may otherwise seem transactional or distant when their interactions only take place online.

The advice above is tool and platform agnostic. Specifically, I'd recommend:

Put Disqus (or equivalent) on everything; never let a comment go unresponded to (no page should ever have only 1 comment, ever!)
Start a development blog (as described above)
Continue engaging in spaces like this one

"
documentation - Standards for documenting gaps in data?,"
I do not know of standards in this area, but I do know that many data owners document missing data or other known issues in a dataset.  This is generally documented in either the site from which the dataset is linked or in the metadata of the dataset itself.
"
usa - What are the sources of data on Data.gov?,"
The list of all organizations that are contributing data (or metadata) to Data.gov can be found in two different places. 
One place is on the Federal Agency Participation link at the bottom of the page.  Clicking on an agency or sub-agency name takes you to a page with all of the raw datasets and tools submitted by that agency.  
There is also a listing of all organizations (which includes non-Federal entities) at the top of the Catalog page, and shows a description of the organization as well as the listing of all datasets and resources from that organization on Data.gov.
(Disclaimer: I am the Evangelist for Data.gov.)
"
data request - Is there a list of hot-spots and free wifis in Germany?,"
There is a wiki at http://freewifiwiki.net/index.php?title=Germany which is a listing (not a database).  There are links at the bottom of that site to additional sites in German and English appear to augment this overarching listing.
"
data request - Downloadable archive of weather conditions for Europe?,"
The European Centre for Medium-Range Weather Forecasts (ECWMF) has a rather impressive data collection available via batch scripts, downloadable files or even tailored formats. The first two are freely available (under specific conditions) for non-commercial research but registration is necessary.
Just to give an example, the GRIB dataset lists 4115 parameters ... including your requested temperature, humidity, precipitation and location (latitude/longitude).
For more casual data retrieval, you might try weather services which focus on specific areas, such as Yr from Norway.
"
releasing data - Alternative to GTFS (General Transit Feed Specification)?,"
Transmodel is a not very widely used format for schedule data (alternative to GTFS).
For real time data (alternative to GTFS-realtime): SIRI is an XML protocol used most heavily in Europe.
You'll want to consider what formats developers are most aware of and any possible performance issues.

TRANSMODEL has been adopted as the European experimental standard ENV 12896 in 1997.
  (La dernière version révisée du document TRANSMODEL (version 5.1) est une norme européenne disponible auprès de l’AFNOR. Elle a été élaborée en 2006, et sa traduction française a été publiée par l’AFNOR en janvier 2012 sous la référence NF EN 12896.)

"
usa - Where can I get bulk access to IRS 990 filings for US non-profits,"
Resource.org has gathered reports dating back to 2002 and says that they process new data monthly. Bulk data can be pulled from here.
For those who prefer a simple search, the Economic Research Institute (ERI) has a database of forms dating back to 2003. 
In at least one case, a journalist on the NICAR-L list reported back that there was a form in the ERI database which was not in the Public Resource set, so proceed with due caution.
"
tool request - What does CKAN stand for and what does it do?,"
CKAN stands for Comprehensive Knowledge Archive Network. CKAN is a self-described data portal platform that allows an organization to manage, publish, and share data and for others to find and use that data. 
In general, data portal platforms provide a structured solution of software, policies, and guidelines that let an organization (often a government entity) share data. The services embedded in these platforms may include data management, content management, data publishing, data discovery, visualization, and workflow. Other examples of such products include Socrata (http://www.socrata.com/), Junar (http://www.junar.com/), DKAN (http://drupal.org/project/dkan), and the Open Government Platform (http://www.opengovplatform.org/).
CKAN installation guidelines are detailed on the site at http://docs.ckan.org/en/latest/maintaining/installing/index.html
You can run it standalone (as noted above) or in a hosted instance. CKAN provides a hosted service, which lets you essentially run the software in an instance on their servers (more at http://ckan.org/datasuite/services/hosted-slas/).
(Disclaimer: I am the Evangelist for Data.gov which participates in OGPL. Data.gov also utilizes CKAN and Socrata.)
"
us census - How to normalize the data when mapping crime reports?,"
I asked a data analyst at the Bureau of Justice Statistics who provided this answer:  
""I would say that the answer really depends on what information they are trying to show. There are many different way to normalize crime data and even multiple different ways of doing population based rates.
For example,  I've even seen some people playing around with creating rates using the ""flow"" of people through the area, where the denominator for the rate calculation is the number of people who pass through a given area during the day--for example at an airport which has no population per se, but does have counts of the number of people who go through the airport during a certain period of time.
You can also create rates for specific crime times as a proportion of all crime in a given area, which helps to identify areas where particular crimes are more likely to occur than other types of crime. This is often done in types of hot-spot mapping where the interest is to identify across a given area where whether burglary (for examples) is more common than other types of crime and how that differs by city block.""
Direct contact information and help is at AskBJS@usdoj.gov and they are happy to work directly with folks in this community on such issues.  In the future, as this private beta goes public, I'll invite such experts to answer directly in this forum.
(Disclaimer: I'm the Evangelist with Data.gov)
"
data.gov - Have genetic algorithms been applied to Open Data?,"
Not sure if this answers the question, but there are two aspects: (1) using algorithms to analyze the holdings in a data catalog or a large (big) dataset; and (2) gathering data around genetics and genomics.
(1) There are many tools and programs underway on data analytics. Check out a large solicitation from NSF and an interagency big data initiative that references useful sites at the end.
(2) There is a growing set of data on Data.gov related to genetics and genomics, with particular recent emphasis on agriculture (Agriculture.Data.gov). And, open access to federally funded research data is a new directive from the White House in addition to the Executive Order on Open Data.  Research.Data.gov is starting to categorize and organize data related to that latter directive.
(Disclaimer: I am the Evangelist for Data.gov)
"
Are there best practices that government API producers should follow more so than non-government API producers?,"
You don't identify what kind of areas you're looking for advice on. But I'll highlight several that I think are particularly relevant for Government sources:

Where feasible, data should not just be made available via an API, but also available for download. This supports other kinds of uses. I think this is important as one goal of Open Government Data is to drive innovation and creation of new businesses. This is easier to do if the entire dataset is available. For example third-parties can offer value-added services over that data.
APIs should be accessible without usage restrictions or API keys. This avoids having any barriers to entry. Scaling may be an issue, but it'd be better to offer an open, unsupported API and then offer alternatives for higher-volume usage (and this is the kind of value-add that a third-party can offer)
Data must be clearly licensed. And that licensing information should be included not just in the API documentation but discoverable from the data itself. E.g. linked in API responses. Users should be crystal clear on how they can re-purpose the data. This is important for all Open Data, but particularly so for government.
Use of Open Standards. Again, this is a benefit in all cases, but government providers should avoid using proprietary formats or data protocols that might impede usage.

"
What would be particularly useful basic APIs for the US Federal Government to offer?,"
Getting the USPS to open up basic information about ZIP codes, either through an open API or even better as a bulk download, would be a big help to a lot of us dealing with geo problems.
The ZIP code API you link to is a great example of how not to do it. It has so many restrictions on its use that it's not possible to use it with most open data sources - ""User agrees to use the USPS Web site, APIs and USPS data to facilitate USPS shipping transactions only."". You have to phone the postal service before they'll even enable your access!
The US Census does a great job of giving us their best guesses at ZIP code locations, but it's missing a large proportion of rural areas, and the boundaries aren't very accurate. 
"
nonprofit - How does the Sunlight Foundation relate to Open Data?,"
From the about page:
The Sunlight Foundation is a nonprofit, nonpartisan organization that uses the power of the Internet to catalyze greater government openness and transparency, and provides new tools and resources for media and citizens, alike. We are committed to improving access to government information by making it available online, indeed redefining “public” information as meaning “online,” and by creating new tools and websites to enable individuals and communities to better access that information and put it to use.
In practical terms, Sunlight's open data work has several facets.  We do policy advocacy around encouraging openness in government (of which the POIA stuff is an example, as is the DATA Act), work with government agencies to figure out how best to expose data they've decided to open, and host an annual transparency-oriented conference of which open data is a major component.  Our Labs team, the technical arm of the organization, also both consumes and produces open data via our tools, which try to make government information more accessible to the public.
As for who works here: here's a staff list.
(Disclosure: I'm a developer at Sunlight.  Sorry if this post comes across as overly self-promote-y.)
"
data request - Is there a better public version of USA's Social Security Death Master File?,"
As noted, there is no official public version of the file, because at this time the NTIS only provides it to subscribers.
As for parsing the file, I made a schema compatible with csvkit's in2csv utility. The schema can be downloaded from https://github.com/JoeGermuska/ffs/blob/master/us/ssa/death_master_file.csv 
Once you've installed csvkit and downloaded a copy of the schema, the command would be
in2csv -s death_master_file.csv ssdm1 > ssdm1.csv

where ssdm1 might vary based on which of the ZIP files you retrieved from ssdmf.info
For more on the in2csv script, see readthedocs
"
Are there publicly available sources listing uses of open data?,"
Data.gov has a developer showcase with links to 300+ applications that have been made with their data.
update: As does uk.data.gov (per D Read).  
Also, if we get into articles, every NASA mission keeps a list of peer-reviewed publications to justify their continued funding.  (eg, SOHO, STEREO).  The astronomy community calls them 'telescope bibliographies' and currently has a draft circulating for comment (until 24 June 2013) on Best Practices for Creating a Telescope Bibliography
  There's also been discussion of ADS building tools for people to manage publication lists, but that's still a ways off.
(disclaimer -- I help to manage the SOHO & STEREO web servers.  I also remember seeing a chart of Hubble papers that did/didn't include any PI team members, and a few years into the mission, the rate of publication without a PI overtook those with.  It was one of the arguments for opening data, but I'm not having luck putting my hands on that presentation)
"
standards - Capturing development/aid project perfromance in IATI format,"
You attach a result to a specific activity, so in this case the Legal and Procedural Change and Communication Activity. The  should be nested within that activity. You can have many results, which are just siblings.
A single result should look something like this:
<result type=""2"">
  <title>Stakeholders reached by public outreach efforts</title>
  <indicator measure=""1"">
     <period>
       <period-start iso-date = ""YYYY-MM-DD"" />
       <period-end iso-date = ""YYYY-MM-DD"" />
       <target value = ""14100"" />
       <actual value = ""43632"" />
     </period>
     <baseline year = ""YYYY"" value=""0"" />
  </indicator>
</result>

A few points:

values should not include commas
the start and end dates could just be taken from the start and end dates of the activity
the baseline year can just be the start year of the activity.
check that you are happy with these assumptions:

result type=""2"" refers to the Result Type codelist - states that it is an outcome
indicator measure=""1"" refers to the Indicator Measure codelist - states that it is a unit

for the All Activities indicator, I would attach that to the parent project. Where you have ""pending"" for the actual element, you should leave the element out for now.

"
Reporting non-country specific administrative spending in IATI standard,"
I think the best way to do this is to create a new activity with these classifications:

recipient-region:

code: 998
text: Bilateral/unspecified
see IATI regions codelist

sector:

code: 91010
text: Administrative costs
but maybe have a look to check there's not a more appropriate code: IATI Sectors codelist


If there are administrative costs related to a particular project, you could just include that in the description of the transaction.
"
licensing - Checklist for soliciting non-identifiable data from FOSS-users for open data project,"
Do you believe that these data can be licensed? If so, then that would imply that you should ask the user to verify that they are authorized to accept your licensing terms.
"
"data request - Is there a list, database or API that contains the URLs for United States city and town websites?","
(Disclaimer: I work for the U.S. Treasury but am writing in my personal capacity.)
I do not know of an API that has what you are looking for off the shelf, but I would recommend you look into what information you can already access or request from the official system governing registration in the .gov domain. The .gov domain registration process for cities appears to be managed by General Services Administration (GSA) through the .Gov Domain Name Registration Service. (Note: GSA also manages Data.gov.) Cities appear to have to apply using a separate form. Several points of contact are given on the welcome page of the site, including registrar@dotgov.gov and a toll-free number: 1-877-REG-GOVT.
You may also consider suggesting that this API be made available under the question asked on this site, ""What would be particularly useful basic APIs for the US Federal Government to offer?""
"
legal - How should I adapt a Schema.org microdata format for legislation?,"
I'm not aware of any list of metadata in microdata format especially for legislation. Certainly there doesn't seem to be one at schema.org, although some parts of GovernmentOrganization would be applicable.
Concomitant with the existence of a governmental entity is, of course, the jurisdiction with which it's associated. For some uses, AdministrativeArea will serve, particularly where ""state"" is used as the primary administrative level below the country level. (Of course, even with the United States, you'd have to mislabel such entities as Puerto Rico and DC or omit them.) Purely geographical and/or geophysical schemes don't adapt especially well to identifying jurisdictions either.
The AkomaNtoso schema was created to provide a common structure for ""parliamentary, legislative and judicial documents."" (Version 3.0 is being prepared and should be released this summer.) As such it provides markup for all aspects and phases of the legislative process. You could make use of the metadata elements in your own schema, insert them as RDF or RDFa (as described here), or make use of them as microdata, as described at [schema.org]{http://schema.org/docs/datamodel.html).
The <meta> element consists of eight different types of metadata, as shown here:
a list of meta elements http://www.akomantoso.org/docs/akoma-ntoso-user-documentation/images-akoma-user-documentation/figure-6-2013-structure-of-meta-container
And then you'll have to figure out not only how you want to classify the legislation but by what mechanism you're going to select the appropriate category.
(Affiliation disclosure -- I belong to the OASIS technical committee for LegalDocML, which is working on version 3.0 now.)
"
usa - What finance data sets would be particularly useful to release?,"
World Bank Donor Data - how donor funding is being spent by project/country/donor/implementer etc.
"
What are the standards for data in terms of Open APIs?,"
Every standard is intended to serve a slightly different purpose.  Even if we're talking messaging standards over HTTP, you have both REST and SOAP.  (and before I get all of the SOAP haters commenting ... there is a ton of bad SOAP implementations, but not everything is documented oriented and meshes well with REST)
Before we had SOAP there were standards like WDDX for encoding your structures as XML, then XML-RPC ... so standards evolve over time, and API providers may not change their systems to support the flavor of the week.
If you're working within a single discipline, there may be a set of standards, but again, as they each have a purpose, the odds of there being only one is quite slim unless that discipline's data is both homogeneous, and there's only one good way to search or visualize it.
For instance, in Astronomy, the International Virtual Observatory Alliance has over 40 standards.  They all serve slightly different standards, for search query formation, serialization of results, etc.  And that doesn't even include all of the work on FITS (Flexible Image Transport System, a file format with dozens of registered extensions) or WCS (World Coordinate System, used with FITS and other astronomy data)
I gave a talk at ASIS&T a few years back on standards and protocols in earth and space sciences and we're starting to recycle, converging, and otherwise simplifying things ... but it'll never completely converge, especially as people add new 'unifying' standards.
"
"government - Where can I find a list or directory of foundations, investment firms or individuals interested in financially supporting Open Data initiatives?","
There are a lot of organizations that fund open data projects for various reasons. A few that have provided funding in the last year or two are:

Governments: Ministries and organizations that are looking to create a specific solution.

US government: Grants and business opportunities from the US government can be found at https://www.fbo.gov/ and http://www.grants.gov/ Challenges are found at http://challenge.gov  These change almost daily, so you can set up alerts for topics you are interested in.

Ford Foundation (such as http://www.webfoundation.org/2012/08/announcing-ford-foundation-ogd-grant/)
IBM (http://www.ibm.com/ibm/responsibility/initiatives/grant_programs.shtml)
Sunlight Foundation (such as http://sunlightfoundation.com/participate/)
Microsoft (such as http://research.microsoft.com/en-us/collaboration/focus/cs/seif.aspx)
Google (such as http://www.google.org/projects.html)
Omidyar Network (http://www.omidyar.com/)
William and Flora Hewlett Foundation (http://www.hewlett.org/grants/grantseekers) 
Open Society (http://www.opensocietyfoundations.org/grants)
Knight News Foundation (you noted): https://www.newschallenge.org/open/open-government/evaluation/ 

"
Ethics of publicizing public data,"
The ethics of posting mug shot photos online has been widely discussed. Some links:

LA Times: Tampa Bay mug shot site draws ethical questions
Poynter: Archived Chat: The Ethics of Posting Mug Shots Online
Source: Matt Waite on the ethics of a news app: Tampa Bay
Mugshots

As it's the most recent, and as it's written by Matt Waite, who built the Tampa Bay mug shot site in the first place, I'll quote this bit from the ""Source"" article:

So before you write the first line of code, ask these questions:
   - This data is public, but is it widely available? And does making it widely available and easy to use change anything?

Should this data be searchable in a search engine?
Does this data expose information someone has a reasonable expectation that it would remain at least semi-private?
Does this data change over time?
Does this data expire?
What is my strategy to update or delete data?
How easy should it be to share this data on social media?
How should I deal with other people who want this data? API? Bulk download? Your answers to these questions will guide how you build
  your app. And hopefully, it’ll guide you to better decisions about how
  to build an app with ethics in mind.


"
"usa - Appealing a ""Mosaic Effect"" restriction?","
The answer depends very much on the legal framework of the jurisdiction where the data is released.  The question seems to be US-specific.  I do not know the legal framework in the USA, but close examination of relevant US privcy laws (which I understand are very fragmented and difficult to understand) should tell you about what means of appeal there exist in the USA (on federal and on state level).  The privacy laws of your jurisdiction should give both tell you what means of appeal there exists, and on what grounds you may make an appeal.
However, since this is an international forum, and by means of example on how privacy laws work in the specific jurisdiction I am familiar with, I would like to point that in the EU and EEA, this regulated by the Data Protection Directive (Directive 95/46/EC) where each member state must set up a supervisory authority, to make decisions about, among other things, the release of data that may affect privacy.  Decisions by the supervisory authority which give rise to complaints may be appealed against through the courts (art. 28).
In Norway (EEA member state) the ""supervisory authority"" is known as ""The Data Inspectorate"", and there is also the ""Privacy Appeals Board"", which is the primary means of appeal for decisions made by ""The Data Inspectorate"" (decisions made by the ""Privacy Appeals Board"" may be appealed against through the courts, so there are also a secondary means of appeal available).
As for case law in Norway, the Mosaic Effect is often cited as grounds for not releasing data.  What usually happens is that the ""The Data Inspectorate"" requires the data to be aggregated to the point where the Mosaic effect can longer be used to indentify individuals before the data is allowed to become public.  However, aggregating data also removes information and therefore makes the data less useful, so a requirement to aggregate is often appealed against.  EU/EEA law requires the supervisory authority to weigh the privacy risks against public utility when making a decion.  In other words, if the privacy risks are low (e.g. the Mosacic effect will not expose sensitive personal data), and the public utility is high, a decision may be made to allow the data to be released without aggregation despite the Mosaic effect.  Vica versa, data where the privacy risks are high, and the public utility is mariginal, the data may not be released at all, or only released as aggregates.
(I am a member of the ""Privacy Appeals Board"" in Norway, and has during my term of service heard several appeals where the Mosaic effect has been relevant.)
"
usa - Open and proprietary data in determining federal funding eligibility,"
There are two aspects I see to this question: (1) access to proprietary data, and (2) ability to crowdsource the verification of any data.
For the first, proprietary data from private companies is sometimes made available as open or restricted data. Some companies' business model is based on selling this data, and some companies will offer at least a small portion of their data openly. In only very specific cases, does the government ask for data directly from companies (incorporation data, financial regulations, etc.).  However, there are some interesting examples related to your question about supermarkets:

Baltimore open data on supermarkets and locations
A blog on the value of supermarket data for consumer confidence in the UK
An interesting post and comments on this topic

As to the second, there are some great examples of crowdsourcing the validation of open data. USAID did so for food security and Google Earth did the same for crowdsourcing land grabbing in Ethiopia.  These both were well received and validated lots of data that would have been very difficult and costly to gather traditionally.
"
Data about biases in city service requests,"
I understand your question and it does relate to open data. It seems like you have a piece of open data: municipal service requests (i.e. ""fix the pothole in front of my house!""). Your followup question is a good one: given the number of service requests, are these people just cranky, or are there actually more needed items to be fixed in a certain area?
I searched a while for data on the responsiveness of public works departments in municipalities. Not surprisingly, this data is not collected yet in an open fashion that I could find. It is alluded to in annual reports, but even there, the numbers are sparse.
Some alternate possibilities:

Try to get the total number of requests historically, before and after the open data request system opened up.
Get the number of ""potholes fixed"" (public works improvements?) with and without an actual service request (this is harder I would imagine, and you will need to clarify what projects are and are not included).
Get the number/type of service requests and overlay demographics (maybe old people call more, or maybe rich people, soccer moms, etc). A lot of this will depend on how the request is geo-located (by zip, by block group, etc)

As an aside, I think it might actually be a good thing to have some sort of ""responsiveness"" metric for public works departments; maybe you could make one. Every department seems to claim that it is responsive, but I was unable to prove this by looking at their annual reports since they didn't use numbers.
"
government - Is there a roadmap for opening all the data for a city or municipality?,"
Great question! Cities follow many different paths, but some best practices are starting to emerge.  Two particularly helpful guides/roadmaps are published by:

Open Data Field Guide from Socrata
Open Data Handbook from the Open Knowledge Foundation (available in multiple languages)

Both provide a nice how-to guide and future plans for cities and localities with open data.
"
Open data community ideation tools?,"
Another avenue I'd offer for your consideration is the ""Data Jam"" model of generating great Ideations and then the companion ""Datapalooza"" showcase for spurring creation of actual products.  Boston and the NC Research Triangle are in the middle of such an experiment.
Also, here is a draft handbook for how anyone familiar with a Hackathon can host their own Data Jams and Datapaloozas.
"
government - Is there (or should there be) a standard way to categorize procurement data at the municipal level?,"
I think it would be foolish to try and replace the NAICS system. NAICS is the federal government's categorization system, and in my experience, it is also in use at the municipal level in the United States. Here's a longer description:

NAICS was developed under the auspices of the Office of Management and Budget (OMB), and adopted in 1997 to replace the Standard Industrial Classification (SIC) system. It was developed jointly by the U.S. Economic Classification Policy Committee (ECPC), Statistics Canada , and Mexico's Instituto Nacional de Estadistica y Geografia , to allow for a high level of comparability in business statistics among the North American countries.

It's easy to hate on a system that only gets as specific as ""Custom computer programming services"", but in reality, it was developed with a lot of effort by a bunch of smart folks.
In my opinion, there are two challenges when trying to compare prices across governments:

NAICS is not always specific enough, especially when it comes to technology. 
Units are not standardized. 

While I would love to see a data standard that fixes these issues, think it's inevitable that there is always going to be a large amount of discretion/manual comparison involved. Because of this, I'd be much more interested in a system that allows a user to view aggregated pricing data from multiple cities, filtered by query or NAICS code. It would pull from multiple sources like the City of Chicago link in the original post.
As far as NAICS goes -- maybe we could create additional subcategories for codes that are way too broad, such as ""Custom computer programming services""? Or taking this idea further, would there be a way to create a superset of NAICS that extends it so it never goes out of date?
EDIT: There's a discussion going on about this at https://github.com/dobtco/NAICS/issues/1
"
data request - Seeking real-world networks that have an approximately linear structure,"
The road system of New Zealand is obtainable in a computer readable form from OpenStreetMap. There are APIs available to query for just the objects you want, in this case the roads, see http://overpass-turbo.eu/s/cG for an example.
"
documentation - Self-documenting RESTful APIs: examples with WADL?,"
Code generation is often considered to be an anti-pattern for REST APIs: the goal is to allow clients and servers to evolve independently as much as possible. Generating client code from a WADL document, as you might do from SOAP, will make the client brittle to server-side changes. It'd be better to ""bootstrap"" the client by consuming the WADL at run-time rather than compile time.
Having said that, here's a couple of quick examples of WADL used on some public APIs (I've not checked the data licensing, so aren't necessarily ""Open Data"" APis):

Launchpad: https://help.launchpad.net/API/Hacking
D&B direct: http://dnbdirectapps.com/docs/1.0/rest

"
What are the most common ways that users find out about new data sets?,"
How to stay up to date with UK government data releases:

Office for National Statistics release calendar
Parliamentary releases mailing list  
Planning alerts mailing list
Press releases
RSS feeds
Twitter

For example:

The Office for National Statistics release calendar is excellent because it allows you to see weeks in advance what data is going to be published. http://www.statistics.gov.uk/hub/release-calendar/index.html
Parliamentary releases mailing list lets you select exactly which committees and types of reports you want to be alerted to, and if you want them immediately or daily. https://subscriptions.parliament.uk/accounts/UKPARLIAMENT/subscriber/new?
Planning data has a great email alert, excellent for local data journalism.
http://www.planningportal.gov.uk/inyourarea/

"
What examples are there of Linked Data/RDF being used for open data applications?,"
I would love to answer this, but I have no idea what ""strong linked data"" means.  These techniques have been used in a lot of products, the dbpedia.org system has been used by a number of systems, ranging from Watson (maybe IBM is considered too academic) to Siri (I don't think Apple is an academic group).  Schema.org and Facebook's Open Graph Protocol are also big users of linked data vocabularies and web linking schemes.
Going to government, there's been a lot of work using linked data in various ways.  The Brits are the lead, a number of their open sites, based on the Ordnance Survey maps among others, use linked data.  Within the US, we have demonstrated a lot of uses at hackathons, in some of the competition winning apps, and in some the info sources available on line.  
There's an article Jeanne Holm, George Thomas, Chris Musialek and I wrote that covers some of this - IEEE Intelligent Systems and some thoughts about what we are doing at data.gov 
So my fast summary is that like many technologies this is being used as a component in many apps, it is not by iteself proposed or fielded as a be all and end all - but without the URIs, the data doesn't make it to the Web, and then we cannot exploit many of the powerful things that open data allows.  
(I have a column in the soon-to-be-released issue of Big Data called ""Peta vs. Meta"" that says a bit more about how the schema.org stuff is used - see also schema.org/Dataset)
"
Are there best practices about data lifecycle management involving citizens?,"
On Data.gov we have a several different ways for people to provide feedback, most of which are publicly viewable.

Suggest a new dataset and see what others have suggested (all the dataset suggestions are tracked to completion, although not all requested datasets exist or can be released)
Comment on or rate a dataset (see an example for earthquake data)
Ask a question in our Developer's Community (these are moderated for spam)
Send an email
Host or participate in events (see a listing on our community page)

(Disclaimer:  I serve as the Evangelist for Data.gov)
"
"What companies, projects, and researchers are using the Consumer Financial Protection Bureau (CFPB)'s API of data on consumer product complaints?","
Our team at Beyond the Arc makes extensive use of the CFPB database.  You can see our latest analyses on our blog.
"
best practice - Displaying Trust and Data Provenance?,"
My issue would be what the purpose of displaying the provenance is.
As I've suggested in some of my questions on here, some of my concerns are about tracing the issues that might be in the data, and sometimes you have to go back and look to see how it's been processed and what it's derived from to tell what the possible issues might be.
(eg I've run into at least once case where the folks calibrating the data hadn't considered big vs. little endian when they converted from 68000 to PPC processors ... the issue wasn't caught 'til they went from PPC to intel and realized that all of the PPC processed data (years worth) was defective. ... but architecture the processing was run on isn't always captured when people talk about provenance)
Personally, I'm of the opinion that for the sake of open data, you shouldn't just share the provenance of the data set in question, but you should try to describe that data's relationship with all of the rest of the data that you serve.  (rule #4 in ""5 star data) After all, it might be that the data that someone found has some more processed form that would be better for their needs ... or formatted / packaged differently.  ... and you're not going to get that from only tracking provenance, as that only goes in one direction.
(disclaimer : years ago, I gave a talk at the AGU on the need for a model to discuss relationships between data (warning: 18MB PPT file) ... unfortunately, I've gotten bogged down with other stuff for the past few years ... but the DataCite schema has RelatedIdentifier and a decent list for relationType to get people started)
"
data request - Are there any open mappings of train station identifiers in the UK?,"
There are a number of codes used to identify train stations. There is a good summary of the various codes here:
http://nrodwiki.rockshore.net/index.php/Identifying_Stations
That site includes links to reference that that you may be able to use in addition to NAPTAN, see:
http://nrodwiki.rockshore.net/index.php/Reference_Data
For example this site correlates CRS, NLC, TIPLOC and STANOX codes and has a MySQL dump available:
http://trains.barrycarlyon.co.uk/data/locations/
I think that covers what you're looking for.
"
metadata - How can I track updates on the release of new open data sources across the world?,"
There are two additional options here. 

If you know of an open data portal for a country or topic you are interested in, many allow you to subscribe to their data updates or releases.
If you do not know that a source exists and are looking for one, several aggregator sites are referenced in a recent answer to your related questions on international aggregators for NGO data.  Additionally, on Data.gov we track official government open data portals, and I can certainly look into creating a subscription feed there.

"
data request - Wikipedia dump files in SQL format,"
No, there are no SQL versions of the XML dump files.
The page Data dumps/Tools for importing on meta.wikimedia.org describes how to work around that: You can either use ImportDump.php to import the XML file directly (apparently suitable only for small wikis), or you can use a tool like mwdumper to convert the XML into SQL and then import that.
"
Evidence for the economic impact of open data?,"
There are several studies on the economical impact of Open Data. The most recent I know of is a study done in 2011 on data held by all public bodies in the European Union called Review of Recent PSI Re-Use Studies Published [docx] (PSI stands for Public Sector Information), the study is also know as the Vickery study. One of its main findings is that the EU's current usage of PSI results in €30 billion of economic activity and that opening up more PSI could increase this to €70 billion.
Mentioned in the Vickery study are amongst others:

Measuring European Public Sector Information Resources (MEPSIR), 2006
Commercial exploitation of Europe’s public sector information (PIRA), 2000

"
Tales of woe from fixed-format (non-delimited) ASCII data distribution,"
I gave a talk many years ago (2008) at the joint AGU/SPD meeting on problems trying to read catalogs, but I don't know that it's directly applicable, as many of the cases were due to manually maintained files that were expected to be read by humans, not machines.
Our community does have a standard for documenting the files in a language-independant way (example), but some of the problems that I ran into was that the documentation was just flat out wrong -- the documentation was written years before, and someone changed the table format without updating the docs.
I did keep some other documentation of the issues that I ran into with trying to read catalogs, but I instead formatted it as a checklist of recommended practices, not as specific 'tales of woe'.
"
What does OpenRefine offer that other data-parsing tools don't?,"
I'm a programmer and I use OpenRefine all the time.  Some of the advantages it has over breaking out Python or some other language include:

results of transformation expressions are previewed interactively with live data
quick, interactive, filter facets which allow for easy browsing of instances/rows which match a variety of filters
exploratory analysis of data to do quick visualization via facets and explore interactions among columns
reconciliation of text data against reference data services containing strong identifiers (Freebase, OpenCorporates, any SPARQL or RDF, etc)
simple linking of reconciled entities to other info sources like Wikipedia, MusicBrainz, IMDB, etc
complete provenance/undo history of all modifications
combination of machine smarts and human review for tasks like clustering of names.  
wide variety of input & output formats including both file formats and online repositories like Google Spreadsheets & Fusion Tables
one click selection of record boundaries to produce a grid of data from a JSON or XML API is great for exploring new API endpoints

And I'm finding more cool stuff all the time.  Just the other day I discovered that the scatterplot facet rotates 45 degrees allowing me to select any area on the diagonal of an OCR character accuracy vs OCR word accuracy scatterplot to investigate in more detail.
Disclaimer: I'm the project lead for OpenRefine, but most the good stuff was done by the original author David Huynh.
"
calendar - Are there any regular Open Data conferences?,"
Conferences:

TransparencyCamp, by the Sunlight Foundation, annually since 2009.
Open Knowledge Conference (OKCon), annually since 2005. Open data has been central since its inception - in 2012 this expanded to be the Open Knowledge Festival (OKFestival).
Open Government Data Camp, by the Open Knowledge Foundation, annually since 2010. From 2012 the camp has been merged with the Open Knowledge Conference.
European Open Data Week, annually since 2012.
European Public Sector Information Platform (ePSI) Conference, annually since 2012.
European Data Forum, focused on Linked Data, annually since 2012.
Health Datapalooza, focused on U.S. open health data, annually since 2010
National Day of Civic Hacking, annually since ~2010
International Open Government Data Conference, started in 2010
Open Data Day is an annual event since 2009
Open Data Exchange started in 2013, and will become an annual event.
The Computer-Assisted Reporting conference (often referred to as NICAR) is held annually by Investigative Reporters and Editors and focuses on obtaining and using open data in a journalism context.
IEEE Big Data, a journal on big data which includes calls on open data
OpenSym, OpenSym includes a track specifically for open data
Open Data Science Conference began in 2015, held several times each year (i.e. 2-4) on both the east and west coasts of the U.S. as well as internationally (e.g. Kiev, Bangalore, Tokyo, etc.).  Emphasis on data science but also touches on open ideas, software, and data.

(people might want to expand this community wiki)
"
What is the major difference between Open data and Linked data?,"
Data can be open but not linked, or linked but not open.
""Linked data"" refers to data that is machine readable, semantic data, that a machine can 'understand'.  The ""semantic meaning"" comes from the links, hence the names.  ""Open Data"" refers to data that is accessible to anyone (e.g. without monetary cost to access) with a permissive license on reuse (e.g. public domain or CC0).
Development in linked data usually focuses on tools that provide meaning to data as microdata, RDFa, or RDF, and ontologies that provide meaning of terms.  Open data focuses instead on tools that allow users to access the data conveniently, focusing on tools such as RESTful APIs, and formats that allow a user to query and subset data such as JSON or XML.
Tim Berners-Lee suggested this five star rating system to help think about linked data:

★ make your stuff available on the Web (whatever format) under an open license
★★    make it available as structured data (e.g., Excel instead of image scan of a table)
★★★   use non-proprietary formats (e.g., CSV instead of Excel)
★★★★  use URIs to denote things, so that people can point at your stuff
★★★★★ link your data to other data to provide context

In Tim's system, five-star linked data has to be open (first star, also open format, third star); The linking is really in stars four and five.
By contrast, most open data systems provide the first three stars (e.g. a RESTful API providing data in JSON format), but don't necessarily hit the fourth and fifth star.  From this we might surmise that all linked data is open data, while all open data is not linked data.
One could arguably claim that they provide linked data (URIs, links to other data) behind some proprietary firewall for internal use only, and thus it is not open data.
"
data request - IPA phonology database,"
Might want to double check the license, but the baseline standard is the CMU Pronunciation dictionary, which is freely downloadable and also ships with many NLP libraries, like NLTK (python).
For out-of-vocabulary words, I've had great success with Sequitur G2P, which is both trainable and under the GPL.
edit: note that CMUDict (and many other speech processing pipelines) represent pronunciation in ARPAbet. I apparently don't have enough points to post more links, but google ""FAVE ARPABET"" and you'll get a handy cheat sheet.
edit 2, in response to OP's edit:

Converting from arpabet to IPA is deterministic, so again, wikipedia is your friend as long as broad transcription is acceptable (see note below) 
Depending on the language, you may not need a pronunciation dictionary. german, japanese and korean are examples of languages that have a deterministic mapping of grapheme to phoneme. english orthography is a hideous mutt of historical accident, so sometimes there's really just no way to tell how a word will be said without just memorizing it. french is horrible, too. i'm not sure about arabic. i'd ask people who do automatic speech recognition in your target language (googling should bring you some researchers' homepages)

""note below"": 99.99% of the time, in real-world engineering usage, it is. IPA transcription can get insanely narrow, describing phonetic attributes things like aspiration, specific articulatory gestures, etc that don't ""exist"" in a speaker's conscious knowledge of their language because they're not phonemic, meaning that they can't be used to signal the difference between words with two different meanings
"
usa - Does any US Government agency (like FDA) publish a list of approved food products and ingredients?,"
The USDA maintains a National Nutrient Database with 

nutrient information on over 8,000 foods using this new and improved
  search feature. You can now search by food item, group, or list to
  find the nutrient information for your food items.

The Nutrient Data Laboratory gives food composition and allows you to browse foods by nutrient. 
FDA 'approval' of food products is somewhat less straight forward- see What does FDA regulate? section on food. 
"
geospatial - Looking for Open Data Source to Correlate Address to Latitude/Longitude (geocoding),"
Openstreetmap has an API which gives coordinates for an address, see Stackoverflow for an example
"
"data request - Is there a list, database or API that contains the all the product information in India","
For the ISBN's, you may find interest in Worldcat. It allows searching with ISBN codes, and by language. Hindi is one of the searchable languages. They also offer an API, but to perform unrestricted search requests you have to be affiliated to a Library, otherwise it's free. The other alternative is also to write your own API that scraps the results off web pages.
As for the UPC codes, there is the UPC database that has millions of items (though, not sure how many Indian UPC codes there are). Otherwise you can also use the Google Search API for Shopping, which is deprecated, but offered until September 2013. To search by UPC codes, you have to use the gtin parameter in the URL. 
If you are interested, read this article about the GTIN - Global Trade Item Number, which explains how it is related to UPC. If you are not, I'll make it short, the GTIN-12 format is UPC.
"
data request - English news dataset for sentiment analysis,"
Could you explain more about what you need the data for? I'm not aware of any pre-built data sets, but you could attempt to construct your own. You'll need to break the problem into two parts though.
The easiest route to identifying the entities is the OpenCalais API, which despite its name is a closed-source service, but has generous usage limits. You can also look at the American National Corpus, which contains a large number of automatically-tagged entities in an open data set.
You'll then separately need to figure out the sentiment associated with each entity, which is still an AI-complete problem to do totally accurately, especially in an example like yours where it would require understanding the meaning of the sentence. Most sentiment analysis techniques look at the frequency of particular words or small sequences of words, you can find a good overview of the algorithms here, along with some datasets matching words with their sentiment. 
"
Where can I find data on sales of celery varieties in Europe?,"
Eurostat is a European institution which collects data from its member country's statistical institutions and gathers data itself, amongst others stats on the production of vegetables. Here are the stats for celery and celeriac production across all EU countries. A summation across all countries for the years 2000-2011 shows the following production in 1000 tonnes:

celery: 3194.9 
celeriac: 3817.2

It is interesting to see though that the production data provided by Eurostat on The Netherlands differs in some cells from the Statitistics Netherlands (CBS) data.
"
data request - Is there a global database of all products with EAN 13 barcodes?,"
The Open Product Data project is a comprehensive source for open barcode data. As of May 2014, they have close to a million products in their database. The data is accessible online, through an Android app (source code), and available for download under the Open Data Commons Open Database License (ODbL).
"
"data request - Geolocation - UK places with 100,000 people within a 30 mile radius","
From this Q&A at GIS SE (Worldwide population density data not grouped by country) (updated by me to reflect link changes):

One of the best gridded data sets is CIESIN's Gridded Population.
See Gridded Population of the World (GPW), v3 for more details. The best resolution is 30 Arc seconds (The global data set has resolution of 2.5 arc minutes - Deer Hunter).

To find the places, have a look at this question at GIS SE: Algorithm for finding population for a given center point and radius in US
In general, I would advise looking for geospatial data at GIS Stack Exchange before going here. The chances are quite high the question has already been asked and answered there.
EDIT: through gracious assistance of ldodds, UK-specific reverse geocoding facility (at Ordnance Survey) has been identified:
http://beta.data.ordnancesurvey.co.uk/datasets/os-linked-data/explorer/search
with documentation available at http://beta.data.ordnancesurvey.co.uk/docs/search. This would be the last stage in the processing pipeline.
"
usa - Ownership of US county property tax/assessment data?,"
In the USA there are no sui generis/fruits of labour provisons protecting data.  While compilations of data may be protetected if the author has used creativity with respect to which facts to include, in what order to place them, and how to arrange the collected data so that they may be used effectively by readers (re: Feist vs. Rural telephone), this (very mariginal) copyright protection afforded collections of data in the USA will not be relevant when you create a new collection reusing current and historical property tax data.
In addition, most states in the USA regard works (which may be collections of data) that has been compiled by the agencies of government or its subdivisions, the property of the people. For detail, see Wikipedia about copyright of US state governments.  This is probably not relevant to you, as transformatory use of state tax data is not under any circumstance breech of copyright in the USA, but gives you extra assurance that it is legal for you to reuse and aggregate this data for the purposes you describe.
But to make sure (never rely on legal advice on the Internet), you may contact the agencies of government or its subdivisions that releases the data you plan to aggregate, and ask if they claim ownership to it.
"
Does there exist an authorative definition of an open dataset?,"
Yes, there is: the Open Definition defines openness for data (and content). The Definition was produced in 2005, heavily based on the Open Source Definition, and revised minimally since.
The key part of the Open Definition states:

A dataset [work] is open if its manner of distribution satisfies the following
  conditions, which simultaneously delimit the characteristics of a suitable
  open license:
1. Access
The work shall be available as a whole and at no more than a
  reasonable reproduction cost, preferably downloading via the Internet
  without charge. The work must also be available in a convenient and
  modifiable form. The license may require the work to be available
  in a convenient and modifiable form.
Comment: This can be summarized as 'social' openness - not only are
  you allowed to get the work but you can get it. 'As a whole' prevents
  the limitation of access by indirect means, for example by only allowing
  access to a few items of a database at a time. An example of 'reasonable
  reproduction cost' is the cost of a blank DVD required to
  distribute a complete database.
2. Redistribution
The license shall not restrict any party from selling or giving away
  the work either on its own or as part of a package made from works from
  many different sources. The license shall not require a royalty or
  other fee for such sale or distribution.
3. Reuse
The license must allow for modifications and derivative works and
  must allow them to be distributed under the terms of the original work.
Comment: Note that this clause does not prevent the use of 'viral'
  or share-alike licenses that require redistribution of modifications
  under the same terms as the original.
...
7. No Discrimination Against Persons or Groups
The license must not discriminate against any person or group
  of persons.
Comment: In order to get the maximum benefit from the process, the
  maximum diversity of persons and groups should be equally eligible to
  contribute to open knowledge. Therefore we forbid any open-knowledge
  license from locking anybody out of the process.
Comment: this is taken directly from item 5 of the OSD.
8. No Discrimination Against Fields of Endeavor
The license must not restrict anyone from making use of the work in
  a specific field of endeavor. For example, it may not restrict the work
  from being used in a business, or from being used for genetic research.
Comment: The major intention of this clause is to prohibit license
  traps that prevent open material from being used commercially. We want
  commercial users to join our community, not feel excluded from it.
Comment: this is taken directly from item 6 of the OSD.

(Disclosure: I helped draft the first version of the Open Definition and have helped curate it since along with other members of the Open Definition Advisory Council)
"
data request - Is there an open movie and/or music database available for commercial use?,"
I'm chosing to answer this from the perspective of ""what open datasets are there for movies/songs"".
Its worth noting that IMDB and MusicBrainz offer commercial usage agreements, assuming you're happy to pay.
MusicBrainz is an excellent starting point for music metadata. The Core data, which covers the artists, releases, songs is all in the public domain under a CC0 license. It is only the additional ""supplementary data"" that is published in a CC-BY-NC license.
More information on that here:
http://musicbrainz.org/doc/MusicBrainz_Database
Open Data for movies is more scattered. Wikipedia contains a lot of data on movies, actors and directors, all of which should be available in Dbpedia.
Dbpedia is available for use under an Open license.
I think all other sources of movie data will likely require you to pay for some kind of commercial usage. 
"
"usa - Is there data on the types of cars bought and turned in during the ""cash for clunkers"" program?","
The stats seem to have been quoted on a number of sites. The official name of the program was the ""CARS program"". You can find additional statistics on the transactions here:
http://www.nhtsa.gov/Laws+&+Regulations/CARS+Program+Transaction+Data+and+Reports
The ""Final Paid Transaction Database"" appears to be what you're looking for. It's available as a Microsoft Access database or a CSV file (described as 'text' on the site.) Here is the list of columns in the CSV file (as generated with csvcut -n)
  1: vendor_id
  2: dealer_name
  3: address_line1
  4: address_line2
  5: address_line3
  6: address_line4
  7: city
  8: state
  9: ZIP
 10: area_code
 11: phone
 12: invoice_id
 13: invoice_num
 14: invoice_date
 15: sale_date
 16: disposal_status
 17: disposal_facility_nmvtis_id
 18: disposal_facility_contact_info
 19: sales_type
 20: invoice_amount
 21: trade_in_VIN
 22: trade_in_vehicle_category
 23: trade_in_make
 24: trade_in_model
 25: trade_in_year
 26: trade_in_vehicle_drive_train
 27: trade_in_mileage
 28: trade_in_title_state
 29: trade_in_registration_state
 30: trade_in_registration_start
 31: trade_in_registration_end
 32: trade_in_insurance_start
 33: trade_in_NMVTIS_flag
 34: trade_in_odometer_reading
 35: new_vehicle_VIN_trunc
 36: new_vehicle_category
 37: new_vehicle_make
 38: new_vehicle_model
 39: new_vehicle_year
 40: new_vehicle_drive_train
 41: new_vehicle_car_mileage
 42: new_vehicle_MSRP

The New York Times posted some summary statistics:
http://wheels.blogs.nytimes.com/2009/08/26/the-final-numbers-on-clunkers/?scp=3&sq=cash%20for%20clunkers&st=cse
"
How can a data governance framework be adapted for Open Data?,"
If you're talking data governance frameworks, I think you'll want to look at the Data Management Body of Knowledge (DMBOK) published by DAMA International. The functional point of view in DMBOK is pretty comprehensive. As Federal government guidelines on roles and responsibilities go, the best baseline framework is what the Office of Management and Budget (OMB) and agencies put together in response to the information quality act. OMB's guidelines are available here: http://www.whitehouse.gov/omb/fedreg_final_information_quality_guidelines
Every federal agency was required to develop information quality guidelines, but here's an example from the US Department of Transportation: http://www.rita.dot.gov/bts/sites/rita.dot.gov.bts/files/subject_areas/statistical_policy_and_research/data_quality_guidelines/index.html
The guidelines generally ponder a data quality administrator and data quality officials matrixed across the organization. DOT adapted most of that goodness into its data dissemination policy, DOT Order 1351.34. Which you can find using a google search because I don't have enough reputation points to link to it. :-)
(Disclaimer: I am a contractor at the US Department of Transportation)
"
data request - US Government API Usage,"
There is a growing catalog of APIs from the federal government available at Data.gov's Developer community.  Activity on each dataset will be made visible in the activity stream area (not yet visible as the new catalog was just created last week).  In the meantime, you can see API activity as follows:

EPA Envirofacts API: dataset page and activity stream
Previous metrics (temporary solution) until the activity streams are populated

(Disclaimer: I am the Evangelist for Data.gov.)
"
data.gov - What are the data quality measures for open data?,"
I think the question, as phrased, is impossible to answer well, but I will try.
Q: ""How does a consumer know they are getting good data?""
A: Let me answer with more questions. How does a consumer know they are getting a good search result from Google? How do they know when the news is of high quality? It depends. As consumers get more interested and informed about something, they do better. The most savvy and informed consumers will compare a data set against a known source. Others have to rely on some degree of trust.
Q: ""Are there standard frameworks for grading the quality of an open data set?""
A: In practice, there are defacto standards for metadata. For example, data.gov uses Dublin Core along with additional attributes. CKAN has many of the same attributes.
Also, for each type of data (or subfield) there are often industry standards or at least conventions. Good luck enumerating those!
A post from the Sunlight Foundation, Government Data Sets - Managing Expectations is a high-level gloss; it breaks down ""dataset quality"" into provenance, data quality, responsibility, maintenance, and documentation.
The above article is somewhat naive; the quality of a data set is not an independent thing. As Wikipedia - Data Quality points out, the quality of a data set  depends on the question asked of it. There is no ""one"" measure of data quality. Rather, there is a subjective 'appropriateness' for each question you might ask of a data set. You can't ignore the subjective nature of data quality.
Q: ""Should there be metrics published around accuracy, completeness, timeliness or validity of the data?""
A: There are advantages to doing so, sure, but there are costs too. Who provides the resources to do it? This question cannot be answered well in the abstract. This is a question of leadership and resources. If you want it, act. Advocate for it. Or do (hack, write, whatever) something for your city, country, state, province, or country.
Q: ""Should there be a minimum set of controls on the part of the publisher?""
A: Maybe. This is complex.

Perhaps it is smart to release the data sets you have, regardless of quality, in whatever format you have available.
Perhaps later, over time, and perhaps with incentives, improve them and/or convert to better formats.
Increasing standards of data quality may act as barriers to publishing data; reducing your data inventory. This may be good or bad.
Some data releases may be criticized in any number of ways; for reasons in or out of your control.
Data may be used in ways that the government or collecting agency does not agree with.
Beware of publishing data that may be sensitive. It is difficult to anonymize data in the general case.

In summary, this is a hard problem with many pitfalls. Even well-meaning organizations may need significant prodding to make data releases happen.
"
usa - Sources of political voting records at the county level?,"
The open elections project is trying to collect this type of information from official sources, though it isn't yet available.
This is commercially valuable information that typically comes with a licensing fee and can be problematic for republishing. ""Election Data Services"" is one of the usual sources for this data. Also see Dave Leip's political atlas store where a site license for 2012 county-level presidential results appears to be about $200. 
This data doesn't exist for ZIP codes, of course; they don't match polling districts. 
"
best practice - How to publish real-time open data,"
You have a few options for real time (or ""near real time"", which is when you have a delay between the collection & time to serve it, or for those that sample at a lower cadence)
There are a lot of considerations when dealing with 'real time' data:

Who is the intended audience?  (and do they already have standards for serving this type of data?)
Is the data of value over the long term, or only ephemerally?  
If the data is of value long term, at what cadence does it need to be at to be of value?  Does that cadence change as the data ages?
If the data is ephemeral, what is the maximum age for which it's of value?
What is the maximum latency acceptable for the data?
How large is each record / observation?
How would the intended audience expect to work with the data?
How many people are going to be calling this API?
Is the data useful individually, or only as part of a larger sensor network?

... I'm probably missing a few things ... but without knowing the answers, I can't say if it makes sense to :

have an API that queries the sensors in real time
have the sensor report in every few minutes and serve the latest value
have some sort of data logger and serve the last 10 or 100 values
log to a database and have a protocol for requesting data from a given period of time
append each entry to a log, and roll the log every day (or hour, or some other time period).

If you're just looking to put up a few graphs on a website to show how much sunlight you get ... then just use RRDTool or something similar.
If you're trying to contribute back to a citizen science project ... you could look into the National Weather Service's Cooperative Observer Program.  I don't know if they deal with solar irradiance though.  DOE does, as they have their Solar Energy Potential map, but I don't know how they get the input for it.  You can also try to find if there are any local mesonets in your area that need a node near you.
Also be aware that for some types of real time and NRT (near real time) data, there are APIs for reporting significant events, and interested parties connect and subscribe to the types of alerts they're interested in.  Or individual sensors report back to some central clearing house, and people can then query that for data of interest.  For instance, in astronomy there's the Gamma-ray Coordinates Network, for US earth science data, there's the NOAA Observing System Architecture.
"
data request - Are there any open datasets with technical specifications for photographic equipment?,"
I think it's semi-abandoned, but there are various levels of details for 1,300 models here: https://www.freebase.com/digicams/digital_camera?instances
If nothing else, the schema might provide a starting point for informing the types of information to collect.
"
data request - Open database APIs for journal article metadata,"
There are several different potential sources of information. I don't think any are completely comprehensive and few would count as strict ""open data"": apart from Open Access titles, licensing is likely to vary between publishers.
Having said that you could look at some of the following sources:

Springer API
Nature Linked Data Platform
PubMed API
PLOS API
ArXiv API
CrossRef Search API -- this would be a good starting point

Essentially there are publisher specific APIs, subject aggregators, and a few cross-industry services. CrossRef as a DOI registry are a good starting point.
Its worth noting that given a DOI, you can now get structured metadata about the article using content negotiation, i.e. a simple HTTP request. This includes all CrossRef and DataCite DOIs. More information, including examples.
"
tool request - Keeping track of updates to open data published as CSV,"
You might want to consider maintaining your cleanups as a set of operations or diffs which get applied to the source data.  This would help isolate you from changes to the source and allow you to reapply them to a new dataset.
OpenRefine maintains a history of operations, but you could do something similar with a set of version controlled scripts in your favorite scripting language.  You also might be able to use a set of patch files produced by something like DiffKit.
"
data request - Sources of topograpical maps for use in LaTeX documents,"
Have you had a look at openstreetmap?
http://wiki.openstreetmap.org/wiki/Osmbook
Not tested myself, they use Latex to generate the book.
"
Best practices for huge explorable linked data directories,"
I would suggest:
When dereferencing the root URL, point to a metadata document (via RDFa and/or conneg), say http://yoursite.com/meta. When dereferencing this document, provide a description of datasets using DCAT. every URI there, when dereferenced can show metadata (file size, creation dates, etc) and include a dcat:downloadURl link with the actual data. I'm not sure about the paging issue, but all of the above can be done in say, apache + flat files. Of course there are other more sophisticated solutions but I think this pretty much covers most of what you ask for.
"
Is there a site where USA road traffic historical data would be available?,"
There are a lot of datasets related to road safety and general patterns.  Here you can find many of the National Highway and Department of Transportation datasets and ones specific to traffic in various locations.
(Disclaimer: I am the Evangelist for Data.gov.)
"
usa - Where can I find U.S. train traffic data?,"
As a supplement of open data, the Federal Railroad Administration in the U.S. provides data on incidents, casualties, and a listing of the rail crossings. FRA also provides geospatial data on the location and maps of rail networks.
It could be possible to interpolate train traffic between two grade crossings as well. You could experiment with the FRA grade crossing data. The grade crossing file captures day and nighttime movement counts, as well as crossings where there is less than 1 train per day. The specification is available.
"
usa - Data Source for Speed Limits,"
The best source at a broad level is from the Federal Highway Administration.
There is a also variety of open data on traffic safety. Some of this has embedded speed limit information as well as other information, such as fatalities and accidents. There are also some state speed zone data zones, such as those for Virginia.
There was a question on another Stack Exchange about this as well: ""you can derive an approximate max speed limit by looking at the national speed limit for the type of road in the country of interest"" derived from OpenStreetMap.
The DOT National Transportation Library has FAQs about the National speed limit and speed limit laws. Speed limits are governed by state and local jurisdictions. Some roadway data from a subset of states is made available through the Turner-Fairbank Highway Research Center (TFHRC) (or pull a data request). Reference librarians are available to help with specific data requests.
(Disclaimer: I am the Evangelist for Data.gov. A contractor with Dept. of Transportation also provided information for this answer.)
"
data request - Open Web Crawling Dumps,"
If the crawled data doesn't need to be very recent, the Internet Archive provides 80 terabytes of archived web crawl data from 2011 for research. Unfortunately, they don't say under which license they release the data, so it might not be Open Data as defined by the Open Definition.
"
"Standards for capturing organisational data like budgets, procurement, salaries","
XBRL is a standard derived from XML that is gaining momentum for describing financial and business transactions. A good definition and set of practices is represented by the Security and Exchange Commission.  It provides examples, APIs, and other technical information for accessing data from the SEC and for machine readable financial data using XBRL.  The site notes, ""The Commission also has published final rules requiring certain nationally-recognized statistical rating organizations (NRSROs) to provide rating information on their websites in XBRL format.""
"
business - Open Address Data for Restaurants,"
OpenStreetMap has quite an easily accessible database of restaurants (and other places), which you can easily query using their Overpass API. An example query for Overpass's Query Form which gets all restaurants in greater London:
<query type=""node"">
  <has-kv k=""amenity"" v=""restaurant""/>
  <bbox-query s=""51.28"" n=""51.686"" w=""-0.489"" e=""0.236""/>
</query>
<print/>

An easier way of creating a query is to use Overpass Turbo, which allows you to navigate a map to reflect your area of interest, again an example of all restaurants in greater London (click on Run and move to the Data tab on the upper right to see the 'raw' data).
Another source is OpenCorporates, which often holds official registrations of companies and whether they are active or not. Many restaurant registrations don't have their address listed, but you might want to use their data to find out whether restaurants are still active or not (see their video on how to use Google/OpenRefine to reconcile names from a certain dataset (in this case OpenStreetMaps) with the OpenCorporates dataset).
"
"releasing data - Preservation of blog posts, articles and essays","
Great question.  I agree that permanent archiving of blog posts and other digital content is an important challenge in open data. It might be helpful to break this down into parts:
Persistent identifiers
Having a persistent address at which potential users/machines can reference your content is crucial to good archiving, and most of the issues you list refer to this (losing your domain name, provider going bankrupt, etc.)  One promsing way to address this is by registering a PURL or Persistent uniform resource locator.   PURLs are widely used for major web ontologies like Dublin Core: http://purl.org/dc, to make sure these resources have consistent links.  Users must register and they are relatively easy to set up (for instance, I have registered http://purl.org/cboettig as a partial redirect to each of the pages in my online research notebook. If you are familiar with DOIs for published literature these are largely analogous technology.  
Archiving content
Making sure your link always resolves is not the same as guaranteeing your content continues to exist.  Robust, geo-politically distributed archiving services like CLOCKSS or LOCKSS are probably the gold standard here, but not accessible to individual authors.  Having your own distributed backup copies on public repositories is still a good idea.  Depositing copies in an appropriate repository, such as fig*share* for scientific research content, is one way to achieve this level of archiving.  
Tools like Git/Github can help archive the version history of your content, not just the most recent version.  
Good metadata
Having good machine-readable metadata on your site will help search engines index it accurately and can help users/machines actually make use of it.  Consider identifying author, titles, dates, tags, and other such data using RDFa, though even vanilla HTML5 has quite a few semantics available.  Some examples here
You've already mentioned good licensing, which is key in making your posts useful as open data.  See that the license information is properly embedded in machine-readable metadata as well. 
"
"usa - Which, if any, U.S. Federal Government agencies offer a Service Level Agreement (SLA) for their APIs?","
SLAs suggest a quality of service that costs money to maintain ... in the business world, you have to pay to get an SLA.
The only time that I've been involved with something approaching an SLA would be an MOU (Memorandum of Understanding) between two federal agencies.  In some cases,  the agency requiring a given level of service helping to pay for the upkeep of the network and processes that go into making the data available ... in others, it's just an agreement that the connection to retrieve the data exists, with no guarantees on availability.
(Disclaimer : I work for the Solar Data Analysis Center, but I'm not the 'Joseph' listed on their website; I'm also not involved with these MOUs, other than knowing that they exist (and which hosts not to block if they start acting up))
"
What's the difference between Open Data and Big Data,"
They are not the same at all. Datasets are Open if they are available under a free license to everyone. Datasets are Big if... well, they are. Typically big beyond where common software can handle them in real time. 
For example Facebook and Google work with Big Data that is not Open. 
Most Open Data sets are actually an example of Small Data: The datasets themselves are not huge, but there is a large number of them that can be correlated to increase their value. 
"
data request - Finding an index of food prices,"
It sounds like you want Consumer Price Indices for Food and Beverage for various metropolitan areas. The Bureau of Labor Statistics calculates just such indices: http://download.bls.gov/pub/time.series/cu/cu.txt
Metropolitan Areas
area_code   area_name   
A101        New York-Northern New Jersey-Long Island, NY-NJ-CT-PA1  
A102        Philadelphia-Wilmington-Atlantic City, PA-NJ-DE-MD
A103        Boston-Brockton-Nashua, MA-NH-ME-CT
A104        Pittsburgh, PA
A207        Chicago-Gary-Kenosha, IL-IN-WI
A208        Detroit-Ann Arbor-Flint, MI
A209        St. Louis, MO-IL    
A210        Cleveland-Akron, OH
A211        Minneapolis-St. Paul, MN-WI 
A212        Milwaukee-Racine, WI
A213        Cincinnati-Hamilton, OH-KY-IN   
A214        Kansas City, MO-KS      
A311        Washington-Baltimore, DC-MD-VA-WV       
A316        Dallas-Fort Worth, TX       
A318        Houston-Galveston-Brazoria, TX  
A319        Atlanta, GA     
A320        Miami-Fort Lauderdale, FL   
A321        Tampa-St. Petersburg-Clearwater, FL     
A421        Los Angeles-Riverside-Orange County, CA 
A422        San Francisco-Oakland-San Jose, CA  
A423        Seattle-Tacoma-Bremerton, WA    
A424        San Diego, CA   
A425        Portland-Salem, OR-WA   
A426        Honolulu, HI        
A427        Anchorage, AK       
A429        Phoenix-Mesa, AZ        

"
usa - What level of government/governing agency determines the terms of use for transportation map/gis data in the U.S.?,"
The national governing body for geospatial data in the US is the Federal Geographic Data Committee.  Data provisioned through the FGDC is part of the overall Data.gov corpus of data.  Data provided by federal agencies (such as NASA and NOAA) are provided without charge and without restriction. (Note the data policy that states ""Data accessed through Data.gov do not, and should not, include controls over its end use.) 
However, data provided by other organizations, such as the Milwaukee city data you referenced, are governed by that entity and may vary. Non-federal data is clearly marked as such on the site in both the metadata and with a banner across the right side of the dataset in the search results.
The new Open Data Policy from the US government explains more about the definition of open licensing.  
(Disclaimer: I am the Evangelist for Data.gov)
"
data request - Open database of domain registration information?,"
You could try a service like http://www.whoisxmlapi.com/reverse-whois.php or http://www.domaintools.com/
The short answer is: it's complicated. The whois system (which is used to query domain registration data) is decentralized similarly to the DNS system - individual registrars keep the whois data for their clients so there isn't a meaningful way to query a central database. Additionally, the whois protocol is designed to be incredibly simple: query for a resource and receive the associated data. Think of it as a dictionary... You can easily search a dictionary for the definition of a word, but you'd need a different tool to search through all the definitions to find words with similar meanings. Companies like the ones mentioned above do the heavy lifting by aggregating the individual whois data and then providing a means of searching the meta data. Since not all whois responses are the same however, most results by reverse whois searches are considered ""best guess.""
As with everything, there are caveats to the explanation above, but without getting too unduly complicated - there you have it.
"
tool request - How to publish open data on my website? (Or: from CSV to RDFa),"
If you already have a nicely formatted CSV then why not publish that? You can publish in both formats if you really want to do the RDFa too.
"
licensing - License for data that precludes government/surveillance use,"
Such a license would require that the publisher provide one on one individual approvals for data use and analyze or have the potential user express all the impacts of the use of their application or analysis.  This essentially would not be open data.
Trying to stay with the theme of this group, there is a license that could apply:

CC BY-NC and BY-NC-ND which restricts commercial use

An example of the licensing used by Thomson Reuters and many of their partners might provide insights as well.
"
usa - Additional Detail from IPEDS Data Source,"
The entry for IPEDS on data.gov (https://catalog.data.gov/dataset/integrated-postsecondary-education-data-system-ipeds-data-center) provides a point of contact. Recommend emailing the dataset POC directly.
"
Census County Commuter Flow Data?,"
Here is a link to county to county commuting flows:
http://www.census.gov/population/metro/data/other.html
For older data, you can take a look at the raw files from:
The 1990 Census: http://www.census.gov/population/www/socdemo/jtw_workerflow.html
The 2000 Census: http://www.census.gov/population/www/cen2000/commuting/index.html
"
usa - Are there good examples of requiring open data in RFPs?,"
Clear and simple, I can do ... re-usable I'm not so sure of.
NSF in 2011 put a requirement on all grant applications to submit a 'data management plan', explaining what data would be produced and made available by the project.  Note that it doesn't actually require the data to be 'open', and it's entirely possible that the scope of the project may be such that the PIs believe that there isn't any 'data' produced.**
There's also been questions as to what instructions NSF has been giving to the grant review panels about data management plans, and how much weight they have in the final scoring of the grant proposals.
NASA has various 'data policy' statements, depending on the field.  The heliophysics policy states:

Two overarching principles also essential to achieving the goals of current Heliophysics programs are:

Embracing NASA's open data policy that high-quality, high-resolution data, as defined by the mission goals, will be made publicly available as soon as practical ...


... while the NASA Earth science policy contains:


NASA commits to the full and open sharing of Earth science data obtained from NASA Earth observing satellites, sub-orbital platforms and field campaigns with all users as soon as such data become available.
There will be no period of exclusive access to NASA Earth science data. Following a post-launch checkout period, all data will be made available to the user community. Any variation in access will result solely from user capability, equipment, and connectivity.
NASA will make available all NASA-generated standard products along with the source code for algorithm software, coefficients, and ancillary data used to generate these products.
All NASA Earth science missions, projects, and grants and cooperative agreements shall include data management plans to facilitate the implementation of these data principles.
NASA will enforce a principle of non-discriminatory data access so that all users will be treated equally. For data products supplied from an international partner or another agency, NASA will restrict access only to the extent required by the appropriate Memorandum of Understanding (MOU).


My understanding is that all NASA missions require a 'PDMP' (project data management plan)
** although, that then gets us into the question of what is 'data', which is much too long and off-topic for this post.
(Disclaimer : I work for a NASA heliophysics archive; I haven't served on a grant review panel for years and never for NSF)
"
Spend transaction data formats,"
I think it was best said in John King's wrap-up for the public hearing on public access to federal data last month at the National Academies.  To paraphrase:

If you require people to do stuff for which they get no benefit, they're going to spend the minimum effort in doing so, and you'll get a crappy result.

So ... rather than focus on the format ... focus on what benefit they could get out of it.  I've been a big proponent of tool building to support data formats -- create some great tool that people want to use, but to use it, they have to put their data into a proscribed format.  Maybe you could give them a tool to do the reformatting, so that they're the ones maintaining it should their fields change.
Right now, their different formats support their existing systems.  They actually need those systems to get their job done.  The extra reporting is just an extra burden that's been placed on them.
Talk to the various IG (Inspector General) and finance departments ... maybe there are some common analysis that they all do that's challenging with their existing tools.  If it's more cumbersome than the data reformatting, you have a potential way in.  If you can't find some way that you can actually improve someone's life (with that someone being in the power to make the change), you're just going to be treated like another unfunded mandate. 
"
tool request - Is there a Git for data?,"
I recently stumbled on this article by the Open Knowledge Foundation regarding the design of a graphical interface to diff tabular data called daff.
It can also be tested and forked on GitHub.
"
usa - Linking FCC documents from ECFS to the Federal Register,"
I've found the Federal Register Ruby gem to be useful:
result_set = FederalRegister::Article.search(:conditions => {
  :agencies => ""federal-communications-commission"", 
  :docket_id => ""12-375"", 
  :type => ""PRORULE""
})

To provide programmatic linking, a mapping of ""NOTICE OF PROPOSED RULEMAKING"" (ECFS) to ""PRORULE"" (Federal Register) is needed.
This seems to work well enough, but a potential pitfall could be proceedings that have multiple documents of the same type (e.g. multiple NPRMs). In those cases, I'm not sure how to distinguish documents.
"
data request - Bioequivalent drugs in the US and EU?,"
A useful subset of the bioequivalent drugs would be approved generics. Don't know if you're also looking for unapproved (yet) but these would be a bit harder to find as a complete collection. Keep in mind that the pharmacokinetic test routines differ slightly between the US and Europe.
Keeping to the approved generics, the FDA is required to maintain and post a list quarterly. The data is in PDF but at least it's selectable text (not just scanned images). For Europe, the EMA has a search page for generics (etc). Search results can be downloaded in excel format.
"
usa - Availability of APHIS Data as API or bulk dump,"
It's likely not want you are hoping for, but that website is an API, just an undocumented poorly designed one. You'll want to make easier to work with by writing a screen scraper. 
It looks like at least one other person, maybe you, is working on one at ScraperWiki
https://scraperwiki.com/scrapers/aphisacis/
"
"data request - A database for dog, cat and other pet names?","
I've compiled some resources for a blog post, I'll just post the relevant content here:
Hundenamen aus dem Hundebestand der Stadt Zürich
This one is from the city of Zürich, Switzerland, where I live. I've seen a recent Twitter post about this dataset, so that may have planted the idea that dog names can be open data.
Data goes back to 2015, and each year is one CSV file. To get an idea of the dataset size, I choose the complete year of 2019. 7647 records. It may be hard to find trends in so few dog registrations. Additionally, the Paw Patrol trend is slowly making it here to Switzerland. Since it started in North America, I'll go to look there.
Anchorage Dog Names over Time
Only 16k total names between 2017 and 2019. That's not enough dogs when there are so many possible names. And starting in 2017, I may not get a good before snapshot.
Seattle Pet Licenses

A list of active/current Seattle pet licenses, including animal type (species), pet's name, breed and the owner's ZIP code.

This might be a good dataset because records go back to 2000 and are updated through 2019. I can get snapshots before and during the PAW Patrol era. But I counted dogs registered in 2019 and it was 11k. In 2018, 7k. Still not enough.
NYC Dog Licensing Dataset
This could be it. Recently updated. 24.1 MB CSV file. 345k total rows going back more than 10 years. 79k dog registrations in 2019. Explore the data here.
the fine print:

Each record stands as a unique license period for the dog over the course of the yearlong time frame.

What does this mean for my data? It means that dog names are assigned at least once per year. If I count unique dog names over multiple years, I'll be over counting. 
and

Each record represents a unique dog license that was active during the year, but not necessarily a unique record per dog, since a license that is renewed during the year results in a separate record of an active license period.

This means that dog-names within a given year may actually be duplicate as well. If this was a real project, in order to fully trust my data, I would first count how many names are repeated. To do this, because there is no column dog ID which would uniquely identify a dog, I would have to create a surrogate key based on the columns such as AnimalBirthMonth, AnimalGender and BreedName, and perhaps also the geographical data Borough and ZipCode.
"
best practice - Let's suppose I have potentially interesting data. How to distribute?,"

Suppose that I have some sort of specialized data, perhaps that I've collected myself or been a part of the collection. And suppose that nothing prevents me from handing this data out to people. In what method should I go about distributing/storing this data so that others will be able to find it and use it, whenever this time may be?

Targeting specialised repositories as per @Joe's answer is indeed an excellent way to go about disseminating data, but what if no such specialised repository exists or you do not wish to target only one specific community in particular?
A methodology to expose Open Data using generic principles is the 5-star Open Data scheme originally proposed by Tim Berners-Lee here.
The core rationale of 5-star Open Data is that you make your data more easily accessible, processable and interoperable with each successive star:
★ Put your data on the Web in some format with an Open Licence. People can access it through their browsers and spend some time to figure out how they can download/access/process/use it. (Avoid problems for your client like this.)
★★ Put your data in a machine-processable format. For example, having a table in Excel is better than having a snapshot printed in PDFs or images because people can download it and start running experiments over it. (Avoid problems like this.)
★★★ Use non-proprietary formats. For example, providing data as a CSV is often better than as an Excel file because CSV can be directly processed by a wider range of (free/open source) tools and programming languages. (Can't find anyone complaining about Excel on here yet but, e.g., this is a similar problem.)
★★★★ Use URIs to denote things. For example, let's say you provide a bunch of pollution measures for cities and somebody would like to specifically reference the pollution measure for London. Assigning a URI for London in your local data provides a global unique identifier for that city that people can reference and point to. There are, for example, related proposals for embedding URI fragment identifiers in CSV files. (Avoid problems like this or this.)
★★★★★ Link your data to other data to provide context. So you have created a URI for London in your data and people can point to it. However, which London are you referring to? London, England or London, Ontario? If you link your local URI for London to the Wikipedia page about the London to which you refer (or, even better, to the DBpedia URI for the specific place to which you prefer), this provides context as to what you mean. (Avoid problems like this.)
The shift from ★★★ to ★★★★(★) is quite an ambitious one and technical proposals are still being made on how best to achieve this, but five star Open Data is great because now your data are available on the Web under open licences with open structured formats where everything of importance is given a URI that can be referenced and linked across the Web, allowing for future discovery and re-use. A common methodology to create five star Open Data (again proposed by Tim Berners-Lee) is Linked Data, which assumes RDF as a common interoperable data format. But if that all sounds too much, getting as far as ★★★ data is still great.
Again, you can check out this description of 5 Star Open Data for more information and a related question here. 
A useful resource for the generic cataloguing of Open Datasets is the CKAN project, where the related DataHub repository is a great place to list and publicise your dataset. You can check out a bunch of 5-star Open Datasets here.
"
Should data APIs require registration and API keys?,"
The other answers so far are all terrific. I'll reiterate one point, and make a new one:

The openness of an API is always important, but when complete, quality bulk data is available some of these access issues become a lot more tolerable. An API is not a substitute for bulk data. The federal government has become very API focused, and many of them have throttling, API key registration, and even attribution requirements. The need for accompanying, complete bulk data is a point I hope the community will continue to press.
There's an important political aspect to API key registration, which is demonstrating (especially internally) that the API is a success, and worth continued investment. I work at the Sunlight Foundation, and this is one of the reasons (in addition to abuse, contacting devs, etc.) we require registration of an API key. This is both quantitative (measuring hits), and qualitative (it's nice to have logos of big organizations on the sidebar of our API homepage).

Of course, government agencies are in a fundamentally different situation than non-governmental organizations. They're funded directly by taxpayers and are the original producers of information that literally belongs to the public. 
When agencies consider the benefits of using API keys versus providing open access, the scale should be heavily tilted towards open access to the people's information. Providing free bulk data access in addition to any APIs (like the Census does), and providing key-free API access (like the Federal Register does) are models I strongly encourage.
Update: See FederalRegister.gov's API case study for their rationale for not using API keys:

In our view, API keys can create an unnecessary barrier to rapid experimentation with our public data. We are able to track our API usage via logging mechanisms on our servers and already have infrastructure in place to mitigate any sort of excessive requests. The benefits of using a simple REST-ful API format are that any user can easily try it in their browser (no SOAP that requires complicated XML to be POSTed around, no special headers, etc). The response to our no keys policy from the development community has been extremely positive (http://news.ycombinator.com/item?id=2839137).

"
"data request - Is there an open database of elementary, middle, and high schools in the United States?","
Unfortunately, the key links in the popular answers for this question are all currently broken. It is still true, as David H answered, that the Common Core of Data is the official source, but the links to data.gov are broken.
Now, the only relevant thing on data.gov is the page about the School & District Navigator which links to an interactive map that doesn't itself offer data.
However, the Department of Education has a simple interactive tool which helps get access to specific CCD files, ""fiscal"" or ""non-fiscal"", at the state, district, or school level.
Along the way, I also found public schools as GIS data from the DHS Homeland Infrastructure Foundation-Level Data (HIFLD) website.
The Department of Education also created a polished ""Developer Hub"" referencing supported and legacy APIs, but the dates on blog posts for that are all at least 3 years old right now, so it's hard to know what's actively supported -- and in any case, none of the APIs are for K-12 schools.
"
"government - Data on income information for India, China, and West/East African nations with GIS coordinates","
Speaking as a non-professional, I have seen no such detailed datasets.
Your research should not depend on clueless strangers, though. My first hit on Chinese statistics was: http://www.stats.gov.cn/english/statisticaldata/ , and I'd think they are worth exploring. Same for India.
Gathering detailed income data is fraught with difficulty on many levels: the ones with access to the high-fidelity data (local tax authorities) won't talk for a host of reasons, apart from confidentiality (quite often they won't be bothered to help other government agencies), while aggregation over villages/counties/whatever lowest-level administrative units is not something in high demand from national and supranational decision-makers. I leave aside the obvious problems of tax evasion, non-response, and outright fabrication.
Thus, for countries with less than well-funded statistical agencies, one is forced to look for proxies.
The usual proxy is electricity consumption; while the utilities are loath to give outsiders access, as a rough proxy, one can simply look at the night lighting: http://geology.com/articles/satellite-photo-earth-at-night.shtml.
I'd recommend against turning to land use data - they can't tell you much about city-dwellers or income levels. You can ask folks at GIS SE for details, though.
Access to sanitation is another proxy (albeit a non-linear one). World Bank, local authorities, utility companies, food inspection agencies can possibly serve as sources, I'd guess.
Whatever you do, please remember to compile a list of references, prior and related studies, validate and cross-check your data. Ideally, you would do an on-site survey for that (assuming you have got some money to spend). Beware of systematic bias creeping in (it will!) without you noticing or telling users of your data.
"
Crowdsourcing Data Submission,"
Indeed, quite typically, for transport data, it can be very interesting to gather open data and users feedbacks (tweets, posts, ...) to improve quality and restore confidence towards public services.
There are some tools mixing Open Data and crowdsourcing: OpenStreetMap, OpenEcoMaps
At some midpoint between Open Data and crowdsourcing, you have FixMyStreet: The data released by the users can be used to hydrate future open data DB's. 
Btw, see this blog post: http://blog.okfn.org/2011/05/23/can-crowdsourcing-improve-open-data/
"
data request - Open dataset for a 65-million year temperature history of earth?,"
The Zachos et al. article has supplemental data section, but there are no actual datasets in this section, only sources for ΔO-18 and ΔC-13 isotopic data. The article itself says that the temperature estimates are given for an ice-free world ocean. If you want to cite the data from Figure 2 in the article without all the caveats (and then some) of the original, your readers will be either fooled or offended.
A sample caveat from the supplement:

Sampling Biases: One of the limitations on reconstructing long-term secular variations is the highly uneven distribution of deep-sea stable isotope data in both space and in time. The global signal for some key intervals is based on data from just a few records (emphasis mine - DH). In general, these spatial biases increase with age, moving toward the Atlantic, and shallower water depths. In other words, the Pacific, and abyssal portions of the oceans tend to be under-represented in existing stable isotope records. These biases do not pose a problem for our temperature/ice-volume reconstruction of the late Neogene oceans which were thermally homogeneous. Such biases, however, are a concern for establishing the mean climate-state of some ""warmer"" time intervals when the thermal gradients within the deep-sea were greater.

"
data request - Open alternative to weatherbase.com,"
How about NASA GISS Surface Temperature Analysis? Although they have station-oriented timeseries, global coverage seems quite good and they provide monthly average temperatures globally for a ridiculously long time: random sample of a station dataset in Africa (1946-2012):
YEAR    JAN    FEB  ...   DEC
1946   24.8   25.9  ...  26.4
1947   26.1   27.3  ...  25.3
1948   25.8   27.4  ...  25.7
...
2010   25.9   28.0  ... 999.9
2011  999.9  999.9  ... 999.9
2012  999.9  999.9  ... 999.9

Major cities tend to have one or multiple stations with identical name nearby. Documentation of their data processing is naturally very good. (Meh: the sample station I picked seems to be plagued by missing values [999.9] recently.)
"
legal - Open data for international treaties,"
A searchable directory of international treaties can be found here: http://www.worldtreatyindex.com/index.html
Links to download the raw data of the complete database can be found here:
http://www.worldtreatyindex.com/multi.html
"
releasing data - Recommended BitTorrent tracker/index for dataset release?,"
Consider whether posting your .torrent file to a BitTorrent index site is the best solution for you. If your objective is to publicise your dataset you may be better off simply posting your .torrent file to a website or forum that focuses on open data or, more specifically, the topic to which the data relates.
You should also bear in mind the large amount of upload capacity you'll need for this initially. Depending on the level of interest, you might find it difficult to retain seeders, and so could find yourself uploading the file repeatedly. I assume you've looked into paid cloud storage (such as Mega), but bandwidth limits will be a potential issue.
It might also be worth confirming that you are using the maximum compression possible. Compression software commonly defaults to a moderate compression ratio as a trade-off for quicker compression time.
"
data request - Open Seed for Crawl,"
You can download the top 1 million (ZIP) sites from Alexa.
"
parsing - What data source for cloud coverage available with forecast and how to parse it?,"
Open GIS-ready cloud data are available at various levels of detail, both in time and space.
It is quite tempting to work with satellite photos of cloud coverage; unless you are a professional, don't do that - there are a bunch of hidden snags you have to know about.
For current data on (points) airfields and airports of the world, your best bet is METAR, where you can learn type of clouds and cloud coverage, as well as visibility figures. There are many applications (in Python and Ruby, among others) that access and decipher METARs (you can do that as well, after a bit of training).
If you need in-depth point forecasts of probability that cloud cover will be less/greater than a given threshold, you can use NOMADS (may be overkill for your purposes, though).
For wide-area coverage with forecasts up to 8 days, the ideal stuff comes from GRIB, easily downloadable and parseable with command-line utilities and GUI programs. These are forecasts of most probable weather, though, unlike the extensive data from NOMADS ensemble.
"
usa - Real-time gunshot detection data?,"
The City of Oakland tacitly agreed to make the shotspotter data open, and the firm representative was eager to do so, but they've not followed up by doing so. 
They did release it in bulk in Washington however, not real time.
"
tool request - Anyone have a good way of comparing two large and unstructured lists (~2k entries each) for commonalities between them?,"
Following the discussion in the comment section, I suggest that you have a look at OpenRefine. For a 4,000 rows dataset (two set of 2000 rows each) Refine allow a mix of manual and script cleaning (using fuzzy match). Here is the steps I will follow (based on what I understood) to clean this dataset:
Prepare your data

In a separate tool, merge the two set in a single document with a column for each source
Load the file in OpenRefine
Using the transpose function merge the two fields in one, in the windows option remember to tick append column name (so you can track from which source your data come from) and separate
Using the split function, split your new column based on the pipe |

So now you have a field with your source name and an field with your value, now we will be able to start to clean those value:

Invoke a text facet to list all the value available, and click cluster to do fuzzymatch comparison and search for similar record
Play with the different clustering algo (including Levenshtein, metophone) ... You have to manually select the right matches, discard the other, you are in total control of the algorithm. Do not hesitate to explore the different algorithms available, some are more conservative than other or will match on different parameters.
Once your done with the semi automatic clean up, finish the work manually using the facet windows to list all value available in your list. When you want to correct a value click the edit option in the facet windows to update all matching rows.
Once done export your data, there is various format available.

This is a very high level process and hopefully you will find it useful. If you want more details, you can explore this tutorial for the split and clustering function, or dig through existing step by step doc.
disclaimer, I am part of OpenRefine team
"
data request - Open datasets for product reviews,"
Amazon has an API for this, and then there's always web-scraping.
"
usa - Public access laws used for real-time data?,"
Given most local laws and regulations allow for a 2-3 week response time, I imagine you'd have to request the records before they exist, and word your request in such a way that it can ""never"" be completely fulfilled.
"
tool request - Wikipedia table to JSON (or other machine-readable format),"
You can use Google Spreadsheets ImportHTML formula as detailed in this Liberating HTML Tables (using Google Spreadsheet) tutorial on School of Data by Tony Hirst - it includes a specific walk-through for Wikipedia.
The essence is to do:
=importHTML("""",""table"",N)

In your case you could try:
=importHTML(""http://en.wikipedia.org/wiki/List_of_television_stations_in_the_United_States_by_call_sign_%28initial_letter_K%29"",
   ""table"", 3);

In your case you can tweak 3 to be the table you want to grab and obviously you can repeat this formula for multiple tables.
There a bit more on this and issues with links in this answer on ask.schoolofdata.org (also from Tony Hirst).
"
tool request - How to use the DOI system as an individual?,"
I'm not aware of any groups that will let you create test DOIs unless you're somehow affiliated.
However, the California Digital Library is a member of DataCite, and they operate EZID to allow other groups to mint DOIs.  They have a pricing schedule for groups to get access.  I'd suggest contacting them (see the link on the pricing page). 
"
best practice - Load data from HTML tables into OpenRefine?,"
If you're doing this interactively, most browsers will format tables as TSV when they're selected and cut.  Pasting this into the clipboard dialog of Refine's project creation dialog will allow you to import the data as TSV.
If you've got a bunch to do or need to do this repeatedly, I'd use Google Spreadsheet's importHtml(url,""table"",N) function which will fetch the Nth table on the given page.  Refine can import directly from the resulting Google Spreadhsheet, so you can skip the export step.
If you just wanted little bits of information from lots of different tables, you could use ""Add column by fetching URL"" and then hand parse the interesting data out using Refine's parseHTML() with the necessary CSS selectors, but that would be pretty painful and not recommended.
"
data request - Database of English words pronunciation,"
Wikimedia Commons currently offers more than 20.000 sound files with English pronunciation, around 1.500 of those with British English pronunciation. All of them are published under an open license.
Unfortunately, there are currently no dumps of the media files available. However, there is a page that explains how to reuse the content outside of Wikimedia.
As an alternative, there is Forvo. Their audio files are licensed under the more restrictive (and not entirely open-data-compliant) CC BY-NC-SA license. On the other hand, they do offer an API.
"
releasing data - What is a ready to use wordpress CMS template for serving open datasets?,"
Data.gov has open sourced it's code, which combines WordPress for the front end with CKAN for the open data catalog.  The code is available via Github--for commenting, downloading, or submitting modifications.  
This code will continue to evolve with user-driven updates to the functionality needed.
This will be part of the U.S. ongoing contribution to the Open Government Platform, as well, which is a collaboration between India, Canada, Ghana, and the U.S. and currently is available as a native Drupal, and a Drupal + CKAN capability. The source code is being made accessible via Github.
(Disclaimer: I am the Evangelist with Data.gov)
"
usa - Linking results from the FCC's TV Query API to the FCC's TV Stations Profiles API,"
This is the sort of issue that can arise when trying to combine data from multiple sources, in this case two apparently distinct APIs.
The Developer page gives some brief instructions as you point out, but importantly there seems to be no method for retrieving a list of all valid Facility IDs. This is an oversight in my opinion, as a ""Facility Details"" request, for example, requires a Facility ID as a parameter. It appears the only way to get valid Facility IDs, other than your method, is to use the ""Facility Search"" API method, which allows you to search for stations using ""call sign, frequency, city, state, channel, or Nielsen DMA"", and returns a Facility ID among other data.
You haven't stated whether you're working manually or programmatically, but as a programmer if I were doing this and needed to get a list of Facility IDs, I would probably use the Facility Search to return data for each of the 50 states, then combine these. This would be relatively straightforward in code - loop through the state codes for each of the 50 states, as per the example given for New York:
http://data.fcc.gov/mediabureau/v01/tv/facility/search/NY.json
For each state, scrape off the Facility IDs, which ought to result in a complete list of valid Facility IDs for the USA. This assumes that there are no facilities that are somehow independent of states, or have no state recorded.
Having said all that, I would probably take a minute to send a quick email to developer@fcc.gov, asking whether it is possible to retrieve a list of all Facility IDs via some unpublished API method. The amount of support you can expect for using an essentially free and thus unsupported service may vary of course.
"
What criteria determine a good name for an open data product?,"
Unique.  I can't believe we still have groups naming projects 'GAIA' (as if image processing software, satellites, and other existing projects isn't enough).  But even satelites like 'TRACE' and 'SOHO' are problematic because they're common enough in English.
I'd also look to see if there are standard prefixes or suffixes in the discipline that are significant (eg, you gave two examples that ended in -X). If there are common acronyms for the given type of data you're releasing, you may want to follow the conventions in your field ... but you also have to be wary of it being too similar and someone thinking it's just a typo for some other product.
"
licensing - Why should I care how a (structured) dataset is licenced?,"
IANAL but I believe most open licences are exclusively designed to provide reuse rights that were taken away by copyright (e.g. CC). Some licences also give rights that were taken away by the 'database right' (e.g. ODbL, OGL). (I don't have much knowledge about click-wrap agreements.)
I'm not sure that you can say that most data is factual, and therefore copyright does not apply. Track names for CDs are creative works. Weather readings and locations of items on a map have been defended vigorously against copyright (e.g. before database right came along in '96), although you'd have thought that because they are collected by strict rules, they'd fail the 'creative judgement' test. That the case of the football fixture list had to be referred all the way up to the European Court suggests it is not an easy judgement and that you have to be extremely careful. The telephone directory case also seems hard-won.
'Database right' does not apply to an individual item of data, but the threshold is somewhat lower than the entire database. As soon as you extract a substantial quantity from the database you become liable. So anything more than one item might invite a letter from the lawyers.
Since reusing data without a licence is such a mine-field, anyone who doesn't have a massive legal fund might as well regard it as a no-go area. That's why there is a big push by governments to provide open licenses for their data, to encourage transparency and reuse of these valuable national assets.
It is important to note that you can use data for your own internal purposes, whatever the licence. It's only if you want to republish the data or derived data that you need to study the licence.
"
usa - What open data institutes in the US are working with the Open Government Partnership?,"
You may have already seen this, but on the OGP website there is a Country Commitments page for each country including the United States. The ""Efforts to Date"" tab has some broad information on the government's, well, efforts to date. It appears that the main objective so far has been to increase the utility of the datasets available at data.gov.
Data.gov looks to be a well-designed site with loads of information. For example you can dig down to Open Data sites at the state and county level. I suspect if you browse around you might find something approaching the information you're after.
As an aside, it seems that the OGP does not directly sponsor or financially support the work of individual groups or companies, etc. Rather they are an organisational body with the goal of spurring on governments to increase transparency. If you're interested in how the OGP's budget is sliced and diced, it is available for download. 
"
api - Is there a resource to look up the Standard Industrial Classification codes that companies file with the SEC?,"
On the NICAR-L mailing list, Tim Henderson pointed to an HTML listing, at http://www.sec.gov/divisions/corpfin/organization/cfia.shtml
Matt scraped the data and posted it as a Google document.
Matt Jacob pointed to another potentially useful resource, a listing of what the SIC codes actually mean.
"
Which real-time open data APIs do you know?,"
You can find a wealth of government APIs at the Data.gov developer page.
As far as real-time nature of the data feeds, the APIs vary in their update frequency. For example, flight status from the FAA updates every 10 - 15 minutes: 
"
data request - Is there any free weather database that one could use for correlations in business intelligence software?,"
Two resources:

https://developer.forecast.io/ (free for low use, then paid after that)
http://www.ncdc.noaa.gov/cdo-web/#t=secondTabLink

The NOAA data is used by a developer in Chicago to explore the correlation between weather and crime.
"
data request - Where can I download those bible verses in JSON or XML or SQL formatted file?,"
Disclaimer: I am the author of below github projects.
Two very good (and complete) sources are:
https://github.com/scrollmapper/bible_databases :  
This has many database formats including all three formats you are inquiring of. 
https://github.com/scrollmapper/bible_databases_deuterocanonical : 
This is a newer continuance of the bible databases, but with secondary books (ie, the Deuterocanonical ones). This is just sqlite at present but easy to convert. It is also based in a Django project. 
"
data request - What sources exist for sales tax information,"
There are only pay-per-request API's for sales tax data. The Sales Tax Clearing House offers an interface to pull sales tax information. With a list of zip codes, you could automate the process of pulling in the tax data. I'm guessing they probably wouldn't like that, so it would probably be a good idea just to send them an e-mail requesting a data dump.
This question has also been asked on StackOverflow.
"
"usa - Need clarification: if state or city gov releases data in non-open formats (i.e. book, microfiche), *must* they now also release it in CSV format?","
The Open Data Policy and Executive Order are for federal datasets and does not mandate the same for state or local governments, although such policies does influence more local policies.  
See http://www.data.gov/opendatasites for a large (but not comprehensive) listing of Open Data sites at the international, state and local levels.
For state-level open government or 'sunshine' laws, recommend http://www.rcfp.org/open-government-guide.
"
"usa - What is the significance of Census ACS columns with line numbers ending in "".5"" or "".7""?","
Before seeing this, I actually verified this answer just earlier today with Paul Overberg, one of the leading Census journalists around: the ""fractional line numbers"" represent ""headers"": that is, they are labels that group together subsequent rows but which don't have values themselves.
His example was the ""Median income in the past 12 months --"" line in this:  (from http://factfinder2.census.gov/bkmk/table/1.0/en/ACS/11_1YR/B07411)
"
geospatial - Where could I find open data about ATM locations (in Paris)?,"
OpenStreetMap's database has the ATM tag. Through the Overpass API, you can quickly access the Points of Interest: Map of ATMs in Paris (Overpass API). Click on the points to see additional metadata  (mainly the name of the operating bank).

Bonus: there is a good thread on XAPI call for all ATMS in OpenStreetMap's own Q&A plattform.
"
Who are non-schema.org data standards for?,"
Schema.org is an ontology (""data standard"") specifically for marking up HTML so that search engines can more easily extract structured data from otherwise unstructured data. Schema.org is very popular for marking up products (reusing the GoodRelations vocabulary), articles (reusing the rNews vocabulary), reviews, etc. If all you care about is search engines, then Schema.org is the only vocabulary you need to care about.
However, HTML is just one way to share information. As you mention, you can provide an API or bulk downloads. Popolo (I'm its editor) is currently targeting those channels. If you're curious about why standards matter in those contexts, I can provide a longer answer.
That said, even if you only use Schema.org in your HTML, you'll be offering partial support for Popolo, because Popolo reuses terms from Schema.org. It also uses terms from predecessors of Schema.org that Schema.org subsequently adopted. For example, Popolo's Person class has significant overlap to Schema.org's.
As for adding Popolo terms to Schema.org, that's definitely a possibility. For example, I think it would make a lot of sense to add the dissolutionDate property from Popolo to Schema.org's Organization class, so that companies can dissolve like in real life.
Last point: unless you plan to share data with other developers through HTML, as far as I know, there's not much to gain from using vocabularies besides Schema.org in your HTML. If you want to share data with other developers, you should offer bulk downloads or an API, instead of requiring people to parse your HTML (even if that HTML has beautiful semantic markup).
"
best practice - Examples of metadata for non-uniform collections,"
I don't know of a single all-encompasing standard.  You basically need to look at what attributes are common to all the collections you're tracking, even if they don't necessarily seem similar at first glance.
If you're dealing with data, then DataCite is generic enough to describe collections of data without getting into specifics for each scientific discipline.  (There are other more specific but still collection-level descriptions available; eg, SPASE for space physics data).
For physical objects, you may need to look into why you're interested in the items.  For a museum, they might track collections by who donated them, or where they came from (archaeological dig, etc.), while someone who's operating a store would track that as a supplier and might have information such as lead time needed for orders, what type of things they manufacture, etc.
You also run into strange issues of what exactly is a collection.  I deal with cataloging data for the most part, and there are issues with what the proper aggregation should be for cataloging.  See Laura Wynholds's 'Linking to Scientific Data: Identity Problems of Unruly and Poorly Bounded Digital Objects' for a bit of a background.  (and see Renear, Sacchi & Wickett's 'Definitions of Dataset in the Scientific and Technical Literature' to see we can't even agree on what the collective noun actually means)
To explain this with physical objects --  a car is a collection of parts; to the factory that needs to assemble the car and the dealership that's selling it, they might look at the car as one thing, or an aggregation of multiple components.  Even something as simple as a movie can be broken into multiple scenes, thousands of individual images plus audio tracks, etc.
"
"data request - How many software developers are there in the world, per country?","
The databases at the International Labour Organization (specifically ILOSTAT and LABORSTA) are tantalisingly close to what you're after. For example, go to LABORSTA and select Employment, then select Employment for detailed occupational groups by sex (SEGREGAT). This allows you to select a country and view a breakdown of detailed occupational groups which includes ""computing professionals"". Not quite ""software developers"" but perhaps a reasonable proxy if you're interested in comparing across countries. Note that the main LABORSTA statistics also break down by occupation but sadly the list of occupations lacks detail.
ILOSTAT is the whiz-bang successor to LABORSTA, but it seems the data is no more detailed than described above. Sadly, in neither case does there seem to be an option to download bulk data, rather you are compelled to view results on a per country basis.
"
real time - Programatically request recent close prices for a list of stock tickers,"
While the Google Finance API is officially no longer available,  it's still active in returning requests in XML.
// Dow Jones
http://www.google.com/ig/api?stock=.DJI

// NASDAQ
http://www.google.com/ig/api?stock=IXIC

You will need to consult a programming site like StackOverflow  if you have a programming related question.
"
data request - UK supermarket product nutrition,"
We're building an open crowdsourced database for uk food products.
We have calories, ingredients, photos.
You can use it and contribute to it as well (and let your user contribute) according to the OdBL licence :-)
https://uk.openfoodfacts.org and https://world.openfoodfacts.org
"
"geospatial - Examples of scraping from ""real-world"" data sources using OCR, etc?","
The best example I have heard of is Real-time traffic monitoring using mobile
phone data (PDF).
The idea is to derive road traffic velocity from the position data that the mobile phones within cars ""generate"" when moving from one base station to the next. The frequency of these base station handshakes approximates the travel velocity of the car. Practical application is the detection of congestion without the need for dedicated hardware.
"
data request - Where are the concentrations of digital companies in the UK?,"
The original article from the National Institute of Economic and Social Research (there's also a direct link to the PDF) has more detail on the geographic distribution of UK technology firms in the form of hotspot maps by Travel to Work Area. This is among a lot of other well presented information including breakdown of technology firms by sector, and growth rates of the ""digital economy"".
I suspect the articles you cite in your question did not look beyond the summary of key findings. Always look to the original published report as a primary source.
"
usa - Accessing CATO Deep Bills with Ruby,"
Since Deepbills is based on the official XML that Congress publishes, I think it'd be pretty easy to add support for passing through Deepbills' extra tags and attributes to the Ruby gem I made for working with Congress' bills, us-documents:
https://github.com/unitedstates/documents
It's a tool for stripping a lot of tags out of Congress' bill XML, and giving you semantic-less HTML that can be integrated and styled much more easily than the original XML.
Adding support for Deepbills might be as simple as adding new names to the whitelist of tags to preserve.
"
usa - How and where can I get data on US census block data by city and state?,"
Try NHGIS, it has built a resource that offers what you are looking for.
The race question is available at the block level from the 2010 Census Summary File 1 release; but the income question is available only through the American Community Survey (ACS) Summary File release which goes down to the block group level. If you want the information together, I would recommend using the ACS as a source.
Otherwise, if you have ArcGIS 10.1, you can just download this. It may work in previous versions, but it's a roll of the dice.
Here is what's available in the pre-joined Census data:
Condensed Codebook
Full Explanation of Each Variable
Update:
There are new resources for:
2012(Data)
and
2013(Data, Variables)
"
data request - Metropolitan Railway Datasets,"
The authoritative source for Chicago transit data is http://www.transitchicago.com/data/.
Steven Vance did quite a bit of preparation and organization around Chicago transit data and posted it on his blog in 2010. I don't see any note that indicates the data has been updated since then, but it's still probably useful as an overview to what data there is and what you might need to do with it.
"
usa - How to flag incorrect links to data on data.gov?,"
The datasets that you might be looking for are all school districts and/or the School Universe Survey. To provide feedback on any of the datasets on Data.gov, you can contact the dataset owner listed on the page (contact email, which in this case is jane.clark@ed.gov) or contact Data.gov at the contact link at the bottom of every Data.gov page.
The URL you are linking to is for a new concept site on Data.gov (Next.Data.gov) for which we are eliciting feedback on a set of new approaches and user interface. This site is not fully operational, but in this case the link is clearly incorrect and I'll get it fixed right away. The contact listed on this page is Marina Martin at    marina@marinamartin.com.
"
usa - Is there a tool to match zip codes to cities?,"
There is no simple answer to this question, because ZIP codes do not represent geographical areas. They represent postal delivery routes, which are sometimes simply a bank of PO boxes in a specific post office, and are sometimes an organization like a University which has its own internal mail processing services.
Therefore, not all ZIP Codes can truly be located in a city, and even those for which a geography can be reasonably defined, they are not necessarily located within a single city.
Probably the closest approximation you could get would be to start with the Census Bureau's ZIP code tabulation area (ZCTA) gazetteer file, which can be downloaded here. Then, using as many of the state-based ""place"" shape files as you need, load those into a GIS tool. Personally, I'd use PostGIS, but you might also be able to use QGIS, or if you can get your hands on a copy, ArcGIS (commercial software).
You could then:

look up the ZCTA for a given ZIP code 
extract the INTPTLAT and INTPTLONG values, which are the lat/long for the ""centroid"" of the
ZCTA
use the GIS tool to identify the place geography which contains
the ZCTA's centroid

You will probably have some ZIP codes which aren't in the ZCTA dataset, and it's not really precise, but it's probably good enough for a lot of cases.
"
media - Raw Data Feed for TV Listings,"
Digital TV channels should have an Event Information Table encoded in them, but the time window is quite limited (generally about 12 hrs).
Tribute Media used to offer free TV listings to the public, but when the popularity of MythTV and other PVRs took off, they couldn't support the amount of traffic that they were getting hit by.
As Tribue wasn't interested in dealing with lots of individuals, some folks banded together to form SchedulesDirect.  They distribute the Tribune / Zap2it data to individuals, at a moderate fee (currently $25/yr), but there are restrictions on redistributing the data.
"
"Is ""open"" data free as in speech, free as in beer, both, or something else?","
The Wikipedia page for Open Data states it better than I can. That is, open data should be:

... freely available to everyone to use and republish as they wish,
  without restrictions from copyright, patents or other mechanisms of
  control.

To answer your question, truly ""open"" data should certainly be free as in speech. I would say, however, that open data need not necessarily be free as in beer. In your example, even though a fee might need to be paid to access the data, as long as the end-user is able to use the data as they wish (including for commercial purposes) then I would agree this still could be considered ""open data"". In practise, data that is made available for public use (by government, say) is generally also free as in beer, but I don't see this as a requisite.
The idea of open data is, in my view, synonymous with the notion of open source software. Just as open source software can be sold if desired by any party, I have no problem with open data being used in commercial ways. If a business is able to add value to open data (or software) then they should be able to charge for their product. The original source will always be available to anyone wishing to use it.
Of course, the above view is utopian in some ways. I suspect a lot of ""open"" data these days is in reality controlled in some way via partially restrictive licensing, copyright, and so on.
"
data request - Weed out inaccurate information from 2 million records,"
You might want to take a look at https://github.com/open-city/dedupe. I haven't used it, but looks like it should solve your problem.
"
time series - Where can I find data sets for machine monitoring?,"
With some clever thinking you can find anything.
I searched 'vibration data filetype:csv'.
In the results was some very complete data set: see here for example
"
data request - U.S. Supreme Court record items,"
To my knowledge, there is no complete online archive of historic US Supreme Court documents. 
There are links to recent indices of online material from http://www.supremecourt.gov/default.aspx, and the court actually redirects seekers of online merits briefs to the American Bar Association's ""Preview"" publication A direct index of cases is available for 2012-13 and 2013-14; for earlier material, PDFs of Preview can be downloaded, but only back to the 2007-08 session.
The FindLaw site has online briefs dating to 1999.
I don't have access to WestLaw, but the SCOTUS site suggests that it may have historic briefs. If so, I would not assume that they have them as far back as 1942.
Otherwise, I believe you'd have to visit a law library or use a commercial document retrieval service, which would send a person to the court to make copies of the relevant documents for you. This page lists libraries which are repositories for printed briefs as well as listing services which can retrieve documents for you (for a fee).
"
data request - A hierarchy of all sellable products and services,"
[Update: As of Jan 25 2015, the page linked in this answer no longer provides the download page or the POD site. A search on the website shows something related to accessing this POD but after quite a bit of search, it doesn't appear that any of this data is actually available.]
The most promising open data source at least for sellable products seems the Product Open Database (POD), which tries to publish a very comprehensive product database, indexed by the barcode number.
The POD Download page contains the database itself and a DB scheme overview. Attributes include the GTIN (the barcode number), product name, brand name, country, size and weight of package. They even have thumbnails of product pictures and brand logos.
The terms of use... erm. Good news: These Terms of Use grant you a worldwide, royalty-free, non-exclusive licence to use the Datasets and pictures subject to the conditions below. Please read them carefully. Bad news: The license is not determined yet. But the explicit use of an ""Open Data"" badge gives hope...
"
"data request - Solid Waste Production, Globally? Spatially Resolved?","
I haven't found anything quite like what you want, but you might be able to build it from a bunch of separate datasets.
Many datasets have solid waste production/consumption on the level of buildings.

https://data.weatherfordtx.gov/Government/Solid-Waste-Operating-Statistics/mvy8-6q2t?
https://data.snostat.org/Government/Solid-Waste-Transfer-Station-Data-2012-/xn5u-y9xd?

Here's one with aggregates for the US.

https://data.oregon.gov/Environment/Materials-Discarded-in-the-U-S-Municipal-Waste-Str/3g88-w2ag?

I found all of these by searching on OpenPrism. You'll probably find more if you look through more of that search's results.
"
nonprofit - Open Data for having transparency of expenditures in running of an Orphanage?,"
It's an interesting question. Salaries are the large expenditure. Will the staff be upset if their salaries are public?
I think transparency in a non-profit is essential, especially for charities. If the staff don't want their salaries public, perhaps share the average or median salary as part of an expenditure breakdown.
If you are interested to increase the number of donors, consider fund-raising for specific projects by using crowd-sourcing. One example is http://www.indiegogo.com, where the donations can be matched to specific projects. For donation level you can also have small gifts of appreciation. You can link each amount to specific costs in the organization (i.e. $100 buys one month of ABC for XYZ). This setup allows for giving in levels ($10,$100,$1000+) that appeals to a broad audience (but mostly local).
"
tool request - Crawling user data from different applications,"
I'm going to assume that, given someone's name, you then want to scrape data from sites that they are posting on or otherwise have information on (i.e. Twitter, Facebook, etc.). With that being assumed...
I have been doing this type of thing for quite some time, and have not found any ""out of the box"" scrapers that will do this. I've been writing my own using a combination of Ruby and Python, which ultimately may be the way you have to go.
Note: be sure to check the TOS of each site, especially social sites. Some don't like their data going outside of their private walls.
A Google search will come up with a few tools that require you to learn XPATH or Regex. Both are relatively simple, once you get the hang of it (which took me some time).
If you clarify what type of information you're looking for I'll be able to give you a better answer.
"
government - What cities provide open data on rental building bylaw infractions?,"
The City of Chicago publishes building violations: https://data.cityofchicago.org/Buildings/Building-Violations/22u3-xenr
"
data request - Find reusable images of city X with width>2100 pixels,"
If you don't need programmatic access (i.e. an API), you can use the Google Advanced Image Search. 
You can also get there by following these simple steps:

Go to search.creativecommons.org
Enter your search term (e.g. berlin)
Select your license requirements (use for commercial purposes and/or modify, adapt, or build upon)
Click on Google Images
On Google Images, click on the Size filter, select Larger than… and then 4 MP (2272×1704).

That should do the trick.
Update: If you want to share the URL of the image search results, please have a look at Google image search URL that can be shared?

"
api - Individual bicycle data within a bike hire scheme,"
Capital Bikeshare has this. CitiBike seems to as well.
Also, CityBikes looks really cool.
"
best practice - Source of open trend data,"
Here's the best open data source for trend data I could think of: Wikipedia page view statistics. Derived datasets:

Wikitrends, a daily/weekly/monthly updated list of 10 most popular (as in page views) articles. Bonus: not only absolute, but also relative change top 10 lists (called uptrends/downtrends) are offered.
Wikpedia:5000, a weekly updated list of the 5000 most accessed articles.
Trending articles on Wikipedia finally seems pretty close to what you seek: popular articles in multiple language wikipedias during last 1, 3, 6, 12 or 24 hours. In my region of interest it shows the typical pattern of expected and surprising keywords.

"
usa - Voter Registration data in one place?,"
It's unclear what you mean by ""voter registration data"".  
If it's demographic statistical data, the Census Bureau has reports from 2012 and earlier, and there are more reports at the Election Assistance Commission.  These reports cover the whole US.
if, instead, you are looking for actual voter lists (names, addresses), then it's unlikely to ever be available at a national level (for some states, the sale of the lists generates revenue). 
However, some recent data (only) for a few states has been put online by Tom Alciere:

Connecticut
Delaware
Colorado
Florida 
Ohio
Oklahoma
Utah

These are ugly sites, presumably just automatically produced to generate advertising traffic. But for about half the states, the full dataset has been made available to download without restrictions, and the others are easy to crawl.
"
Are there public transport data for Germany freely available?,"
Again, OpenStreetMap to the rescue: it has a whole tag scheme related to public transport services. Pointers:

ÖPNVKarte (German, real domain [öpnvkarte.de] contains an umlaut, openbusmap.org is just a proxy domain) has a nice rendered world map, showing airport, train stations, rails, buses, subways, trams, ... worldwide (with varying degrees of coverage, of course). The corresponding page on OSM Wiki gives background information.
More general, the article Public transport on OSM Wiki lists all important tags for extracting relevant data from a data dump.

"
data request - I need a KML file for Northern Ireland BT postcodes,"
Tabular data about post code data is at MySociety.org:

ONS Postcode Directory (ONSPD), February 2012 edition (thanks dvdoug!). This include full UK (including Northern Ireland) postcode locations.

Those files don't include the headers, which is annoying. This page has a version from a different vintage which had a header added. If it's 47 columns, hopefully they match, but it's taking an age for me to download.
It looks like Northern Ireland has some more complicated licensing rules for commercial use, which may have contributed to that data being harder to find when the Wikipedia pages you're looking at were originally created.
It looks like someone has approximated shapes for the BT postcodes which you can see on this Open Street Map page. Getting those in a useable format is pretty obtuse, as best I can tell.
To conclude, I'm sorry that the original text of this answer was misleading: the source I identified doesn't actually have what you need. You may have to contact NISRA; maybe more requests from the public will chip away at their data publishing process.
"
data request - Postal codes and city districts worldwide for download,"
GeoNames does indeed provide the relationship between parts of cities and their postal codes. A few examples:

Parts of London with the postal code EC1
Postal codes of districts in Austria called Innere Stadt (Inner City)

You can either use GeoNames' Postal Codes Lookup Tool, or you can download the database dumps in text format for many countries.
The data is licensed under Creative Commons Attribution 3.0.
"
"usa - What is the difference between US Census definitions of ""Urbanized Areas"" and ""Urban Clusters""?","
Look at the National Center for Education Statistics Urban-Centric Locale Codes developed by U.S. Census https://nces.ed.gov/ccd/rural_locales.asp. You might be able to modify that approach to accomplish what you want. They use principal cities to delineate between ""urban"" and ""suburban""

City - Inside an urbanized area and inside a principal city  
Suburb - Inside an urbanized area and outside a principal city  
Town - Inside an urbanized cluster and outside an urbanized area  
Rural - Outside of an urbanized cluster and outside of an urbanized area


"
"tool request - Twitter crawlers for tweets, retweets and social network","
Have you looked at: http://datasift.com/ it is where I would start. 
http://gnip.com/ would be another good option.
"
doi - Persistent publishing of data. My nations DataCite does not cover my field; alternatives?,"
For either users or producers of open data, the Register for Research data Repositories (RE3) can be a useful resource.  In their own words:

The goal of re3data.org is to create a global registry of research data repositories. The registry will cover research data repositories from different academic disciplines. re3data.org will present repositories for the permanent storage and access of data sets to researchers, funding bodies, publishers and scholarly institutions. In the course of this mission re3data.org aims to promote a culture of sharing, increased access and better visibility of research data.

Among other information, the registry specifies what repositories provide the ability to identify particular datasets through DOI, URN, or otherwise.  It allows to search by discpline.  For example, searching exclusively for open access atmospheric science repositories gets 37 results.  Limiting it to those that have persistent identifiers reduces the number of results to 7, such as the World Data Center for Climate.
The registry aims at both producers and users of open data.
"
weather - Systematic bias in NCDC GSOD climate dataset?,"
Without all of the original data, and its metadata, any answer here can only offer a guide as to how to start answering your questions.
Your first question is: ""where is the problem?""
Your second question is: ""is GSOD biased?""
Both of these must start with further statistical analysis.
And you need to analyse the metadata for the datasets you are comparing.  Go through the definitions side by side methodically, and look for overlaps and for differences.
Compare your available data at its most temporally disaggregate: more detailed than what you've done so far - monthly means can hide so much of relevance. If possible, try to recreate the monthly means yourself from the individual readings: that can often highlight issues that cause this sort of discrepancy: e.g. the way that missing data or outliers are handled.
It would also be very helpful to set out your prior.  And it would be helpful to analyse what's happening in the other months too.
As for using linear regression as an analytic tool in this case, do remember that it's a really blunt, unsophisticated tool; it will more often mislead than give useful information. Remember, courtesy of Andrew Gelman, the criteria for its applicability:


Validity. Most importantly, the data you are analyzing should map to the research question you are trying to answer. This sounds obvious but is often overlooked or ignored because it can be inconvenient. . . .
Additivity and linearity. The most important mathematical assumption of the regression model is that its deterministic component is a linear function of the separate predictors . . .
Independence of errors. . . .
Equal variance of errors. . . .
Normality of errors. . .


"
data request - Recent high spatial resolution images of the Tasman Sea,"
WMO OSCAR has a (complete?¹) list of current space-borne high resolution optical imagers, defined as instruments with a ""spatial resolution in the range of less than 1 m to a few 10 m."".  Scroll down to ""Current instruments"".  I'm not an expert in high-resolution optical imagers, but I suspect the problems are as follows:

The higher the resolution, the smaller the field of view.  At high resolutions, it either takes a very long time to cover the entire globe, or data are only acquired on pre-order and for-pay.  Also, it's not free.  Some examples:

GeoEye has a resolution of 0.41 metre, and covers the globe in 6 months.  Otherwise, data are available for pre-order for a specific location, but the field of view is too small so it likely won't help you.  Competing WorldView has similar properties.
AWFS has global coverage in 5 days, but the resolution of 56 metre is insufficient.
CMT has a resolution of 4 metre, and global coverage in 104 days.
ETM+ on LandSat has a resolution of 15 metre, and global coverage in 16 days.
HiRi on Pleiades has a resolution of 0.7 metre, and global coverage in 26 days.  I don't know if they actually acquire information accordingly, or if this is just a theoretical limitation: it would be an awful lot of data.

If you browse through the list I linked above, you will find many more examples.
Data may be difficult or impossible to obtain, and most data are not free and open.  In fact, many high-resolution optical imagers are carried on spy satellites.  Good luck.

I don't know practically speaking how to get the data.  Many instruments are commercially operated.  You might be able to get more information by contacting companies such as GeoEye, RapidEye, or DigitalGlobe.  Perhaps your best bet may be to see if any agency can get recent Pleiades data.  If WMO OSCAR is correct, it might just be good enough: 0.7 metre and 26 days appears to be the best thing out there.  Again, data are not free and open, but search and rescue organisations in most countries should have access to the necessary funds.

¹It seems not to include spy satellites, but those might not have open data available...
"
data request - Where can I find the training logs of (as many as possible) athletes?,"
Try these:

Longitudinal study of the effect of high intensity weight training on aerobic capacity
Effect of endurance training on lung function: a longitudinal study.

"
"geospatial - Where can I find a longitudinal survey that includes sociographic data, including religion affiliation, over a long period?","
Here are some additional studies that may be useful:
Most likely meets most or all of your criteria:

Americans' Changing Lives: Waves I, II, III, and IV, 1986, 1989, 1994, and 2002 (ICPSR 04690)

May meet some of your criteria:

You mentioned ARDA studies already, but the National Study of Youth and Religion (3 separate waves) may be helpful.

"
data request - Chicago Traffic-Related Fatalities,"
This data used to be publicly available through the Illinois Department of Transportation's Safety Data Mart. However, the Department's new Safety Portal is now only accessible to other government agencies. 
For Chicago data, you can get historical data from the Chicago Crash Browser
"
"data request - Statistics of US/Europe Businesses, Advertising Agencies Specifically","
Gut feel, you will have to dig this data up country by country.
This list should be fairly up to date for the UK.
http://www.ipa.co.uk/framework/sections/agency/agencies.aspx?display=list&menu=open (something like import.io or scraperwiki will help you turn the page into data - I don't think they have an API)
I found this http://eaca.adforum.com/search/agency/idCountry/783 which has a variety of search options for agencies. Again, it is a site not a db. So you would have to use a scraper of some variety. There is still no guarantee of accuracy or completeness.
"
data request - Are there any open datasets for soccer statistics?,"
Recently, the paper Linked Soccer Data was published. In it the authors describe how they combined various football-related datasets, such as http://fussballdaten.de/. Some of the data they covered can be viewed through this demo application.
The paper also mentions other relevant sources of football data, including the openfooty API.
"
real time - Realtime Data - Why and Who?,"
Learning:
This is huge and can be applied to anything.
Imagine trying to learn how to play poker.  You can only play one hand every hour.  How long will it take you to learn how to play poker?

""Not too out of date for my use case""

If the company is smart and whose goal is to make a profit, they will change their ""use case"" or even completely revamp the point or meaning of the use case so they can derive insights like what their competitors are doing, and much more outside the scope of this response to, for lack of a better word, win.
With a realtime source feed this doesn't even cost much money.  I actually am 100% broke at the moment ($0) but I could program a few things on my computer or even (lol) go to the public library, rent out an amazon web server for 25 cents an hour that I remote desktop into, and start gathering insights with the correct programming and know-how.  Then, with correctly applied statistics, start deriving insights.
This concept mostly involves repeatedly testing different products or permutations of ideas (which is essentially what everyone does), but in the accelerating marketplace of the online world (and to the retail world as well since they are beginning to blend), competition is not going to go away.  
Whomever learns the quickest wins.  If you have direct competitors, whomever has the economic advantage will win, even if it's an accumulation of several different economic advantages. 
You could say they got that economic advantage in a different way because maybe they:

Already know more about the market
Initially had a longer time frame to learn more
Had better connections/got luckier/hired the smartest people
Are able to learn much faster than their competitors

The internet changes VERY rapidly.  So would you agree with me that numbers 1, 2, and even 3 are becoming more irrelevant nowadays?  I didn't list money/resources because, usually, all they let you do is learn more quickly in some form or fashion.  Sometimes there are barriers to entry (example: oh no, we can only do 5,000 API requests per day and can't pay more for anymore?).  Learn more and you'll figure out a way around anything.
Colleges don't even teach the skills that are in most demand today! - unless its a programming language or something.  And if they do, then the teachers most likely don't know much more than the students!  Think about ""social media.""
Who teaches that?
How many people are looking for experts in it?
Now look at my poker learning analogy.  
If you think money is the biggest problem, then you're not learning quickly enough how to potentially point out trends that could persuade someone to invest, or you're not able to see all the little miniature data points that reveal the hidden picture - the one that could be lowering costs by a factor of 4.  That is, assuming you're able to analyze them.
You'd also not see how the real-time feed allows one to quickly test ideas and throw them out or keep them.  What if you wanted to test permutations of ideas?  Assuming sample size is large enough, you'd only be able to test one permutation (of which you'd probably have lets say 3x3x3=27 of) every day or whatever interval.  How would you sell that to inner management?  Think they'd be more easily persuaded if you could get the answer within an hour and do it cheaply? 
What about factors that may change during your 27 days of permutation testing that would affect the outcome like market factor forces, seasonality, etc.
It could be argued that getting the data over a longer time frame is better because you'd hope all uncontrollable factors would be included in the analysis as an average; but what if you wanted to control one of those factors and, say, just test at 4pm-5pm with all the permutations?  Or any other innumerable cases I could think of.  
The factor of time here is most important in my opinion to be honest.  Why?  You may sacrifice a bit of accuracy by not doing the test for a lengthy period, but that sacrifice will lead you to conduct more experiments with better accuracy...ones that you could immediately implement.
Now you could also argue your sample size isn't large enough.
Most people assume you need a massive sample size.  Something to consider though is the validity of your analysis increases with sample size, BUT it increases at a quickly decreasing rate.  Something to consider - and apply with realtime data.
I love massive sample sizes too, but how do you think drugs get approved in clinical trials or medical research studies are done with 20 people? They can't test them out on thousands and thousands of people.  They use statistical methods.  
Example:  They look at all results of EVERYTHING, even things you may think are completely unrelated they record.  Blood pressure, heart rate, hair growth, skin color change, rashes, change in color preference perceived mood, sleep time, computer time, perceived interest, the combination of each factor and the combination of each factor on each other combination of factors comparing that to known full population averages are...factoring in time..etc..The data in small sample sizes is there (ie. real-time data would be considered a small sample size if you're using little snapshots of it), it's about setting up for it and digging deep.
Many, many other statistical methods are available like fractional factorial, non-parametric, design of experiments, deriving simulated numbers from your numbers based on standard deviation and probability and seeing how they fit into your model, etc. methods as well.  
Ok enough belaboring on my part.  Here are a few examples too:

IP Addresses & What Their Attributes are at that Time

GEO location to IP services such as Maxmind constantly update their database because IP addresses change so frequently.  For example, for a day or even 3 hours you could have your IP be used as a Tor exit node meaning alot of websites will block you, and do it quickly, or list you as using a ""proxy."" This is based upon data from GeoIP databases, which somehow will pick up on you being a tor exit node and will change the attributes of your IP quickly and accordingly to that information.
An attempt to geolocate a potential customer based on the results of a GEO-IP database and thus discovering potential facts about them using a big enough sample size that you regress into a big database of known facts about that area (like avg level of income) using census data.  =x   This involves the central limit theorum.
Thwarting fraud and identity theft.  If your IP is listed as a ""proxy"" in GEO-IP databases, the merchant will likely pick up on that (by doing behind the scenes requests to the their GEO-IP database provider) and reject your attempt to purchase anything.  Essentially they are assuming your attempt to hide your identity must mean you're also hiding other things.  Plus, statistics from transactions reveals this to overwhelmingly be true anyway.  

Imagine you are a thief and stole someone's CC number and are trying to buy stuff online.  Are you going to use your home IP address?  Probably not.

Online Media Buying
I can actually add more to this since I buy media online.  Albeit I admit that much of that does involve bidding and you admit the realtime factor being important for.
However, there's MUCH more to it than that.  One thing that REALLY bothers me when I'm buying media is the lack of real time RESULTS being published as well.  Say I want to raise my bid price really high and see what happens - I'm testing the market.  Most systems will only charge you when you do this as if you were bidding slightly higher than the next person, but sometimes they won't, or in my experience they actually just say they do and charge you some strange inflated ratio.
In our example lets say I get my results every 30 minutes.  During that 30 minutes my bid price was $10 dollars per click.  A very high bid on average.  I need to know the results of this test to judge all sorts of things about the market (and by market it could mean a specific website, or a specific keyword, anything really) like competition, size, potential of market, conversion rate, etc.
I get my results back 30 minutes later and, oh crap, I just spent $10,000 because I was blind during the test.  Had I been able to see it realtime, I could of just done it for 30 seconds and known a good outcome.
You could argue that I should have gradually raised my bid.  If so, I point you back to the poker analogy up top and remind you of all the permutations I have to test as well.

This also gets even more complex when you're working with sales reps at a digital marketing firm and the client or your boss wants answers quickly.  This means you have to conduct a lot of tests on a very, very wide permutation of variables in order to be able to give them the information they seek or just to be able to found out what the profitable combination is.  
I could tell a client or anyone else alot in about 10 minutes with realtime data, using statistical analysis (a skill very few in my field have unfortunately), and trying different permutations...mostly involving macro changes (macro changes let you cover more ground at once - it's essentially a compensation for the inability to do everything real-time).  Without that, it could take a week.  Time is money.
Being able to make a move every 30 minutes or every morning won't work well when you have so many permutations to test (bid, landing page, source, offer, etc. etc. etc.).
As another example, a company I worked for had a tech team that could only change the landing page every 2 weeks - whereas it would take me 30 seconds to make the change I wanted...but I wasn't allowed to do it.  In fact, I wasn't even allowed to talk directly to the person making the change.  It had to go through 3-4 different and usually unnecessary people.
Imagine being able to make a move every 2 weeks and during those two weeks having to run the campaign and deal with the pressure and constant reminder that the campaign was not profitable.  Back to the poker learning analogy.
"
releasing data - What's the best way to host map tiles?,"
Take a look at what OpenStreetMap does. There's a page describing the nature of tile server disk usage. If you go up to zoom level 18 worldwide, you're talking about 91,625,968,981 tiles, which would take around 54000GB of disk space, but would mostly never be viewed.
So I'm not sure if it would ever be a sensible approach, but having said that, I heard that MapBox do pre-generate all their tiles when hosting a tile set. I think they go up to higher zoom levels just in the cities or something like this.
The approach OpenStreetMap tile servers use, is a combination of on-the-fly rendering and caching. The management of this is done with a specially written apache module called mod_tile
Either way, if you want to do things worldwide up to a high zoom level, you need something a little more complicated than a filesystem full of 256x256px PNG images. mod_tile stores files in cache as a 'meta-tiles'. MapBox uses a format called MBTiles to store all the tiles in a database file.
I mentioned MapBox a few times. If you pay them they'll render & host tiles for you. There's various other providers of tile hosting/rendering and other map services
In general you'll find the OpenStreetMap tech community have a lot of experience with this kind of thing. You can contact them in various ways. There's even a question & answer site: https://help.openstreetmap.org
"
"government - Besides Vancouver and Chicago, what cities have open data on rental buildings bylaw infractions?","
A growing number of city open data sets is federated to http://Cities.Data.gov.  There are quite a few rental building violation datasets accessible: http://www.data.gov/cities/Community/Cities/Datasets  You can find some additional information at http://Counties.Data.gov and http://States.Data.gov.  These include non-Socrata based local governments as well.
Specifically, you might look at:

Seattle's code violations
New York's building code violations

(Disclaimer: I am the Evangelist for Data.gov)
"
How can I get the Wikidata inter-language links?,"
The official Wikidata dumps are still in a very early stage.
At the moment, you might find these processed files helpful, especially the one that ends with links.ttl.gz (currently http://semanticweb.org/RDF/Wikidata/turtle-20130801-links.ttl.gz), which is a Turtle file that provides the inter-language links extracted from the Wikidata dump.
The export scripts are available on GitHub, and you might also find the corresponding announcement on the mailing list interesting.
Please also have a look at the related question How can I download the complete Wikidata database?
"
"The Healthplan finder API stopped responding, where can I get support or more information?","
First, verify that you've tested your script on a different system. Then I would proceed to:

Calling HHS at 800-318-2596
Trying a live chat
Leaving feedback on this dataset at the Developers' Center
Sending a tweet to @healthcaregov
Leaving a post/send a message on their Facebook page

"
Open Data Standard for Stack Exchange?,"
I don't know much about open data standards, but as far as I'm aware, no. Stack Exchange itself isn't based on any particular standard.
We provide all publicly-accessible data via an XML-based [data dump, but I believe its schema is simply mimicking the database schema we also expose via the Data Explorer.
"
licensing - Is data scraped from Govt agencies and in public domain considered open data?,"
In order to license the data you have to hold a copyright, which it sounds like you do not.
If you believe the data is in the public domain, then you can consider Public Domain Mark 
"
usa - GitHub license for code written by US Government Employee,"
As a work of the US government, there isn't any license appropriate for the work, because it's already in the public domain (in the United States). So a license like the Unlicense (or CC0), in which the licensor is entering the covered work into the public domain, doesn't work. Some text that acknowledges the public domain status in the US is helpful (and desirable), but it's not a ""license"".
The above situation only applies to the US, unfortunately -- government works are potentially copyrightable in non-US contexts. So, some formal text or license that enters the work into the worldwide public domain is appropriate (and desirable).
My favorite example of this is what HHS has done on their ckanext-datajson extension:

As a work of the United States Government, this package is in the public domain within the United States. Additionally, we waive copyright and related rights in the work worldwide through the CC0 1.0 Universal public domain dedication (which can be found at http://creativecommons.org/publicdomain/zero/1.0/).

"
usa - Survey of illness and symptom data,"
you ought to start reading http://asdfree.com and play around with http://www.google.com/trends/  :)
the easiest county-level data set to work with is the area health resource file.  this won't have individual cases, but it'll have many small-area counts of different things health-related over many years.
the data set probably with most of the characteristics you've described is the surveillance epidemiology and end results cancer incidence data.  it's got data over many years, but only for a handful of (nationally-representative) states
of the other major health surveys - brfss, meps, nhanes, and nhis - only meps has event-level/icd-9 data, only brfss has state- or sub-state-level data, and only nhanes has biological information.
"
extracting - What is the best way to get airline schedule data from pdf files,"
If the pdfs are very similar each time you check them, then the best path may be to write a custom pdf scraper for each low cost carrier. Check out this tutorial, http://blog.scraperwiki.com/2012/06/25/pdf-table-extraction-of-a-table/.
You should also keep your eye on https://github.com/jazzido/tabula, It's not quite there yet, but may be a solution soon.
"
real time - Obtaining ADSB Mode-S Data Feeds for Aircraft from the FAA,"
The FAA does not seem to provide a stream of ADB-S data, but does provide a stream of about airplane location and disposition called Aircraft Situation Display to Industry. Getting access to this stream is not completely straightforward, but you can start with the FAA's page on the program.
"
data.gov - CKAN API questions,"
You've asked three separate questions in one question, I don't think this fits into Stack Exchange's one question, one answer model, should have been three separate posts maybe. But anyway..

It appears that every API response includes a JSON piece with help information. can this be excluded?

No :) Not as far as I know. Just ignore it. I agree it seems a little unnecessary.

The response does not appear to include information on the total number of hits. how can API clients get information on this total number of hits of searches?

The package_search link you posted does actually contain the number of hits, that's the ""count"": 8511 bit.

Does the CKAN API support spatial selection criteria as seems to be implemented at data.gov? it appears the catalog.data.gov UI does do some form of spatial filtering (within/overlap is unclear), but the API returns an error when using the ext_bbox parameter: ""Search Query is invalid: ""Invalid search parameters: ['ext_bbox']""""

The CKAN version that catalog.data.gov is using does not support ext_bbox parameters on GET requests, but later CKAN versions do:
http://demo.ckan.org/api/3/action/package_search?q=test&ext_bbox=-180,-90,180,90
On catalog.data.gov you need to send a POST request as in the following example:
curl -X POST -d '{""q"":""environmental"", ""extras"": {""ext_bbox"": ""-130,30,-50,40""}}' http://catalog.data.gov/api/3/action/package_search

"
business - Job satisfaction data,"
The Health and Retirement Study (HRS) is a longitudinal panel survey which includes job satisfaction questions in one of its modules. It primarily focuses on tracking US individuals 51 years and over.
The GSS also has a few questions on job satisfaction.
"
weather - API for sun radiation / illuminance data?,"
As no one's given anything else so far, so might as well make this an official answer.
I'm not aware of any free APIs specifically, but there are various data sources that might be able to give you what you need:
The US Department of Energy's National Renewable Energy Lab offers maps of solar radiation in the US, both annual and monthly averages, although the data is only through 2009:

https://www.nrel.gov/gis/solar.html

NOAA offers data from 1991 to 2010 from 1500 ground stations in the US, with nominally hourly cadence:

https://www.ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/solar-radiation

OpenEI (Energy Information) maintains a registry that lists 43 'irradiance' datasets and 26 maps (some are the NREL ones I already mentioned):

https://data.openei.org/search?ra%5B%5D=Solar+Power&dt%5B%5D=data&sort=relevance&size=25&q=irradiance

SolarGIS might have it, as they sell products that look to have more granular information, but their only free (attribution required) offerings are annual averages of two types of 'irradiation'. (I'm not sure how that differs from 'irradiance'):

https://solargis.com/maps-and-gis-data/overview

And the company that specifically markets an API (not free) for this data is Vaisala.  If those other resources aren't sufficient, you might have to look at their pricing:

https://energy.vaisala.com/en/support/solar-prospecting-tools/how-accurate-are-solar-prospecting-tools/

PS: What is different between solar irradiance and solar radiation?.
"
government - What are your responses when people tell you they can't open data?,"
""Oops! I'm sorry, I can't be your customer.""
-- by which I simply mean this: they can't, and so I can't. There are plenty of other people who are interested in having an open and honest relationship with their customers; companies who don't, aren't interesting to me, and I'm frankly unwilling to waste my time with people that I can't trust.
"
documentation - What do the permit types in Chicago's Building Permit Data mean?,"
Those are pretty common terms for building permits.
""New construction"" means that they're building (or placing) a new building.
""renovation/alteration"" means that there's an existing building that's being modified.
What qualifies as a renovation can get a little interesting by area ... I remember one of my co-workers telling me that in Virginia if you leave one wall standing, you can declare it to be a renovation, and avoid having to pay all of the fees required for new construction if you're looking to tear down an existing building.
See the city's website for details on the other permit types.
"
transportation - Is there Open Data on car engine efficiency?,"
Thanks for your question. I'm not an expert in this space, but I asked around and was pointed to two resources at NREL you might look at:

The efficiency curves available in the FASTSim model that are posted at http://www.nrel.gov/fastsim. I was told by the managers of this dataset that it may be used for commercial purposes, just not redistributed commercially.
The Caltrans cleansed dataset will be posted at http://www.nrel.gov/tsdc in the next couple of weeks, which will includes a subset of vehicles where OBD data was recorded along with GPS speed profiles.

"
data request - Where to get IMDb datasets,"
Not sure if this would classify as a comment or an answer, but it's useful information nonethelss:
So in reading this question I HAVE to point this out - ever heard of the paper?: 

Arvind Narayanan and Vitaly Shmatikov. ""Robust De-anonymization of Large Datasets (How to Break Anonymity of the Netﬂix Prize Dataset)"". 
  The University of Texas at Austin February 5, 2008.

Full text is at:
http://arxiv.org/pdf/cs/0610105v2.pdf 
It's quite a famous paper and was even on the news when it got published.
Here's the abstract: 

We present a new class of statistical de-anonymization attacks against
  high-dimensional micro-data, such as individual preferences,
  recommendations, transaction records and so on. Our techniques are
  robust to perturbation in the data and tolerate some mistakes in the
  adversary’s background knowledge. We apply our de-anonymization
  methodology to the Netﬂix Prize dataset, which contains anonymous
  movie ratings of 500,000 subscribers of Netﬂix, the world’s largest
  online movie rental service. We demonstrate that an adversary who
  knows only a little bit about an individual subscriber can easily
  identify this subscriber’s record in the dataset. Using the >>>Internet
  Movie Database<<< as the source of background knowledge, we successfully
  identiﬁed the Netﬂix records of known users, uncovering their apparent
  political preferences and other potentially sensitive information.

I looked at their citations for clues but they only thing they cite verbatim is:

IMDb. The Internet Movie Database. http://www.imdb.com/, 2007.

This is also quite a while ago too.  However going through the full text of that article, you may be able to glean some clues as to how they got their data and replicate those - so this could potentially help you.
"
data request - Shaded KML file for US Zip Codes? Doesn't have to be perfect,"
The shading of shapes on a GIS map is a design choice; it's not a characteristic of the data. Not all choropleth maps are opaque, even if some that you've seen are. A designer made that choice.
In Google Earth, you can apply styles to a layer using the ""Edit > Get Info"" command. On that, choose the ""Style, Color"" tab and change the color and opacity in the ""Area"" section. Getting labels like on http://www.usnaviguide.com/zip.htm is not something I know how to do in Google Earth.
Note that I find that file you pointed to at filosophy.org to be ""very laggy."" I think that happens when you have over 50,000 features (shapes).
(edited in response to comment thread; comments may now seem unrelated)
"
Content-driven API or RSS feed,"
Maybe you can find here something; but what would be a business model of an API ""news for free"" provider?
"
social process - How should I respond if a government official says he won't release data because no one cares about the data?,"
Boringness of data sets is not a legal reason to withhold data.
The only thing that matters is: 1) does your state/country have a public records disclosure law? and 2) What exemptions does it have for not disclosing? i.e. most states exempt agencies from disclosing records about undercover police officers and certain kinds of health records.
And 3) of course, does the government actually collect the data that you are asking for?
If so, then they can be compelled to send you the data, as per the state's regulations. If the government doesn't actually collect that data, then no, they don't have to collect it for you. Also, most state laws say that an agency does not have to provide new types of records or queries to you, if those queries aren't generated as part of the government's business.
For example, an agency may fight against a request that asks for pension amounts, aggregated by year/department, if they never conduct that aggregation themselves.
"
social process - How should I respond if a government official says she won't release data because she doesn't think it will be useful?,"
The first three I'd lump together ... the others, I'm not so sure.  For the first three, I'd ask them:

Why did you collect the data?
If the data has no use, why do you keep it?
What do you use the data for?

The next two require more information ... they might be fiscal in nature, and in today's budget situations, it's quite likely that departments are already short-staffed; changing how they do things might have significant costs, especially when the data is actually in high demand.
The last one ... well, if it's a federal agency, I'd be inclined to bring up FOIA.  If it's a state agency, I'd bring up the equivalent law, if it exists.  I know a few people who argue against opening up the data because they've had to deal with too many crackpots who want to discuss the data and/or refute it without actually understanding what the data is.  (it's amazing how many times you can explain compression artifacts to people, and they still insist that it's UFOs, and that you're just trying to cover it up)
"
usa - How do I Get that Juicy Economic Data from BLS.gov (Bureau of Labor Statistics) into Zip Code Format?,"
I used some BLS data for a recent project. It took me a little while to dig through. But there are essentially 3 ways to get access to their data. 

Is from their 'downloadble' sets published as links from HTML. It sounds like you have got to this.
Quandl have some of the BLS data curated. That have made it nicely searchable, filterable and available under REST.
All of the data the BLS publish is here in raw format: ftp://ftp.bls.gov/pub/ the doc folder tells you what each of the sub directories in time.series directory is for. This is the overview. As far as I know this is totality of what they have published at the most granular level. Overview tells us this the cx file and the cx file tell us this.

The work is in curating this data. There are joins needed through structured keys. This is not hard, but you need to go through the process.
To the best of my knowledge, if the data is not here, the BLS don't have it. I would love to be proved wrong and shown where more granular data is.
"
How do you respond when government cites costs for not releasing data?,"
For point one, I would probe more. Custom programming to get data out, a portal, what? 
For point two, I try to make sure if it's actually true. Try to get the details of the software they are using and then figure out what the capabilities of the software are. Talking directly to the vendor is often the best next step.  This excuse is often given by people who are not well informed about what is technically possible, so I would also attempt to talk to the whoever is responsible. Assuming it is true, then ask them to release the data in whatever format they can. If the data is interesting enough, then the community will figure out how to make an obscure data format open, a la http://treasury.io/.
For point three, they are likely already spending substantial staff time responding to FOIA requests. If they make their most common FOIAed documents open by default, they can save staff time.
For point four, yes. They will have to do some different things.
For point five, this really depends upon their current information management practices. If they have good, computerized systems in place for managing their data internally, it should be easy to automate public data releases. If they don't have those internal systems, then, you might be in for a longer effort which involves finding an internal champion for better internal systems and supporting that effort.
"
How do you respond when government cites time concerns for not releasing data?,"
These seems like objections for a government that hasn't started making data public. If that's the case, then the key thing is go get them to make a start.
Real time data is great, but there's lots of really valuable data that changes very slowly, can be easily exported. GIS data and budget data are often good targets, but choose something that makes sense in your local context.
When a government is getting started, they don't need a dedicated open data web site. They can just make a new page on one of their existing sites with links. Make sure though, that they have a way of keeping track of traffic and downloads. You'll want to be strategic in selecting data sets that will get used so you can make the case for releasing more data later.
The last point seems different than the others, because it seems like you are requesting a particular piece of data, not just advocating for open data. If you get this request, I would try to find out what the software is and talk to the vendor to see what's possible.
"
"How do you respond when government says it should be selling its data, not opening it?","
Depending upon the situation, you could respond by saying

That violates the applicable FOIA law which usually says that records must be provided only at the cost of responding to the request. This might be substantial, but cannot, legally, be a money maker.
Even if it's legally permissible to charge for the data, the overhead of taking and processing payments is likely to wipe out any profit
The data that people are most likely to pay is almost always timely. The data owner can increase their demand by making a staler version available for free and charging for more immediate access. In the private sector, many economic indicators are free, but you can pay for the privilege of getting the data a few minutes before anyone else, ex. https://www.ism-chicago.org/chapters/ism-ismchicago/barometer.cfm

"
How do you respond when government says it needs more proven results to release data?,"
First,
Tell the story of peer cities or agencies that released data and realized benefits. It's hard without a specific target, but by now cities and agencies of very different sizes have released data and seen people run with it.
Code for America's Brigade are a good resource, for telling these stories for cities, http://brigade.codeforamerica.org.
Bigger examples you could point to

GTFS
Weather
Open Street Map
The whole field of geographic market segmentation which largely depends upon census data

Second,
This is what Hackathons are good for. It demonstrates the reality of the premise that if you release these data then people will do things with it, and that the government will get good press for making the data available.
"
How do I escape a single quote in a Socrata SODA 2 API call?,"
From Socrata Tech Support

The [Socrata SODA 2 API] expects strings to be enclosed in single
  quotes. You can use double single quotes to escape it within a string.

http://data.cityofchicago.org/resource/xh8b-g55w.json?$where=license_description='Caterer''s Liquor License'&$limit=1
"
How do I escape an ampersand in a Socrata SODA 2 API call?,"
Escape the & as hex code %26.
http://data.cityofchicago.org/resource/xh8b-g55w.json?$where=license_description='Special Event Beer %26 Wine'&$limit=1
"
"data request - Historical values for the German ""Sonntagsfrage""?","
Just found the Overview of the Allensbach Institute, which has data at least for the current legislative period. The data can be easily scraped, see for example my R script.
This helps a bit, but if you have another answer, I will accept and upvote yours.
"
data request - Database of names of Japanese and non-Japanese people,"
Finally, Wikipedia and its sometimes tragical affection to compiling lists on everything and anything becomes handy: 

List of most common surnames in Asia#Japan (Wikipedia). Every continent and major country seems to contain a pretty extensive list, sometimes even with estimates on number of occurrences.
Category:given names by culture looks pretty extensive: male, female and neutral names cover over 1,000 articles. If they are still incomplete, looking for categories of famous Japanese persons might be complementary.

"
machine learning - Data Set for Predictive Modelling,"
If you're already using a Python toolchain, then the easiest option is probably skdata. See the data sets it provides.
"
data request - Product catalog datasets,"

BestBuy publishes product data through Products API or in RDF/XML dumps (see sitemap here).
Linked Open Commerce is an attempt to aggregate data from e-shops, including descriptions of products.
An older (2009) attempt to publish product data is ProductDB.

"
Data on Android/iPhone apps by user?,"
authors of the BAM application for android may have some data you are asking for.
It can recommend apps based on other users that have the same set installed.
https://play.google.com/store/apps/details?id=com.bestappsmarket.android.bestapps&hl=en
E.g., blind users if they connect - they would recommend this way apps to each other.
"
ethics - Is metadata about data from my clients my data or the clients?,"
I think you should check first if the agreement you have with your customers allow you to reuse and mix their data and in so in which conditions.
You can also look at the standard and guideline promoted by of the Dutch DPA or the European Data Protection Supervisor in term of privacy. 
"
transportation - Stream Airfare data,"
Airlines distribute fare information through something called a global distribution system (GDS). An example of one such system is SABRE. There are lots of different kinds of GDSs out there, and you can find them with a simple internet search. The International Air Transport Association (IATA) is probably your best resource for finding out more about GDS and trends there.
"
data request - Northern Ireland electoral wards shapefile,"
If you go to the NISRA website then you'll get all shapefiles in both .shp and .tab formats for 2011 census as well as some other geogrpahies.
Be awere of the copyright.
"
geospatial - Bathymetric contour data for North America,"
OpenStreetMap to the rescue! Its cycling map shows contour lines and shaded relief (even in Canada):

(source: opencyclemap.org)
Like most open data projects (probably Natural Earth, too), they use NASA's SRTM (OSM Wiki) dataset that has global coverage, as far as I know. As you seem to need contour shapefiles, refer to the article Contours (OSM Wiki) for a workflow on how to convert raw SRTM data to shapefiles using GDAL.
Jackpot: Apparently, OpenDEM went through the effort of preparing contour shapefiles worldwide from SRTM with 25m precision. They offer a pretty convenient download for an arbitrary region. (Download manager or wget recommended.)
How I found the data: For all my spatial data needs, usually before googling, I have a look at the OpenStreetMap Wiki and search for relevant keywords, in this case: contour, height, SRTM.
"
language - Frequency Analysis Character Distribution Data,"
Second try: Google Ngram Viewer contains raw counts of 1-, 2-, ...-grams of text, retrieved from its book scanning endeavor. The section 1-grams contains counts of the occurence of lettres, numbers and even punctuation. They are provided as tab-separated value files, so the frequencies should be derivable with modest scripting efforts.
Found via Wikipedia article Text corpus.
"
geospatial - UN/LOCODE Copyright Status,"
I believe minopret is incorrect on the use restriction of UN/LOCODE to non-commercial use w/o permission. There is no where on the UNECE site that indicates such a restriction. All the verbage would indicate the contrary. The terms of use simply indicate that the material is w/o warranty and the user will indemify the UN.
This statement can be found on the web page: http://www.unece.org/cefact/locode/locode_since1981.html 
UN/LOCODE is freely available to all interested users. It can be consulted and downloaded from the web-site www.unece.org/cefact/locode and users are welcome to propose additional locations; for this purpose a new, automated request procedure has been introduced, as described below: 
Sometimes users get confused on what code lists from the UN and ISO are free to use and which are copyrighted and you need to purchase. For example, the ISO 3166-1 codes are free to use, but the ISO 3166-2 subdivision codes are not. 
EDIT: Feb. 15, 2014
The ISO 3166-1 Country Codes will no longer be freely available from the ISO website after Feb. 20, 2014. You will need to purchase a subscription for 300 CHF (http://www.iso.org/iso/home/standards/country_codes/country-codes_new-product-info). 
I have placed an archived version (Feb. 15, 2014) of the English and French datasets in TXT and XML format at:
http://www.opengeocode.org/archive.php
"
How to get daily updates from Wikipedia?,"
Right now, there is no good solution. The options I can think of are:

Just download the normal dump whenever it becomes available. This means you won't get daily updates at all. For the English Wikipedia, a new dump is generated about once a month.
Use the adds/changes dumps (already mentioned by ojdo). There are two problems with using this:

It doesn't include all changes. Specifically, information about moves, deletes and some undeletes are not included.
It's an experimental feature and I wouldn't be surprised if it were discontinued in the near future (because a better option is coming, see below).

Use the API (possibly combined with the IRC recent changes feed) to get the text of all new revisions to a wiki. I think this might work for very small wikis, but it's certainly not feasible for huge active wikis like the English Wikipedia.
Use binary incremental dumps (which is a project I built over this summer). This will do exactly what you want: it will allow you to download only changes since the last dump and it should allow creating dumps much more often (the hope is for daily dumps). The only problem is that this is not live yet, so you will have to wait before using this (I have no idea how long, but I would expect it to go live this year).

"
data request - World gas/petrol prices at the pump,"
My best guess is World petrol prices on MyTravelCost.com. They give you a large, configurable bar chart of gasoline prices. However, their source description can be described vague at best:

The data are drawn from a variety of sources including official government materials, oil companies, online resources specializing in gas prices, and others. These sources provide reliable information about fuel prices in a large number of countries. For the other countries, we provide an estimate using previously published data.

Bonus: If you want to access the raw data, simply right-click on the graphic an open the (incredibly long) image URL in a new tab. It contains the raw data for the bar chart like this:
http://www.mytravelcost.com/graph.php
?data=0.02,0.09,0.15,0.18,...
&titles=0.02|0.09|0.15|0.18|...
&outsideGraphTitles=Venezuela|Iran|Saudi%20Arabia|Qatar|...

"
data request - Crowdsourced local consumer prices,"
Gas Buddy, the Price of Weed website (SE does not allow a link) and Craigslist all come to mind.
"
usa - Government shutdown causing linked open data to go away,"
Archive.org might have already crawled the site for you. Check out the Way Back Machine: http://wayback.archive.org/web/query?type=urlquery&url=&Submit=Go+Wayback!
"
usa - Federal Shutdown: Raw SNAP (Food Stamps) participation data source,"
Is this helpful? 
Monthly Data -- National Level:
FY 2011 through June 2013
http://webcache.googleusercontent.com/search?q=cache:r9JbtaubWlMJ:www.fns.usda.gov/pd/34snapmonthly.htm
Annual State Level Data:
FY 2008-2012

Persons Participating
Households Participating 
Benefits 
Average Monthly Benefit Per Person
Average Monthly Benefit Per Household

"
data request - Stemming of long English text,"
Given that language is not a fixed thing, I'd hesitate to put much stock in a fixed database of ""definite"" stems. 
Here's the source code for the NLTK (python) Porter stemmer (GPL). It looks like it has no serious dependencies on anything else in NLTK -- just an interface that you could discard and some stuff for unicode compatibility that you could adapt. So if you're using Python (and can work within the GPL), this could get you started pretty quickly without the weight of the entire NLTK library.
If you still feel that the stemmer is likely to make mistakes, it wouldn't be hard to add an override that checks a dictionary of 'definite' stems before dropping back to the algorithm. Maybe you deploy it with an empty dictionary and then add corrections as you find errors.
PS this Stack Overflow post ""Stemming algorithm that produces real words"" looked like it might relate to your question.
"
data request - Looking for lat/long coordinates of proposed Northern Gateway pipeline route,"
It looks like Pipe Up Against Enbridge has the data your after, i.e., http://pipeupagainstenbridge.ca/the-project/map 
My colleague @HughStimson is contacting them about the data in the hopes that we can get permission to host as part of the GeoDataBC collection (https://github.com/geodatabc). (PS We're looking for contributors!)
In the meantime, it appears that the necessary data is linked from that map, e.g.:
http://pipeupagainstenbridge.ca/images/map/pipelineroute.kml
You'll probably want to check-in with them about the license / source for the data.
Good luck! 
Phillip.
"
What models and data is used for traffic predictions?,"
In this MIT project you can find a model about traffic prediction. Also, they mention that they took the data from Minnesota Department of Transportation. So, my guess is that if you want the data from a specific city, you have to contact with the local department.
"
licensing - Can you use data if no data license is explicitly stated?,"
The content of a database is generally covered under copyright law, so broadly speaking… no. You cannot assume that copying and re-use is implicitly allowed by default.
Almost all major countries follow the Berne Convention. In the US (for example), almost everything published after April 1989 is considered ""copyrighted"" by default and protected whether it has a copyright notice or not. You should assume that any works that are not explicitly licensed for reuse may not be copied unless you know otherwise. 
""Knowing otherwise"" is where it gets tricky. There is a lot of data that simply is not copyrightable. But you have to understand copyright law and the laws governing reuse if you are going to act without explicit license. There are Fair Use provisions which allow a certain amount of re-use of original works. Copyright law doesn't generally protect mere listings of things (like ingredients, formulas, telephone listings, etc)… but copyright protection may extend to substantial literary expression within those listings. Some countries recognize separate property rights for databases which are somewhat distinct from copyright. Also, only original works of authorship are protected by copyright. Compilations of others' work may not.
If you don't see an explicit license, you should assume copyright by default — and then proceed cautiously to determine whether the work itself is actually copyrightable or if your application is covered under Fair Use.
"
government - Where can I find normalized data on governmental spending on science?,"
Try the OECD's Main Science and Technology Indicators
"
programming - What is a good editor for linked data?,"
My currently favoured text editor jEdit has a simple yet effective word completion feature (Menu Edit > Complete word; default shortcut Ctrl+B). It takes its word list from the opened document and includes keywords of the file's programming language (in case it is code). Word-delimiting characters can be user-defined as described on the page Working with Words (section What's a Word?) in the comprehensive jEdit User's Guide.
"
biology - Is there a list of linked data from resources mentioned in the annual NAR database issue?,"
Yes there are several resources.
www.bio2rdf.org is probably the best known.
To easily browse linked data try
www.distilbio.com
"
"From an entrepreneur's perspective, are there reasons to open data?","
Example: I only know of the company Cloudmade because they provide (a now outdated, but fine at the time) download portal of ready-to-use shapefiles derived from OSM data. Though this service might have helped its competitors, they probably earned much more in terms of visibility, which might spawn e.g. development contracts for custom-made solutions. 
This argument is more verbosely given for open source software in the excellent blog post Yes, You Can Make Money with Open Source. Just replace OSS with open data, and the argument stays valid.
"
geospatial - Is there a tool to convert geoRSS into geoJSON?,"
The most versatile tool for geo format conversion is ogr2ogr in gdal. Here's an online front end that uses ogr2ogr to convert to and from GeoJSON, and it supports GeoRSS.
"
data request - Where to get school district boundaries?,"
1) google to find the exact website of the census.gov that contains the information you're looking for.  google's search results still contains all of the shuttered web pages
2) paste that url into the ""wayback machine""
3) choose the most recent blue circle that's pre-shutdown
Example: Here is the US Census 2017 School District TIGERLine/Shapefile Web Interface saved via Wayback Machine.  
Note: the Census' FTP server will not save in the Wayback Machine; In very recent past, the team behind the Wayback Machine had started bypassing US .gov server restrictions in order to preserve sites/data, but for reasons unknown, Wayback is still blocked by Census FTP.
"
data request - Creative commons licensed audio files of basic French vocabulary,"
Many of the pronunciation files on Wiktionary are from the Shtooka Project, that offer colllections of audio files for basic vocabulary in many languages, licensed under the CC-BY license.
"
data request - Large bibliographic database of research papers,"
CiteSeerX and DBPL are two of the most commonly used data sets used in the academic literature.
"
usa - Is Data.gov down due to the government shutdown or is it down for good?,"
Data.gov is down because of the US government shutdown. You can see almost identcal language on the census.gov website, which explains that the shutdown is causing the site to go offline.
Here is an article explaining more details about what is and is not available during the shutdown.
And if you are interested in seeing what APIs are available for when the website comes back online, try the Internet Archive for a list.
"
usa - How to convert risk adjustment scores into dollar amounts from the Medical Expenditure Panel Survey?,"
This is a complex calculation. Several factors are considered before the risk score is created, including age, original reason for entitlement, Medicaid status, and primary payer information.  The data behind these include the Minimum Data Set Long Term Institutional File and the Common Medicare Environment from the Beneficiary Demographic Input File. The Medical Expenditure Panel has a query interface.
""Diagnostic Data is used in risk score calculations and is obtained from both plans and FFS providers.

The Risk Adjustment Processing System (RAPS) Database contains the diagnostic data submitted by Medicare Advantage plans, PACE organizations, and cost plans.
The National Medicare Utilization Database (NMUD) contains the diagnostic data submitted by fee-for-service providers.

The Risk Adjustment System (RAS) calculates risk scores for all Medicare beneficiaries."" (Medicare Managed Care Manual)
"
geospatial - Is there anywhere to get US gps pipeline route data?,"
You can find geospatial data about this on Data.gov (this links to gas pipelines, but you can look for other types of pipelines). To find the geographic region you are looking for, you can draw a boundary box over the map in the upper left.
(Disclaimer: I am the Evangelist for Data.gov)
"
"medical - Data for ""Driving is Why You're Fat"" graph","
Let's try to follow the sources one by one:

Trust for America's Health has obesity ranks (sense probably inverted compared to the graphic) and the proportion (%) of obese people per state, directly linked from their homepage.
U.S. Census American Community Survey should have this data. They have per-state transport mode usage statistics.
Streetsblog is both a blog about (public) transportation and a network of grass root organisations promoting alternate (as in not passenger car) transport modes. I could not find any prominent study, statistic, survey or data source on their pages.

Sorry for the poor return, but I tried!
"
best practice - A Python guide for open data file formats,"
Here is a guide for each file format from the Open data handbook and a suggestion with a Python library to use.
JSON is a simple file format that is very easy for any programming language to read. Its simplicity means that it is generally easier for computers to process than others, such as XML. Working with JSON in Python is almost the same such as working with a Python dictionary. You will need the JSON library, but it is preinstalled to every Python 2.6 and after.
import json
json_data = open(""file root"")
data = json.load(json_data)

Then data[""key""] prints the data for the JSON.
XML is a widely used format for data exchange, because it gives good opportunities to keep the structure in the data and the way files are built on and allows developers to write parts of the documentation in with the data without interfering with the reading of them. This is pretty easy in Python as well. You will need the MiniDom library. It is also preinstalled.
from xml.dom import minidom
xmldoc = minidom.parse(""file root"")
itemlist = xmldoc.getElementsByTagName(""name"")

This prints the data for the ""name"" tag.
RDF is a W3C-recommended format and makes it possible to represent data in a form that makes it easier to combine data from multiple sources. RDF data can be stored in XML and JSON, among other serializations. RDF encourages the use of URLs as identifiers, which provides a convenient way to directly interconnect existing open data initiatives on the Web. RDF is still not widespread, but it has been a trend among Open Government initiatives, including the British and Spanish Government Linked Open Data projects. The inventor of the Web, Tim Berners-Lee, has recently proposed a five-star scheme that includes linked RDF data as a goal to be sought for open data initiatives I use rdflib for this file format. Here is an example.
from rdflib.graph import Graph
g = Graph()
g.parse(""<file root>"", format=""<format>"")
for stmt in g:
   print(stmt)

In RDF you can run queries too and return only the data you want. But this isn't easy as parsing it. You can find a tutorial here.
Spreadsheets. Many authorities have information left spreadsheet documents, for example Microsoft Excel. This data can often be used immediately with the correct descriptions of what the different columns mean. However, in some cases there can be macros and formulas in spreadsheets, which may be somewhat more cumbersome to handle. It is therefore advisable to document such calculations next to the spreadsheet, since it is generally more accessible for users to read. I prefer to use a tool like xls2csv and then use the output file as a CSV file. But if you want for any reason to work with an .xls file, www.python-excel.org is the best source I had. The most populars are xlrd and xlwt. There is also another library, openpyxl, where you can work with .xlsx files.
Comma Separated Files (CSV) files can be a very useful format, because it is compact and thus suitable to transfer large sets of data with the same structure. However, the format is so spartan that data are often useless without documentation since it can be almost impossible to guess the significance of the different columns. It is therefore particularly important for the comma-separated formats that documentation of the individual fields are accurate. Furthermore, it is essential that the structure of the file is respected, as a single omission of a field may disturb the reading of all remaining data in the file without any real opportunity to rectify it, because it cannot be determined how the remaining data should be interpreted. You can use the CSV Python library. Here is an example:
import csv
with open('eggs.csv', 'rb') as csvfile:
    file = csv.reader(<file root>, delimiter=' ', quotechar='|')
    for row in file:
        print ', '.join(row)</pre>

Plain Text (txt) are very easy for computers to read. They generally exclude structural metadata from inside the document however, meaning that developers will need to create a parser that can interpret each document as it appears. Some problems can be caused by switching plain text files between operating systems. MS Windows, Mac OS X and other Unix variants have their own way of telling the computer that they have reached the end of the line. You can load the txt file, but how you will use it after that depends on the data format.
text_file = open(""<file root>"", ""r"")
lines = text_file.read()</pre>

This example will return the whole txt.
PDF Here is the biggest problem in open data file formats. Many datasets have their data in PDF, and unfortunately it isn't easy to read and then edit them. PDF is really presentation oriented and not content oriented. But you can use PDFMiner to work with it. I won't include any example here since it isn't a trivial one, but you can find anything you want in their documentation.
HTML. Nowadays much data is available in HTML format on various sites. This may well be sufficient if the data is very stable and limited in scope. In some cases, it could be preferable to have data in a form easier to download and manipulate, but as it is cheap and easy to refer to a page on a website, it might be a good starting point in the display of data. Typically, it would be most appropriate to use tables in HTML documents to hold data, and then it is important that the various data fields are displayed and are given IDs which make it easy to find and manipulate data. Yahoo has developed a tool, YQL that can extract structured information from a website, and such tools can do much more with the data if it is carefully tagged. I have used a Python library many times called Beautiful Soup for my projects.
from bs4 import BeautifulSoup
soup = BeautifulSoup(html_file)
soup.title
soup.title.name
soup.title.string
soup.title.parent.name
soup.p
soup.p['class']
soup.a
soup.find_all('a')
soup.find(id=""link3"")

Those are only a few of what you can do with this library. By calling the tag, it will return the content. You can find more in their documentation.
Scanned image. Yes. It is true. Probably the least suitable form for most data, but both TIFF and JPEG-2000 can at least mark them with documentation of what is in the picture - right up to mark up an image of a document with full text content of the document. If images are clean, containing only text and without any noise, you can use a library called pytesser. You will need the Python Imaging Library (PIL) library to use it. Here is an example:
from pytesser import *
image = Image.open('fnord.tif')  # Open image object using PIL
print image_to_string(image)</pre>

Proprietary formats. Last but not least, some dedicated systems, etc. have their own data formats that they can save or export data in. It can sometimes be enough to expose data in such a format - especially if it is expected that further use would be in a similar system as they came from. Where further information on these proprietary formats can be found should always be indicated, for example by providing a link to the supplier’s website. Generally it is recommended to display data in non-proprietary formats where feasible.. I suggest to google if there is any library specific for this dataset.
Tab Separated Values (TSV). A tab-separated values file is a simple text format for storing data in a tabular structure (for example, database or spreadsheet data). Each record in the table is one line of the text file. Each field value of a record is separated from the next by a tab stop character – it is a form of the more general delimiter-separated values format. Unfortunately, I haven't found any good working Python library only for TSV. Until now, I have worked with CSV library like the following example:
import csv
with open(""tab-separated-values"") as tsv:
    for line in csv.reader(tsv, dialect=""excel-tab""): #You can also use delimiter=""\t""

Shapefiles are files used to represent spatial data such as polygons that define a city, a neighborhood etc. You can use the libraries  fiona and shapely (pip install fiona shapely) to help with this job. For example, if you want to load a shapefile, simplify its polygons (to reduce size) and then export to GeoJSON (so you can plot in your Web browser using JavaScript libraries such as LeafLet), you can use this code:
import json

import fiona
import shapely.geometry


shapefile = fiona.open('my_shapefile.shp')
shapes = shapely.geometry.shape(shapefile['geometry'])
simplified_shapes = shapes.simplify(0.01) # 0.01 is the simplification factor
geodict = {'type': 'FeatureCollection',
           'features': shapely.geometry.mapping(simplified_shapes)}
with open('my_geojson.json') as fobj:
    fobj.write(json.dumps(geodict))

VOTables is a mix of HTML and XML used most of the time in astronomy. This kind of data contains metadata that it is vital for you. It is pretty simple to extract them with Python using the following library.
from astropy.io.votable import parse
votable = parse(""votable.xml"")

Additional Information. Maybe you will find the Pandas library useful, whose I/O capabilities integrate and unify access from/to most of the formats: CSV, Excel, HDF, SQL, JSON, HTML, and Pickle.
"
language - Data for word hierarchies,"

ConceptNet is a semantic network containing lots of things computers should know about the world, especially when understanding text written by people. 

Trying to reproduce your relation-sequences yields:

finger PartOf hand IsA body part, which looks surprisingly ""dead end"".
chair IsA seat RelatedTo furniture MadeOf wood ...

"
data request - Where can I get Tectonic Plate Shape Files,"
I solved this problem using QGis with the following steps:

Vector -> Geometry Tools -> Check Geometry Validity
Vector -> Geometry Tools -> Simplify Geometry
Saved as a new Shape File

It loaded just fine after this
"
data request - Dictionary of misspelled words,"
There is actually a Wikipedia page about this very topic, and they maintain a machine-readable list that you can use as a starting point if you're writing a tool.
"
data request - Birthday and Marriage Information,"
Something like this? 
You can find the spreadsheet on the bottom of the page.
"
transportation - Where can I find data on aircraft?,"
NASA does quite a bit of aircraft testing.  (It's the National Aeronautics and Space Administration, after all).
I'm not familiar with repositories of that sort of data, but you can find technical papers, including those on Blended Wing Bodies in the NASA Technical Report Server, and that would give you contacts to request the data if it wasn't specifically deposited separately.
(disclaimer: I work at a NASA center)
"
Data standards for election results,"
This isn't widely used, because we just started working on it this year, but our Knight Foundation project, OpenElections, is developing specs for election results in the United States. You can see the latest specification on our Github wiki. We also have a format for election metadata. Project details at openelections.net.
"
usa - What does 'subregion' mean in Illinois State Report Card data?,"
I think these subregions are CPS 'networks': 
From http://www.cps.edu/fy13budget/pages/Schoolsandnetworks.aspx

Networks/Collaboratives District-run schools in CPS are organized into
  five geographic collaboratives – North/Northwest, West, Southwest,
  South, and Far South – and then further divided into 19 Networks,
  which provide administrative support, strategic direction and
  leadership development to the schools within each Network. There are
  13 elementary Networks, four high school Networks, one K-12 Network
  that serves both elementary and high schools, and one alternative
  Network for alternative schools.

Network                                   No. of Schools 
Austin-North Lawndale Elementary Network  32
Burnham Park Elementary Network           36
Englewood-Gresham Elementary Network      33
Fullerton Elementary Network              41
Fulton Elementary Network                 30
Garfield-Humboldt Elementary Network      28
Midway Elementary Network                 36
O'Hare Elementary Network                 42
Pershing Elementary Network               31
Pilsen-Little Village Elementary Network  26
Ravenswood-Ridge Elementary Network       40
Rock Island Elementary Network            29
Skyway Elementary Network                 40
Far South Side K-12 Network               37
North-Northwest Side High School Network  25
South Side High School Network            19
Southwest Side High School Network        19
West Side High School Network             27
Alternative Schools                        6

"
usa - What to do when license statements conflict?,"
The Massachusetts Open Data Initiative seems to provide only a link to the data, not the data themselves. Their terms of use couldn't be really applied here.
The terms and conditions of the Massachusetts Archives website refer to the documents, not to their data collections.
As to historical vital records, this page states that ""[t]he Archives' collections are public records and are open to all for research."" Besides, it states

Publication 
The Archives requires patrons who copy materials for
  publication to complete a permission to publish form agreeing to use a
  standard citation for archival materials and to give a copy of the
  published work to the Archives. There are no charges for publication.
Copyright 
Records created by Massachusetts government are not
  copyrighted and are available for public use. Copyright for materials
  submitted to state agencies may be held by the person or organization
  that created the document. Patrons are responsible for clearing
  copyright on such materials.

There is also A Guide to Massachusetts Public Records Law on their website.
I'd suggest contacting archives staff to clarify the issues with a permission to publish form and a proper citation.
Hope it helps.
(edited in response to Rob's clarification)  

Sorry, I focused on this precise example and missed your more general question. Let's make clear some terms, shall we? For any data distribution process, there are the following participants:

data collector (a person or persons who actually collected the data); 
author/creator/owner/principal investigator - it might be either the collector herself or an organization, e.g. government agency, university, research institute, which the collector has collected the data for;
data publisher (keeps the data and provides them for someone's use);
data distributor (it might be the same organization as publisher, but also might be another organization, say, with broader opportunities to disseminate information)

Now, the copyright always remains with the author/owner. And whatever other participants do, the data belong to the principal investigator.
Data provider/publisher shall comply with the owner's terms of use, especially regarding confidentiality.
Data distributor shall comply with the data publisher's terms of use, hence, automatically, with the owner's.
When someone looks for data, one should apply the terms of use attached to the data, whether they are written by the owner, publisher, or distributor. If the terms of use are missing or conflicting, one should apply the terms of use of the participant, which is one step higher on that list - publisher's instead of distributor's, or owner's instead of publisher's.
In this case, the data owner should be, in my understanding, the Massachusetts Registry of Vital Records and Statistics, the data publisher should be the Massachusetts Archives, and the Massachusetts Open Data Initiative goes as a distributor.
Can you download the data from the MODI? No. There is only a link, so forget about their terms of use. Can you get the data from the archives? Yes. OK, then you should comply with their terms of use. Specifically: not with the terms of use of their website but with the terms of use of their data collections under the Massachusetts law.
According to my experience, a possible citation for these data might look something like this: Massachusetts Registry of Vital Records and Statistics, 2013, Vital Statistic Records (1841-1910). Boston, MA: Massachusetts Archives [distributor], though I'd hesitate to use it without contacting the archives.  
What is the purpose of an Open Data directory if the collections it links to are not open data? Excellent question. Well, public records are open data, are they not? The fact that you should do some paperwork for the data publisher doesn't make these data closed or private. But besides this, I totally agree with you: the ultimate goal of any open data distributor should be to provide open data, not to complicate the whole process.  
As discussed above, 2 out of 3 terms of use are not applicable here (one refers to a link, another to a website, but only the third one mentions the actual data).
Have to say, I really sympathize with you about what you are going through. (The archives didn't answer even to phone calls? What's that again, the government shutdown?)

"
"data request - I'm looking for military discharge rates, the more focused the better","
I'm not an expert in this area, but I was told about this site:
http://www.va.gov/VETDATA/Veteran_Population.asp
Which has some data on veteran populations in the U.S. I'm not sure if it has discharge rates, but it might be a good place to start. You could try emailing some of the contact folks referenced in the datasets on that VA site, as they might have a better idea.
"
Do you know any open/standard resume format?,"
ResumeRDF seems to be the most ""ontological"" approach to normalization of CV information. The W3C has another article about ResumeRDF with further links.
"
What are the Units of Measure for the ZAREALAND & MAREALAND fields in US Census Data?,"
Since the file is a relationship file between ZCTA and MSA, it represents the area in square meters that the whole ZCTA (ZAREALAND) covers, and the area that the whole MSA (MAREALAND) covers. If you notice a difference between ZAREA and ZAREALAND, or MAREA and MAREALAND, this is because there is a waterbody in that area and the *AREALAND counts only land area instead of all.
The Census Bureau uses scientific measurement units, hence meters.
"
tool request - Suspected dirty data in Wikipedia articles indirectly belonging to a category,"
As far as I can see, there is no link from the category Japanese people to the article John Andru (at least based on dump from 1 October 2013), so I'm not sure why did the tool tell you otherwise.
But the category structure is pretty messed up. For example, John Andru is in the category 18th-century German writers, through chain of categories like this:
Category:18th-century German writers → Category:Immanuel Kant → Category:Kantianism → Category:A priori → Category:Analysis → … → Category:Statistics → … → Category:Anatomy → … → Category:Memory → … → Category:Design → … → Category:Justice → … → Category:Anti-corruption measures → … → Category:Students → … → Category:New Left → … Category:Free will → … → Category:Technology → … → Category:History → … → Category:Millennia → Category:2nd millennium → … → Category:1932 births → John Andru
"
api - Social Networks other than facebook and twitter that allow access to their data to developers?,"
Instagram
Foursquare
Livejournal
Flikr
Pinterest
LinkedIn
Blogger
Wordpress
Hope it'd be enough to begin with.
"
Data licensing question,"
The answer depends heavily on the context of you work and on potential users of your data.
First, there is no ""must"" about sharing your dataset. If the users want to get out more data than your website/app would allow them to do, they could always go to the original sources as they've been doing previously.  
Secondly, if you decide to share your data (be it the whole database or just a part of it referring to a specific ""car"" with its ""parts""), you must do it under the same licenses as the original sources do (or you might try to find one license, which satisfies them all, but it's not the easiest thing in the world). And as far as I can tell without more details, there might be already a potential problem with combining GPLv2 and LGPLv3.  
Thirdly, it really depends on your area whether you should or should not distribute openly your work. In academia, for instance, it's highly recommended not only because we all stand for open data and global knowledge but because it will bring you citations, mentions, and scientific prestige. No fear that the others will use your work because you want them to use your work as widely as possible and to give you credit for it.
In private sector, however, it might be different, so it's up to you what to decide. 
Finally, I should note that any data might be ""scraped"" from a website or an app. If somebody is really interested in getting a lot of data, what would stop them from doing it? You can try to build a protection against data scraping, of course, but at the end of the day, it might be more costly than sharing the data. Thus, you might want to add a way to contact you directly for big amount of data (say, ""Here is my email, please send me an inquiry, describing your interest, your company/institution, and the intended purpose of data use"").
Hope it helps.
"
data.gov - Data Standards for Campaign Finance,"
Political Disclosure Standard Electronic Reporting Format (PDSERF) is one such example. It may very well constitute the first such format.

Question and Answer on DBA.StackExchange
Github project with parser

Note: I'm the author of the question, answer, project parser.
"
research - PhD for open data in Europe,"
The Open University and the University of Southampton definitely have their hands in open data. For more concrete ideas you'd have to look at the appropriate department (e.g. Computer Science) and find a supervisor.
"
tool request - How can I work with a 4GB csv file?,"
This is the kind of thing that the csvkit was built for:
csvgrep -c ""Healthcare Provider Taxonomy Code_1"" -r '^282N' npidata_20050523-20131110.csv > hospitals.csv


csvkit is a suite of utilities for converting to and working with CSV, the king of tabular file formats.

A little more efficiently, you could do: 
zcat NPPES_Data_Dissemination_Nov_2013.zip | grep 282N | csvgrep -c 48 -r '^282N' > hospitals.csv

"
Do customers ever access data from a company or is everything they access just information?,"
Sounds like the question distinguishes between ""information"" which the customer receives, and ""data"" which is how the company itself internally organizes and manages the ""information"" product it delivers. Might be better termed ""digital products"" and ""product management information/data.""
An example would be Netflix, which delivers information in the form of streaming media to customers but customers never see the data that Netflix uses internally to manage and deliver the steaming media. 
Regardless of the type of digital product delivered, even if it something like raw demographics data, the company will always have data for internal use that the customers never see. From the IT perspective, there would be compartmentalized systems and likely functional units, one which concentrates on product delivery and one that concentrates on internal operations. 
"
data request - Text message corpus for American english,"
I don't know a corpus, but I know a way to create one. If you know how to program you can use the Facebook API and download all the public facebook status from USA with their comments. Then you can use them as a corpus.
Info about Facebook API
"
metadata - Is it a good idea to think of defining a DCAT vocabulary in other languages?,"
I would not recommend translating standard schema terms to other languages. This would make datasets not portable across toolsets that incorporate the schema. Instead, I would suggest mapping the terms to other languages in the visualization (e.g., viewer) of the data.
For example, an spreadsheet view in English might show ""title"" while in Spanish how ""titulo"". But the field naming in the raw data would not change.
"
best practice - What type of software is used by the Open Data community?,"
You cannot find ""THE"" best answer for this question. It always depends of what you want to do with the data.
Statistic Analysis
If you want to make a statistic analysis, you can use python like the question you mentioned. Otherwise, weka, spreadsheet (like microsoft excel), R or Rapidminer are a few tools I have worked with them.
Geodata
If you have geodata and the only thing you want to do is to visualize them in a map, you can use cartoDB.
Visualization
A pretty cool tool is the open spending. You can create awesome graphs most of the time for financial data.
Clean data
Many times, you will find data that needs cleaning such as removing unwanted parts or fix mispelling labels etc. For this kind of work, Google refine or Open Refine as it is called now is perfect.
I cannot remember something else now. If I refresh my memory with anything else, I will update the post.
"
historical - Data about when cities and neighborhoods were established,"
'Established' can mean different things (first settlers, formally founded & named, formally chartered or incorporated, etc.)  My town was incorporated in 1870, but founded in 1706, and settled sometime near 1695.  
'Neighborhoods' are even trickier, as they're not as formal.  You still have issues of multiple dates (when did New York's 'SoHo' start being called that, vs. when was it settled?) but the date and boundaries are much more ambiguous.
 Camp Springs, Maryland has been fighting with the County to be recognized, as they're spread across multiple county council districts.  Subdivision are easier to deal with, as there'd have been some sort of planning process and establishment of homes (although there may be older homes in the subdivison).
For municipalities, I'm not aware of any one source, but I'd look to either state archives, or state municipal leagues.
"
data request - Public source for financial company customer counts?,"
I am not sure you will find the number of customer per company but you can asses their size using other more accessible metrics like their revenue or number of employee.
"
data portal - Difference(s) Between Datahub.io and CKAN,"
As you say, http://datahub.io is a particular instance powered by CKAN, so you can't really compare both. In terms of what is particular about datahub.io, it is a community centred and powered site where everybody should be able to host their data, rather than the more default scenario of one or more organizations publishing data on the portal.
It is in the middle of a migration process to a newer CKAN version, so I suggest checking the blog or contacting the maintainers if you want to know more about its status.
"
What's a good resource for learning OpenRefine?,"
The documentation page on OpenRefine website point out to the official wiki a list of tutorials and the discussion list for your project specific questions. 
Like @Joe pointed out, I am not sure what are your looking for exactly. Maybe you can explain what you are tying to learn and why the book or existing documentation doesn't address your challenge. 
(disclaimer I am part of the OpenRefine team)
"
Catalog.data.gov using CKAN API with python requests package,"
The POST syntax is fine - you get the same results using the web front end:
http://catalog.data.gov/dataset?q=sea_water_temperature&sort=score+desc%2C+name+asc&ext_location=&ext_bbox=&ext_prev_extent=-254.53125%2C-75.84516854027044%2C54.84375%2C83.19489563661588
However the underscores are being treated as spaces, so effectively your query becomes: ""sea"" OR ""water"" OR ""temperature"". This is simply SOLR default syntax, and that has its pluses and minus. I think could successfully argue that the OR thing is not what you might expect, leading to all those results, but I'm not sure that the underscores as spaces is useful to users. (Can someone explain this aspect of SOLR?)
To get what you're after, try adding quote marks:
http://catalog.data.gov/dataset?q=%22sea_water_temperature%22&sort=score+desc%2C+name+asc&ext_location=&ext_bbox=&ext_prev_extent=-254.53125%2C-79.43237075914709%2C54.84375%2C80.87282721505686
This gives 52 results, starting with ones with exactly this tag, which is what I think you're after.
If you're really just interested in this tag in particular, you can use just search this field using 'tags:' instead of 'q:' (web URL) or inside of the q value in the API.
You can save faff of using requests and JSON decoding by using ckanclient. Once you get the hang of it, it is dead easy:
>>> import ckanclient
>>> from pprint import pprint
>>> ckan = ckanclient.CkanClient('http://catalog.data.gov/api')
>>> pprint(ckan.action('package_search', q='tags:sea_water_temperature', rows=1))
{u'count': 51,
 u'facets': {},
 u'results': [{u'author': None,
               u'author_email': None,
               u'extras': [{u'key': u'bbox-east-long',
                            u'value': u'-94.867'},
                           {u'key': u'resource-type', u'value': u'dataset'},
                           {u'key': u'bbox-north-lat', u'value': u'8.085'},
...

"
Geospatial Open Data from Sri Lanka or India,"
Did you try to look for it at GeoCommons.com?

India states
India state boundaries
India states

"
Where can I find a repository of open data stories?,"
Alright, some links!

This was started years ago, but didn't seem to take off: http://opendatastories.org/
Before the Beyond Transparency book, CfA did Engagement Stories: http://commons.codeforamerica.org/engagement-commons (The Commons hasn't been getting a ton of love recently though)
OKF has a (more recent) living doc exploring how open data improved data quality: http://bit.ly/opendata-betterdata

"
data request - Estimate how many linux operated computers are currently online,"
There is the good old Linux counter project1 which estimates around 69 million linux users. Now this is not exactly the same as number of online linux machines, but it should at least give some reference for what the number might be.
1 Actually this is a newer incarnation of the original counter.li.org.
"
usa - Is it still possible to download Census 2000 data at the level of the Metropolitan Statistical Area?,"
It looks like the geographic categorization is different for MSA between 2000 and 2010. If you use this link, and select ""BACK TO ADVANCED SEARCH"", it will bring you back to the search bar with the geographic level I specified. Since this level only seems to apply to 2000, you'll be getting table listings for the 2000 Census.
In the future, if you open the Topics tab --> expand the Year drop-down list, select 2000 and then go into the geography tab, it will only list available geography levels in 2000, and MSA appears on the list.

"
"Geospatial, temporal and keyword search using CKAN API on data.gov using ckanclient","
ckan.action will pass all the keywords parameters onto whatever API call you are using, so you can pass any parameter supported by package_search to it (I'll use PDF instead of WMS to get some results):
import ckanclient
q = 'tags:sea_water_temperature'
ckan = ckanclient.CkanClient('http://catalog.data.gov/api/3')
d = ckan.action('package_search', q=q, rows=10, fq='res_format:PDF')

You can change slightly your code to pass a whole dict of parameters, which makes easier to see what is going on:
import ckanclient                                                                                                       
search_params = {                                                                                      
    'q': 'tags:""sea_water_temperature""',       
    'fq': 'res_format:PDF',                         
    'rows': 10,                                                                                        
}                                                                                                      

ckan = ckanclient.CkanClient('http://catalog.data.gov/api/3')                                          
d = ckan.action('package_search', **search_params) 

Use these parameters for the other query types that you needed:
# sea_water_temperature + PDF + modified since + bounding box
search_params = {                                                                                      
    'q': 'tags:""sea_water_temperature"" AND metadata_modified:[2012-06-01T00:00:00.000Z TO NOW]',       
    'fq': 'res_format:PDF',                                                                            
    'extras': {""ext_bbox"":""-121,45,-120,46""},                                                          
    'rows': 10                                                                                        
}

Hope this helps!
"
usa - What are good examples of how open data is driving community development?,"
I'm not sure about specific examples in the USA, but the OpenStreetMap had some outstanding success in various countries. There are bound to be disaster stories in the US as well.
A famous example is the Haiti earthquake. It also has nice visuals.
Less known is perhaps the community mapping in Dar es Salaam, Tanzania:
http://www.opendta.org/Pages/Initiatives/initiative-mapping-dar-es-salaam.aspx
http://blogs.worldbank.org/ic4d/node/537
An open data platform that is cited numerous times for its success is Ushahidi.
"
data request - Legacy/obsolete UK postcodes,"
This site UK Data Service has historical postcodes from 1988. But I don't know if it is open data since I have to login. I hope this will help you :)
"
extracting - Where can one find developers interested in a PDF data extraction hackathon?,"
People from ScraperWiki and OpenKnowledge Foundation sure will like it! They develop and maintain a software called pdftables which extracts tabular data from PDFs.
There is also an article on ScraperWiki's blog about research in identifying tabular data in PDFs (since PDFs do not have information about data semantic, only positions, font etc.).
To contact people directly you see members of OKFN's GitHub organization, ScraperWiki's GitHub organization and also pdftables contributors.
"
security - Aggregated data for port number and vulnerability,"
OpenVAS have a really good repository for ""Network Vulnerability Tests"" that contain all the information needed to carry a research like this.
Downloading the archive from the nvts repository you can parse the file in the database,and make your own statistic of port sorted according vulnerability.
With a few bash lines of code you can filter all the files that contain at least one ""CVE-XXXX-XXXX"" and one ""port"" directive, make some research inside the filtered files (warning: there can be more than one CVE and more than one port per file) and then sort and filter the results.
"
"network structure - Data mining social group's interests, demographics, and geography","
There are also academic surveys. Check ICPSR for open-access survey results.
"
data request - Speech audio databases with phonemes labelled,"
I am not 100% sure if the following links are useful for you. Please let me know if you are looking for something else.
http://www.iitg.ac.in/ece/emstlab/SRdatabase/introduction.php
http://accent.gmu.edu/howto.php
http://www.signalprocessingsociety.org/technical-committees/list/sl-tc/spl-nl/2012-05/the-rss2015-speech-corpus/
"
usa - After FOIA is fulfilled where can it be found?,"
Short answer: No. As other people have said, the recipient of the FOIA information is not obligated to share it with others. There are some FOIA allowances for the needs and benefits of providing information to media, primarily with regards to fee waivers, but the recipient can do whatever they like with the information once they have it... including keeping it all to themselves.
That said, there are various initiatives to try to make these requests more useful by sharing them with a larger audience once they have been made. On the Federal side the most prominent that I am aware of is FOIA Online, which serves both as a conduit for requests and a place where others can see the responses. Unfortunately the participation seems to be voluntary and not all agencies participate.
On the public side you have projects like Muckrock which has submitted requests and responses listed and possibly FOIA Machine, which was kickstartered into existence earlier this year by the Center for Investigative Reporting. I'm unsure of FM will make public responses to other people; I can imagine a case for journalists to not necessarily make other journos aware of what they're working on.
In voluntary participation people often put results into Scribd or DocumentCloud and that might be your best avenue here - contact the people who made the request and ask them to put the results into there and share them. Here in the DC area people at the City Paper put supporting documents in one of those and attach them to the bottom of their online story. Your best bet may be to convince those folks that doing so would enhance their reporting.
"
Source of historic RFPs for the City of Chicago,"
With the assumption that an RFP converts to a contract, they have an Awarded Contract page. From there they have scanned documents for older content and a Vendor, Contract and Payment Search system which looks to have the Specification number for linking back to the RFP/RFQ/RFI
Given the edited questions request to find RFPs pre 2010, I have not been able to find a decent search capability but there is pre-2010 data in the system as evidenced by Contract #13221. It was awarded in 2007
"
Data Listing Request,"
It sounds like you're just looking for any large structured dataset to play with. If you could be more specific of the type of data you're interested in, we could probably provide some more targeted recommendations.
This gist provides a good list of sample open data sets, including some curated collections by data scientists.
I particularly like Hilary Mason's research-quality dataset list.
My advice is to pick a dataset related to a subject you're interested in. Good luck!
"
tool request - Building maps using ESRI shapefile,"
My favorite is cartodb. It is free and you can create wonderful visualizations.
Also, you can read this tutorial about using ESRI shapefiles in cartodb.
"
best practice - Optimal Data Format(s) for Open GIS Data Repositories?,"
I would say that geojson is in the top of my personal list. JSON is really easy to use it with a programming language, especially with Python that I am familiar with. Also, it is easy for conversions. If someone has another system and he has to use the data in another format, he could convert it to that format from JSON pretty easy.
Also, shapefiles are very popular and I feel it would be my second choice.
"
"education - Has any MOOC (Coursera, edX, Udacity or others) publicly released some of their student data?","
From my knowledge, Coursera uses Backbone.js for their site, so you can find several JSON endpoints with a lot of data. Unfortunately, I am not familiar with Backbone and the only links I know are from Google
List of all courses - https://www.coursera.org/maestro/api/topic/list?full=1
Another list of courses - https://www.coursera.org/maestro/api/topic/list2
List of all universities - https://www.coursera.org/maestro/api/university/list
Information about a specific course - https://www.coursera.org/maestro/api/topic/information?topic-id=compdata
Maybe if you are familiar with those technologies, you can find other links with more details and maybe with non-personal data you may find interesting.
UPDATE
Also, this user seems to have a lot of data about MOOC (completion rates etc). Maybe you can contact with him and ask him to share them with you.
http://moocmoocher.wordpress.com/2013/02/13/synthesising-mooc-completion-rates/
"
standards - Is there an open data format for screen/play scripts?,"
The Text Encoding Initiative's XML language has a module for ""Performance Texts"" by which they mean plays, screenplays, and other sorts of scripts. If you're looking for very prescriptive rules, TEI will disappoint you (there's usually more than one way to do anything in TEI!), but if you want a hospitable markup language with a lot of existing scaffolding, TEI should work nicely.
The best way to get your head around TEI is TEI By Example, which has a module tailored to drama.
Good luck!
"
data request - Cellular Network database,"
This is a list of mobile phone network and satellite phone network operators measured by number of subscribers which is free in wikipedia with link
Furthermore, there are a few more paid services in any case:
1) researchandmarkets.com
2) deveo
"
data request - Shape Files (Free downloads) for districts of the world,"
There are 3 ways to do it
1) Download shapefiles from here for cities boundaries. A single file  for each country.
2) Use Twitter API to get the coordinates and polygons for each city.
3) Use wikimapia for the same thing such as Twitter API
"
calendar - Is there an open data api or service for dates of official government holidays?,"
The system for (US) federal holidays is rather simple. As described in 5 USC 6103, it's basically just a short list of specific dates and rules for weekends.
This could certainly be integrated into your calendar but you might want to find a more complicated data source to practice with an API. To keep on the same theme, how about religious holidays? (example)
"
data request - German - English dictionary,"
If GPL licenced is acceptable to you, then you can use the German <-> English dictionary included in the Ding package:
http://www-user.tu-chemnitz.de/~fri/ding/
"
"data request - Set of handwritten, labeled characters","
I created a dataset with of on-line data. It has 369 symbols (including a-z A-Z 0-9 \alpha-\omega), but it is online data. You will have to create the rendered versions yourself:
http://write-math.com/data
Each class has at least 50 recordings. (I don't have much data for letters, so it will probably not be more.)
edit: I've redered it. See The HASYv2 dataset.
"
data request - Where can I get private companies' financials?,"
There is no comprehensive source of open data as you describe, but there are public and restricted-access open data sets that can illuminate certain aspects of private company finances. Privately-held companies often disclose information to the public in accordance with federal, state, and local laws. Some of this information directly involves company finances (such as public bankruptcy records; public property records) and some of it may serve as an indirect indicator of a company's financial circumstances (e.g., the U.S. Department of Labor publishes disclosure forms with rich information on the retirement plans maintained by U.S. employers, including 401(k) plan assets).  
Certain sectors of private companies are subject to more comprehensive financial reporting, such as banks (see, e.g., the extensive federal open data on U.S. banks published by the banking regulators). Data.gov's search feature can allow you to explore data available on particular sectors, such as finance.
Depending on the purposes you have, you may also be able to take advantage of restricted-access data sets on individual firms. For example, the U.S. Census provides researchers with access to nonpublic data at a number of Census Research Data Centers (RDCs). Restricted use data includes economic data for business establishments and firms, including: 

Economic Census
Longitudinal Business Database (LBD) 
Annual Survey of Manufactures (ASM)

The list of Census restricted use data sets is available on the Census Center for Economic Studies site.
"
data request - Getting legal information,"
This is an example of a question that we won't be able to answer until the CourtListener archive is complete. Using the data that is available, one could run queries there for particular statutes by citation or name.
For example, if interested in the Convention Against Torture, do a query at CourtListener for that with surrounding quotes, or if interested in cases invoking ""42 U.S.C. § 1983"" query that.
But cases aren't always ""filed under"" a particular statute. When filing a federal lawsuit one is required to fill out a cover sheet and choose a ""Nature of Suit"" (NOS) code that categorizes the case somewhat, but many lawsuits involve more than one issue, so a case alleging both copyright and trademark infringement might only be listed under the copyright NOS code and so even if you could search across all the NOS codes you wouldn't get exactly what you want. A separate problem illustrated by this example is that Copyright law is vast. Cases dealing with 17 U.S.C. § 107 (fair use) are often totally different from cases dealing with 17 U.S.C. § 109 (the first sale doctrine). 
So, in some ways your question is not exactly framed in an answerable form. What could be done (when CourtListener's archive is complete) is we could create a giant list of every opinion that mentions ""17 U.S.C. § 107"" and every opinion that mentions ""17 U.S.C. § 108"", ""17 U.S.C. § 109"", etc. through the entire United States code. That would be really interesting. What is the most litigated part of the code? What statute is cited the most? etc.
The best collection of federal district court opinions that you could download/scrape would be the RECAP collection from the good people at CITP, hosted by the Internet Archive here: https://archive.org/details/usfederalcourts
However, that collection includes any kind of document that might appear on a federal docket, including complaints, briefs, declarations, etc. and so it would take some additional effort to sort out the opinions from the rest. One day CourtListener will ingest that collection and make this easier for people trying to tackle questions like yours.
"
data request - Practice Hadoop?,"
Here you are:
A MOOC from coursera, but you have to wait until they will run it again or
You can start from dozens of courses here in big data university.
"
data request - WHO's Defined Daily Dose guidelines,"
The ATC index DDD is copyright protected, so I do not believe one could legally make an open-data version.
You can find information on their licensing on this FAQ page:
http://www.who.int/classifications/help/FAQOther/en/index.html
You can find their copyright statement at:
http://www.who.int/about/copyright/en/
"
usa - Raw data from US Census and BLS,"
The native raw datapoints comprising some of the more popular tables related to your search are accessible through sites at both agencies.  As examples:

Summary reports on current employment statistics and detailed data behind the report from the Bureau of Labor Statistics
Summary reports on firm size, employees, and payroll and detailed data behind the report from the U.S. Census

A collection of all the datasets by organization can be found at the Data.gov catalog.  For the two you note:

Bureau of Labor Statistics
U.S. Census

(Disclaimer: I am the Evangelist for Data.gov.)
"
usa - How can I convert from Census 2010 tracts to Census 2000 tracts?,"
Missouri Census Data Center provides a (SAS-based) app that does this pretty smoothly.
"
data request - Congressional Record in electronic form,"
The Sunlight Foundation makes a lot of Congressional Record data available through its Capitol Words API:
http://capitolwords.org/api/1/
It's focused on speeches, and doesn't bring much order to chaos beyond that.
But you're also looking for votes, and bills, and bill text? Rather than plumb the depths of the Congressional Record, you can get that from this project:
https://github.com/unitedstates/congress
Which has various scrapers that assemble this from around Congress. This is the data that ends up in GovTrack and in the Sunlight Foundation's Congress API.
"
usa - Panel data for public companies' financials? Alternative to Capital IQ Compustat?,"
The SEC publishes standardized machine readable data in XBRL format on public companies and other entities, such as mutual funds. Public company financial statements are among the data sets available. These data sets are available on the interactive data home page of the SEC, www.xbrl.sec.gov. 
In addition, the SEC maintains a market structure and data analysis site. (See announcement.) This site, www.sec.gov/marketstructure, contains data such as metrics for individual securities. The individual security data sets provide metrics for more than 4,800 securities. Columns include: Ticker, Date, Security, McapRank, TurnRank, VolatilityRank, PriceRank, Cancels, Trades, LitTrades, OddLots, Hidden, TradesForHidden, OrderVol, TradeVol, LitVol, OddLotVol, HiddenVol, TradeVolForHidden.
"
File formats by scientific community?,"
The BioSharing Standards Database may be of some use to you.
You're going to have a hard time with this, though, because of the scatteredness of the information and because quite a few instrument-science file formats are proprietary.
"
"data request - What are the different types of Degrees, Diplomas and Certifications and Industry types?","
I'm not entirely sure what exactly you are after, but in any case, looking at the ESCO-standard would probably get you in the right direction.
ESCO is a EU-standard published in October, linking the NACE sector classification, the ISCO occupational standard and the EQF qualifications standard.
For instance the information about the European Hairdressing Certificate  contains standardized metadata about the awarding bodies, the corresponding qualifications, the related occupations, etc.
The complete standard is available as open data, and can be downloaded as RDF.
"
best practice - Captcha in a website with open data,"
It might be consider illegal. Autofilling captcha is a blackhat toolbox and in most of the cases you should avoid it. The best thing you can do is read the ""term of uses"" of the site and if you cannot find anything there, then you can always contact with them and ask for a certain reason to send you the data directly.
"
web crawling - Alexa top dataset,"
Alexa is a division of Amazon.  They still have an API, but you need to access it as a paid service through AWS:
Alexa Web Information Service
http://aws.amazon.com/awis/
and
Alexa Top Sites
http://aws.amazon.com/alexatopsites/
"
data request - Amazon price history dataset,"
www.keepa.com is a similar website to camelcamelcamel. I cannot find a way to download the data but maybe you will.
In addition, Terapeak provide a free package with limited api calls (500/months) where you can find historical prices.
"
data request - Indian Movie Database?,"
Hmm I don't know if you could find something for a commercial use. But here are a few suggestions.
1) imdb if you do not want to play with api, you can download all the database. But not for commercial use.
2) An alternative api for movie database rotten tomatoes
3) An alternative api for movie database anditson
Since you need a commercial use, read very well all the term of uses, just to be 100% sure.
"
best practice - What are examples of people sharing personal data as open data?,"
Stephen Friend of Sage Biometrics gave a talk at the Symposium on Global Scientific Data Infrastructures  in which he mentioned that people were putting their medical data out there to try to help find cures for their ailments.  (and often paying of of pocket for their DNA to be sequenced so it could be shared)
Sage Biometrics operates Synapse to help distribute  and analyze that and other public biometrics data.
update More info on the effort is at http://sagebase.org/bridge/ :

Toward this goal, Sage Bionetworks, with support from the Robert Wood Johnson Foundation’s Pioneeer Portfolio is building BRIDGE a web-based, open-source platform that will allow patients to provide their data and insights as research partners on the health problems that matter most to them.
On BRIDGE, citizens, patients and researchers will be able to use online tools that connect people, their data and their stories to build communities focused on defining the research question that matters most to patients and their families. Participants will be able to use BRIDGE’s consent tools and data portal to contribute their health data into open research projects such as those that Sage Bionetworks is enabling with its Synapse data repository and collaborative work space. The insights that come from the data will then be reported back on BRIDGE and also drive new rounds of research collaborations.

"
Journals for Open Data research,"
It seems research is interdisciplinary. 

For technical aspects I would look, for example, at journals that publish research on linked data. 
For business models etc management journals.
For public sector information, journals that focus on pubic administration.

More research might be in Information Systems and others.
We at the ODI started a Zotero group for open data papers, but early stages.
"
data.gov - Trying to locate US Census Shapefiles for all MSA's (Metropolitan & Micropolitan Statistical Areas),"
You have been misled, CBSA files are in fact what you are looking for. Check the documentation detailing what each type of shapefile contains.
Want more verification? Here is the description of the GEOID field for the CBSA shapefile: 2010 Census metropolitan statistical area/micropolitan statistical area code found in CBSAFP10. And here is the description for the CBSAFP10 field : 2010 Census metropolitan statistical area/micropolitan statistical area code. You may want to use the NAME10 field to reference each specific MSA.
Here is another tip for you, all of the data released by the Census (as far as I know) comes labeled with a year in which it is measured. If your data is a 5-year average, the most recent year it covers is the shapefile year you should attach it to. If it is one year data, then the shapefile should come from the same year. I am going to assume you want population statistics and that the year of data you have is for 2012 or is for 2008-2012, therefore, I believe you want this.
"
government - Involvement of central statistics bureau with national open data portal,"
I have a CSV dataset you should find useful. It contains primary government website URLs for all countries in the world. The third field is to the country's statistical division.
http://www.opengeocode.org/download.php#govweb
"
data request - A dataset for Google Books,"
Google Books publishes an API. https://developers.google.com/books/docs/v1/getting_started
Of course one of the prohibitions in their ToS (https://developers.google.com/terms/) is ""you will not...Scrape, build databases or otherwise create permanent copies of such content"" so if anyone built an open database from this data, which I think is what you're asking for, it may be in violation of the ToS.
"
government - Crowdsourced Open Data,"
I aggregate datasets from different sources together frequently. I sometimes use a hybrid database/table representation, that follows some adaption of this basic approach:

Split the fields into two table schemas.
A. one for things that are invariants (e.g., place name, ISO codes, etc)
B. one for things that are variants (e.g., population, coordinates, etc).
C. the 2nd table schema uses a foreign key to tie its records to the first table schema.
I then make an instance of the 2nd table for each dataset source.
When querying, I join the first table with an instance of the second table depending on which data source I want to use.

I avoid blending data for the same field from different data sources if there is a potential to be different, and I hate the idea of adding N fields for the same value in one table to cover each dataset. This way, if I decide to add a new dataset source, I can simply create another instance of the table schema. Other benefits I have:

Don't have to update existing table schemas.
Don't have to re-optimize storage of in-production databases.
Don't have to modify queries in my middle-ware.

"
Is there any documentation or case study of scaling up an Open Data API?,"
You can have a look at the main data portal in the UK: data.gov.uk
An example of the published statistics is below.
Dataset                                         Views   Downloads
English Indices of Deprivation 2010             45173   15046
Bona Vacantia Unclaimed Estates and Adverts     38263   24112
Lower Layer Super Output Area (LSOA) boundaries 35547    8785

"
data request - Real Newbie Question,"
There's Data.gov for U.S. federal government open data, and Datahub for international open data, as a start.
"
api - Techniques for Pulling Prices for a Large Number of Amazon Items,"
As I can understand from this blog post they split the database in groups of interest and they update products in base of these groups.
Without Ι know more than this blog post, several companies provide the option to contact with them and arrange a different paid package for their API. I don't know if this is the case for Amazon, but why not?
"
data request - Music partitions for piano,"
I found this website (musopen.org) the other day and I bookmarked it. Looks interesting. From the about section of their website:

We provide recordings, sheet music, and textbooks to the public for free, without copyright restrictions. Put simply, our mission is to set music free.

"
usa - Occupational Outlook Handbook dataset,"
http://www.bls.gov/data/#api
You should be able to use the API to get the raw data.
Douglas
"
government - A file listing adjacent congressional districts?,"
I just solved a problem exactly like this with the United States' 2012 Census Block Groups. I used ArcGIS to breakdown the borders to their individual line segments (in other words each line that consists of only two vertices), and captured the line length of each segment of each area. I then transformed the lines into midpoints and did an intersect analysis. This method I found was exponentially faster than near table analysis using the whole polygons. I then selected records that where only Block Group GEOID's from GEOID1 did not match the id in GEOID2.
This took ArcGIS too long so I had to do it in SAS. I then dissolved the records by GEOID1 and GEOID2 and summed the line length. This produces the total length of borders that are shared by GEOID1 and GEOID2. I then compared this length with the total length of the original Block Group. This helped me to solve the question: What percent of Block Group 1's border is shared by Block Group 2's border? And it was for all Block Groups. This also tells you what Block Groups are adjacent to each other.
I realized based on some of the answers provided that I was not including adjacent districts where only one corner from each was touching, so I broke down each district by vertex instead of line midpoint and did a similar analysis. I then joined the first analysis based on line midpoints to the vertex analysis and give a proportion score of zero to any that were not caught in the first analysis (because if only corners touched they do not share a common border, but a corner).
This file includes my analysis. The first column is the State and District number of the reference district; the second column represents the districts adjacent to it; the third column represents what proportion of the reference district's borders are shared with the adjacent district. If the proportion is zero, then they are adjacent by touching corners.
Source of data.
"
data request - Results of past NCAA games,"
I created a git repo with the data I have collected from the past few seasons for the NCAA mens basketball division I. 
Check it out here: 
https://github.com/mgoldwasser/ncaa-basketball-historical-final-game-scores
"
data request - Database of trademarked words,"
Check the following two resources (sorry, only US info):

http://www.uspto.gov/ip/officechiefecon/tm_casefiles.jsp
https://explore.data.gov/Business-Enterprise/Trademark-Daily-XML-Applications-Assignments-and-T/eqbw-esys

Hope this helps.
"
government - How to acquire data about the speed and density of vehicle on freeways?,"
I couldn't find one dataset for both of your request.
Here is the ITOworld project which contains data about speed limits.
And here is a dataset for UK about traffic. Maybe from this dataset you can find the density.
"
CKAN vs. Socrata,"
I'm also quite biased because I work on CKAN and DKAN, but four things to add:
1) Both CKAN and Socrata are often integrated in various ways with a Content Management System such as Wordpress (data.gov) or Drupal (data.gov.uk), in order to more readily provide a more full featured web portal including use cases like telling stories around data, creating groups to collaborate around the data, etc. 
2) If this CMS use case is relevant for you, and especially if your organization already uses Drupal for its websites (and/or has a PHP-savvy than Python-savvy team), do look at ""DKAN"" as well (http://nucivic.com/dkan).  DKAN seeks to mimic the features and API of CKAN natively within Drupal, so there's just a single LAMP stack of software to deal with rather than an integration of two different open source platforms, as in the case of Wordpress+CKAN or Drupal+CKAN.  
3) I think open-source / lack of ""vendor lock-in"" is a big deal; even if you're buying a turnkey SaaS solution, I would always go with a one that you could always change your mind and take over direct responsibility for if you ever need to.  For now, CKAN and DKAN are stronger in that regard.
4) Both CKAN and DKAN are available in the cloud on IaaS platforms like AWS and Azure, as well as in SLA supported SaaS versions.  Open source + SaaS = OpenSaaS (http://opensource.com/government/14/1/opensaas-and-government-innovation).
"
api - Tool to extract the main concepts/topics from web pages,"
I ended up creating my own system combining a bunch of APIs. Here is what I did:

I pulled the homepage text from each website and cleaned up all the html, javascript and styles
I pushed this text out to various APIs to process and stored the results. I used alchemyapi.com, textrazor.com, opencalais.com. Those APIs have a lot of options but mainly I focused on keywords, entities and topics.
I then manually scored a bunch of websites if they were on topic or not
I exported the data to a CSV file and uploaded it to BigML.com a SAAS machine learning tool. Using their very slick user interface, I was able to easily create a dataset, create a model or ensemble of models, run a 80/20 evaluation of my model and finally predict results.
I originally used Google Prediction, but it was very complex to setup, had very poor documentation and examples in their API for prediction. and was black box - meaning It worked but who knows why.
I'm getting around 80% accuracy in my predictions for new URLs to see if they are on topic or not.
Oddly enough I wrote this all in PHP as a WordPress plugin mainly because that is what I know and the end result of this will be a multisite wordpress website. A better choice would have been Python as there are many good text processing tools for that language.

"
computing - User interface data,"
We can make some available from Retail systems, where the logs are the checkout operator. These are logs of main messages sent via PreTranslateMessage to the application. The operator tends to use the same screen all day, but there are a number of popup and child windows that show.  Our logs only show the inputs such as keystrokes and clicks; they do not show any output such as error messages. http://www.fieldpine.com/docs/tech/datasets.htm
I am not sure if these will contain enough information, depending on what your aim is. To fully understand and analyse you might also need to know some details about what button is positioned where. Maybe we can provide that too if needed but this is then getting quite specific. We have instrumentation all through the code and can track from user action, to processing flow, to final outcome. From your sale receipt we can track back to exact keystrokes if we want too.
At the moment I have only loaded a small sample so you can verify it has what you need, I will get some larger samples online next week.
"
Where can I find a complete list of Census Bureau Summary Level (SUMLEV) definitions in table format?,"
The page Missouri Census Data Center - Census Geography and Summary Levels has a link to a listing of all the sumlev codes known to them as a text file.  
Although the text file is a SAS source code file it would be trivial to edit out the code before and after the list, and edit the rest to csv.  
It's unclear what the license is for their text version of the list.
"
geospatial - Obtaining longitude/latitude boundaries for Google Maps regions,"
Google Maps data comes with many restrictions so isn't really suitable as a base for further use.
Although it can be of varying quality, Open Street Map (OSM) increasingly includes boundaries as well as ways (roads) and points, and is Open Data (subject to their terms of use including attribution).
The OSM data is in XML format (download link at bottom of the left column in the example) that lists the relations, ways and nodes in the boundary that was selected.  In a nice simple case like that Acadia University boundary, utilities like osmtogeojson should be able to convert it into a mapping format like geojson, but it might struggle with a more complex nested boundary.
By comparing Google Maps with the OSM data you'll see there are some differences with the campus boundaries (especially north of Hwy 1 / Main St).  It's debatable which is more accurate, the official campus map doesn't show any boundaries but does include (for example) buildings west of Westwood that neither Google or OSM include within the campus.
For a more general solution, many countries now have open data boundary files online, such as GeoBase for Canada.  These are more likely just to be government admin areas (provinces, municipalities, ridings and so on), and can be a large amount of data to deal with.
"
Download Wikipedia articles from a specific category,"
The Wikipedia Special Export feature does exactly this.
More details in this answer.
"
Using the API from Healthcare.gov to access healthcare plan data,"
(Caveat - I have not used this API, but have used the raw datasets).
I read the technical documentation for this API. It can be found at: https://finder.healthcare.gov/services
This is not a ""REST API"". It is similar, but instead of using parameters on the URL request, you place your request in XML as party of the body in a POST request to the healthcare API url.
How you do this will depend on what programming language you are using? Curious, are you a HS or college student.
"
real time - Telemetry data feeds,"
(note that 'telemetry' can mean a few different things ... for satellites, it's typically information about the location and other data about the operating status of the spacecraft)
NASA's STEREO mission makes their telemetry available.  See : 

http://www.srl.caltech.edu/STEREO/attorb.html

Most science missions have such information available, as the spacecraft location and pointing is necessary for proper interpretation of the data.  They'll also have other 'housekeeping' information (voltages, temperatures, etc.) which may be necessary for propler calibration.
You can find ephemeris data from other space science missions in the VSPO ... use the restriction 'Measurement Type' of 'Ephemeris'.
You can also compute spacecraft locations using SPICE.
"
data request - Twitter open datasets,"
While you will find some exceptions, sharing archived tweets is against Twitter Developer Policy.

Take all reasonable efforts to do the following, provided that when requested by Twitter, you must promptly take such actions:

Delete Content that Twitter reports as deleted or expired;
etc


If a user deletes their own content, the archive should reflect that deletion (which is a massive effort to continuously check).
For that reason, Twitter data sets are often shared as simply two fields: user_id and tweet_id. Then to reconstruct the dataset, one would query the API with those two keys. Then Twitter can ensure that if the tweet was deleted after the initial grab, the content won't show up in the second.

As an exception...  the banning of Politwoops, a service from Sunlight Foundation that archive deleted tweets from politicians. It has since been reinstated.

It doesn’t appear that Twitter is changing its terms for developers. Instead, the company has reached an agreement with the Sunlight Foundation and the Open State Foundation, another group involved in versions of Politwoops around the world, to offer the an exception.



If you run the live stream, you can collect your own (you'll get more tweets than you can deal with).
edit: I can add that if you want to 'search' for all tweets, you can use 'lang:en' as your search parameter. See here. Then you can loop over desired langauges.
"
tool request - How might I go about visualising historical temperature CSV data?,"
Data
Here is my take on it: I use R and its IDE RStudio.
The hard part, cleaning the data, is luckily done. Sharing the CSV via a dropbox link is not bad. The file is well structured. To improve it you could add a licence and provide a bit more information about the source. For more information see our certificates.
If you want to publish in a more ""professional"" way, you can use a platform such as Socrata or datahub.
Analysis
Use the power of visualisation to get a sense of the data. For example, three histograms across all years. 

Then I would calculate summary statistics for groups such as years or months. That should inform you about outliers.
We can also look at a calendar visualisation. 

The winter in 2010 seems to be particularly cold. Winter is coming...
Now that you have a sense of the data, you can explore ideas of how to create an algorithm that finds interesting patterns and outliers.
Code
I have run similar analyses before therefore I have some code that I was able to re-use.
We share all of our code on GitHub. The relevant repository is here:
 https://github.com/theodi/R-playground/tree/master/weather-centuries
This is the syntax file for the graphics.
(Note that calendarHeat.R is from Paul Bleicher.)
"
government - Demography vs. political preference data sources,"
Two resources you might consider are:

The European Social Survey
The Comparative Study of Electoral Systems

"
government - Understanding City Budgets,"
open budget and open spending are what you are looking for:
https://github.com/adstiles/openbudgetoakland
https://openspending.org/
openspending csv format is basic, but still quite confusing, at least for me:
you only need three columns: date, amount, unique id. from what i can tell, there is no way to automate this, and you're going to have to literally get all up in the guts of budget documents, and rip out what you need. i created a google doc you can see/copy here:
https://docs.google.com/spreadsheet/ccc?key=0Aq0hZevysGfkdDNUVjVwazJ0M0NSZm9MdVBpTkhJdEE&usp=drive_web#gid=0 
there's more to openspending than three columns, plus my explanation is quite vague, so loading data into openspending in-depth guide here:  https://docs.google.com/document/d/1YBXX6du4rOV6OutZncT7gyJeOA7zHml3cC1TtWJW65w/edit 
openspending is where you want to submit actual spending, so final budgets, and openbudget is where you want to release budget submissions, proposals, etc.  
"
data request - Population Within Radius,"
I have not done this before, but I do not think you can get exactly what you want (population + GDP) within a radius. You can get population/GDP/demographics down to a census tract. Here is what I would suggest as a rough method (in pseudo code), assuming your radius stays within a US county.

Get the census tract KML datasets from US Census.
For the point of interest (POI)
A. Determine which county the POI is in.
B. Determine which census tracts are in that county.
For each census tract determine if the census tract polygon is following within the circle defined by the POI and radius.
A. If so, include the population and other data as part of the radius
B. If not, determine if the polygon intersects the circle.
C. If so, use a rough approx. of how much of the population data for the census 
     tract to include (e.g., 50%).

The US Census KML datasets can be found here:
http://www2.census.gov/geo/tiger/KML/2010_Proto/
"
data request - Numeric facts database?,"
I think the highest quality database you can find is wikipedia. Now bear with me for a moment: for every article wikipedia usually provides a very vast set of numeric facts along with the textual facts. The only thing you have to do is to download a wikipedia dump and using the markup language that wikipedia uses, extract the numeric facts.
"
data request - How can I get a list of currencies from Wikidata?,"
Yes, there is.
There are several ways to get this list:

You can, in Wikidata website, look at every entity that has a link to currency (Q8142): https://www.wikidata.org/wiki/Special:WhatLinksHere/Q8142. The problem of this is that you will have some entities that makes a link to currency other than instance of.
The best way is to use an external tool called AutoList, developed by a German guy called Magnus Manske. The link of his tool is here:
https://tools.wmflabs.org/autolist/autolist1.html

This tool uses a query language WDQ. Its documentation is here:
https://wdq.wmflabs.org/api_documentation.html
Here is how it works:
The property ""instance of"" has the identifier P31.
The value ""currency"" has the identifier Q8142.
If you want every Wikidata entity with the condition (P31 == Q8142) equals true: 

copy-paste claim[31:8142] in the form field called Query;
click on the Run button.

"
usa - Open API for SEC data?,"
You should take a look at Rank and Filed, a relatively new (Feb 2014) site about EDGAR filings, with lots of data export options.
"
data portal - Search CSW for opendata without using an anytext filter?,"
The following python 2.7 code...
# -*- coding: utf-8 -*-

import requests

def main():

    get_records = """"""<?xml version=""1.0"" encoding=""UTF-8""?>
<GetRecords
    xmlns=""http://www.opengis.net/cat/csw/2.0.2""
    xmlns:csw=""http://www.opengis.net/cat/csw/2.0.2""
    xmlns:ogc=""http://www.opengis.net/ogc""
    xmlns:ows=""http://www.opengis.net/ows""
    xmlns:dct=""http://purl.org/dc/terms/""
    xmlns:gml=""http://www.opengis.net/gml""
    xmlns:gmd=""http://www.isotc211.org/2005/gmd""
    xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
    service=""CSW""
    version=""2.0.2""
    maxRecords=""{max_records}""
    startPosition=""1""
    resultType=""results""
    outputFormat=""application/xml""
    outputSchema=""http://www.isotc211.org/2005/gmd""
    xsi:schemaLocation=""http://www.opengis.net/cat/csw/2.0.2 http://schemas.opengis.net/csw/2.0.2/CSW-discovery.xsd"">
    <Query typeNames=""gmd:MD_Metadata"">
        <ElementSetName typeNames=""csw:IsoRecord"">full</ElementSetName>
        <csw:Constraint version=""1.1.0"">
            <ogc:Filter xmlns:ogc=""http://www.opengis.net/ogc"" xmlns:gml=""http://www.opengis.net/gml"">
                <ogc:PropertyIsLike wildCard=""%"" singleChar=""_"" escapeChar=""\"">
                    <ogc:PropertyName>apiso:Subject</ogc:PropertyName>
                    <ogc:Literal>%opendata%</ogc:Literal>
                </ogc:PropertyIsLike>
            </ogc:Filter>
        </csw:Constraint>
    </Query>
</GetRecords>"""""".format(max_records = 1) # minimal 1 !

    r = requests.post('http://gdk.gdi-de.org/gdi-de/srv/eng/csw', data=get_records, headers={'content-type': 'application/xml'})

    xml_string = r.content

    with open('for_opendata_stackexchange_com___output.txt', 'w') as file_out:
        file_out.write(xml_string)

if __name__ == '__main__':
    main()

...will result in a text file starting with...
<?xml version=""1.0"" encoding=""UTF-8""?>

<csw:GetRecordsResponse xmlns:csw=""http://www.opengis.net/cat/csw/2.0.2"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://www.opengis.net/cat/csw/2.0.2 http://schemas.opengis.net/csw/2.0.2/CSW-discovery.xsd"">

  <csw:SearchStatus timestamp=""2014-02-23T21:03:24"" />

  <csw:SearchResults numberOfRecordsMatched=""1053"" numberOfRecordsReturned=""1"" elementSet=""full"" nextRecord=""2"">
...

So the German geo data catalogue has 1053 open data sets.
"
tool request - Parsing Curriculum Vitaes,"
I'm a python fan, so that is the path I normally take.
If the CVs are in PDF format, then I use pdftotext to convert them into .txt files (without formatting).
Once in .txt files, to find email addresses, I usually split the lines into individual strings, and then look for strings that contain the '@' character. Since that may include twitter names or other noise, I then include only the strings that have a '@' character between two alpha-numeric characters.
To get phone numbers I look for clumps of numbers separated by '.', '-', or including parentheses. To get addresses I look for integers and then alpha strings. Names are usually the first alpha string in the .txt file. I have some synonym list for Skills, Education, etc.
This process goes on and on, and gets more complicated for getting sections and tagging appropriate sections.
I do the process iteratively, meaning that I scan 100 CVs in a training set. This tests my algorithm. If the CVs are from a particular industry (or academic), then my algorithm is tuned along the way to take into account different options.
edit: It's a dirty process, but it works for me. I value volume over accuracy.
If the CVs are in HTML format, like from indeed.com, then I use BeautifulSoup to parse the fields from the text. This is usually easier because the fields like Education are wrapped in an HTML tag that includes the string 'Education'. The only difficulty is being aware of all fields that may exist, since not everyone has all the sections.
"
tool request - Open Product Data sources and importing into SQL,"
Regarding your first question, Open Product Data provides guidelines for data importing which suggests using software such as BigDump to handle importing the data.
Now, to answer your second question:
Outpan is my personal project focusing specifically on creating a free database of all barcoded products. The database currently includes more than 18 million products and an API to easily access the content programmatically. As I mentioned before in another answer our top priority is to keep this completely free (hopefully even open source) and accessible for everyone.
NOTE: I am affiliated with Outpan.
"
tool request - How do I share Open Data with others on this SE site?,"
We have recently created the OpenData StackExchange organization on datahub.io.

Welcome to the Datahub, the free, powerful data management platform from the Open Knowledge Foundation, based on the CKAN data management system.
The Datahub provides free access to many of CKAN's core features, letting you search for data, register published datasets, create and manage groups of datasets, and get updates from datasets and groups you're interested in. You can use the web interface or, if you are a programmer needing to connect the Datahub with another app, the CKAN API.

OpenData meta discussion

Update 2017: I've learned that there is a 100MB limit per file (see answer), and there is an open question about how to host larger files.
"
data request - Famous people dataset,"
Wikipedia has a list of lists of people, from that you can focus on the specific lists that are of interest to you.
They also have a very functional API for collecting data, see here for details. For your lists of interest you can collect whatever data you like for individual ""famous people.""
"
data request - Free database or API of all North American businesses,"
This question was previously answered for non-profits, and I think the same answer works here at least partially.  
In addition, you can find all U.S. companies at the Securities and Exchange Commission's EDGAR site. Technical documentation is also available.
Information about Canadian companies is accessible through their open data site, but seems to be segmented by industry.
Mexico does not seem to have corporate content available publicly, but an explanation of where they are is at the Open Data Index.
There's a nice overview on Programmable about ways to access this type of information.
"
data request - Where can I find dataset around child labor in cocoa production?,"
Great question!
I'll assume a dataset doesn't exist because it makes the creation of a data-story more interesting.
Here are some steps that I would take to collect the data and start to analyze it. Remember, a question is needed before we can ask the data for answers. For that reason, I'll make up the question: ""Do cocoa producers have higher rates of child labors than other agricultural exporting countries?""

I would do some background reading. Wikipedia has a section about cocao and child labor. From this reading I learn that Ivory Coast has approx 200,000 child laborers in the cocao industry (wow). From this article and other I'll start by making a list of cocoa producing countries.
I would then make a spreadsheet with rows for each cocao producing countries and columns for data like population, Gross Domestic Product (GDP) per capita, amount of cocao exported, amounts of total agricultural exports, etc. CIA World Factbook is one way to collect data, although it doesn't break down exports as percents of GDP, or of a total amount. I'd have to dig more for a more detailed data source.
Now that I have columns of economic data, I would add columns for child labor statistics for those countries. There may exist a database but you can also manually collect data for the 10-15 countries that are cocoa producers. Here is the data for Ivory Coast. Wow, 35% child labor between ages 5 and 14 (definitions).
Now comes the fun part - playing with the data. Scatter plots are easy and a great way to explore data (see the image blow). For this concept, I would start with the econonic indicators (best option is ""cocoa production as percentage of agricultural exports"") of each country on the x-axis and the child labor stats on the y-axis. In this case, maybe I'll find that countries that have more cocoa (large x) also have high percentages of child labor (large y). In that case, maybe the scatter plot shows a linear relationship (correlation). I don't expect my first guess to be the final product, but I start to get a feeling for the data. I have to understand the data, for example, when comparing countries, it's often the case that I have to normalize by population (per capita).
Now that I'm familiar with the data, I should probably clean things up. This may mean including other agricultural countries that don't have cocoa production as a comparison. Now my scatter plots have two colors. Do these countries also have high child labor rates? Are there outliers (cocoa exporting countries without high rates of child labor)? Ask I ask these questions, it becomes essential to collect other data. And the process continues until I can pull the data together and tell a story.


"
data request - Open Source alternative to IMDB,"
Sadly, Freebase has been made ""read only"" March 31st 2015.
Alternatives are:

https://www.themoviedb.org/
Actively maintained by a large community and used by a broad range of apps under a propriatary license that states free use:

TMDb is committed to free and open access to our APIs for commercial and non-commercial purposes. However, providing the APIs does have real costs for TMDb. For uses of TMDb APIs over a certain rate or for certain types of commercial applications, TMDb reserves the right to charge fees for future use of or access to the TMDb APIs.

http://www.omdbapi.com/
Run by Brian Fritz under a CC-BY 4.0 licence.

The OMDb API is a free web service to obtain movie information, all content and images on the site are contributed and maintained by our users.


"
data request - Free high quality geolocation database,"
check out geonames, that's the only thing that comes to mind
http://www.geonames.org/
"
JSON file with jquery - Stack Overflow,"
I think this is a question for Stackoverflow, but in the meantime I can point you to an intersting tutorial. It's pretty comprehensive.

We use jQuery’s getJSON function, which, by definition, loads “JSON-encoded data from the server using a GET HTTP request.”

"
spending - Estimate of total public expenditure from governments around world?,"
This is the type of question that wolframalpha.com is really handy for.
Enter ""total government expenditure"" in the input box, and see the results. The results also provide the information sources used.
Edit:

"
data request - Translation dictionaries,"
For free Japanese to English datasets, I think this stackoverflow question may have some answers for you: https://stackoverflow.com/questions/2716792/freely-available-dictionary-data-for-chinese-japanese-cjk-characters
The referenced JMDict project may be what you are looking for.
"
"data request - Where can I find a dataset of songs labeled with their genre, BPM and key?","
I haven't used them before. I've just had them in my bookmarks for any case.
1) http://echoprint.me/
Free: 'Use our data for whatever you want (commercial or non, research, personal use)'
2) http://the.echonest.com/
Free for non-commercial: The Echo Nest APIs are free for non-commercial purposes. To use them commercially, contact us and we will go over licensing options.
I think that both of them are services to identify a song like Shazam does. But they let you download their data or use their API.
"
"data request - Where can I find a dataset of albums labeled with their release date, band name, genre, and number of sold copies?","
I think you can find an API based on the answers to the question here.
It may be harder to find the number of copies sold, so you may need to collect from two sources and join them together. In particular, the API listing site ProgrammableWeb lists many music APIs.
"
data request - Where can I find a dataset of academic conferences?,"
I would suggest that you scrape the data on SSRN by looking by field/location/date.
"
data request - Where can I find a dataset of drones attacks?,"
I found at least one non-profit that has a website (updated link) that contains statistics based on specific country. I don't know how accurate they are because many attacks are probably still unknown. I don't know if you can download their data as a database, but I think with finite data points, you can reconstruct the data set yourself.
For the optional stuff:

Demographics: are sort-of there (Militants killed, Civilians killed, Unknown killed, Target organization)
Reason for attack: you may have to dig up media stories for specific attacks and then join that info with the data set
Organization/country piloting the attack: this website has organization and is sorted by country.

I would start by reaching out to this organization (or any other similar ones) and ask if you can take an export of their data set. They use a Creative Commons license, so they encourage non-commercial use with attribution.
"
"data request - Dataset with biggest events in the world/region, reporting number of attendees, geolocation and date","
Here's a slightly off-topic, but perhaps interesting pick from Wikipedia on the (estimated) Largest peaceful gatherings in history, mainly for religious occasions. Some examples:

An estimated 20 to 25 million people visited the shrine of Husayn ibn Ali in Karbala, Iraq during Arba'een in December 2013.
An estimated 4.2 million people attended a concert given by Rod Stewart in Rio de Janeiro, Brazil on 1996-12-31.
An estimated 1.25 million people attended a Papal mass given by Pope John Paul II in the Phoenix Park, Dublin, Ireland on 29 September 1979.

Similar lists by topic could get you started on multi-sport evens (like the Olympic Games), running events, the largest concerts ever...
"
"What can ""open data"" with a CC-BY-NC-ND (Creative Commons Attribution NonCommercial NoDerivatives) license be used for?","
I don't agree with your conclusion completely in one respect:
I think there is a scenario where you can use CC BY-NC-ND (version 3 or 4) data and still stick to the license restrictions if you:

Use it for a non-profit/ non-commercial use case such as a web offering by a public body or an NGO
You do not change any of the triples in this data set
You do not extend the data set by triples which use their ontology (classes, properties)

I think the third restriction might be a little bit too cautious but I just want to make sure there is really no potential infringement to the license.
If this is assured, I think there is no problem in importing this data together with your data in a database or a triple store as this is not a derivation of the external data set but merely a physical representation of the data.
"
transportation - Free database of vehicle data and images,"
For the United States, FuelEconomy.Gov provides a seemingly comprehensive database of vehicles. You can use the API to pull down select vehicles or all vehicles in the database. In is primarily to communicate fuel economy, but a number of other vehicle features are also reported (transmission, engine type, etc.).
Full disclosure: I'm the developer of an R package aimed at extracting data from the API.
"
telecom - Any CDR (call data record) dataset?,"
Try with d4d challenge from orange Senegal:
http://www.d4d.orange.com/en/home
""2.5B anonymized records of 5 million mobile phones""
"
finance - Banking open data from Credit Agricole?,"
I think you are looking for the Crédit Agricole API, which has a page here. As far as I can tell, it's designed for app designers so each account would require individual authentication. Maybe
There are tons of news stories about it, check out examples here, here or here.
"
data request - Vehicle Miles Traveled in New Jersey before 1981,"
National statistics for miles traveled on different roads and for different uses have been compiled for the United States by the Department of Transportation.

Historical listings from 1936-1995
Detailed listings from 1995-present
National Household Travel Survey (online data extraction tool and tables)
Specific to New Jersey (after 1995)

Even at the New Jersey state level, the statistics only go back to 1995, but they are complete for miles traveled.
"
best practice - A web API user's guide for free and open data,"
APIs are often offered by websites so that developers can use the web-based data for apps, without having the uncertainty and difficulty of scraping the HTML. But it's not necessary to use the data to build apps, and this means that APIs can be a great source of data for research and analysis. Just to name a few types of API data: weather forecasts, historical stock prices, the twitter live-stream, wikipedia page views, ... and the list goes on and on.
Full list of APIs: ProgrammableWeb has a comprehensive list of APIs (more than 10,000). The list is so comprehensive that it's hard to know where to start. In the answers below you'll find details about popular APIs as well as some instructions for how to connect to data.
Authentication: Some APIs are free and open and don't require authentication, although they will often block your IP address if you make too many requests or request too fast. Other APIs require authentication but are free for basic usage. Each API has its own terms of service, and can restrict unlimited access by either capping the total calls per time period (i.e. 1000 per day), or by using a rate-limiting quota (i.e. 1 call per second).
Data formats: Data from APIs usually flows via a RESTful interface, and common formats are JSON, XML, and sometimes CSV (comma separated values).
Software and code: With a correct URL/URI, anyone can access the raw data from the browser (for example, JSON, XML, or even a CSV file). Here is a handy overview for non-programmers. Programming skills are usually needed to repeat or automate a process. There are some open source tools such as KNIME that allow non-programmers to connect to a RESTful interface and then collect and analyze data. For Python, see the Open Data Python Guide for examples of how to read and use the data files described above.

Here are just a handful of examples of the diverse and useful web-based API data
Wikipedia page views:

stats.grok.se

Wikipedia page views are useful for many things, including predicting changes in a stock price (paper)

A flood of views to a company's Wikipedia page may be a sign that their stock price is about to plummet. (news article)

You can access the raw data without any authentication, either by download or via an API. 
For the API, you construct a URL like this: http://stats.grok.se/json/en/201306/Data , which would give you the page views in JSON format for the article 'Data' from June, 2013.
It helps very much to first verify that you page and title exists. It does.
Sample python code to access page views for two articles ('Advisor' and 'Adviser') for all months between 2008 and 2014 (my reason). Note that days that don't exist (i.e. June 31st) will return 0 (zero) page views.



Weather data (historical):

Wunderground.com 

500 free API calls per day (with authentication), and offers historical detailed daily weather data going back to 1997 for my location (details).
International weather, so it is not only restricted to the US.
Data comes in JSON format (example data, click ""show response"").
Sample python code for current and historical weather.



Stock quotes

Markit On Demand

Free and authentication-less API for obtaining stock quotes and historical data
Documention
example GET request with JSON output



The Internet Archive

Wayback Machine

Archives HTML from old websites - example: NY Times from Feb 24, 2001
API Documentation - No authentication needed (!)
Example python code that finds historical New York Times front pages that include a certain string, and prints a link to those archived pages.



Google Maps Geolocation API

Pass search string and receive latitude, longitude and other data.
100 free requests per 24 hours (no authentication required)
Example lookup
Python code sample


"
data.gov - Federal Holiday Calendar Service,"
Here are a few things that might be helpful.

Federal holidays through 2020
Current status with XML and RSS feeds

Alerts and hurricane warnings are also listed.
"
data request - Drink Recipe Database,"
The Wikimedia project Wikibooks has a book on bartending, including a full chapter of cocktail recipes. Not a database, but pretty comprehensive and possibly a good start for a proper database.
"
computing - A resource for wearable data?,"
Kaggle had a competition that involved ""biometric data,"" which came from sensors in people's cell phones.  The data is available at http://www.kaggle.com/c/accelerometer-biometric-competition/data
"
data request - Where can I find a dataset that lists all the businesses and their addresses operating in a given city?,"
I would look into Google Places API for this. You have to register for an API key and the quota is limited to 100,000 requests a day (plus some multipliers on some requests), but it's a rather verbose database.
Example:
Here is a request URL to get all ""food"" places within 500m of the specified latitude-longitude that has the word ""harbour"" in it:
https://maps.googleapis.com/maps/api/place/nearbysearch/json?location=-33.8670522,151.1957362&radius=500&types=food&name=harbour&sensor=false&key=AddYourOwnKeyHere

The response is JSON so you will need to do a little parsing unless you use their JavaScript API or use some other wrapper.
If you're using Java, I actually developed my own Java wrapper around this API which I believe successfully maps the API 1 to 1 if it fits your purposes: https://github.com/windy1/google-places-api-java
With this library you can just do something like:
GooglePlaces client = new GooglePlaces(""apiKey"");
List<Place> places = client.getNearbyPlaces(lat, lng, radius, GooglePlaces.MAXIMUM_RESULTS);
for (Place place : places) {
    System.out.println(place.getName());
    System.out.println(place.getAddress());
}

Hope this helps!
"
web crawling - Is it legal to crawl research papers from ACM/IEEE?,"
From the ACM terms of usage page

To copy otherwise, to republish, to post on servers, or to
  redistribute to lists, requires prior specific permission and/or a
  fee. Send written requests for republication to ACM Publications,
  Copyright & Permissions at the address above or fax +1 (212) 869-0481
  or email permissions@acm.org.

Thus, I believe that you shall contact with them before doing anything else. Also, without looking for the copyrights on IEEE, something similar probably would be the case.
"
"data request - Product database for firearms, guns, ammo, weapons, munitions?","
SIPRI (Stockholm International Peace Research Institute) has some databases available on weapons that were traded between countries or political groups. I think the weapons are going to be military-grade, so I'm not sure how useful for your question. Also, the data is there but quiet buried under manual extraction process and a format that is not easily machine readable.

SIPRI Arms Transfers Database


Shows all international transfers in seven categories of major conventional arms since 1950, the most comprehensive publicly available source of information on international arms transfers.

The data at this level is aggregated, so for particular weapons, we have to go one level deeper. 

Trade registers


Provide information on each deal included in the database. Information provided includes, inter alia, the suppliers and recipients, the type and number of weapon systems ordered and delivered, the years of deliveries, and the financial value of the deal.

The screen should look like this:

The exhausting effort will be to create all the combinations of countries, but you can narrow the search to countries that manufacture weapons as the supplier.
Then, when you've gone through all ∞ combinations, the data turns out to be in RTF files (which you can convert to text and then parse and structure).
Here's a screenshot of the ""raw"" data.


This is a good task for a web scraper like Python's mechanize, scrapy, etc
"
data request - Where can I find a database of hotel property locations?,"
There is a long discussion from 2009 on StackOverflow which you may find helpful.
In particular, I'd check out the Expedia Affiliate Network or try to get TripAdvisor access. 
... And maybe api.hotelbase.org will come back some day.
"
"data request - Searching for a database for spaceships, satellites, drones, etc.?","
I would guess that scraping lists an timelines linked on the summary page Spaceflight lists and timeline (Wikipedia) will cover most satellites, probes and drones operational today and in the past.
"
computing - Data sets from motion capture?,"
Here is a source of many varied subjects, motions, and motion categories: CMU Graphics Lab Motion Capture Database
"
medical - Diabetes patient record data sets?,"
Your question is so general it's hard to be sure I'm answering it correctly. More detail would be quite helpful. Here's a stab, though:
If you're looking for datasets that are already open and are fairly standard social science data, I would look at what ICPSR holds. They have several datasets, in particular this one, contributed by Jens Ludwig at UChicago.
However, none of the files they have probably match what it sounds like you're looking for: in-depth logs from monitoring devices. On that I don't know of any data that's available, at all; to my knowledge, monitoring devices (at least in the US) don't tend to be wired at all.
"
images - Is it legal to make an open face recognition database of public people like celebrities?,"
Why wouldn't it be?
You can take pictures of people in public and post them online legally in the U.S..  
A better question around this topic is:
""What are you planning to do with an open face recognition database of public people/celebrities?""  
And an even better question for this topic:
""Are your plans involving creating an open face recognition database of public people/celebrities ethical and moral?"" 
"
education - Where can I find data on university expenditures?,"
Every year the US Department of Education requires all accredited post-secondary education institutions to complete the IPEDS survey. It's huge amount of data. You can get canned and customizable downloaded datasets here. I've used this site a lot. 
http://nces.ed.gov/ipeds/datacenter/
The datasets contain information on past, current and projected costs of education per institution, including breaking it down by in-state, out-of-state, foreign, tuition, books, fees, on and off campus housing, etc.
"
uk - When is open data not open?,"
All open data are good, but some are better.
In a colloquial way: open means that everyone can use it, for any purpose. The idea is that you have as few technical, financial and legal barriers as possible.
Quite a popular definition is the one from the OKF:

“A piece of data or content is open if anyone is free to use, reuse, and redistribute it — subject only, at most, to the requirement to attribute and/or share-alike.”

I believe this becomes more clear when we enter the ""grey areas"":

Open usually means free. However, sometimes it may include a nominal fee when the publication of data is costly.
Open usually avoids a non-commercial licence. Excluding commercial activities brings the uncertainty of the exact definition of ""non-commercial"" and its potential legal ramifications.
Twitter is also somewhere in between. Some people argue it's ""open"" (e.g J. Gurin in Open Data Now) perhaps because of its public nature. However, there are serious restrictions of who and how you can use Twitter data.

Glasgow
On their site they write

Glasgow City Council data is now open by default, freely and easily
  accessible to all.

Unfortunately, the link to ""open by default"" goes nowhere at the moment. The licence would explain what you can and cannot do with it. I would assume (and I may be wrong) that the Open Government Licence applies. It is relatively generous and easy to read.
"
tool request - Collect a list of open data systems,"
I've compiled a fairly extensive list of open government portals both in US and around the world in the last few days. I am working on code to support crowdsourcing the catalog. I also have a CSV version for download. There is about 700 sites listed so far:
ONLINE CATALOG: http://www.opengeocode.org/opendata/
DOWNLOAD: http://www.opengeocode.org/opendata/opendata.csv
I've setup the catalog for crowdsourcing, so feel free to send us your suggested portals to add to the catalog.
"
data request - Popular News Database,"
The New York Times has a web-based archive of news stories going back to 1851. Headlines are available without being a paid member. I'm sure other individual media companies will have a similar archive, but I'm only familiar with the NYTimes one.
After searching a time range, we notice that the URL of the results page can be used to create a scraping alogirthm.
http://query.nytimes.com/search/sitesearch/#/*/from19140101to19140131/

contains results from Jan. 1, 1914 to Jan. 31, 1914. You can even a summary of each article and download the full PDF!
Unfortunately, it's not a neat and clean database... Here is a sample scraping algorithm I wrote in order to get the Google results count from the HTML page (link). The important part is here (download and parse)
text = requests.get(url).text # get google html
m2 = re.search('About ([0-9,]+) results', text) # search for results

You may find that the number of searches is limited (so be gentle with the scraping).
"
data request - Datasets with Practical Applications,"
My opinion: Most of machine learning isn't wasting cycles but asking a question and then designing a path to an answer.
If you want to 'waste' cpu cycles, consider running one of the distributed computing projects. Folding@Home is one of the best known examples.
If you want to find something new, then find a topic that interests you and then ask a question that can be answered by collecting data. There are seemingly infinite projects and data for those projects exist(s) all around us. Sometimes it's as simple as running a code or collecting web-based data, like N-grams or Search Trends. Other times it means downloading a 'traditional' data set and creating a unique analysis or new ways to visualize the data. And other times it requires being a 'data journalist' who gets dirty digging through data to uncover a story.
If you want to see what people are doing outside of Kaggle, check out datatau.com, /r/machinelearning, etc. Or get involved with the Open Knowledge Foundation. Or find local people and start to see where there is local need.
"
tool request - Do you know a technique for text mining judicial decisions?,"
If you are interested to just scrape some data from web pages, then like @magdmartin wrote, you just need to write some code to download the HTML (python requests package) and then to parse the HTML (BeautifulSoup in this case).
import requests
from bs4 import BeautifulSoup
import re
r = requests.get('https://www.canlii.org/en/ca/scc/doc/2007/2007scc4/2007scc4.html')
soup = BeautifulSoup(r.text)
print soup.find(text=re.compile('Present:')).encode('utf-8')

This python 2.7 code finds the names of the judges based on finding the string 'Present' in the HTML-encoded text.

The way I approach a problem like this is:

Find one website that has as many court cases as possible.
Get familiar with the HTML. In Firefox, control+U is how to view the page source.
Develop a scraping algorithm that starts small (like the code above that finds the judges' names), and then gets more ambititious as the code develops.
Deploy your code to as many pages (court cases) as possible (not manually, but with a crawling algorithm).

Also, always look for APIs that would allow you to automatically download the data you want without the messiness of HTML scraping.

If you start to do full-blown text-mining (analysis on the text), you will need a legal ontology. One example is here.
"
"usa - List of data sources at the state, county and zip / zcta level","

Health

Behavioral Risk-Factor Surveillance System (BRFSS) - A health-related survey that asks respondents about health and disease risk factors.

Unit of Analysis: County

Area Health Resource File (AHRF) -  A compilation of Census Bureau demographic information, along with information about hospital utilization, health professionals, and natality/mortality.

Unit of Analysis: County

Health Professional Shortage Areas (HPSA) - An estimation of areas where the population may be underserved by healthcare systems.

Unit of Analysis: Varies from Census Tract to County

SNAP Participation -  Enrollment Data for the Supplemental Nutritional Assistance Program by Year.

Unit of Analysis: County

Food Deserts - An analysis of populations that are far from stable food sources, and are thus reasoned to have low access to supply.

Unit of Analysis: 2000 & 2010 Census Tract

Food Security - Indicators of inequality of access to food sources.

Unit of Analysis: County


Crime

National Incident Based Reporting System NIBRS - ""The National Incident Based Reporting System (NIBRS) is an incident-based reporting system for crimes known to the police. For each crime incident coming to the attention of law enforcement, a variety of data are collected about the incident. These data include the nature and types of specific offenses in the incident, characteristics of the victim(s) and offender(s), types and value of property stolen and recovered, and characteristics of persons arrested in connection with a crime incident.""

Unit of Analysis: County


Education

Integrated Postsecondary Education Data System (IPEDS) - the primary source for data on colleges, universities, and technical and vocational postsecondary institutions in the United States.
Common Core of Data (CCD) - ""The Common Core of Data (CCD) is the Department of Education's primary database on public elementary and secondary education in the United States. CCD is a comprehensive, annual, national statistical database of all public elementary and secondary schools and school districts, which contains data that are designed to be comparable across all states.""

Unit of Analysis: Individual Institutions

Private School Survey (PSS) - ""The target population for the survey consists of all private schools in the U.S. that meet the NCES definition (i.e., a private school is not supported primarily by public funds, provides classroom instruction for one or more of grades K-12 or comparable ungraded levels, and has one or more teachers. Organizations or institutions that provide support for home schooling without offering classroom instruction for students are not included.).""

Unit of Analysis: Individual Institutions


Economic

Longitudinal Employer-Household Dynamics (LEHD) - This is an extremely useful dataset. Based on data from the Quarterly Census of Employment and Wages, the information published comes in three forms: The LEHD Origin-Destination Dataset (LODES), the Workforce Area Characteristics Dataset (WAC), and the Residence Area Characteristics Dataset (RAC). The WAC posts demographic information on jobholders by the area that their job is located; you will be hard-pressed to find this level of detail ANYWHERE else. The RAC posts demographic information on jobholders by the area that they live in. The LODES indicates the combination of the residence and worksite locations by each jobholder's job.

Unit of Analysis: 2000 & 2010 Census Block

County and Zip Code Business Patterns (CBP / ZBP) - ""County Business Patterns (CBP) is an annual series that provides subnational economic data by industry. This series includes the number of establishments, employment during the week of March 12, first quarter payroll, and annual payroll. This data is useful for studying the economic activity of small areas; analyzing economic changes over time; and as a benchmark for other statistical series, surveys, and databases between economic censuses. Businesses use the data for analyzing market potential, measuring the effectiveness of sales and advertising programs, setting sales quotas, and developing budgets. Government agencies use the data for administration and planning.""

Unit of Analysis: Zip Code, County, MSA, State

Quarterly Workforce Indicators (QWI 1,2) - ""The Quarterly Workforce Indicators (QWI) are a set of economic indicators including employment, job creation, earnings, and other measures of employment flows. The QWI are reported using detailed firm characteristics (geography, industry, age, size) and worker demographics information (sex, age, education, race, ethnicity).""

Unit of Analysis: County, MSA, State

Local Area Unemployment Statistics (LAUS) - A predictive model of county-level unemployment by year. It is based on an estimation using the Current Population Survey, state payroll, and state unemployment insurance sources.

Unit of Analysis: County


Migration

SOI Tax Stats - County-to-County Migration Data Files - This source of information is unique from Census Bureau Migration data because it estimates emigration from areas as well.

Unit of Analysis: County



"
data request - Where can I find the dataset that has all open research publications (just titles and abstracts)?,"
Restricting this question to ""Computer Science"" (as suggested in the comments) and reducing ""all possible"" to the ones which were actually published, one resource is The DBLP Computer Science Bibliography. Note that this source follows ODC-BY 1.0.
The DBLP has a search interface with an attempt to identify coauthor communities. They also offer bulk downloads in XML format.
Some other research subjects have good sources, such as PubMed for biology, medicine, etc.
"
best practice - What are good examples of open data dashboards?,"
You already listed a bunch. There's also

Philadelphia's Open Data Pipeline
Whitehouse Scorecard

"
biology - Cannabis Data Set,"
Try the sites linked at the bottom of http://www.theplantlist.org/tpl1.1/record/kew-2696480, particularly NCBI one.
Just about the seeds: http://data.kew.org/sid/SidServlet?ID=4439&Num=5E7
"
data request - Database of self-inflicted injuries?,"
Sounds like very interesting research.  
There is a database of self-inflicted injuries and suicide from the U.S. Centers for Disease Control and Prevention.  Faststats has both summary and detailed information about this.  There is a related set of data on unintentional injuries that may also be helpful.
"
usa - Automobile accident data in the US,"
California Polytechnic State University has a downloaded dataset on fatal accidents on the national highway system for the year 2007:
https://wiki.csc.calpoly.edu/datasets/wiki/HighwayAccidents
Data.Gov has a number of datasets on accident data by state:
https://explore.data.gov/catalog/raw?tags=crash
The NHTSA has summary statistics for 2012 on a state by state basis here:
http://www-nrd.nhtsa.dot.gov/departments/nrd-30/ncsa/STSI/USA%20WEB%20REPORT.HTM
The Insurance Institute for Highway Safety has accident/safety ratings per make/model/year, but you will have to scrap it off their site:
http://www.iihs.org/iihs/ratings
Some of the city level open data portals also publish traffic accident datasets:
Denver: http://data.opencolorado.org/dataset/city-and-county-of-denver-traffic-accidents
Seattle: https://data.seattle.gov/Public-Safety/Traffic-Accidents/7ayk-pspk
The UK data.gov publishes accident/road safety datasets from 1979 to present here:
http://data.gov.uk/dataset/road-accidents-safety-data
I put together an online catalog of sites covering traffic surveys and traffic accidents: http://www.opengeocode.org/opendata/traffic.php
A list of links from albert,
NCSA Publications & Data Requests http://www-nrd.nhtsa.dot.gov/Cats/index.aspx
 you probably want this guy:
http://www-nrd.nhtsa.dot.gov/Cats/listpublications.aspx?Id=F&ShowBy=DocType 
super in-depth stats
http://www-fars.nhtsa.dot.gov/Main/index.aspx 
crash stats archive
http://ai.fmcsa.dot.gov/CarrierResearchResults/Archives.asp?p=23 
stats and facts
http://www.fmcsa.dot.gov/facts-research/art-stats-facts.htm 
nhtsa data
http://www.nhtsa.gov/NCSA 
federal trans safety somtehing
http://www.fmcsa.dot.gov/ 
national highway traffic safety administration
http://www.nhtsa.gov/NCSA 
nhsta crash stats
http://www-nrd.nhtsa.dot.gov/cats/listpublications.aspx?Id=F&ShowBy=DocType 
more crash stats
http://ai.fmcsa.dot.gov/CrashProfile/CrashProfileMainNew.asp 
http://www.fmcsa.dot.gov/facts-research/art-stats-facts.htm 
http://www-nrd.nhtsa.dot.gov/Cats/listpublications.aspx?Id=F&ShowBy=DocType 
http://ai.fmcsa.dot.gov/CarrierResearchResults/CarrierResearchContent.asp 
"
data request - Looking for a list of major cities of the world,"
Geonames has a tsv with  data of the cities with more than 15k inhabitants. You only need to filter the ones you are interested in. The tsv has a column with the population (the 15th one) so it should be quite easy to get them.
You can download it here
"
Data of vehicle traffic,"
You can find plenty of summary data, but I have not seen any publicly available raw counter data. Here's some summaries:
Federal Highway Administration
Traffic Volume Trends is a monthly report based on hourly traffic count data reported by the States. These data are collected at approximately 4,000 continuous traffic counting locations nationwide and are used to estimate the percent change in traffic for the current month compared with the same month in the previous year.
http://www.fhwa.dot.gov/policyinformation/travel_monitoring/tvt.cfm
Arizona Dept of Transportation
The annualized average 24-hour volume of vehicles at a given point or section of highway is called a traffic count. It is normally calculated by determining the volume of vehicles during a given period and dividing that number by the number of days in that period. 
http://www.azdot.gov/planning/DataandAnalysis
California Dept. of Transportation
The Traffic Data Branch is responsible for the collection and dissemination of historical volumes (counts). We also produce the Mobility Performance Reports.
TRAFFIC COUNTS, also called Traffic Volumes, are available in various formats, and are only for the State Highway System. Highways are signed as Interstate, California State Route, or United States Route.
http://traffic-counts.dot.ca.gov/
Colorado Dept. of Transportation
This is the access point to information frequently used for transportation planning and project development. Information is provided on current and projected traffic volumes, state highway attributes, summary roadway statistics, demographics and geographic data.
http://dtdapps.coloradodot.info/otis
Florida Dept of Transportation
Welcome to the Florida Department of Transportation's Traffic Information site. This site provides statistical traffic information for Florida's State Highway System.
http://www.dot.state.fl.us/planning/statistics/trafficdata/
Indiana Dept of Transportation
INDOT Interactive Traffic Count Map showing the annual average daily traffic (AADT). 
http://dotmaps.indot.in.gov/apps/trafficcounts/
Maine Dept of Transportation
Traffic Monitoring is responsible for the collection of all types of traffic data including traffic volumes, vehicle classification, turning movements and special studies as requested by the Department.  The reporting of traffic volumes is accomplished through two distinct methods involving the Continuous Count and Coverage (i.e. short term) Count programs.
http://www.maine.gov/mdot/traffic/tc.htm
Massachusetts Dept of Transportation
The Massachusetts Highway Department conducts an annual traffic data collection program. This data is available online by autoroute and city/town list or as an interactive map. You can view data for a specific town from the Town Index, for numbered routes from the Numbered Route Index, or you can use the interactive map to browse data by area. You can also download the complete spreadsheets 
http://www.mhd.state.ma.us/default.asp?pgid=content/traffic01&sid=about
Michigan Dept of Transportation
2012 Average Daily Traffic (ADT) Maps
https://www.michigan.gov/mdot/0,1607,7-151-9622_11033_11149---,00.html
Minnesota Dept of Transportation
Thousands of traffic counts are collected on Minnesota roadways each year. This information is used to produce volume, classification, speed and weight data as well as traffic forecasts, reports, maps and analysis. 
http://www.dot.state.mn.us/traffic/data/
New York State Dept of Transportation
The Highway Data Services Bureau is responsible for the collection and dissemination of information on the extent, use and condition of the public roadway system in the State of New York. The Bureau consists of three sections: Highway Data, Traffic Monitoring, and Pavement Data.
https://www.dot.ny.gov/highway-data-services
North Carolina Dept of Transportation
Annual Average Daily Traffic (AADT) Traffic Volume Map presents the traffic average for the year at specific points on North Carolina highways. Data is collected at more than 40,000 locations throughout North Carolina using Portable Traffic Count Stations. AADT map is typically published at the later part of the summer.
http://www.ncdot.gov/projects/trafficsurvey/
Ohio Dept of Transportation
Traffic Count Information & Maps includes Traffic Survey Reports and Traffic Survey Flow Maps. The Traffic Survey Reports list an estimate of Annual Average Daily Traffic (AADT) volumes separated by cars (Pass &A Com’l) and trucks (B&C Com’l) for all Ohio Interstate, US and State highway system routes.
http://www.dot.state.oh.us/Divisions/Planning/TechServ/traffic/Pages/Traffic-Count-Reports-and-Maps.aspx
Oregon Dept of Transportation
The Transportation Systems Monitoring (TSM) Unit has the mission to formulate a system to collect and process traffic related data on Oregon´s Highways. TSM provides traffic volumes, flow maps, trends, manual counts and vehicle class on state highways to Federal, State, Local, private and public constituents.
http://www.oregon.gov/ODOT/td/tdata/Pages/tsm/tvt.aspx
South Caroline Dept. of TransportationThe Traffic Polling and Analysis System provides traffic data and graphs from traffic counting devices on South Carolina's Highways. This system is intended for the general public to view current and historical traffic information during hurricane evacuation events
**
http://www.scdot.org/getting/trafficcounts.aspx
Tennessee Dept of Transportation
An AADT traffic volume is used throughout the project planning process to provide projected volumes of traffic.  It is based on a 24 hour, two directional count at a given location.  This raw traffic volume is then mathematically adjusted for vehicle type, determined by an axle correction factor.  Then this volume is statistically corrected by a seasonal variation factor that considers time of the year and day of the week. 
http://www.tdot.state.tn.us/projectplanning/adt.asp
Washington State Dept of Transportation
The Annual Traffic Report (ATR) summarizes traffic data maintained by the Washington State Department of Transportation for the State Highway System. The report includes Annual Average Daily Traffic (AADT) figures and truck percentages, when available, for locations where data collection has occurred within the past four years.
http://www.wsdot.wa.gov/mapsdata/travel/annualtrafficreport.htm
Wisconsin Dept of Transportation
Wisconsin Department of Transportation (WisDOT) traffic counts are now part of an interactive map that allows you to view counts anywhere in the state.
http://www.dot.wisconsin.gov/travel/counts/
Chicago, IL TransportationAverage Daily Traffic (ADT) counts are analogous to a census count of vehicles on city streets.
** 
https://data.cityofchicago.org/Transportation/Average-Daily-Traffic-Counts/pfsx-4n4m
Arlington, VA TransportationArlington County Regular Traffic Count Data is available in Portable Document Format (PDF).
** 
http://www.arlingtonva.us/Departments/EnvironmentalServices/dot/traffic/counts/EnvironmentalServicesCounts.aspx
Delaware Valley, PA Delaware Valley Regional Planning Commission
http://www.dvrpc.org/webmaps/trafficcounts/
British Columbia, Canada Ministry of Transportation
The Ministry of Transportation and Infrastructure's Traffic Data Program monitors traffic volumes at several locations throughout the province. This information is used by ministry staff to help support planning, design, construction, and operation of the Ministry road network. 
http://www.th.gov.bc.ca/trafficData/
United Kingdom (UK) Dept of Transportation
Traffic counts provides street-level traffic data for every junction-to-junction link on the 'A' road and motorway network in Great Britain.
http://www.dft.gov.uk/traffic-counts/
New South Wales, Australia Roads and Maritime
Traffic Volume Data books, sorted by Roads and Maritime Services regions, containing the Annual Average Daily Traffic (AADT) volumes for various roads. 
http://www.rms.nsw.gov.au/trafficinformation/downloads/aadtdata_dl1.html
"
internet - What is the amount of open data that is currently available over the web?,"
Some projects such as the Open Data Monitor aim to answer this question. For convenience, I split the answer into four sections.
Governments
The amount of open data released by governments is reasonably documented. Perhaps not in the sense of the exact amount of Gb.
Examples are:

The Open Data Index by the Open Knowledge Foundation
The Open Data Barometer by the The Web Foundation & The Open Data Institute.
Catalogues of datasets such as datacatalogs.org or the World Bank.

Businesses
This is much harder to quantify. Indicators are:

The GovLab are running a survey, the Open Data 500, analysing companies.
Data about businesses is published with a dual-licence by OpenCorporates. At the moment the database contains over 60 million entities.
Many businesses publish data, often via an API. However, in most cases not as open data. An example is Twitter, where the licence and availability of the ""hose"" is substantially limited.

Institutions
Institutions are publishers of some of the largest datasets on the web. For example:

The 1,000 Genome project is one of the largest open datasets with an extreme size of 260Tb.
The Tiny Images dataset consists of almost 80 million images stored in the form of large binary files. They are used in an academic paper on scene recognition from 2008.
Researchers at the Measurement Lab (M-Lab), who publish over 750Tb of data under a CCZero licence.

Individuals
The amount of personal data that individuals make available on the web is, to the best of my knowledge, negligible.
"
data request - Full list of LinkedIn endorsements,"
The only thing that is available right now on Linkedin is the skills from a certain user and not the endorsements. And in order to download the skills, users have to log in with auth. Unfortunately, you will not find any complete list of skills from Linkedin.
If you want to download the skills from a certain user, here is the code for python using this library.
from linkedin import linkedin

API_KEY = '.............'     # This is api_key
API_SECRET = '...........'   # This is secret_key

RETURN_URL = '.............'

authentication = linkedin.LinkedInAuthentication(API_KEY, API_SECRET, RETURN_URL, linkedin.PERMISSIONS.enums.values())

authentication.authorization_code = code

authentication.get_access_token()

application = linkedin.LinkedInApplication(authentication)

skills = application.get_profile(selectors=['skills'])

"
"data request - Intelligence, Internet usage and Ip addresses","
Going by country is pretty coarse. But as indicated in my earlier comment, I would start with the (coarse) database showing the distribution of IP addresses to countries/cities: http://dev.maxmind.com/geoip/legacy/geolite.
I would then use the World Bank datasets on economic related data per country, such as:

GDP
Economic Indicators
Gender Statistics
Education Statistics
GNI Gross National Income per capita
Health and Nutrition Indicators

http://datacatalog.worldbank.org/ 
"
"data request - ""Official"" list of business branches","
You are looking for SIC codes (e.g., Used by Bureau of Labor) and NAICS codes (e.g., Used by US Census). The NAICS is gradually replacing the older SIC system. It is updated every 5 years and is developed/maintenance in conjunction with Canada and Mexico (hence the name North American Industry Classification Standard). One good place to start is at this US Census link:
https://www.census.gov/eos/www/naics/ 
This link has some downloadable definitions:
https://www.census.gov/cgi-bin/sssd/naics/naicsrch?chart=2012
This is the BLS link describing how they have moved away from the SIC system to NAICS:
http://www.bls.gov/bls/naics.htm
Dutch SBI System (thanks @Martjin):
http://unstats.un.org/unsd/cr/ctryreg/ctrydetail.asp?id=1182
European NACE System (thanks @Martjin):
http://epp.eurostat.ec.europa.eu/statistics_explained/index.php/Glossary:Statistical_classification_of_economic_activities_in_the_European_Community_(NACE)
United Nations ISIC (thanks @Martjin):
https://unstats.un.org/unsd/cr/registry/regcst.asp?Cl=27
"
data request - publicly available spam dataset of social networks,"
From the Machine Learning Repository at UCI there are some datasets that may get you started. 

emails
SMS

For emails you may also find this StackOverflow discussion helpful, or this 300+MB dataset.
For blogs marked as spam, check out this site.
Twitter data are not allowed to be publically shared. You can share the analysis or the visualization, but not the raw data of tweets. I imagine Facebook has a similar policy.
But, you can collect more than you ever dreamed of by using the public live stream. Here is an example python code to collect many, many tweets as they are sent. There should be plenty of spam if you let it run for a couple minutes.
"
data request - Clickstream sample dataset,"
Dataset 1: 
Wikipedia Releases Clickstream Data
Wikipedia has released a data set of clickstream data for January 2015. A clickstream is the path a user requests to get to a desired web page or article by using a referer—clicking on a link or performing a search. The dataset contains 22 million referer-article pairs from the English language, desktop version of Wikipedia—just a sample of the 4 billion total requests made in January. Clickstream data is a valuable analytical tool as it can determine things like the most popular links in a web page and how users navigate through a website.
Download data from: https://figshare.com/articles/Wikipedia_Clickstream/1305770
Dataset 2:
Visualize Website Clickstream Data
These website log files contain data elements such as a date and time stamp, the visitor’s IP address, the destination URLs of the pages visited, and a user ID that uniquely identifies the website visitor.
Download data from : https://s3.amazonaws.com/hw-sandbox/tutorial8/RefineDemoData.zip
Dataset 3: 
Sample Clickstream Data
Download data from : https://gist.github.com/matthayes/4614332
"
geospatial - APIs for oil and gas well location/production data?,"
Some of the States provides datasets for oil and gas well locations:
Illinois (ArcGIS, Shapefiles)
http://certmapper.cr.usgs.gov/data/noga00/natl/spatial/doc/ilcells06g.htm
Louisiana (XML)
http://lagic.lsu.edu/data/losco/oil_gas_wells_ldnr_2007_faq.html
New York (CSV download)
http://www.dec.ny.gov/energy/1603.html
Nevada (University of Nevada, Reno - 2011)
http://www.nbmg.unr.edu/Oil&Gas/NVWellInfo.html
Texas Railroad Commission (charge ~$20/dataset)
http://www.rrc.state.tx.us/data/datasets/WellData.php
Utah
http://gis.utah.gov/data/energy/oil-gas/
Production Summaries by State:
Distribution and Production of Oil and Gas Wells by State
https://explore.data.gov/download/dwne-h7vw/XLS
ArcGIS free dataset (CC-BY) for Alaska 2008
http://www.arcgis.com/home/item.html?id=ccfcd4a89a234275a18408b9d7fe9fe0
UK on/off shore oil wells/rigs (MS-Excel)
https://www.gov.uk/oil-and-gas-digital-data-exchange-format
There are also commercial providers (paid), for example:
PetroView http://www.psg.deloitte.com/ProductsPetroviewData.asp
"
api - Climate data from stations in Canada,"
The CKAN API is documented here: http://docs.ckan.org/en/latest/api/index.html.
"
Any Open Data Sets for the (Football) World Cup (in Brazil 2014)?,"
Edit: A new initiative on the web open_data_world_cup
Edit 2: Gerald also started collecting a list of other football projects on GitHub.
Edit 3: Here is a list of the football players ages in a spreadsheet format. 
You can create data by putting the information from the wikipedia page into a more structured format.
There are plenty of tools available that make scraping easier. A starting point might be ScraperWiki, but it depends a bit on your interest and skills.
Here's a quick proof-of-concept with GoogleDocs' ImportHTML function.

"
api - Using Open Graph to see what comments are being made,"
As I said before, I made a small research about your question. Here is what I've found.
Your url give me this response:
URL: http://graph.facebook.com/?id=http://thanetstar.com/article/what-s-great-about-thanet-2
Response: {
   ""id"": ""http://thanetstar.com/article/what-s-great-about-thanet-2"",
   ""shares"": 19
}

But if I use the same call for another website, I get this response
URL: LINK
{
   ""http://www.hollywoodreporter.com/live-feed/scandal-shonda-rhimes-kerry-washington-olivia-pope-308845"": {
      ""id"": ""http://www.hollywoodreporter.com/live-feed/scandal-shonda-rhimes-kerry-washington-olivia-pope-308845"",
      ""shares"": 67,
      ""comments"": 2
   }
}

You can see that in your link, even if you have comments in the site, response doesn't include them. I don't know why. Maybe you haven't used the OG meta tags with the right way.
Also, I found that this link will return the comments from a OG object:
URL: https://graph.facebook.com/comments/?ids=http://thanetstar.com/article/what-s-great-about-thanet-2
My source: Quora
"
computing - Is there any public domain Controller Area Network trace data from a real vehicle?,"
I found this research paper and in the abstract, the authors write that they use trace data from CAN. You should contact them and ask them to share the data with you.
Here is the research paper.
"
Database with monthly climate/weather data by country,"
The UNDP gathers and reports data along the line of what you are looking for. I think its a matter of finding the right tables. They collect historic data as well as predicating climate changes to countries.
The following link is a repository of some reports and raw data for 61 countries, mostly africa, middle east and south america. Each country has a zip file (AllData). Search through the subfolders looking for time series.
http://www.geog.ox.ac.uk/research/climate/projects/undp-cp/
Note, this copy of the data is hosted by University of Oxford, Department of Geography and the environment. The time series are on a year basis (no monthly) and range from 1960 to 2006.
"
"data request - Large French dataset for NLP (not formal, rather discussions/reviews)","
I've been thinking about this question a lot and I have another solution. You correctly wrote that Wikipedia articles have too much quality due to the editing.
But, the discussion (talk) pages are full of raw text data and are not prone to being edited, or even correct.
An example for the French language Open Data page (source): 
Y a-t-il une raison pour laquelle le terme Open Data est utilisé partout dans l'article alors qu'il a pour titre Données ouvertes ? Je propose d'être uniforme dans tout l'article et d'opter pour la langue de ce wiki, c'est-à-dire le français. À moins d'une opposition claire, je vais procéder d'ici quelques jours. Dirac (d) 18 février 2013 à 19:48 (CET)

It's straightforward to download all the Discussion/Talk pages not only for articles, but also for Users, Files, Help, etc.
Download:

Wikipedia has a (massive) data dump at dumps.wikimedia.org (details). Be careful, the files are huge and won't open with most text viewers. It's best to have a programming approach for parsing the data.

What's available?
English latest, French latest.

The specific files you want are labeleled with something like enwiki-latest-pages-meta-current1.xml-p000000010p000010000.bz2. The key part is pages-meta and then some sequence for the individual files of the split data.

Parse:

The header of these XML files contains these (selected) keys:
<namespace key=""1"" case=""first-letter"">Talk</namespace>
<namespace key=""3"" case=""first-letter"">User talk</namespace>
<namespace key=""5"" case=""first-letter"">Wikipedia talk</namespace>
<namespace key=""7"" case=""first-letter"">File talk</namespace>
<namespace key=""9"" case=""first-letter"">MediaWiki talk</namespace>
<namespace key=""11"" case=""first-letter"">Template talk</namespace>
<namespace key=""13"" case=""first-letter"">Help talk</namespace>
<namespace key=""15"" case=""first-letter"">Category talk</namespace>
<namespace key=""101"" case=""first-letter"">Portal talk</namespace>
<namespace key=""109"" case=""first-letter"">Book talk</namespace>
<namespace key=""119"" case=""first-letter"">Draft talk</namespace>
<namespace key=""447"" case=""first-letter"">Education Program talk</namespace>
<namespace key=""711"" case=""first-letter"">TimedText talk</namespace>
<namespace key=""829"" case=""first-letter"">Module talk</namespace>

Wikimedia's guide to parsing the XML files, a guide to SQL import, and tutorial about how to parse with python: link and source.

"
data request - Cynthia Wessell's Dictionary of Affect,"
Well, you picked a very commercial word list so even 'finding' a copy of the dataset could lead to legal problems or results you can't publish.
But, to get a taste, there is a web-app that I found after chasing down some broken links. It's apparently endorsed by Cynthia herself. Check it out here.
Now, interestingly, the creator of that site posts a link to more info. 
And, in that more info section is a URL that may help you greatly:
dead link http://compling.org/cgi-bin/DAL_sentence_xml.cgi?sentence=This+is+a+great+web+app
replacement? https://sail.usc.edu/dal_app.php
So, there you have it, a web interface to the DAL without having the DAL itself. For a sentence in the URL with words separated by a '+' sign, you get in return XML:
<sentence>
 <word>
  <token>This</token>
   <emotion>
    <measure type=""DAL"" valence=""1.7500"" activation=""1.3333"" imagery=""1.0""/>
   </emotion>
 </word>
 <word>
  <token>is</token>
   <emotion>
    <measure type=""DAL"" valence=""1.8889"" activation=""1.1818"" imagery=""1.0""/>   
   </emotion>
 </word>
 <word>
  <token>a</token>
   <emotion>
    <measure type=""DAL"" valence=""2.0000"" activation=""1.3846"" imagery=""1.0""/>
   </emotion>
 </word>
 <word>
  <token>great</token>
   <emotion><measure type=""DAL"" valence=""2.6250"" activation=""2.1250"" imagery=""1.0""/>
   </emotion>
 </word>
 <word>
  <token>web</token>
   <emotion>
    <measure type=""DAL"" valence=""1.7778"" activation=""1.8750"" imagery=""2.8""/>
   </emotion>
  </word>
  <word>
   <token>app</token>
    <emotion>
     <measure type=""DAL"" valence="""" activation="""" imagery=""""/>
    </emotion>
   </word>
  </sentence>

"
data request - Police car chase dataset,"
Suffolk Police Department's annual reports document most of this data, but only for a couple of years. pretty sure they have it for 2006-2008 or 2009. you'll have to check, but the data is there:
http://www.suffolkva.us/spd/inside/annualreports/
"
best practice - Which namespace should be used for an open vocabulary published by a private organization?,"
I would use purl.org yes. That's weird that they're not answering. They answered pretty quickly when I tried myself. There's also https://w3id.org/ if you're looking for stable and persistent identifiers. 
The best however could also simply be to publish the vocabulary yourself. One of the issues with using URIs for the vocabulary of your data is that there's so many to choose from already: http://vocab.cc/
Instead of creating yet another vocabulary make sure the concepts you're trying to identify don't already exist. Also http://lov.okfn.org/dataset/lov/ is a good place to search for existing vocabularies.
If you feel you need to publish new URIs, then try to go with something persistent unless it's so specific for your use-case, then you should perhaps use your company's domain name.
"
data request - Analyst Estimates for Earnings on the S&P 500,"
I am not familiar with this crowd-sourced project, but they claim to be crowd sourcing estimated earnings:
https://www.estimize.com/
Their tagline is: 
The Most Comprehensive Earnings Forecasts
Crowdsourced estimates from over 4,500 hedge fund, brokerage, and independent analysts
According to Crunchbase, they are Venture funded.
Funding Received $2.6 Million in 3 Rounds from 8 InvestorsMost
Recent Funding$1.2 Million Series A on March 26, 2014
Headquarters:New York, NY
Description:Estimize is an open financial estimates platform which facilitates the aggregation of fundamental estimates from independent analysts.
Founders:Matthew Jording, Leigh Drogen
Categories:Crowdsourcing, 
"
historical - Open Data: Dataset containing records of events throughout history?,"
An application that uses a dataset like this one is Day Like Today from the OKFN Greek chapter. It makes use of DBpedia and retrieves information from Wikipedia infoboxes. For example dates of wars, deaths of popular persons etc.
Thus, I do not know a database that you can download and use, but you can use DBpedia to retrieve historical events from Wikipedia.
"
usa - Seattle City Buildings Data Set,"
In addition to the building permit database, Seattle.gov provides a dataset for historic buildings at:
https://data.seattle.gov/browse?tags=historic+register
Any building in Seattle that is a public housing or multi-family housing inspected for public housing assistance program will have info in this HUD dataset:
http://www.huduser.org/portal/datasets/pis.html
The footprints and building heights are commercially available ($1571) at:
http://market.weogeo.com/datasets/nokia-here-buildings-seattle-wa-metro-region.html
Other than that, the best way to get this information free (or near FREE) is to go to the property tax office for King County and see if they provide a electronic copy (e.g., like on a CD) for a fee.
UPDATE: I found some more links on building lot outline in major cities open data. This comes from stackexchange question: https://gis.stackexchange.com/questions/2046/where-can-i-find-building-footprint-data/2050#2050
Chicago: http://data.cityofchicago.org/Government/Boundaries-Buildings/w2v3-isjw
Seattle: http://data.seattle.gov/dataset/2009-Building-Outlines/y7u8-vad7
Bellingham: http://www.cob.org/services/maps/gis/index.aspx
Bloomington: http://bloomington.in.gov/documents/viewDocument.php?document_id=1870
Spokane: http://www.spokanecity.org/services/gis/data/
Kitsap County: http://www.kitsapgov.com/gis/metadata/
There is also some additional data for several cities across the U.S. and federal data on building permits from Data.gov.
"
data.gov - Looking for prior work parsing NCDC's Integrated Surface Data,"
Neal Lott of NOAA/NCDC  is familiar with Integrated Surface Data.   He can be reached at Neal.Lott@noaa.gov 
Peter Grimm
NOAA/NESDIS
Silver Spring, MD
Peter.L.Grimm@noaa.gov 
"
usa - Census data at every block level in US cities,"
Block level data is only available from the Decennial Census, which means that you can only get fundamental demographic and housing data. This amounts to age, sex, race, household/family structure, home ownership and home vacancy rates. Some Decennial Census tables are only tabulated at the census tract level and higher, because of restrictions based on releasing data which can personally identify respondents
Questions about poverty, income, education, etc are now collected in the American Community Survey, which only tabulates data down to the block group level. Furthermore, because it's a survey and not a complete count, for many estimates at the block group level there is a high margin of error.
"
data request - A database of drugs and their targets,"
Try the following:
DrugBank: http://www.drugbank.ca/ 
openFDA API to get some data online:   http://open.fda.gov/
"
data request - VAT Rates of EU or better all Countries,"
You can find this information in the OECD Tax Database. The rates are provided in an Excel sheet.
Unfortunately the table is confined to 33 OECD member states, among which 21 are also member of the EU.
"
data request - Language Similarity Heuristic,"
Check out the World Atlas of Language Structures.  http://wals.info/
"
data request - Download a sample database of calls in telecommunication,"
Here's a CDR sample:
MSIDN:IMSI:IMEI:PLAN:CALL_TYPE:CORRESP_TYPE:CORRESP_ISDN:DURATION:TIME:DATE 
068373748102;208100167682477;351905149071;PLAN1;MOC;CUST1;0612287077;247;12:07:12;01/01/2012 
068373748102;208100167682477;351905149071;PLAN1;MTC;CUST2;0600000001;300;12:15:09;01/01/2012 
068373748102;208100167682477;351905149071;PLAN1;SMS-MO;CUST1;0613637193;0;12:18:18;01/01/2012 
068373748102;208100167682477;351905149071;PLAN1;SMS-MT;CUST1;0612899062;0;12:21:07;01/01/2012 
065978198280;208100310191699;356008289837;PLAN3;MOC;CUST1;0612283725;90;12:00:00;01/01/2012 
065978198280;208100310191699;356008289837;PLAN3;MOC;CUST1;0613069656;82;12:02:27;01/01/2012 
065978198280;208100310191699;356008289837;PLAN3;MOC;CUST1;0613481951;78;12:04:41;01/01/2012 
065978198280;208100310191699;356008289837;PLAN3;MTC;CUST2;0600000001;92;12:07:13;01/01/2012 
065978198280;208100310191699;356008289837;PLAN3;MTC;CUST2;0600000002;94;12:09:40;01/01/2012 
065978198280;208100310191699;356008289837;PLAN3;MTC;CUST1;0612063352;114;12:12:40;01/01/2012 
065978198280;208100310191699;356008289837;PLAN3;SMS-MO;CUST1;0613103364;0;12:13:42;01/01/2012 
065978198280;208100310191699;356008289837;PLAN3;SMS-MO;CUST1;0613751973;0;12:14:44;01/01/2012 
065978198280;208100310191699;356008289837;PLAN3;SMS-MO;CUST1;0613672843;0;12:15:44;01/01/2012 
065978198280;208100310191699;356008289837;PLAN3;SMS-MT;CUST1;0612769488;0;12:16:42;01/01/2012 
065978198280;208100310191699;356008289837;PLAN3;SMS-MT;CUST1;0613164676;0;12:17:39;01/01/2012 
065978198280;208100310191699;356008289837;PLAN3;SMS-MT;CUST1;0613399901;0;12:18:39;01/01/2012 
067599860569;208120276653317;353297808290;PLAN2;MOC;CUST1;0612089847;116;12:00:00;01/01/2012 

"
data request - Where can I find a large list of English books published in the last 50 years?,"
There are lots of options, though Project Gutenberg is probably not one of them because of the time range. You could try using LibraryThing, Google Books (orderBy=newest), Goodreads or Amazon APIs to run search queries. Or try to get access to OCLC WorldCat or Ingram OASIS - talk to your friendly local librarian.
"
geospatial - Looking for geo localization data of golf courses,"
You could start with the 25000+ objects tagged with leisure=golf_course on OpenStreetMap. To do this, you could extract the geolocations with GIS tools, or create a custom map like they did with bicycle tags. OSM data is ODbL licensed.
"
data request - Searching a proxy for monthly GDP,"
The UNSD's Joint Oil Data Initiative (JODI) contains oil production, export/import and consumption on a monthly basis for all the countries in the world since 2002. I have a tutorial here on using the dataset:
http://opengeocode.org/tutorials/UNSD.php
The UNSD also publishes annually a very comprehensive energy statistics yearbook for 215 countries (currently). 
'The 2010 Energy Statistics Yearbook is the fifty-fourth issue in a series of annual compilations of internationally comparable statistics summarizing world energy trends. which commenced under the title World Energy Supplies in Selected Years, 1929-1950. Annual data for 215 countries and areas for the period 2007 to 2010 are presented on production, trade and consumption of energy for solid, liquid, and gaseous fuels, electricity, and heat. Per capita consumption series are also provided for all energy products. Graphs are included to illustrate historic trends and/or changes in composition of production and/or consumption of major energy products. Special tables of interest include: international trade tables for coal, crude petroleum and natural gas by partner countries; selected series of statistics on renewables and wastes; refinery distillation capacity; and a table on selected energy resources. ""
http://unstats.un.org/unsd/energy/yearbook/default.htm
"
education - Collection of messy data,"
I collected some answer from friends on twitter, they suggested:

MG-RAST: http://metagenomics.anl.gov/
Interaction web database: http://www.nceas.ucsb.edu/interactionweb/ - just a collection of csv files basically, same general format but all from diff researchers
Interesting dataset of 2127 articles that the Wellcome trust paid for in Europe for open access article fees to publishers

lots of messy data see this blog post: http://biomickwatson.wordpress.com/2014/03/25/biologists-this-is-why-bioinformaticians-hate-you/
the data http://figshare.com/articles/Wellcome_Trust_APC_spend_2012_13_data_file/963054


"
products - Where can I find data from released computer models?,"
I think in your search you should use either the term ""specifications"" or ""specs"", as in ""computer specs"". ""Information"" is too vague and in the jargon of technology specs/specifications is the way to go.
To get you started, I would suggest looking for non-commercial groups that are collecting data on hardware. For example, the groups doing the Energy Star rating, groups collecting data for import and selling purposes, groups conducting safety tests, etc.
One quick example: As part of the old EU Energy Star program, there is an Excel file with the type of specs you are looking for. Unfortunately, the manufactorers are masked. But it's a start.

Overview
All Downloads
Computers version 5.0 - Final - Masked Dataset [XLS]

If there is any newer data, you'll have to register: link.
"
data request - Dataset of personal names,"
I'm going to expand on the comment of @Joe above because there isn't room to add too much in the comments.
IMDB offers textfile datasets with enormous amounts of data (details). Choosing one site at random, you see a list of the files (link). Choose biographies.list.gz and uncompress. The biographies.list file has sections incuding:

NM: 'K', Murray the
RN: Murray Kaufman
NK: The Fifth Beatle
DB: 14 February 1922, New York City, New York, USA
DD: 21 February 1982, Los Angeles, California, USA (cancer)
BG: Murray the K was born Murray Kaufman in New York, New York, on 14... 
  (abbreviated)
SP: * 'Jacklyn Zeman' (qv) (14 February 1979 - 1981) (divorced)
TR: * Legendary disk jockey who made his name at WINS (New York) in the 1950s... (abbreviated)

Some categories are easy to decode (BG = background). Here is the full list of possible categories, with some annotation based on my skimming. It seems there are about 500k unique people included. The data is a little dirty, but you can surely get names, nationalities, countries, ages, genders, marital and family status after a little text mining.

2050717 BG: (biography)
1438983 TR: (trivia)
585662 NM: (name)
560287 OW:
373177 DB: (date of birth)
329587 QU: (quotes)
155847 HT: (height)
138481 RN: (real name)
132678 AT:
130380 SP: (spouse)
119293 DD: (date of death)
96901 BY: (biography written by)
74378 CV:
66683 NK: (nickname)
61704 PT:
52134 IT:
29016 BO:
27863 TM:
11012 PI:
6348 SA:
6042 BT:
1 WN:

Overview stats via some awk/grep/sort/uniq/sort:
awk '{print $1}' biographies.list | grep -v '^$' | sort | uniq -c | sort -rn -k1


Update: Instead of downloading the file and parsing it, you can use one of the unofficial APIs: discussion.

one
two
three

"
usa - What do the summary files in the ACS FTP drive mean?,"
To answer your question very specifically: here is the technical documentation for the ACS2012-1yr summary files: http://www2.census.gov/acs2012_1yr/summaryfile/ACS_2012_SF_Tech_Doc.pdf
But, the comments on your question provide sage advice: if you don't know why you need the summary files, you may be better off using the American Fact Finder. 
Alternatively, I've been part of a project called Census Reporter, which aims to make ACS data easier for journalists to use. We hope that it's useful to non-journalists as well. Here's our profile page for New Orleans You can also get bulk data for different tables. We're gradually working on some help documents that try to put census concepts into clearer, if sometimes less technically precise language.
If you do want to go deeper with the summary files, the first question is ""why do you want data for both the 1-year and the 3-year""?  They are not meant to be compared to each other. If you want to compare data for New Orleans this year and three years ago, use the 2012 and the 2009 1-year data sets.  If you want data for different areas inside New Orleans, you'll need to use the 5-year dataset to find census tracts or block groups inside the city.
The root of the Census Bureau's ACS documentation is at http://www.census.gov/acs/www/
Just to get a general idea of what goes on in the ACS data, I found the ACS Handbooks for Data Users quite helpful, and there are several versions each slightly tailored to a different audience. Coming from a journalism background, I was impressed that the ""media"" version included quotes from several respected expert census journalists.
"
data request - Coding homework dataset,"
Including to other answers, here is one that may help to create a database.

Project Euler is a series of challenging mathematical/computer
  programming problems that will require more than just mathematical
  insights to solve. Although mathematics will help you arrive at
  elegant and efficient methods, the use of a computer and programming
  skills will be required to solve most problems.

As the about section said, project Euler is a collection of many mathematical problems and users try to solve them one by one with programming languages. Unfortunately, there isn't a limitation on the programming language you have to use. But if someone track them and collect only those that want, it would create a nice database.
"
linked data - Why isn't RDF more popular within the private sector?,"
While there is clear power with RDF and other formal ontologies, web technologies are showing a tendency towards simplicity -- things that are easy to code, read, manipulate, etc. RDF has none of those qualities. So while a language like Ruby might evolve on its own, it gains more power, popularity and community when a platform that makes development more simple (like Rails) starts to use it.
"
usa - Data mining for US demographics,"
You can find demographic data on Metropolitan Statistical Areas through American FactFinder. Depending on how accurate, timely, or complete you want your data to be, you can decide between 1-Year averaged estimates, 3-Year averaged estimates, or 5-Year averaged estimates. The 1-Year averaged estimates cover areas with populations over 65,000 (with an average sampling rate of 1.5% of the population); The 3-Year averaged estimates covers areas with populations over 20,000 (with an average sampling rate of 4.5% of the population; and the 5-Year averaged estimates will have the largest sample with an average sampling rate of 7.5% of the population.
"
"usa - Where can I find data by ""census tract numbers""","
I think I know what is going on with the weird Census Tract numbers you are getting. Census tract numbers come in many forms. However, the number you are shown in the Seattle city violent crimes appears to be both the tract number and the block number (e.g. ""4700.4003""). I am guessing that the first number prior to the period is the tract number, which is expressible in its GEOID form as ""004700"", which also shows up as ""Census Tract 47"", or tract number 47.00. It is inherently 6 digits long, although the last 2 digits can be thought of as decimals and the leftmost numbers may disappear because they are leading zeroes. The numbers after the period are 4 digits long, because block numbers are always 4 digits long no matter what.
Given this information, I would forget about worrying what the Census tract number is. If you have the latitude and longitude I would suggest taking a different approach to matching the locations to areas with demographic data.
Make an account with NHGIS.org (They are a free publicly-provided service) and download the latest data from the 2008-2012 ACS along with corresponding shapefiles to go with it. With these shapefiles in your hands, and the lat/long coordinates of the crime incidents, you may be able to perform some type of intersect analysis to find which Census tract each crime was located in. If you are so inclined, you can do this instead by Census block group which is a smaller geographic division than Census tract, but may suffer from a larger degree of statistical error than Census tracts.
I hope this was helpful.
"
usa - Archive of ZIP code changes,"
I've never used data that old for addressing. One place I would start is with the US Census 1970. It used real zip codes back then (vs. ZCTA), but it is before the census published tigerline shapefiles. So without the geographic shape, I am not sure how you can solve the problem. I did some googling the census archive. I think this is where the census data for this is:

Census of Population and Housing, 1970, Fifth Count File 5B National
  Archives Identifier: 594232 Data Files: 10 (one per ZIP Code Area,
  based on first digit of ZIP code) Geographic Areas: 12,500 ZIP code
  areas for Standard Metropolitan Statistical Areas (SMSAs) Technical
  Documentation: 44 pages and 82 supplemental pages

I would google to see what ""commercial"" providers who have converted data from archives into shapefiles.
"
usa - USDA Plant Hardiness Zone Map (PHZM) data,"
There's an Open Plant Hardiness Zones (OPHZ) project on Github where various people have reverse-engineered a pdf of the hardiness zones (pdf, really large) to produce a GIS file (SHP) of the zone boundaries.  The ophz-c version is the latest. It's public domain.
You could use GIS software or tools, and a list of the center point of each zip, to find the hardiness zone for each zip. (you're no doubt aware there are no firm boundaries for each ZIP, so this would just be an approximate answer, the latitude and longitude of a street address would be better). 
Doesn't even have to be GIS software. You could convert the SHP file to geojson if needed, import it into a geo-aware database such as MongoDB or Postgres, import the zip code list as well, and use the database GIS queries to give you ""zip is within .. zone"" answers.
Probably a more complex solution than you'd like.  Maybe somebody has done these steps already and released the results, but I'm not aware of it.
"
computing - Where to find key log data for keyboard usage?,"
There are a few academic papers that have used keylogger data set. An example from 
""A Case-Study of Keyloggers and Dropzones"" (PDF link):

We found more than 33 GB of keylogger data, containing stolen information from more than 173,000 victims.

For security reasons, the authors gave the data to AusCERT (Australia’s National Computer Emergency Response Team), and it's mostly impossible that this data set is ever released. But the paper is very interesting in its explanation of how they collected the keylogger data.

Another paper, ""A Metric for the Evaluation and Comparison of Keylogger Performance"" (PDF link): 

(The authors have) developed a framework to assess the performance of a keylogger. This paper provides the documentation on how such a study can be conducted, while the required source code is shared online.

The analysis code (python) and keylogger data (sql database format) is available as a download from the Security in Telecommunications group at TU Berlin. The data is from tablets and smartphones.

The toolkit also holds the test-data from the performed use-case study and a demo keylogger.

The git page is here, and the direct download link is here. The data files are called KeyStrokesExperiment.sql and KeyStrokesRealExperiment.sql in the ""data"" folder.
"
How much data is available on the Internet?,"
Have a look at the Measurement Lab.
One estimate I've heard is that researchers at the Measurement Lab (M-Lab) publish over 750Tb of data under a CCZero licence.
"
licensing - What open data about tunes and melodies exist? Do copyrights apply here?,"
It seems like http://echoprint.me/ is the service you want:

Echoprint is a music fingerprint or music identification service. It
  listens to music signals and tells you what song is playing. It’s
  backed by a huge database of music that grows with the community and
  further partnerships. On launch we’ve partnered with Musicbrainz. 
...
Does it work “over the air”, identifying songs over a microphone?
Yes - Echoprint has been designed from the ground up for OTA, and our
  informal tests have demonstrated many successful and promising results
  for this scenario. The system still needs a little more tuning,
  however, and is under continued development to further improve
  accuracy and performance.

Data license (at http://echoprint.me/data):

Echoprint data (for ingestion into your own server) is available under the “Echoprint Database License.” The intent of the license is simple: Use our data for whatever you want (commercial or non, research, personal use); If you download our data and then add to it, you are required to contribute data back to us. ; There is a good reason for this. We want Echoprint to be able to resolve every song in the universe. If you add to the database of resolvable tracks, the Echoprint community needs to know about it.

"
data request - Population movement datasets?,"
It's only a subset of population, but many bikesharing programs openly offer their data.
One example, Capital Bikeshare in Washington DC has a data page. This data will include geolocation of each bike with exact time, as well as status of each station and more. There was a recent presentation about using KNIME with the data (PDF link), and you may find their analysis an interesting start.

Other cities: New York City, Chicago, Bay Area, etc...
List of bike sharing programs (Wikipedia)

"
usa - How to make a successful FOIA request?,"
The basics for FOIA are:

be very clear in your request
be respectful
be tenacious

Remember that while there are government officials who are hostile to records requests, not all of them are. There are many people in government who want to do the right thing.
There are a few websites designed to help you file FOIA requests. In the US, Muck Rock has been around for several years, and FOIA Machine is another newer one. In preparing this answer, I also learned about iFOIA.org. There is also a project called Alaveteli, an open source project to make it easier to set up sites like these. Their GitHub wiki has a list of running sites around the world.
One way to have a hint about data systems that must exist is to consider forms that you already know are filed. You can use what you infer from the existence of the form and the fields it collects to explain what you are requesting, and to make your request more confidently. As mentioned in the other answer, requesting data schemes or other system documentation can also sometimes be helpful.
In response to Albert's answer: While there are always going to be cases where people in power work to contain information, there is a long history of important news being broken based on information obtained via FOIA. Here are just a few lists:

Noteworthy News Stories made possible by FOIA
documents
(National Security Archive) 
The FOIA Files: Stories that FOIA made
possible
(Sunshine in Government Initiative)

But also, in respect for Albert's point, I offer this tweet from ProPublica's Jeff Larson: 

Don’t use ‘related to’ in your FOIA request. Just got:
  ‘all documents 'relate' to others in some remote fashion’ as a reason for denial.

"
data request - Is there a database that provides lengths of books?,"
You can get page counts from OpenLibrary and word counts from the linked editions on Internet Archive.  Of course the latter is only going to be for public domain editions and if you are focused on review sentiment (as a proxy for purchase desirability?), you are probably more interested in modern non-public domain editions.  The other drawback to IA word counts is that  you'll need to download the full text to get them, but if you pull down a compressed text-only file, it shouldn't be too bad.
Here's the chain of links you need to follow:

OL work page https://openlibrary.org/works/OL3686173W/Lorna_Doone
OL edition https://openlibrary.org/books/OL13522117M/Lorna_Doone
OL API https://openlibrary.org/books/OL13522117M.json
IA page https://archive.org/details/blaclornadooneromanc00rich
IA files https://ia600308.us.archive.org/9/items/blaclornadooneromanc00rich/
IA text file https://ia600308.us.archive.org/9/items/blaclornadooneromanc00rich/blaclornadooneromanc00rich_djvu.txt

Counting words is as simple as:
curl https://ia600308.us.archive.org/9/items/blaclornadooneromanc00rich/blaclornadooneromanc00rich_djvu.txt | wc
  34410  280815 1488652

So this edition of Lorna Doone is 687 pages with (approximately) 280K words.  The word count is done over OCR'd text, so it will not be 100% accurate, but it should be close enough for this type of project.
"
data request - A dataset of resumes,"
indeed.com has a résumé site (but unfortunately no API like the main job site). You can build URLs with search terms:
http://www.indeed.com/resumes/data-science

With these HTML pages you can find individual CVs, i.e. link. You can search by country by using the same structure, just replace the .com domain with another (i.e. indeed.de/resumes)
The HTML for each CV is relatively easy to scrape, with human readable tags that describe the CV section:
<div class=""work_company"" >
...
<p class=""work_description"">

Check out libraries like python's BeautifulSoup for scraping tools and techniques.
"
data request - Looking for an UTF-8 table,"
With python you can lookup each unicode character by its integer code using unichr.
import sys
with open('unicode.csv','wb') as output:
    for i in xrange(sys.maxunicode):
        output.write(unicode(i))
        output.write(u',')
        output.write(unichr(i).encode('utf-8'))
        output.write(u',')
        output.write(unichr(i).encode('ascii', 'xmlcharrefreplace'))
        output.write(u'\n')
print sys.maxunicode

This gives you a file (unicode.csv) which has the unciode integer, unicode representation, unicode character, and HTML escaped character (for non-ascii).
For example, each line looks like this:
64058,u'\ufa3a',墨,&#64058;

I put the code and the unicode.csv file on github for easier access.
Note: Because the unicode character set includes newline characters, CSV is not really the best format. (See lines 10 to 13.) I also added a python code to generate a JSON file, which is more safe than CSV for storing unicode characters.
"
government - Where can I find the 2014 release of Medicare data from 2012?,"
It looks like this might be the data:
http://www.cms.gov/Research-Statistics-Data-and-Systems/Statistics-Trends-and-Reports/Medicare-Provider-Charge-Data/Physician-and-Other-Supplier.html
"
"data request - Where can I find all cities, regions and cantons of Switzerland?","
For detailed statistics, bfs.admin.ch is the official Swiss source of population stats and data. They have many English-language resources, but the one you are looking for is in German or French only. Excel files are easy to understand without speaking German or French.
The source page is here and the Excel file is here. It is called ""Bilanz der ständigen Wohnbevölkerung nach Bezirken und Gemeinden"" or ""Bilan de la population résidante permanente selon les districts et les communes"".
The Excel file has the structure as follows 1. Kanton/Canton (i.e. Zurich), 2. Bezirk/District/Distretto (i.e. Affoltern), 3. Municipality/Gemeinde/Commune (i.e. Aeugst am Albis). Note that the municipality code is not the PLZ.
- Zürich
>> Bezirk Affoltern
......0001 Aeugst am Albis
......0002 Affoltern am Albis
......0003 Bonstetten
......0004 Hausen am Albis
......removed for clarity
......0014 Wettswil am Albis
>> Bezirk Andelfingen
......0021 Adlikon
......0022 Benken (ZH)

For basic lists of municipalities (cities) and their PLZ (zip code) and Canton (two-letter code), you can use the resources from post.ch. See the download link here and this particular file here. They also have a list of municipalities by PLZ and municipality number.

If you are looking for mapping data, the government swisstopo site offers tons of data. There is a GitHub repo called swiss-maps where you can find tools to create geoJSON and topoJSON files based on the swisstopo data.
"
data request - List of public holidays by countries?,"
Just came across Azure open dataset:
https://azure.microsoft.com/en-us/services/open-datasets/catalog/public-holidays/

Worldwide public holiday data sourced from PyPI holidays package and Wikipedia, covering 38 countries or regions from 1970 to 2099.
Each row indicates the holiday info for a specific date, country or region, and whether most people have paid time off.

"
sql - What is the best tool for doing explorative analysis of databases?,"
looks like possibly same question on stackoverflow:
https://stackoverflow.com/questions/12835572/how-to-use-pure-sql-for-exploratory-data-analysis 
most results i found recommend using R
http://www.renzeconsulting.com/presentations/exploratory-data-analysis-with-r.pdf 
looks like these two solutions cost:
http://science.nature.nps.gov/im/units/arcn/documents/documents/nps-arcn-dmsop-2009-03_exploratory_data_analysis_using_sqlserverstudio_v1.0_20090210.pdf 
http://spie.org/Publications/Proceedings/Paper/10.1117/12.907097
"
data request - Source for photographs of sample checks,"
This is an interesting question. Here is a non-direct path (assuming you are using Windows).
TL;DR print hard copies with check font, scan/photograph them, use OCR.
I used Python to write the few MICR characters in Unicode. I use the utf-16-le (little endian) encoding with BOM.
import codecs
with open('micr.txt','wb') as output:
    output.write(codecs.BOM_UTF16)
    for ichar in xrange(9280,9280+11):
        line = ( u'1'+unichr(ichar)+u'\n').encode('utf-16')
        output.write(line)
    output.write( (u'0123456789'+u'\n').encode('utf-16') )

The output looks like this (I suggest using Notepad++ or another high-level text editor). It's required to have some character before each of the non-numeric MICR unicode characters, I just chose '1' for simplicity:
1⑀ 
1⑁ 
1⑂
1⑃
1⑄
1⑅ 
1⑆
1⑇
1⑈
1⑉
1⑊
0123456789

These are all the possible unicode characters for MICR. Of course, the font is wrong. (A simpler method would be to copy paste from here)
The MICR font costs money, unfortunately, but there is an open version: GnuMICR. You can download the font directly here: GnuMICR.ttf. To install, just double click and then restart and running programs where you want to use it.
I then took my output text from above, and pasted it into LibreOffice Writer (other MS Word-like programs will work, too). Highlight all the text and change the font to GnuMICR. I've shared the LibreOffice Writer .ODT file here (you must have the GnuMICR font already installed).
The screenshot below shows what my screen looks like.

From here, you can use the Python code or lots of copy/paste to generate unlimited routing and account numbers. Let me know if you want a python snippet to generate fake routing/account numbers.
To really test the OCR, generate your own MICR documents by physically printing your digital ""checks"" and then scan/photograph (re-digitize) them. Between printing and scanning you can have some fun by folding, spilling coffee, etc.
You can also digitally damage the images, for example, by using Python Imagine Library (PIL) and methods such as discussed here.
"
"data request - Where can I find historical aerials for some areas in Dona Ana County, New Mexico?","
USGS EarthExplorer is often a fantastic resource for this.
Check out, in particular, these datasets:

Aerial Photo Single Frames (usually pre-1980)
DOQ (1990s)
NAPP (1980s-1990s) 
NHAP (1970s-1980s)

"
data request - Administrative boundary of Colorado State Forest,"
best way to do this is search for locality admin site. then search it for gis. that gave me this:
http://www.arcgis.com/home/group.html?owner=rsacco&title=Colorado%20Parks%20and%20Wildlife%20-%20Administrative%20Boundary%20Data
"
socrata - Contains filter in SODA2 API call,"
Update: I realized my answer is for the browser and not related to Socrata. I'll probably delete it soon.

Your core URI is 
http://data.cityofchicago.org/resource/4ijn-s7e5.json

which is actually just a large JSON file.
It's possible to use the browser to filter the JSON file. After the core URI above, you put a question mark (?) and then the filters. For example, to get inspection_id # 1434955, the URL will be constructed as follows:
http://data.cityofchicago.org/resource/4ijn-s7e5.json?inspection_id=1434955

The key from the JSON file is inspection_id and the value is 1434955. These strings must match exactly, so it's most useful to use the complete JSON as a reference.
You can combine filters with ampersand (&). For example, this URI gives a JSON that combines all Zip Codes being 60649 and with Inspection Type being ""Complaint""
http://data.cityofchicago.org/resource/4ijn-s7e5.json?zip=60649&inspection_type=Complaint

The keys are things like zip and inspection_type, and the values are things like 60649 and Complaint. There is also the ability to give the browser JSON with more complicated filters (greater than, less than), but those will require some documentation from the data source. Normally you put a JSON-like piece of text in your URI (for example). If you use a browser tool like discussed here, I think then you can easily search text in the JSON file.
If you don't know programming, consider learning cURL (don't mind the tacky website, it's really a core resource). Also, there are browser extensions that may make the process simpler (for example).
"
"data request - Cafe, library ambience noise database","
nature sounds offers kitchen audio...which may not be close enough. it allows for exporting 30min file
http://naturesoundsfor.me/ 
ok. i can't find anything longer than a few minutes. check out some of the results in the open audio community on archive.org:
https://archive.org/details/opensource_audio
"
data request - Database of English words difficulty,"
The New General Service List (updated just a few weeks ago) is a list of ""the most important words for second language learners of English"".
It ranks the top three thousand headwords (plus more words within each headword, such as plurals and tenses).  
So it won't cover all the words in your list, and it's really indicating ""usefullness"" rather than difficulty.  But I think useful words are more likely to be used, so more familiar, so ""easier"".  It's certainly something you could use as a major factor, along with other factors, to determine difficulty.
The data is in a spreadsheet, and the license is CC-BY. 
For (more difficult) words beyond that list, you could use word frequency as a major component of a difficulty rank.  One source is subtitles word frequency lists (CC-BY).  That lists a few hundred thousand words by how often they appear in English-language TV and film subtitles.  Again not the same as difficulty (and it's spoken English, rather than written), but it's an indication of how likely it is that somebody has heard the word before.
"
data request - Database of Japanese words difficulty,"
You can start by gathering Japanese books or documents which you know the difficulty level of (e.g. reading level of the book). Match your list of words to your set of books/documents to derive the difficulty of your words. This will probably give you a pretty decent classification to start with. You can then improve your classification by looking at additional things like the ones on your list.
"
"data request - IRS Statistics of Income, machine-readable","
Been slightly inattentive here, but for posterity's sake, I wanted to post the results of my cleaning effort: machine-readable SOI data.
Like most intensive data cleaning I've been a part of, it wasn't the result of a single programming effort. For example, a fair bit of consideration went in to how to reconcile the individual annual series. The coverage.csv in the repo shows the series coverage for 1997-2011. Enjoy!
"
data request - Dataset of domain names,"
If this still makes sense, there's a dataset that I'm maintaining:
https://github.com/tb0hdan/domains
TLD kinds: 1522
Country TLDs: 245
Generic TLDs: 1277
Total domains in dataset: 1,789,946,688
"
data request - Structured movie/TV dataset,"
About the TV shows I know a great site (eztv) with a list of airplay, info, trailers etc, but I cannot find anywhere about copywrite. Maybe you can contact with them and ask them about it.
Link: https://eztv-proxy.net/showlist/
"
tool request - Wiki site for data tables,"
scraperwiki lets you do this, plus push to ckan databases. there are limitations for free accounts. it will break your huge file up into multiple csvs.
google drive also lets you do this, a little bit more limited, but you can still do what you want by creating a new doc (spreadsheet) and then importing your data.
tabula is another sweet tool, published by knight foundation, that you can use on your desktop...i've had minimal problems with file size, but for each one, if i simply wait it out, as in let the program consume my resources as much as it needs....it always works.  
as for hosting: datahub.io, google drive, will host off the top of my head. so i guess on that note, amazon's free service and dropbox too 
"
data request - IP addresses of backbone routers,"
I suggest you use data from Caida (Cooperative Association for Internet Data Analysis).   In particular their Macroscopic Internet Topology Data Kit (ITDK) may suite your needs.
http://www.caida.org/data/internet-topology-data-kit/
Quoting from the above page:
The ITDK contains data about connectivity and routing gathered from a large cross-section of the global Internet. This dataset is useful for studying the topology of the Internet at the router-level, among other uses.
The latest ITDK release, 2013-07, currently consists of

two related router-level topologies,
router-to-AS assignments,
geographic location of each router, and
DNS lookups of all observed IP addresses.

"
government - Accurately Using Census Tract Data and Total Population,"
Note that your API request is for the population of all tracts in Allegheny County, not just in Pittsburgh. But I don't think the API supports tracts-in-place right now.
I don't know the region that well, but I don't see any problems with your method. Note that you're using 2010 ACS estimates, when you could be using 2012.
http://api.census.gov/data/2012/acs5?key=XXXX&get=B01003_001E&for=tract:*&in=state:42+county:003
Remember that the ACS data are estimates, so there is always a risk of sampling error. I don't think you can get the error as part of the Census API query, but you could get it from the American FactFinder, or from our project, CensusReporter: here is B01003 for all tracts in Allegheny county.
Our (Census Reporter's) advice to journalists is to use caution when the error is > 10% of the estimate. That is the case in 123 of the 402 tracts in Allegheny county. But also, the total of the tract-level population estimates from your API call is 1,223,066, which is quite close to the 2010 population from the Decennial census, listed as ""estimates base"" in the Allegheny County Quick Facts.
Do you have personal knowledge of the places with low population that makes you suspicious?
"
data request - Change of real estate ownership,"
It would be difficult to track a single individual's home ownership pattern and reasons over time, primarily because these are considered personal data in most countries and would not be released as such.
However, in the U.S., the following datasets may be helpful:

U.S. Department of Housing datasets
Data related to families moving to areas of opportunity
The American Housing Survey
FactFinder from the U.S. Census on housing information
Detailed data for New York City for specific reasons for moving (via @EreiSegalHalevi)

"
data.gov - Looking for data on bankruptcy prediction,"
In the U.S., bankruptcy data is found in several places. Depending on which parameters you decide are important for your tool and algorithm, some of the following datasets should be helpful to you:

Recent bankruptcies, findings, and maps from the U.S. Courts
Bankruptcy statistics from the U.S. Department of Justice
Public bankruptcy cases from the U.S. Securities and Exchange Commission

The challenge with any predictive modeling is not where to find the data, but which data will be true indicators of the item you are trying to predict. In this case, the data above show the companies that have become bankrupt. Your challenge is finding those factors that led to their bankruptcy...a much more difficult problem.  In this case, it may be that data such as:

Chapter 7 Trustee reports from U.S. Department of Justice
UCLA Bankruptcy Research Database

These may point to the underlying causes and complexity in the area you are researching.
"
web crawling - Why and how are certain websites able to state in their terms of service that you can't crawl their data?,"
IANAL, TINLA. If you really need to know, get a lawyer.
In the United States, adult human beings are presumed competent to enter into contracts while protecting their own interests. Ergo, contracts can say almost anything you can imagine and the law will enforce them. Terms of Service agreements are contracts. Ergo...
(There are certain specific limitations to the power of contract. If you want to know more, talk to a lawyer.)
Re copyright: in the United States, compilations of facts are not copyrightable (Feist v Rural Telephone Service) because they do not meet copyright's standard for originality/creativity. So base facts about a restaurant... not covered by copyright. Reviews, however, likely pass the originality test, so scraping them has copyright-infringement implications.
"
data request - Where to download a dictionary of medical terms,"
Merriam-Webster's Medical Dictionary with Audio
Free for noncommercial (which here means not generating any revenue) use provided you submit less than 1000 queries a day. Other forms of licensing are available upon request.
description, licencing and fees
"
data request - Dataset of researchers CV,"
I don't know if it would have all of what you're looking for, but there are a couple of efforts to try to give identifiers to researchers.  Most of them link to publications, and some have CV building tools.  They might not be 100% what you're looking for, but they may satisfice your need:

http://orcid.org/
http://researchgate.net/
http://www.researcherid.com/

(note that I haven't looked at their terms for extracting their data; I know when I signed up for ORCID, it asked me what info I wanted to make public.)
"
data request - diabetes complications dataset,"
UCI Machine Learning Repository provides three datasets for diabetes:

Diabetes Data Set
Pima Indians Diabetes Dataset
Diabetes 130-US hospitals for years 1999-2008 Data Set

If these don't meet your needs, you may want to go through the papers that cite these datasets and see what else they have used.
"
data request - Dataset: text and music,"
Have you seen http://freemusicarchive.org/search/ which has an API (http://freemusicarchive.org/api/docs/) as well as categories like eerie, happy, and sad
"
data request - Age of consent per country database,"
The UN publishes such as list for age of consent for marriage:
http://data.un.org/Data.aspx?d=GenderStat&f=inID:19
Note, the age of consent for sex is in many countries lower than consent for marriage. The UN publishes this as well, but I was not able to find it at the moment.
"
data request - Are there any open datasets of board games that allow commercial usage?,"
Depending on what type (and the structure) of the metadata you're looking for, have you thought of tapping a more general product API from the likes of Google, Amazon, eBay, etc.?
Also: consider contacting the API you found that has terms not to your liking. Perhaps you can create a special arrangement with them.
"
data request - Historical Weather Forecasts,"
The NOAA / National Weather Service has an archive from the Weather Prediction Center, but it only started archiving most products a few years ago:

http://www.hpc.ncep.noaa.gov/archives/web_pages/wpc_arch/get_wpc_archives.php
https://www.wpc.ncep.noaa.gov/archives/web_pages/wpc_arch/get_wpc_archives.php

Most of what's archived are weather maps (like what you'd see behind the weatherman on the news as he's giving his forecast), and not specific temperatures.  Their 'national high/low' product just reports where the highest & lowest temperature on a given day was recorded, not the high/low for lots of cities.
For many years, they only maintained a 'rolling archive', where they'd keep maybe 30-60 days of forecasts, and then purge them.  It's possible that something like archive.org might have some of it, but it'd likely be an incomplete record.  If you wanted to go that route, take a look at the list of NOAA's national centers:

http://www.hpc.ncep.noaa.gov/html/othersites.shtml
https://www.wpc.ncep.noaa.gov/html/othersites.shtml

"
network structure - Data sets with both social and attribute data,"
I'm not sure if you care what domain the data are from, but I suspect the Add Health dataset may have what you're looking for. It contains a bunch of variables on health and child wellbeing and includes data on respondents' school-based networks.
"
film - Movie/Tv Show api (with posters) for commercial user,"
A question like this was asked on Stack Overflow a few years ago. 
Besides TheMovieDB (which was the accepted answer), and OMDB, which you've found,  people suggested the Rotten Tomatoes API, which appears to have posters, but which also looks like it's integrated into OMDB...
"
data request - african newpapers for the past 20 years for machine learning,"
Here's a good starting place (compiled by Columbia University):  
Nigeria:
http://library.columbia.edu/locations/global/virtual-libraries/african_studies/countries/nigeria/online.html 
Tanzania: 
http://library.columbia.edu/locations/global/virtual-libraries/african_studies/countries/tanzania/online.html
"
data request - Word list for Common European Framework of Reference for Languages (CEFR),"
There is a Wictionary list for Spanish A1 vocabulary that would be easy to parse in the print version. 
"
"data request - Standing and seating capacity of clubs, bars and restaurants","
I've only came across one dataset that contained the seating capacity. It is the restaurant inspections for Seattle and King County:
http://info.kingcounty.gov/health/ehs/foodsafety/inspections/search.aspx
"
"usa - How to construct a database with the underlying real estate data displayed by Redfin, Zillow, or Trulia?","
I don't know specifics on how Zillow, etc acquired their data - other than assuming they (or 3rd party) obtains the data on a per county basis.
As cities/counties open up open data portals, this data will become more accessible to developers.
When searching these portals, you want to look for tax lots, property tax, or parcels.
A lot of these datasets are still in Shapefile formats, but some are being provided in more readable formats (CSV). Some examples:
Denver, CO
http://data.denvergov.org/dataset/city-and-county-of-denver-parcels
Madison, WI
https://data.cityofmadison.com/Property/Assessor-Property-Information/u7ns-6d4x
New York, NY
http://www.nyc.gov/html/dcp/html/bytes/dwn_pluto_mappluto.shtml
Vancouver, BC
ftp://webftp.vancouver.ca/OpenData/csv/property_tax_report_csv.zip
"
Looking for a large data set of French data,"
I also think that Twitter data is the best way to get >100Gb french dataset.
OpenStreetMaps gives really great open data but you can't reach 100Gb easily : the Integral Metropolitan France dataset is 2.6 GB.
There is no open weather data by MeteoFrance (you have to pay the licence) but there is an Open Meteo Forecast that you can try. I never had the opportunity to use it but I guess it can provide you large amount of data.
You also have the dataset ""Tous les documents"" from the BNF that is 1.5GB and looks quite interesting.
Hope it can help.
Cheers
"
usa - Fatality Analysis Reporting System,"
First you will need to define the schema of your final database and determine how you will map each file to your schema.
Then I recommand using ETL tool (Extract Transform and Load) such as Talend Open Studio (TOS) or Penthao (both are open source solution). Using such tools will allow you to:

automate the process (and not process each file individually)
create mapping for each file between the original format and your SQL schema

In the case of talend you will do something like this:

find the right component to connect to the input file (I don't know the format made available by FARS)
connect this component to a tmap
connect the tmap to a tSQLout component to update your database

Then using a tLoop and a putting your file name as a variable, you can loop through the different file and process them and define a different route depending on the file format from FARS (and update the tmap accordingly).
"
data request - Dataset of international marriages,"
The United Nations Statistics Divisions publishes tables on marriages per country. While they do not break it down by nationality, they do provide a variety of demographic data.
http://unstats.un.org/unsd/Demographic/sconcerns/mar/mar2.htm 
The BLS also publishes tables on marriage that contain some ethnicity information for the US:
http://www.bls.gov/opub/mlr/2013/article/marriage-and-divorce-patterns-by-gender-race-and-educational-attainment.htm 
"
data request - Dataset of allergies,"
A list of statistics is available for U.S. persons:

Medical treatment triggered by allergies and hay fever. 
Allergy by age and ethnicity for 1998-2012 (specifically at 205.207.175.93/HDI/TableViewer/tableView.aspx?ReportId=59)

An interesting related article about the need for more data on allergy prevalence in developing nations may be informative.
"
data request - Dataset of crosswords,"
There was a really super visualization recently from vizual-statistix. See the whole post here.

Anyway, his data source was XWord Info, which shares all the NYTime Daily Crossword Puzzles. You can see an example (most recent) of JSON format:
http://www.xwordinfo.com/JSON/Data.aspx?date=9/11/2008

Details here. With a snippet of code you can search over all valid calendar dates, then download and parse.
There is also the XPF format which is designed for crossword puzzles in order to fit unusual features.
For NYTimes puzzles, the difficulty generally increases from Monday to Sunday.
"
analysis - Open Data for Economic and Business Research within Sport Management,"
Here are the slides from the vienna.rb talk (I did not attend it) about Open Football(soccer) Data and the World Cup 2014. The presentation is really interesting and it gives a lot of ideas I think.
Open Football on GitHub proposes a large collection of open football datasets.
Here is a brilliant article called ""ANALYZING A NHL PLAYOFF GAME WITH TWITTER"". It's not really Open Data but it also gives ideas and some pieces of code !
An interesting article about data future in football (it's more economy than tech but I thought it's what you asked).
My selection is a little football-oriented, and I'm sure there are a lot of data in sports like baseball or NBA.
Hope it could help !
"
data request - Converting Geographic Coordinates to New York City neighboorhood names,"
tamu has a killer geocoder, as well as a definitive list of other free geocoders. all you have to do is upload your data
http://geoservices.tamu.edu/Services/Geocode/BatchProcess/
http://geoservices.tamu.edu/Services/Geocode/OtherGeocoders/ 
"
data request - Dataset of adulteration incidents,"

Here is a Food Fraud Database. I haven't used it, but for some
reason it was in my bookmarks.


The first publication of the database in 2012 includes more than 1300 entries based on more than 650 articles. This includes information on more than 350 different food ingredients. Each record in the database is a publicly reported unique combination of food ingredient, adulterant, and where available an analytical detection method published in one literature reference.


Here is a Wikipedia list with a few dozens of incidents.
Here is a paper (Development and Application of a Database of Food Ingredient Fraud and Economically Motivated Adulteration from 1980 to 2010). They said that they would created a Database. Maybe you could contact with them or read the paper and find any citation.

"
data request - Human Mortality database around the world,"
The World Health Organization Mortality Database is probably what you're looking for.
"
finance - Data set for operating cost and revenue for small business?,"
In Infochimps, you will find hundreds of business related datasets. A few of them are from USA about tax return, receipts and net income. You will find also about revenues for certain business categories.
Visit here: Website Link
P.S. If you cannot find what you need in the link, please update your question with more details. Where you have already searched or/and an example of what exactly the dataset you want to contain.
"
Survey Questions for Data Mining Project JUSTNN,"
I would consider asking questions similar to the data collected in the IPEDS surveys by the US Department of Education. Each year, every accredited university/college and trade school fills this survey out. It includes a large number of questions, including: tuition, books, housing on/off campus, degrees offered, size of freshman class, student population by ethnicity, etc.
You can download standard or customized tables from the US Dept of Ed here:
http://nces.ed.gov/ipeds/datacenter/ 
I also have a number of these tables converted into CSV format, which anyone is free to use:
http://www.opengeocode.org/cude1.1/US%20Education/IPEDS/
"
contributing data - How do I add a topic to Freebase?,"
Copied directly from Freebase (source)...

Many times when you think you need to Create a New Topic, you probably will NOT need to.
IF YOU'RE SURE YOU NEED TO CREATE A NEW Topic in Freebase:
Think about what sort of thing your topic is. For instance, is it a Person, Film, or Skyscraper.
On Freebase.com, navigate to that Type (the Search interface at the top of each page is an easy way to do this)
When you are on the type page, click ""Add more topics"" in the upper right of the page
Enter the name of your new topic
Once it is added, you can click through to it and then edit it to add more information.
New topics usually appear in a Search within a minute or so, but this can be longer if the system is under high load.
SEE ALSO: FAQs about Topics

"
data request - Download list of the name of every country in Western languages,"
A list of the names of each ISO country code in up to 114 languages was produced by Jonah Ellison in 2011 (the number of languages available varies by country).
There's a link to the UTF-8 CSV file on his page: 21,640 Translated Country Names.
Each line contains the ISO country code, the ISO language code, and the name of the country in that language.  
For example, here's Luxembourg (country code LU) in around 75 languages, after loading that file into a spreadsheet:
LU  af  Luxemburg
LU  am  ሉክሰምበርግ
LU  ar  لوكسمبورج
LU  az  Lüksemburq
LU  be  Люксембург
LU  bg  Люксембург
LU  bn  লাক্সেমবার্গ
LU  bo  ལཀ་ཛམ་བོརྒ།
LU  ca  Luxemburg
LU  cs  Lucembursko
LU  cy  Lwcsembwrg
LU  da  Luxembourg
LU  de  Luxemburg
LU  el  Λουξεμβούργο
LU  en  Luxembourg
LU  eo  Luksemburgo
LU  es  Luxemburgo
LU  et  Luksemburg
LU  eu  Luxenburgo
LU  fa  لوکزامبورگ
LU  fi  Luxemburg
LU  fo  Luksemborg
LU  fr  Luxembourg
LU  fur Lussemburc
LU  ga  Lucsamburg
LU  gl  Luxemburgo
LU  gsw Luxemburg
LU  gu  લક્ઝમબર્ગ
LU  he  לוקסמבורג
LU  hi  लक्समबर्ग
LU  hr  Luksemburg
LU  hu  Luxemburg
LU  hy  Լյուքսեմբուրգ
LU  id  Luxembourg
LU  is  Lúxemborg
LU  it  Lussemburgo
LU  ja  ルクセンブルグ
LU  ka  ლუქსემბურგი
LU  km  លុចហ្សំបួរ
LU  kn  ಲಕ್ಸಂಬರ್ಗ್
LU  ko  룩셈부르크
LU  ln  Luksamburg
LU  lo  ລຸກແຊມເບີກ
LU  lt  Liuksemburgas
LU  lv  Luksemburga
LU  mk  Луксембург
LU  ml  ലക്സംബര്‍ഗ്
LU  mr  लक्झेंबर्ग
LU  ms  Luksembourg
LU  mt  Lussemburgu
LU  my  လူဇင်ဘတ်
LU  nb  Luxembourg
LU  nds Luxemborg
LU  ne  लक्जेमबर्ग
LU  nl  Luxemburg
LU  nn  Luxembourg
LU  or  ଲକ୍ସେମବର୍ଗ
LU  pl  Luksemburg
LU  pt  Luxemburgo
LU  ro  Luxemburg
LU  ru  Люксембург
LU  se  Luxembourg
LU  sk  Luxembursko
LU  sl  Luksemburg
LU  so  Luksemboorg
LU  sq  Luksemburg
LU  sr  Луксембург
LU  sv  Luxemburg
LU  sw  Luksemburg
LU  ta  லக்ஸ்சம்பர்க்
LU  te  లక్సంబర్గ్
LU  th  ลักเซมเบิร์ก
LU  tr  Lüksemburg
LU  uk  Люксембург
LU  vi  Lúc-xăm-bua
LU  zh  卢森堡

Scroll down the above list to see all the languages.
A more authoritative source, but in a less convenient form, would be the Unicode CLDR files you mention. In the file for Portuguese (for example) country names are listed as ""names / territory"" starting at about line 2340. 
"
data request - High Resolution Mineral Maps,"
The US government portal for this data is mrdata from from USGS:
Mineral Resources On-Line Spatial Data.
They also include some world maps with a list of external databases.
In particular you may be interested in the Mineral Resources Data System (MRDS).

In addition to interactive maps, they also offer a list of API web-services.
Just a couple of the services:

Data records near a geographic location
Data catalog records for a topic category

"
english - Relationship Advice/Advice Column Corpora?,"
English Language&Usage has some material that might interest you, under some of its tags like this one:
https://english.stackexchange.com/questions/tagged/politeness
The data is licensed under cc by-sa 3.0 with attribution required.
"
"data.gov - Weather radar data for Europe, especially Latvia","
Weather radar data specific to Latvia can be found using NOAA's Radar Data tool.  Just enter ""Republic of Latvia"" in the search box and you're off and running.
There are a large number of datasets covering international areas at the U.S. National Oceanographic and Atmospheric Administration (NOAA), which maps some of the global daily indicators. The Global Historical Climatology Network-Daily (GHCN-D) is the data you are seeking.
International indicators of climate change and data collected from many organizations are available as well. 
(Disclaimer: I am the Evangelist for Data.gov)
"
images - Where to contribute pictures of election leaftlets?,"
Unlock Democracy runs a project called Election Leaflets.
You can take photos, scan the leaflets, or even post them to their address.
They've got a good repository of leaflets.
"
tool request - Best way to convert Excel Files to Open Data Formats,"
I don't believe that you will find a ""ready"" library that will be the case for all your excel files. My advice is to create a script by yourself in base of your file structures. Especially if the updates will still have the same structure.
I cannot help you in PHP, but in Python you can find an answer for what you need here: A Python guide for open data file formats. If you decide to do it, you can update your question with an example of your files and let users help you with the code.
"
IP Geocoding Data Sources and/or APIs,"
There are three sources that may be helpful:

The answer on Stack Overflow on how to Geocode an IP Address
Free GEOIP has a RESTful API for this specific task
The Google Map API
A geolocation XML API from PInfoDB

"
What decentralized open data project has the most contributors?,"
I would say Wikipedia, with 21,436,641 named editors (and more anonymous editors), but there might be bigger projects.
The only other hard-numbers references I could find are the Ohloh project, which has tons of similar data, but it is only about open source projects, not all open data.
"
programming - PHP script automatically convert content in open data formats,"
I found this Wordpress Plugin WP-CSV
Details:

More than 50000 lines can be imported/exported (the only limit is
your server) 
Posts, pages, and custom post types
Tags, categories,   and custom taxonomies
Custom fields (simple and complex)
Thumbnails
Flexible filter system to easily control which fields export
Simple User Interface (if you know Excel or another spreadsheet
program, you will find this plugin quite easy) The plugin should now
be usable with most plugins that are fully WordPress compliant.

"
data request - Domain Name System Record A database,"
The DNS Census 2013 is an attempt to provide a public dataset of registered domains and DNS records.
The torrent is about 15 GB (uncompressed: 157 GB).
I am pretty sure it contains domain names and IPs, but I am not 100% sure as I have not downloaded it to check. Please let us know, thanks!
You can extract TLD from domain name.
"
data request - Indian caste dataset,"
Since the caste system has been officially banned, there have changes in Indian society that seem to complicate a straightforward connection between caste and last name.  Some of those changes include people choosing to change their last name, inclusion of other cultural norms in naming, and mobility within society.
However, there is still data available, but you might need to merge several datasets and sources together to achieve what you are looking for. There is a strong relation between region, caste, and name, and so sorting by region should give you better insights. The following might be helpful:

Scrape region, name, and caste from the nice compilation at, unexpectedly, Wikipedia and in Edgar Thurston's Ethnographic Notes in Southern India.
Datasets on caste-related information at Data.gov.in (but in general these do not include the last name of respondents or surveyed individuals)
There is additional data on caste in relation to many other variables from the Indian National Election Study (ranging from 1967-1985). 

These guidelines may be helpful in looking at the structure of Indic name data sources.
"
geospatial - Redistributable Twitter-like data,"
stackexchange data has practically all of those requirements:
http://data.stackexchange.com/stackoverflow/query/new
flickr does all of that, although i'm not sure if their use of tags constitutes hashtags, but i'm sure thats easily convertible.
you could also use my personal twitter history that i've downloaded. i could careless. maybe you could get a few more people to download theirs also, combine them into a repo on github?
you could also do the same with my delicious account, although there are no geotags in the set.
i think you could actually apply this to all of the social networks that you can download your personal data from and whose datasets match your requirements. 
identi.ca comes to mind. although i think they are diaspora now.
"
data request - Examples or datasets of evolving networks,"
One of my favorite (dynamic) networks to study is Wikipedia. You have plenty of data to analyze and it is evolving every second :)
Here is an example of the edits in Wikipedia articles: http://rcmap.hatnote.com/#en
However, if you want more options, in this repository, you will find dozens of networks and you could sort the table by ""Dynamic"" networks to find only those you need.
"
Video game meta-data (supplement for Steam API),"
There is an unofficial API for metacritic which can provide most of the data you request. You'll have to register.
Here is an example cURL request of the 'find game' endpoint:
curl --include --request GET 'https://byroredux-metacritic.p.mashape.com/details?url=http%3A%2F%2Fwww.metacritic.com%2Fgame%2Fplaystation-3%2Fthe-elder-scrolls-v-skyrim' --header ""X-Mashape-Authorization: <mashape-key>""

To loop over all desired games, you'll have to pass this type query for each game URL at metacritic. Creating that list of URLs may be a small project in itself (although at least they are logical).
That gives a JSON output:
{
  ""result"": {
    ""name"": ""The Elder Scrolls V: Skyrim"",
    ""score"": ""92"",
    ""rlsdate"": ""2011-11-11"",
    ""genre"": ""Role-Playing"",
    ""rating"": ""M"",
    ""platform"": ""PlayStation 3"",
    ""publisher"": ""Bethesda Softworks"",
    ""developer"": ""Bethesda Game Studios"",
    ""url"": ""http://www.metacritic.com/game/playstation-3/the-elder-scrolls-v-skyrim""
  }
}

To get more than one result per query, you'll have to construct some clever searches using the 'search games' endpoint.
They provide code for common languages (i.e. python), so you can perhaps use search to get and store multiple games.
"
data request - 3-dimensional dataset to test tri-clustering method,"
A good place to start looking for datasets is the Machine Learning Repository from UCI.
There are a few datasets that stand out for your method. Although they may not match exactly, you may get some ideas.

The YouTube Codedy Slam Preference Data
Character Trajectories
User Identification from Walking Activity

You can use the table columns ""Default Task"", ""Attribute Types"", and ""# Attributes"" to help narrow down more possible data sets.
"
data request - Total screen time and first on-screen appearance by actors in films,"
I don't have an answer but your request reminds me of a Slate.com article about characters shared scenes in Friends. The journalist explained his method:

To determine which characters shared scenes, I downloaded transcripts of all 236 episodes from this remarkably comprehensive fan site. For the purposes of this inquiry, I treated these transcripts as authoritative, as watching all 10 seasons was impractical. Due to inconsistencies in the transcripts, which do not always list all of the characters present in a given scene, I used the following methodology: If a character spoke a line in a scene, I marked him or her as present. Admittedly, this is an imperfect approach, as even with this gregarious group, there were scenes in which a character was present but did not have a line. However, in most instances, all parties present in a scene end up uttering a line, so I’m confident my analysis is sound, even if it missed a moping Ross here or a pouting Rachel there.

Interestingly, the author mentions the limitations of his method.
This method won't help you to measure exactly how much time an actor appear on-screen. But it may be a good basis to calculate how much time an actor is present in a scene, by measuring the time between his first and last sentence in a scene, then repeating it for every scene to get the total time in the movie. It may be a good approximation of the time an actor spent on-screen.
"
CKAN database scheme,"
CAVEAT: I am not an experienced user of the CKAN software.
From reviewing the online documentation, it appears that CKAN supports storing data in two methods: FileStore and DataStore. FileStore is storing and retrieving an entire datasets (e.g., CSV file, PDF document, etc). It does not involve a database.
The Datastore uses what they call an ad-hoc database for storing and querying individual elements of a structured dataset. In one mode, it appears to be fairly generic, creating a table and entries based on the column names in the structured data, and supporting an API query based on the column names.
http://docs.ckan.org/en/1117-start-new-test-suite/datastore.html
I believe it has more advanced capabilities and relationships in the structured data is linked data or RDF and is constructed using a vocabulary supported by CKAN, which include the Dublin Core, DCAT, VoID and SCOVO.
http://docs.ckan.org/en/1117-start-new-test-suite/linked-data-and-rdf.html
"
"government - Now I have an API key, how do I access OpenFDA?","
I'm one of the core team members for openFDA. Really sorry to hear that you find the documentation cryptic - one big goal that we have is to make the API as easy to understand as possible!
The main documentation for drugs has a number of sample queries: http://open.fda.gov/drug/event/
You can hit ""Run Query"" next to any of those and it will show the results right there in the page. You can edit the query, hit Run Query again, and it will update accordingly.
In regards to the API Key, it is optional. When you sign up, you will get an email from api.data.gov, who provides API keys for the entire federal government. We're working on customizing those emails to include an FDA example.
How would you recommend we improve the documentation?
"
json - Data In CSV Format from OpenFDA,"
Joe, we don't support CSV downloads for the time being. Drug adverse event data is highly relational, a format that does not easily lend itself to being represented in a format like CSV. For instance, a given record may have 5-10 different drugs associated with it and an additional 5-10 different reactions, each of which all have their own additional metadata.
If you have a recommendation on how we could easily represent this data in CSV, please do let me know!
"
data request - Are there any open datasets for Wrestling statistics,"
FILA keeps an online database of all matches for Amateur/Professional wrestling tournaments worldwide. These are ""Olympic"" class wrestlers (not WWE). It does not though contain collegiate events. It does have all Olympic related records dating back to 1896
http://www.fila-official.com/index.php?option=com_content&view=article&id=768&Itemid=100236&lang=en
The NCAA publishes it's national championship results here:
http://i2.turner.ncaa.com/dr/ncaa/ncaa7/release/sites/default/files/external/gametool/brackets/wrestling_di_2014.pdf
"
api - How to get total count of adverse effects events by manufacturer in openFDA?,"
Direct count queries are supported: e.g.
https://api.fda.gov/drug/event.json?count=patient.drug.openfda.manufacturer_name.exact
works without an accompanying search.  
I can confirm that it's not returning the full list of manufacturers.  The 1000 entry cap is to avoid the backend server overloading, but it should only be relevant for non-count queries: count queries are supposed to return all of the results by default, which is why skip is disabled for them.
Thanks for reporting this -- I'll file an issue to figure out what's going on.
"
api - Format for exchanging open data catalogs,"
I would propose that each site that hosts a Catalog of Open Data Portals has a downloadable version of the catalog in CSV format. To ease exchangeability, I would recommend the following layout:
Basic Level
url, short name, formal name, political entity, admin division, category, license
url : Url to the open data portal
short name: A short name for the data portal (abbreviation, acronym, etc).
formal name: formal name of the open data portal (e.g., City of Madison Open Data ....)

political entity: country or dependency (e.g., Canada)
admin division: administrative division type (e.g., state, province, county, town,...)
category: type of open data portal. some examples:

Data Portal (multi-category)
Transparency Portal (government finances, budget, payroll, expenditure)
GIS/Gazetteer (Geographic Information Systems, Maps)
Census/Demographics (Population Statistics)
Health
Education
Commerce/Transportation
Science
Historical
Military

license: type of license to use the data. examples:

CC0 (public)
CC-BY
ODbl
UK OGL
PDDL

Level II
This level would add information for geolocation and geographic scope that the portal covers.
url, short name, formal name, political entity, admin division, category, license, coord, area
coord : lat/lng of area centroid
area  : total area in sqkm
This should be fairly easy to looked up and added.
Level III
This level would add population scope that the portal covers.
url, short name, formal name, political entity, admin division, category, license, coord, area, pop, year
pop: population
year: year of population statistic
Level IIII
This level would add information on the amount and scope of information in the data portal:
url, short name, formal name, political entity, admin division, category, license, coord, area, pop, year, ndatasets, nrecords, subcategories
ndatatsets : the (approx) number of datasets in the portal
nrecords : the (approx.) total number of data records in the portal.
subcategories: a list of subcategories of the type of data available (e.g., crime, bus routes, business licenses, etc).
This would take a crawler to periodically crawl the portal to determine size and scope.
"
data.gov - Where do I type in my query in the API in openFDA.gov,"
@Joe Germuska - we built something like you mentioned (user-friendlier web interface to search Open FDA adverse events API) at http://searchopenfda.socialhealthinsights.com
"
government - Search for False advertising/drug name associated with patient complaint of false advertising,"
Joe is correct here. We don't currently offer data on patient claim of false advertising. In our scope for the future, we plan to offer data on product recalls and product labels. We're definitely listening to the community, though, and will add patient claims of false advertising to our list of potential datasets for the future!
Sean Herron, openFDA Team Member
"
Australia District/County level data needed,"
The link below is to the National Geospatial Agency (NGA) Geographic Name Server (GNS) for geographic features of Austrialia. The dataset has been converted to a linked CSV format and should be easy to parse. Your interest will be in records that are administrative divisions (NGA/GNS FCFC=A). The value of NGA/GNS DSG will tell you the type of administrative division:
ADM1: provinces and territories
ADM2: 2nd level divisions (e.g., shires)
ADMD: other smaller division (I think this includes districts)
http://www.opengeocode.org/cude1.1/NGA/GNS/AS.zip
The link below is to the Australian Statistical Geography Standard fact sheets. These PDF documents explain how the Australian Census is broken down into geographic areas and contain links to various downloads. The smallest unit are mesh blocks, which consist of approx. 30 to 60 dwellings. 
http://www.abs.gov.au/websitedbs/D3310114.nsf/home/ASGS+Fact+Sheets
"
data.gov - Seriousness values on OpenFDA,"
If you take a look at our API documentation (http://open.fda.gov/drug/event/reference/), you'll see a complete reference for every field we return and the range of possible values. We follow the E2b specification for adverse drug reporting (http://www.fda.gov/downloads/Drugs/GuidanceComplianceRegulatoryInformation/Surveillance/AdverseDrugEffects/UCM350390.pdf).
For the specific examples you provided, here's what our documentation reads:
seriousnesshospitalization

This value is 1 if the adverse event resulted in a hospitalization,
  and absent otherwise.

serious:

1 = The adverse event resulted in death, a life threatening condition,
  hospitalization, disability, congenital anomali, or other serious
  condition.
2 = The adverse event did not result in any of the above.

primarysource.qualification:

An encoded value for the category of individual submitting the report.
1 = Physician
2 = Pharmacist
3 = Other Health Professional
4 = Lawyer
5 = Consumer or non-health professional

"
programming - What is a good Python CKAN Tutorial?,"
Have you looked at https://github.com/ckan/ckanapi ? It is the official and current python client for CKAN. 
"
data.gov - Inconsistency in API results?,"
The first query only returning 1 result is probably a bug with the API. I would suggest reporting it. The total number of records for this query currently is 286,115. You can get more records using the limit parameter. This query will get you the first 50 records.
https://api.fda.gov/drug/event.json?search=patient.drug.openfda.pharm_class_epc:""nonsteroidal+anti-inflammatory+drug""&limit=50
You can use the &skip parameter to walk through the results. Yesterday, you could request fairly large limits. I noticed that today you get an error message if you set the limit to above 100 records. I think they retuned the number of records and limit after they launched on Monday.
They added this to the API Basics on the &limit option:
Return up to this number of records that match the search parameter. Large numbers (above 100) could take a very long time, or crash your browser.
"
openfda - Search for deaths by a specific drug,"
Here are two ways to do your query:

Using the OpenFDA API directly by specifying a brand_name and reactionmedrapt value: https://api.fda.gov/drug/event.json?search=patient.drug.openfda.brand_name:(tysabri)+AND+patient.reaction.reactionmeddrapt:(death)&limit=50&skip=0
Using a tool I helped build which is a searchable web interface to the OpenFDA data at http://openfdasearch.herokuapp.com/?drugbrandname=tysabri&patient_reaction=death

Hope this helps!
"
geospatial - Data on business locations,"
Can't comment so sharing advice in an answer: Can you be more specific about what type of location data?

If every type of category, you're probably best using APIs/licensed data from the likes of Infochimps, Google Places, Yelp, Foursquare, etc.
If you're doing something smaller scope like hospitals and clinics or maybe gas stations, you can probably find some open data about it that covers the majority of the data points and ultimately saves you money.

"
government - Model documents for presubmission meeting with FDA about mhealth pilot trial and risk assessment,"
While I am not part of the Open FDA team (I am a member of the community), I am pretty sure this is not part of the current Open FDA initiative which is at http://open.fda.gov. 
If I were you, I would start at the following links:

Rock Health's ""FDA 101: A Guide for Digital Health Entrepreneurs""
FDA page about ""Mobile Medical Applications""

"
api - OpenFDA adverse event counts by dates do not add up,"
Mark is correct - you need to specify the date field that you want to search on. receivedate tends to work best. 
"
api - How to get all the DrugList,"
The current OpenFDA Adverse Event dataset isn't really well suited for a query like that. An API for the drug SPLs (Structured Product Labels) is coming soon according to https://open.fda.gov/about/ though.
For now, check out the following resources for the data you're looking for:

NLM Pillbox API (http://pillbox.nlm.nih.gov) - recommended - most developer friendly 
NIH DailyMed bulk downloads and APIs (http://dailymed.nlm.nih.gov/dailymed/help.cfm#webservices)
FDA National Drug Code Directory bulk download and web search (http://www.fda.gov/drugs/informationondrugs/ucm142438.htm)

"
geospatial - High resolution population density maps in the US,"
As part of PL94-171, the canonical source for population data for drawing legislative districts is the Decennial census. 
To get the high resolution data you want, you can download shapefiles of blocks by state with population included from http://www2.census.gov/geo/tiger/TIGER2010BLKPOPHU/
Of course, if you want to get into demographics of the population of those blocks, that is also available from the decennial census. You can get race and basic age from the files released for PL94-171 -- that data is the first release from each decennial census so that redistricting can get started promptly. Of course, now that the decennial census is several years in the past, you can get complete demographic data, although dealing with it at the block level is not a simple task.
"
data request - Dataset with a specific demographic distribution due to user interface,"
Not a user-interface problem, but a related consequence: in Blue states watch more porn than red states, bad data collection led to the conclusion that Kansas consumes more porn (from pornhub.com) than any other state. 
Further review revealed that the IP geolocation service defaults to Kansas when it can't locate an IP address, skewing the data.
"
data request - Demographics by Zipcode,"
The Census Bureau has far easier avenues of extracting data through the American FactFinder (AFF). There are many tutorials for how to use AFF, but it is generally not been 'intensely hard' to use for me. By downloading the information in .csv format, you would be able to download the latest information from the ACS, or the 2010 Census.
"
data.gov - Weekly raster data for temperature and precipitation in US,"
NOAA puts out a weekly division dataset on temperature, precipitation, and drought that should answer your need.
In addition, weather radar data an be found using NOAA's Radar Data tool, and you can request data for any date range, including weekly.
There are a large number of datasets covering all areas of the world the U.S. National Oceanographic and Atmospheric Administration (NOAA), which maps some of the global daily indicators including the Global Historical Climatology Network-Daily (GHCN-D).
International indicators of climate change and data collected from many organizations are available as well.
"
economics - Should we open our data?,"
Quick answer with what comes to my mind. They might be several others.
Advantages

Since you lower the barrier to access the data (no more scraping), you will see more people using it and new usage of your data will appear
You will also receive feedback regarding your data quality (since there is more eyes looking at it under different angles)
This can be a marketing opportunity to introduce your company to new audience via partner website and application (I used your data on my website / application in return I mention you as the source)

Disavantages 

A competitor can reused those data and present it in a more user friendly was, with nice functionality and get straight after your current business.
Depending on the format used someone can reverse ingeneer your internal process / data structure and reuse it on a different market.

I think the next question you will need to asnwer is the license and release format (API, csv or database dump ...). Those can be different questions on OpenData SE)
"
Access to text entry speed data?,"
I am not aware of any dataset like this one. But if you Google ""typing speed test data"", you will see many websites that does exactly this. And most of them have a contact page. You shall contact with them (maybe all of them) and ask if they are willing to share their data.
"
data request - List of all ecommerce websites,"
Based on checkout usability performance these are the top 100 E-Commerce websites
http://baymard.com/checkout-usability/benchmark/top-100
"
data request - Resolve company name / TLD to industry?,"
DUNS is the best business list that I can think of -- but I have no idea if it has industry info in it.  Years ago, when you wanted an SSL certificate, registrars wanted your DUNS number to verify you weren't mascarading as another company.  Unfortunately, it's not an open list.
For something that's more likely to be free, for the U.S., there's a the EIN, which the government uses to track employers.  I'm not aware of any searchable list, though.   The SBA's website   points to the SEC's EDGAR for public companies, and GuideStar for non-profits.  This would miss private companies, though.
"
OpenFDA API : can we count on several fields?,"
Not at this time. I'm not 100% sure how the team behind OpenFDA is fielding feature requests but I've been putting ideas and requests like this on Github and they have been labeling them as ""enhancements."".
Link to OpenFDA github: https://github.com/fda/openfda/issues
EDIT: While OpenFDA was in beta, I built a drill down tool (see http://recordit.co/Hn65jq.gif for screencast) but it's currently broken because the API changed quite a bit.. if something like that seems useful to you I can work to resurrect it :)
"
usa - What data on violence against women in the U.S. is available?,"
The FBI provides summary data on crime statistics. These tables cover a variety of categories and geographic regions (national, state, county, Metropolitan Statistical Areas (MSA)).
The start page is here:
http://www.fbi.gov/about-us/cjis/ucr/crime-in-the-u.s/2010/crime-in-the-u.s.-2010/violent-crime
The world bank has a Excel spreadsheet for violent crimes against women on a per country basis. The data is summary (percent/totals). 
http://web.worldbank.org/WBSITE/EXTERNAL/TOPICS/EXTSOCIALDEVELOPMENT/EXTCPR/0,,contentMDK:22488819~menuPK:6835249~pagePK:148956~piPK:216618~theSitePK:407740,00.html
"
Where is the list of fields available through the openFDA API?,"
There are a few sources for the information you're looking for (essentially a data dictionary if I am understanding your question correctly):

List of OpenFDA Fields (data OpenFDA has added/annotated for developer and user convenience on top of the base FAERS data fields)
FAERS/AERS Mapping Notes which are part of the Github project and document some caveats about fields like primarysource.reportercountry
Lastly, the one of the great parts of the OpenFDA team using ElasticSearch as a backend is that you can query by literally any field you see returned to you. So you could look at the sample queries, look at the fields, and query that way.

Hope this helps! 
"
weather - What is the best source of open data on sea level rise in Bangladesh and India?,"
The NOAA National Oceangraphic Data Center (NODC) maintains data (and datasets) for sea levels and tides. You can get current (hourly) or historic data back to 1994. The data is collected from 289 stations around the world.
http://www.nodc.noaa.gov/General/sealevel.html
"
api - openFDA: Can we print results for multiple patients at once?,"
You're looking for the limit URL parameter which maxes out at a value of 100. 
For example: https://api.fda.gov/drug/event.json?search=receivedate:[2012-08-09+TO+3000-01-01]&limit=100
This is explained at the ""Query parameters"" section of the API documentation at https://open.fda.gov/api/reference/#query-parameters
"
data request - What is the national median salary in Poland?,"
Have a look at Eurostat's earnings database. There you will find the mean, median, 10th percentile and 90th percentile for hourly, monthly and annual earnings. These are gross earnings. For the moment there is data for 2002, 2006 and 2010. According to Eurostat, an update is due in 2016, with figures for 2014.
"
usa - USGS Various API Calls,"
I looks like you have your left/right longitude values mixed up. Try fixing this and post back your results.
"
medical - Is there a way to extract the brand names of drugs available in USA?,"
After downloading the zip file, use only the ""prescribable"" portion. (that are drugs currently approved for use, without many ""historical"" drugs)
The main file is RXCONSO.RRF which is 28MB.
It has 113290 rows and contains many different types of rows (e.g., synonyms).
To filter only ingredients, using R, do this:
conso<-read.delim(file = 'RXNCONSO.RRF', sep='|',header = F, stringsAsFactors=F)

conso[3:7] <- list(NULL)  #not used in this distribution, only in full UMLS

column names can be seen here 
http://www.nlm.nih.gov/research/umls/rxnorm/docs/2014/rxnorm_doco_full_2014-2.html#s12_4
names(conso)[1] <- 'rxcui'
names(conso)[3] <- 'rxaui'
names(conso)[7] <- 'sab'  #12
names(conso)[8] <- 'tty'  #13
names(conso)[10] <- 'str' #15

Use TTY term type column - value BN=Brand Name
#only brand names
bn<-subset(conso,tty=='BN',select=c('rxcui','str'))

and in bn data.frame is your result.
Sample result would look like this:
> head(bn,5)
    rxcui       str
1      38  Parlodel
73    332  Adipex-P
107   479   Alfenta
187   756 Anafranil
188   769   Anaspaz

In fact, to get only generic ingredient names, a similar query can be used like this:
#get only ingredients and CUI and name for each intredient
ingr<-subset(conso,tty=='IN',select=c('rxcui','str'))

"
"usa - Docket Information call returns ""Not Acceptable"" error in windows form app","
Use a network sniffer like Wireshark to compare:

The request that fails
The request that succeeds

There is probably a tiny difference in the request, that you can find out and fix.
Network sniffers are easier to use on HTTP, but if you are forced to use HTTPS, there are still various solutions.
"
dataset - 'Panel' data without unit identifier - Cross Validated,"
This is what is called a ""pseudo-panel"" data set.
Look up
Pseudo Panels and Repeated Cross-Sections
and 
Verbeek, M.: 1996, Pseudo Panel Data. Chapter 11 in: Matyas, L. and Sevestre, P.(eds.): The
Econometrics of Panel Data. A Handbook of the Theory with Applications.
If you have a model, then as an estimation benchmark I would suggest to run also basic pooled OLS (which is an estimator that doesn't really care whether your data are well ordered or not).
"
api - Results data for Basketball World Cup,"
There is a question about a soccer API which has answers with many sports APIs - LINK.
Also, check out my answer there about the ESPN Developer Center API, which should include the FIBA World Cup.
Note: The ESPN Developer APIs have been retired.
"
Is the Healthcare Finder API broken?,"
Sorry about that - there was a firewall rule blocking download of that file. We have fixed this and the link to the schema document should now be working.
"
How can I get raw occupational data from the census long form?,"
The American Community Survey (ACS) Public Use Microdata Sample (PUMS) is likely what you are looking for. The ACS includes occupational codes for respondents based on questions that people responded to in this questionnaire (page 11). In the PUMS dataset, there is the INDP variable which houses 2012 NAICS occupation codes which you look at in a data dictionary (e.g. The 2012 1-Year ACS PUMS (page 60)). The raw files are present on the Census Bureau's FTP server here.
However, if you are looking for the most approachable method to diving into the ACS PUMS, I recommend doing it through the University of Minnesota's iPUMS project. The data is open access, registration is free (although there is wait time for registration approval wait period that might take about a day). It allows you to download data that you can import into Stata and SAS.
"
openfda - open.fda.gov JSON search terms,"
See my answers to a question similar to yours here: 'Where is the list of fields available through the openFDA API?'. You could also just do a query for a random event report like https://api.fda.gov/drug/event.json?limit=5&search=morphine and then use a JSON visualizer such as jsonviewer.stack.hu look at the structure and fields in a more friendly way.
Hope this helps!
EDIT: Also, see https://open.fda.gov/drug/event/reference/ which includes an anatomy of a typical response. 
"
data request - List of English words,"
You can download an Aspell Dictionary, then  convert it to simple list of words: 
aspell -d en dump master | aspell -l en expand > my.dict

A few other dictionaries.
"
data.gov - What are open data sources about current sea state?,"
I answered a question similar to this that has a useful resource for current sea level information:
What is the best source of open data on sea level rise in Bangladesh and India?

>
  The NOAA National Oceangraphic Data Center (NODC) maintains data (and datasets) for sea levels and tides. You can get current (hourly) or historic data back to 1994. The data is collected from 289 stations around the world.
>
  http://www.nodc.noaa.gov/General/sealevel.html

"
"data request - High resolution, small area, maps that characterise natural terrain","
Elevation Data
For high resolution DEMs, which I define as 1/9 arc-second (approx. 3m) or better in the horizontal plane, i use a combination of the National Map viewer (http://viewer.nationalmap.gov/viewer/) where you can select and view the availability of 1/9 arc-second data across the US.  Here's a screenshot of the availability:

LIDAR collection is ongoing at the state and federal level (search USGS 3DEP program) but won't be complete for years.  Individual states can sometimes provide higher resolution LIDAR or DEM data, and most have GIS clearinghouses.  NY State's: https://gis.ny.gov/   Image of LIDAR availability at present for NY State:

Areal Imagery
The National Map viewer can also display the 1-foot aerial imagery coverage:

And and example of NY State's coverage:

Land Use
Other than the NLCD data set, which is too course for your purposes, I only know about the USFS Urban Tree Canopy assessment program (look it up, i can't post more links cause of my reputation).  Here's a snapshot of what that data can look like:

The bottom panel is what I think they create.
Doesn't appear to be readily accessible; may have to request downloads.  Only focused on urban/metropolitan areas.  May provide the plant ground cover data you are looking for.  You could also use the (non-spatial I think) Forest Inventory Analysis data from the USFS (http://www.fia.fs.fed.us/tools-data/), which provides species information, I think, and randomly assign it to different locations within either the UTC maps or other maps?  Just a thought.
Edit: Just noticed the Philadelphia imagery is 4 inches!!  I think this is a link to it, from checking out the metadata: http://www.pasda.psu.edu/uci/MetadataDisplay.aspx?entry=PASDA&file=PhiladelphiaCityMosaic2010.xml&dataset=1040
Also, now you have a link to the PA GIS clearinghouse.
Ground level photography?
The google street view API?  Have no idea if that would be useful for you.
Hope this helps.
"
"data request - World city database (with longitude, latitude) and population per year","
The United Nation Statistics Division publishes population totals and by demographics per country on an annual basis. This is called the UN Demographic Yearbook. It is normally in PDF format, but there are various areas on the unstats.un.org site that you can download EXCEL and CSV files.
A good start is here. This has downloadable tables between 2007 and present.
http://unstats.un.org/unsd/demographic/products/dyb/dybcensusdata.htm
"
data request - Programming titles,"
I would take a short list of programming langauges and then search those terms on a job website.
For example, searching indeed.com for 'python' returns job titles of 'Data Developer', 'Build Automation Engineer', 'Software Engineer - Data Infrastructure', and 31502 more.
I mention indeed.com because they have an API that you can use code to perform searches - LINK. You have to register to get the key, but it's free.
Once you've collected the thousands of job titles (and summary, descriptions, location, etc) for each programming language, you can do some basic stats to see which job titles are showing up most commonly (maybe the top 20 or something).
"
"data request - List of pop music genres, with sound samples, easy to download, open","
I'm afraid the curation of a dataset like this is an expensive proposition, especially if you take into account the complexity of copyright.  I certainly think it's optimistic to hope that it's already assembled into the format you describe.
The Free Music Archive (FMA) is an extensive source of free audio files, and it has 15 genre tags (including spoken). You'd be on your own to produce clips. This page on their site also has links to articles about the FMA, some of which sound as though they might also cover other free music resources.
Otherwise, I'd suggest a literature review of academic study of music genre. Maybe a paper will provide a lead on a resource like you describe that is not as readily found by Google directly.
"
"data request - open database of elementary, middle, and high schools in Latin America, Africa and Asia?","
The Education Policy and Data Center (EPDC) collects and summaries education related data from 200 countries in the world. While I don't believe they publish school location data, they do publish the sources of their data, which include link's to the countries dept. of education, census/demographic data, etc.
http://www.epdc.org/about-help/data-sources
"
tool request - Search engine for graphs where we can specify the axis we are looking for,"
Although not strictly open data, the in my experience most fruitful way to search for something like this is:

Search for the raw data (timeseries, tables, values) in studies/statistics oneself and prepare the graph oneself or
do a simple Google image search like graph hdd storage price.

"
machine learning - Free data of connections between role and skill,"
There are two free data sources that you can use, albeit both are pretty noisy and the job title is a part of a free text field, not an enumerated one. 
The first is crossing the user profile page and tagged posts from the Stackoverflow public dump. See Stackoverflow downloads from the internet archive and 
Database schema documentation for the public data dump.
The second is scrapping the LinkedIn public API for profile properties. These include both job descriptions and skills. Contrary to the first, you may have to wait for a while, because of usage limitations.
"
data request - Pokerstars hands database,"
As I know, buying hands are illegal for several poker sites. However, there are a few free databases that probably will help you.
1) University of Alberta A database with more than 10 million of poker hands
2) HandHQ.com A 70GB database of poker hands.
"
usa - What would be the cleanest / easiest method of collecting rental property data?,"
HUD offers a lot of rent related information (by County). For example, the 50th percentile estimates contain the medium rent per studio, 1, 2 and 3 bedrooms per county.
http://www.huduser.org/portal/datasets/pis.html
I also keep a copy, converted to our linked CSV vocabulary, of some of these datasets here:
http://www.opengeocode.org/cude1.1/HUD/PHA/index.php
"
data request - Dataset for Named Entity Recognition on Informal Text,"
Although the entity set is more restricted than you are looking for, the following might be useful:
https://github.com/sandeepAshwini/TwitterMovieData
The data is referenced in the following paper by Ashwini and Choi, which discusses and evaluates the general approach: http://arxiv.org/abs/1408.0782
"
Multiple attributes available in a single OpenFDA query,"
I think your question is very similar to this one: OpenFDA API : can we count on several fields? but here are some additional thoughts for you:
The short answer, as you will see in that question's answers and the asker's own edit, is that no, this is not currently possible in the openFDA API. You have to do multiple API calls which is how we* do it on ResearchAE.com. I can't speak about how they do it on the openFDA site though but if you take a screenshot or send a URL of what you're talking about I can try and dig into their site's code and let you know how they are doing it (the source code for open.fda.gov is on Github for anyone to see).
Full disclosure: my business partner and I built ResearchAE.com and presented about it and the power of the openFDA API at Health Datapalooza alongside the openFDA team.
"
data request - Dataset on donations to charities?,"
I am not sure if this is something that is required to be reported. 
Data like does this does exist for campaign/PAC contributions and lobbying money spent by corporations. The Sunlight Foundation has a great site called InflueceExplorer.com (which has a web UI, bulk downloads, and API). For example, a search for Microsoft.
I also heard from a contact well entrenched in the NPO space that major donors might be shown on NPOs' Form 990 that they are required to file with the IRS but I am not seeing it on the sample 990 at http://www.irs.gov/pub/irs-pdf/f990.pdf. You can contact and look at data from 990++ aggregators such as GuideStar, the National Center for Charitable Statistics, and CitizenAudit.
EDIT: Found this useful FAQ: ""Q: Where can I find out who has donated money to a particular nonprofit organization?"". Short answer: ""The list of donors filed with Form 990 is specifically excluded from the information available for public inspection, except for donors to private foundations and political organizations.""
"
data request - Dataset of football (soccer) penalties,"
I think FIFA used to keep this level of details on its archived statistics page. But now when you go to the url (http://www.fifa.com/worldcup/archive/) you get a blank page! I found this reference from The Guardian that aggregated the statistics per team going back to 1930 thru 2006 and made available to download as a spreadsheet. It does not have individual player stats:
https://docs.google.com/spreadsheet/ccc?key=0AgdO92JOXxAOdGtRLThiUUhYSnhackhXVm9qbm5aQ0E#gid=0
If somebody can find copies of the archive, Brenna Curley, Iowa State University, wrote a paper (April 2012) for analyzing the dataset using R.
http://www.public.iastate.edu/~curleyb/Stat585_Project_FinalPDF.pdf
"
data request - Where can I find sales figures on pharmaceutical drugs listed by manufacturer?,"
This is not open data but from my experience looking for data like this for quite a while even on a site with a lot of terms and conditions, the following is a relatively robust source:
http://www.drugs.com/stats/top100/units has sales and unit figures for the top 100 drugs of each quarter going back to 2011 and then top 200 from 2003 through 2010. The source is listed as IMS Health (Midas). 
Here are three Quora answers that were somewhat helpful to me while researching IMS' offerings a little bit further:

https://www.quora.com/IMS-Health/How-much-does-IMS-data-cost
https://www.quora.com/IMS-Health/How-is-a-contract-with-IMS-Health-structured-How-does-it-charge-pharma-companies-100M+-each-year
https://www.quora.com/IMS-Health/How-does-IMS-Health-get-their-prescrption-data


In terms of actual open data, the best I can find is usage by prescription drug class from the CDC: http://www.cdc.gov/nchs/hus/contents2012.htm#092 . It is table 92 ('Selected prescription drug classes used in the past 30 days, by sex and age: United States, selected years 1988-1994 through 2007-2010') of Health, United States, ""an annual report on trends in health statistics""
"
Where is data on stable currents of world ocean?,"
I am by no means a subject matter expert on the type of data you are looking for but am good at finding open data :)
Have you tried http://www.oscar.noaa.gov/datadisplay/oscar_datadownload.php?pagetype=nonjava by any chance? Data comes down as a NetCDF file but there are file readers for different operating systems (I used Panoply for Mac) and then you can export to CDL which could be parsed by any number of programming languages. 
NOAA has other ocean current data sets and information at http://www.nodc.noaa.gov/General/current.html too
Is this on the right track for the data you're looking for?
"
data request - Fortune 500 CEOs/executive boards,"

For publicly traded companies, Google Finance or another similar site will list the officers and directors. For example, GOOG: https://www.google.com/finance?q=google
For private companies, I have found investing.businessweek.com to be a good source. For example, Socrata: http://investing.businessweek.com/research/stocks/private/snapshot.asp?privcapId=41354884

Either of these could be screen-scraped within reason or maybe they even have APIs.
"
openFDA adverse reaction Serious vs Expedited?,"

For serious values mappings, see this similar SE question: Seriousness values on OpenFDA. The short answer is (quoting Sean Herron):

1 = The adverse event resulted in death, a life threatening condition, hospitalization, disability, congenital anomali, or other serious condition.
2 = The adverse event did not result in any of the above.

For fulfillexpeditecriteria and other variables you may come across, I suggest referencing the PDF both Sean and I mentioned in the linked SE question: http://www.fda.gov/downloads/Drugs/GuidanceComplianceRegulatoryInformation/Surveillance/AdverseDrugEffects/UCM350390.pdf . That document is not specific to OpenFDA but instead the source system upstream from the API OpenFDA exposes. The short answer is (quoting the PDF):

Value is 1 (1=yes) for identified expedited reports (15-day); 2 for all others. 


EDIT: You can see more information on the fields at http://www.fda.gov/downloads/Drugs/DevelopmentApprovalProcess/FormsSubmissionRequirements/ElectronicSubmissions/UCM149932.pdf as well. For example, the following for fulfillexpeditedcriteria:

Does this case fulfill the local criteria for an expedited report?

Be sure to remember that different countries might have different standards
"
data request - Song lyrics and timings,"
Sheet music and MIDI comes to mind. You could use XML versions of sheet music to get the timing... http://www.musicxml.com/music-in-musicxml/
You need to add personal touches to music  composition to make it soulful. Otherwise, it can come off as being very cold.
You could also look at music videos with closed caption or subtitles as a way to extract lyrics and timing.
"
analysis - Examples of useful applications that are being developed using open data,"
I'm so glad you bring up this question. Please take a look at the following links as well:

http://www.opendata500.com/
http://www.datainnovation.org/
http://www.socrata.com/products/custom-web-and-mobile-apps-government-data/ (scroll down to 'Featured Apps')
http://www.pinterest.com/socrata/open-data-applications/

"
data request - Scraper for Openstreetmap: all south-american schools to mysql-db,"
Here is some information on how to get closer to what you might be looking for.
One well-organized source for information like this is a site called GeoNames.org. GeoNames has a number of APIs as well as downloads (""dumps"") which are discussed on their export page.
For your use case, an export seems to make sense. Each country's POIs (points of interest) are downloadable in .zip frhttp://download.geonames.org/export/dump/. Each includes a README file describing the data. The data should be able to be opened in a spreadsheet application such as Microsoft Excel or Apple's iWork Numbers and similarly imported into database and analytics engines such as MySQL, MongoDB, MS Access, Tableau, and much much more.
If you're interested in only schools, you will want to determine which GoeNames feature codes you are interested in and only import those into your database/collection. These codes/classes correspond to the text files' seventh and eighth columns as documented in the README file under the heading ""The main 'geoname' table has the following fields :""
I imagine someone may be willing to write a program to download the exports for each South American country and import only the schools (note there are several different type of codes which map to schools depending on your definition of what a school is) into a MySQL server or just CSV file. To make it easier, I would suggest you define what exactly you're looking for and maybe attach a bounty to it.
Hope this helps!
"
data request - Rocket attacks dataset in Israel and State of Palestine,"
Try the Global Terrorism Database from the National Consortium for the Study of Terrorism and Responses to Terrorism (START) project at at the University of Maryland. Here's a brief snip from their about page

The Global Terrorism Database (GTD) is an open-source database
  including information on terrorist events around the world from 1970
  through 2012 (with additional annual updates planned for the future).
  Unlike many other event databases, the GTD includes systematic data on
  domestic as well as transnational and international terrorist
  incidents that have occurred during this time period and now includes
  more than 113,000 cases. For each GTD incident, information is
  available on the date and location of the incident, the weapons used
  and nature of the target, the number of casualties, and--when
  identifiable--the group or individual responsible.

The GTD database can be downloaded after filling out a form at http://apps.start.umd.edu/gtd/contact/ -- so ""kind of open data.""
START got the contract to collect terrorism data for the US State Dep.
after WITS: the Worldwide Incidents Tracking System was discontinued in 2012.
Some of the original WITS website is in the Wayback Machine This page of reports has a link for ""exports"" at the top, but they didn't seem to work for me.
Also, I found the RAND Database of Worldwide Terrorism Incidents, although that only covers 1968-2009 according to their database scope page.
"
"data request - Natural hazards in California - Historic time series, spatial resolution (lat/long) & Climate Model Forecasts","
Check out the content at Data Basin, which has a lot of California data.
"
data request - Colors of political parties,"
Here is a list for UK political parties - (link).

By doing similar searches, you can find other lists hosted on Wikipedia.
http://en.wikipedia.org/wiki/Wikipedia:Index_of_Minnesota_political_parties_meta_attributes


I think with a few of these lists you can make a joined list of major parties. Because there are so many parties, the colors will start to overlap and be indistinguishable, so I'd stick to just a few main parties.
"
"geospatial - Seeking Water Quality Data for Lake Ontario that includes Dissolved Oxygen, Nitrogen, Phosphorus?","
The only data I could find on some of these parameters is from http://ontario.ca's 'Provincial (Stream) Water Quality Monitoring Network' which can be accessed at http://www.ontario.ca/environment-and-energy/provincial-stream-water-quality-monitoring-network .
The raw data is available from http://www.ontario.ca/environment-and-energy/provincial-stream-water-quality-monitoring-network-pwqmn-data in MS Access, Shapefile, and MS Excel formats where each row represents a sensor readings which include the following sensor descriptions:

PHOSPHORUS,UNFILTERED TOTAL
NITROGEN,TOT,KJELDAHL/UNF.REA
DISSOLVED OXYGEN

Hope this helps!
"
data request - Standardized tests questions databases,"
Thanks @Unihedron
National Center for Education Statistics releases a subset of questions.
I would not call this truly open, but it is a start.
"
"openfda - ""hypotension"" due to carbamazepine in open FDA?","
One way to query for that is with the following URL:
https://api.fda.gov/drug/event.json?search=patient.drug.openfda.generic_name:carbamazepine%20AND%20patient.reaction.reactionmeddrapt:hypotension&limit=10
If you are looking for a graphical representation, you can use a tool my company developed which is completely free and shows this information in a web page at the following URL:
http://www.researchae.com/adverseevent?from_date=1900-01-01&to_date=2014-07-16&from_age=&to_age=&country=GLOBALLY&patientsex=&manufacturername=&drugbrandname=&druggenericname=carbamazepine&medicinalproduct=&reactionmeddrapt=hypotension&drugclass=&drugindication=&indsubmit=&productndc=&safetyreportid=
"
sports - Any open public data sets for the English Premier League (EPL)?,"
There is a list of soccer datasets and APIs here from this related SO question. Hope that helps.
EDIT: Quoted text from the article linked above, as requested:

openfootball has started a free (open source) public domain football
  database. The data is historical data, meaning no lives scores but the
  data does include the schedule, teams and players for the upcoming
  2014 World Cup along with global league data. This is a very promising
  project and has the potential to be the definitive source for
  historical data for the public. The data is stored in various repos on
  github. Start browsing and contributing at github.com/openfootball.
  See the opensport Google Group for discussion and questions.
footballsquads.co.uk has current and historical squad details for
  clubs and national teams from all across the world for many leagues
  and competitions, including the 2014 World Cup squads.
Rec.Sport.Soccer Statistics Foundation (RSSSF) has massive collection
  of formatted plain text statistics. An example of English Premier
  leagues results.
ESPN API has an API for registered users (free). You can get a list of
  all the players in the EPL. However they are very limited in their
  data. They restrict all fixtures and scores to “strategic partners.”
  However, you can get lists of players and teams.
opta Playground has a developer program that provides very limited
  access to historical data. The site reads “Opta can provide data for
  programmers wishing to develop a mobile app or website with selected
  historical data available to download.” You have to request permission
  in an email. I applied and they sent me the xml data set for 10 rounds
  of games from the start of the 2007/2008 Bundesliga 2. The more
  detailed game data had either x,y coordinates of game events. A very
  impressive dataset but it felt more like an advertisement. The data
  provided I had no interest in and I’m not sure why an indie developer
  would spend time working on a data set they could never afford.
StatsFC used to have an restful JSON API of all EPL scores and
  fixtures. It was about $8 us dollars a month but was recently shut
  down. There is no doubt it was related to data rights. See their
  official statement.
CrowdScores beta is UK company trying to crowd-source the football
  data collection process. You sign up for an account and report game
  events to their servers. They have web/iphone/android interfaces for
  reporting. They reward the top reporter with a season ticket. They
  data collection process is ideal but they might have to work on the
  incentives. I believe a better incentive would be to allow the
  reporters who contribute access to an API of all the data collected.
openfooty API had promising API documentation but a quick look at the
  developer forums shows a stale community and questions about why no
  one seems to actually be able to get a developer key.
football-data.co.uk has made a lot of historical league data available
  as csv files. The data includes results and a lot of betting/odds
  related data. I have tried to aggregate and clean up the data in the
  following repo github.com/jokecamp/FootballData
www.european-football-statistics.co.uk is a visually dated website but
  has a lot of historical football data (mostly an overview of
  league/tournament results) displayed in nice clean HTML tables. Looks
  like they already have 2014 EPL stats.
openligadb.db has an old-school windows asmx web service with methods
  such as “GetGoalsByMatch()”
Linked Soccer Data is a white paper on one group’s attempt to “create
  a dataset including reliable information about soccer events covering
  as many historical data as available including recent competition
  results.” Some dead links but worthwhile to skim.

"
legal - Web crawling to create a business,"
In the UK the Computer Misuse Act from 1990 (section 1) states that it is an offence to access a computer where:

the access to the data is unauthorised
the offender knows that it is unauthorised 

This may extend to web scraping because, in some circumstances, you are accessing the website intending to use the data in a way that is not authorised by the website owner.
Thus, the conservative and recommended way is to ask the owner for permission.
"
Are patient ages in openFDA specific to months of age (for infants) or only age in years?,"

For patient age in months, yes, it is possible but might not be as easy as you'd like it to be. As you will see from https://api.fda.gov/drug/event.json?search=receivedate:[1900-01-01+TO+3000-01-01]&count=patientonsetageunit (and referencing the values for patientonsetageunit from https://open.fda.gov/drug/event/reference/, the majority of reports in the database have ages defined in years but 40,000 or so have an age unit in months, weeks, days, or hours. You could, of course, convert those non-month units into months.
In terms of gestational age, I do not believe that data is available in the public data files which openFDA is built off of.


Use caution no matter what you end up doing. It looks like 46.19% of the 3,643,453 reports in the openFDA API are missing an age
Hope this helps!
"
releasing data - Datasets for smaller towns or villages,"
I was a Town Commissioner for 6 years of a small town (population ~640, but we're also the County Seat, so our daytime population is over 3000; exact number dependent upon how many people have jury duty that day)
Due to the limited staffing, I'd recommend that small municipalities only collect the data that they're being asked for or obligated to provide, and not attempt to guess what data might be useful to people in advance.
In our monthly newsletter, we list crime statistics, breaking them down by general category and if they're in a residential area or downtown.  Originally, we just broke them down by type, but then someone started asking if it was downtown or in one of the neighborhoods.
We report our income and expenditures on a monthly basis, based on the categories that are defined in our detailed budget, along with the status of all cash accounts (amount in the accounts and status of collatoralization agreements).
Due to our small size, we've had to take a number of measures to mask some data.  For example, when we moved towards being self-insured for health insurance, we consolidated line items that had been in each of the departments.  The reason being that we had a department with only one full time person, and so if any money came out of that item, we felt it may leak too much information about that employee's health.  We also have each employee as a separate line item, so it's possible for anyone looking at the budget to determine an employee's pay.  It's doubtful that a larger municipalities would have these issues to be concerned with.
"
tool request - Open data & perl,"
I don't know if there are any specifically in this field ... however, as catalogs go, there are Koha and Evergreen, which are ILS systems (Integrated Library Systems).
It might be possible to modify them for use in data cataloging ... you'd likely need to change out the underlying schemas, as I assume they'd be using MARC.
"
data request - French equivalent of the brown corpus,"
Via Twitter, I asked a friend with expertise in computational linguistics and French. She stated that the French Treebank is the largest tagged corpus. Since both it and the Brown corpus are described as about 1M words, I don't think you'll find another French one which meets your final condition.
It also seems, if I understand correctly, that the French Treebank is all sourced from Le Monde, so even it fails the second condition (""various sources"").
Ultimately, her question was why the French Treebank can't be licensed, as the answer might help to make a better recommendation.
"
openfda - Dietary Supplement data,"
I do not have an official answer (I am just a community member passionate about the project)
Dietary Supplements - YES
If you search for a recent dietary supplement report from http://www.fda.gov/ForConsumers/ConsumerUpdates/ucm153239.htm on the https://api.fda.gov/drug/enforcement.json API endpoint, you do get results. Note this is the drug enforcement report endpoint, not the food enforcement report endpoint.
For example: https://api.fda.gov/drug/enforcement.json?search=bee%20pollen&limit=1 matches the most recent update from the aforementioned FDA link (http://www.fda.gov/ForConsumers/ConsumerUpdates/ucm401676.htm).

Veterinary Drugs - NO
Animal and veterinary recalls and market withdrawals appear to be listed at http://www.fda.gov/AnimalVeterinary/SafetyHealth/RecallsWithdrawals/default.htm. When I do a search on the food and drug enforcement report API endpoints, I don't get any results.
When I do a search on the device enforcement report endpoint, I get some results when I search  keywords for recent animal drug recalls... however they have older dates so they must be previous ones. Bottom line: doesn't seem like recent animal/veterinary drug recalls are in any openFDA at this time.
"
openfda - Data about the safety of Da Vinci Robotic Surgery,"
Jerry,
While I am not comfortable offering advice on the saftey of a device I will offer a method in which you can do your own research leveraging the OpenFDA API. Here is a search of medical device recalls for the term ""Da Vinci"" - http://www.researchae.com/recalls?reporttype=device&from_date=2004-01-01&to_date=2014-11-30&search=Da+Vinci
"
CMIS compatibility: TCK reports data for all ECM products,"
I found nothing so I created one: http://cmissync.org/CmisCompat
It contains for each ECM server:

The raw TCK test data
Some metrics
A calculated score


Creative Commons CC-BY-SA.
Data is hosted on Github, so collaboration is via pull requests.
"
data request - List of programming languages,"
Ohloh is the worlds largest tracker of open source projects, it compares activity level as well as programming languages by lines of code, might be useful if your interested in use stats and trends. https://www.ohloh.net/tools
"
web crawling - How to crawl data from a library website,"
First, you should check for any license for using the site to see if you can use their data for any reason (commercial and non-commercial).
Second, you should read the terms of use and find anything about scraping website. Many websites don't allow scraping.
Third, if non of the above is a problem, you should use a programming language to crawl and scrap the content from the website. Do you have any knowledge of any programming language?
"
data request - Where can I get historic prices for a commodity?,"
Index Mundi has a lot of commodity data (present and historical) available to the public.
But I am not sure if you would find toilet paper on the list (lol) - it does not exactly fit the definition for a commodity:
""a raw material or primary agricultural product that can be bought and sold, such as copper or coffee.""
http://www.indexmundi.com/commodities/
"
data request - Is there any dataset for problems common people are facing to build apps against?,"
It's generally difficult to build applications to scratch someone else's itch without having a vested interest in it (such as getting paid).
You're typically better off trying to find something that bothers you, and figure out how to solve or mitigate it.
If you really can't think of anything, there are hackathons and 'data jams' where they bring together people w/ programming skills & data experts to try to come up with uses for the data:

National Day of Civic Hacking
Capital Code Data Jam (MN)

You may also have local groups working on projects that you can get involved with.  Some meet virtually, others get together for regular 'hacknight' events, instead of it just being an annual or non-reoccuring thing:

Code For America
Code For DC Hacknight
Random Hacks of Kindness

If you're a student, there are various 'summer of code' projects, where various projects solicit for things they need done, and students can get stipends to work on them.  (the other still needs doing, so if you're willing to volunteer, there's likely still work they need doing).  Here are a few, although there are other groups with similar programs:

Google Summer of Code
Rails Girls Summer of Code
ESA Summer of Code in Space

And tomorrow & Wednesday is the Mozilla Science Lab's Summer Code Sprint.
... and these are just a sampling of all of the various groups out there -- there's a whole lot more.  You can also reach out to various community groups and see if they have need for some programming help.
"
data request - Undirected graph datasets with node attributes?,"
Here is short list of attributed networks I know:

DBLP co-author
SocioPatterns

I'm new user restricted to two links...
You can also search for KDD2012 tencent weibo dataset.
"
data request - Streets of Mauritius,"
They have a map (Google tiles) with waypoints but no additional road data: gov.mu/English/Map/Pages/default.aspx I didn't see a data download link on the page, but I'll keep digging.
I did find that their police utilize services from SuperMap (which is not open source/data): http://www.supermap.com/en/html/solutions722161.html
And for the sake of context, here is a paper discussing the need for more GIS in Mauritius not too long ago: http://www.ncgia.ucsb.edu/conf/gishe97/program_files/papers/beedasy.html
"
programming - How do I access data from 3taps API in C#?,"
I would suggest learning about what an API is from 'An Introduction to APIs' by Zapier and then some more specific information about consuming JSON APIs w/ .NET from 'Making JSON Web APIs with ASP.NET MVC 4 Beta and ASP.NET Web API'
Unirest is a simple HTTP request client to look into as well.
"
data request - Database of predominant religion by country?,"
The Factbook includes a religon entry, for example, the entry for Canada reads:
Roman Catholic 42.6%, Protestant 23.3% (United Church 9.5%, Anglican 6.8%, Baptist 2.4%, Lutheran 2%), other Christian 4.4%, Muslim 1.9%, other and unspecified 11.8%, none 16% (2001 census)
You can get all the data in the public domain in JSON from openmundi/factbook.json for example. Canada example in JSON:

 ""religions"": {
      ""text"": ""Roman Catholic 42.6%, Protestant 23.3% (United Church 9.5%, Anglican 6.8%, Baptist 2.4%, Lutheran 2%), other Christian 4.4%, Muslim 1.9%, other and unspecified 11.8%, none 16% (2001 census)""
    }

"
data request - Database of neighborhood between countries?,"
The Factbook includes a land border entry, for example, the entry for Burkina Faso reads:
3,193 km - Benin 306 km, Cote d'Ivoire 584 km, Ghana 549 km, Mali 1,000 km, Niger 628 km, Togo 126 km
You can get all the data in the public domain in JSON from openmundi/factbook.json for example. Burkina Faso example in JSON:

""land_boundaries"": {
      ""total"": ""3,193 km"",
      ""border_countries"": ""Benin 306 km, Cote d'Ivoire 584 km, Ghana 549 km, Mali 1,000 km, Niger 628 km, Togo 126 km""
    }

"
data.gov - OpenFDA API for drug Label changes,"
As of right now, SPL (structured product labels) are not part of the openFDA list of APIs for drugs (https://open.fda.gov/drug/event/ - note how ""product labels"" is greyed out). You can learn more about the SPL standard, guidance, etc. directly from the FDA at http://www.fda.gov/ForIndustry/DataStandards/StructuredProductLabeling/default.htm.

While you wait for the openFDA project to serve SPLs, there is a robust (and actively improved - latest update was February 2014) website (with web services!) maintained by HHS/NIH/NLM called DailyMed. Information on DailyMed web services can be found at http://dailymed.nlm.nih.gov/dailymed/help.cfm#webservices .
From my intermediate knowledge of SPLs, they are versioned by a unique GUID called ""set IDs"" as well as an addition ""SPL version"". There is an API endpoint for getting the history of a specific SPL Set ID as well as an API endpoint for finding the drug record (including set ID) in the first place.
Also note that the DailyMed site maintains an RSS feed for each set ID (such as http://dailymed.nlm.nih.gov/dailymed/labelrss.cfm?setid=c5fdde91-1989-4dd2-9129-4f3323ea2962) as well as an RSS feed for all changes in the past 7 days (http://dailymed.nlm.nih.gov/dailymed/rss.cfm).
Hope this helps!
"
Delay in openFDA's drug recalls/enforcement reports?,"
As has been updated on the github issue (https://github.com/FDA/openfda/issues/24), we're now updating this endpoint bi-weekly.
"
data request - List of all universities by country,"
Wikipedia's sister project Wikidata provides data about more than 18,000 universities with a varying amount of detail.
You can use the Wikidata Query Builder to generate the following SPARQL query:
SELECT DISTINCT ?item ?itemLabel WHERE {
  SERVICE wikibase:label { bd:serviceParam wikibase:language ""[AUTO_LANGUAGE]"". }
  {
    SELECT DISTINCT ?item WHERE {
      ?item p:P31 ?statement0.
      ?statement0 (ps:P31/(wdt:P279*)) wd:Q3918.
    }
  }
}

This returns a list of all instances (P31) of university (Q3918) and any of its subclasses (P279) (such as institute of technology or law school).
By slightly tweaking the above query, you can even generate an interactive tree view of all countries and universities backed by live data from Wikidata.
"
api - seriousnesscongenitalanomaly in openFDA Adverse Events,"
Thanks for the complement on our ResearchAE.com project. Glad you found it useful and as an example of what people can do with the openFDA data.
To answer your question, I believe the query you mean to be doing is this:
https://api.fda.gov/drug/event.json?search=_exists_:seriousnesscongenitalanomali&limit=5

Clickable link for that openFDA query: https://api.fda.gov/drug/event.json?search=exists:seriousnesscongenitalanomali&limit=5
"
data request - How can I retrieve a list of companies that deal in a specific field?,"
You can get a list of UK registered limited companies from Companies House that also contains the SIC Codes for each company.
These aren't always that informative, but they provide a starting point?
"
europe - Open company data for the Netherlands,"
openkvk is a project to provide Dutch company data. The project has been running since 2009 and there's a mature API.
"
government - Where can I find column definitions for ed.gov data?,"
Did you download the dataset directly off of http://www.ed.gov/developers ? If so, a lot of metadata and data discussion can be found in the PDF file linked to the right of the orange CSV, JSON, XML, and API links. Attaching a screenshot below.
EDIT 1: One more trick -- if you are using the API endpoint, switch rows.json in that URL to columns.json and you'll get some more information such as what the name of the column is as opposed to the lowercase/parameterized fieldName but the PDF should still have more information and explanation.
Does this help? If not, which data set are you referring to?

"
economics - How to estimate subjective value?,"
This data-request is very interesting but not trivial at all. (Hence, the somewhat late answer.)
What you refer to as subjective price, economists would often call reservation price. The reservation price is the maximal price a person would pay for a good, or put differently, her valuation for that good. As you rightly observed when you called it the subjective price, the reservation price differs from person to person.
This is where the difficulty starts. If a house is sold at 100,000$, for example, we do not know any person's reservation price. The person who bought the house might well have paid more if needed. Conversely, for the people who did not buy the house it is likely that their reservation price was below 100,000$ (as the seller would have accepted this price) but we cannot say what their exact reservation price is.
This being said, in the market for houses there are some tricks, you can use to get more information about people's reservation prices. Ideally, you could look at auction data. Auctions have the advantage that a lot of people make bids and thus, there will be more than one person that will at some point give away information about her reservation price. If person A buys the house for 100,000$ but before B has made a bid of 95,000$, you know that B's reservation price must lie somewhere between both numbers.
But it gets even better: for you data request it will be ideal to look at Vickrey Auctions (also called second-price sealed bid auctions). These are auctions where everyone gives a sealed bid, the highest bidder wins but only pays the second highest price. This is similar to an auction of ebay where the winner also pays only the second highest bid (and not her own) and where bids are essentially sealed if everyone votes in the last second. Standard economic auction theory shows that every person should truthfully report her reservation price in a Vickrey Auction. Thus, if you simply look at the bids people made in such an auction you will get a very good picture of people's subjective value.
So for your data request, my suggestion is that you look at the bids given in Vickrey Auctions in the housing market. I am not too familiar with the housing market per se, but I assume that especially for very expensive houses, auctioning them off would be one way how they are usually sold. Otherwise, if you are not interested in subjective value for houses but in general, arts auctions might be a good point to start. Of course, such data is not publicly available (without asking) and I cannot provide you with any link but if you (or anyone else, since this is quite old) is really interested in the matter for research, writing to auction houses would be a good starting point.
"
OpenFDA API: Can I perform search by same field with various values,"
Here is the correct query syntax for an 'OR' search:
https://api.fda.gov/drug/enforcement.json?search=classification:(""Class+I""+OR+""Class+II"")

I took a look and the result count equals the count of Class 1 + Class 2 so this should be what you were looking for.
More information on openFDA ""API Basics"" are at https://open.fda.gov/api/reference/#query-syntax though there was not a specific example for an OR search that had spaces in it.
"
usa - IRS Codes in machine readable format,"
It's difficult to know when to use the Stack Exchange ""answer"" field for an inconclusive answer, but after a review of the IRS's website, I don't see much sign of this, particularly for the 1f code on that form, but really, for anything. The most prominent source of structured data from the IRS that I found is the Tax Stats section, which isn't what you're looking for.
There's an extensive section for ""e-file providers"", including some PDF docs that articulate valid code values, but nothing machine readable that I see. Maybe they make that sort of stuff available to approved e-file providers?
Note that the 1099 form instructions you referenced are also available as HTML as well as PDF, although you'd still have to scrape free text.
This seems like a case where you are best off asking the IRS directly, both to get an authoritative answer, as well as to register public interest in more structured data from them. Also, while I know that tech folks are often inclined to simply fill out a form or send an email, this may be a case where a phone call would pay off. Sometimes (but definitely not always), a real-time human interaction helps cut to the chase, and can result in social referrals to people with better information or more access than you can find by impersonal electronic means. However, the IRS has not been very forthcoming with publishing machine readable data thus far.
"
data request - How many Veterans have been or are currently in Congress,"
Vital Statistics on Congress from the Brookings Institute has some data about this available in Excel.
The ""Vital Statistics Chapter 1- Demographics of Members of Congress"" workbook has a worksheet, ""1-8"", which has the title ""Prior Occupations of Representatives, 83rd - 113th Congresses, 1953 - 2014""  Veteran is one of the lines.
"
openfda - Class 1 and 2 medical device recalls,"
The short answer is as follows (here is a clickable link though) which searches for recalls initiated during June 2012, that were classified as Class I or Class II, and had the word ""radiation"" somewhere in the recall documentation.
https://api.fda.gov/device/enforcement.json?search=radiation+AND+classification:(""Class+I""+OR+""Class+II"")+AND+recall_initiation_date:[2012-06-01+TO+2012-06-30]&limit=100 
There are a number of caveats to keep in mind though:

This query relies on ""radiation"" being included in one or more of the fields listed at https://open.fda.gov/device/enforcement/reference/ such as product_description but there is absolutely no guarantee radiation drugs will mention that keyword.
This query also relies on recall_initiation_date be present which appears to only be there for 59% of all the device enforcement reports available in the API (4639 of the 7855). Note that field report_date seems to have 100% coverage. Maybe the recall_initiation_date field is only available after a certain date? More digging needs to be done to make that call.

"
data request - Product Reviews,"
amazon:
This dataset consists of reviews from amazon. The data span a period of 18 years, including ~35 million reviews up to March 2013. Reviews include product and user information, ratings, and a plaintext review.
http://snap.stanford.edu/data/web-Amazon.html
beer advocate reviews:
http://snap.stanford.edu/data/web-BeerAdvocate.html
rate beer reviews:
http://snap.stanford.edu/data/web-RateBeer.html
movie reviews via rotten tomatoes:
https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews
"
How to work with RDF formats,"
RDF has many different serializations (or formats); there is no ""RDF format"".
As you say it’s XML, it’s most likely the RDF/XML serialization.
You should not use a usual XML parser to work with it; you’ll want to work on the RDF triple level instead.
So you need an RDF-aware tool that can input RDF/XML; or you can convert your file from RDF/XML to a more human-readable format like Turtle, and use a tool that inputs this serialization.
To query the data, you’ll want to use SPARQL.
There are many tools (Redland librdf (C), Jena (Java), …), and I can’t give a recommendation, but the sister site 
Software Recommendations might help.
See also a list of tools in W3C’s Semantic Web wiki (also with links to other off-site lists).
"
data request - Tagged addresses,"
http://openaddresses.io/
It's an initiative to collect open (CC0) addresses around Unites States and globally.
You could transform these as you like..
"
Data sets for evaluating identity resolution,"
If you are looking to generate large data sets and don't mind putting in a bit of work you can use the data set generator in febrl from the Australian National University (Project at http://sourceforge.net/projects/febrl/ and documentation for dataset generator http://cs.anu.edu.au/~Peter.Christen/Febrl/febrl-0.3/febrldoc-0.3/node70.html).
It requires that you give it a dictionary of terms for each field with frequencies and possible misspellings etc and then input the probabilities that this row has an error/is a duplicate etc. There are some dictionaries bundled but they are Australian based but they give you an idea of how to create your own.
It then generates a file with the original record and the duplicates which it identifies for you. Could be useful?
"
best practice - Open data build project examples,"
I started the open football project - that collects public domain football data (e.g. World Cup in Brazil, English Premier League, Bundesliga, Champions League etc.).  Nothing new other than having public domain (license-free, no rights reservied) datasets in plain old text.  
What's different is that all tools and scripts including, of course, the build scripts are public domain (and open sourced) too and, of course, part of the project.  
For example, to build a single-file SQLite database e.g.  worldcup.db that includes all World Cups from 1930 to 2014 use the Ruby make tool, that is, rake. Example: 
$ rake build DATA=worldcup

That's it. Cheers.
PS: The same ""system"" works, for sure, for other topics. See, the open beer and brewery project as another ""real-world"" example that includes build scripts that let you build ""The Free World Beer Book"" (e.g. $ rake book).
"
Census API data across superior geographies,"
The answer to your first question, ""Is there a way to issue a request with just tract/blockgroup/block FIPS codes?"" is no, as you found in the documentation.
The answer to your second question, ""If not, could one be implemented?"" is not really a great question for a Stack Exchange site. I don't think Census Bureau folks are monitoring this site, although other open data advocates in the US Government are, so maybe I'm wrong, or maybe that will change and they'll start coming around. (and having them use OpenData.StackExchange.com would be a big gain over http://apiforum.ideascale.com/)
As someone who has been working on a project (Census Reporter) which includes a Census data API, handling API requests for fine-grained Census data is hard to get right and make it perform well. I know that Census Reporter really doesn't handle bulk data for block groups well. The slowest part of our app is calling for arbitrary geojson for maps on our block group profile pages (example).
In short, this is an example when an API is probably not the right way to deal with this open data.
If you can at all handle it, I suggest you look into simply grabbing the bulk data that you need and updating your GeoJSON to include it. It doesn't change that frequently, so making API calls for it is, as you observed, complicated for you, and also likely to be hard to make perform as well as you would like.
"
Where can I find data about sharing propaganda music videos on Facebook?,"
I doubt that there's an open data set for this, since it's a new subject and so many parts of it are changing rapidly. Also, it's not terribly clear what kind of data you're looking for, how it would be quantified, etc. 
I'd suggest looking for academic research in the field. Some of the researchers may have posted their data, or they may be open to requests for access.
Gilad Lotan has recently published some interesting stuff about the network connections.
http://globalvoicesonline.org/2014/08/04/israel-gaza-war-data-the-art-of-personalizing-propaganda/
https://medium.com/i-data/israel-gaza-war-data-a54969aeb23e
Also, I found this one about YouTube: The YouTube Jihadists: A Social Network Analysis of Al-Muhajiroun’s Propaganda Campaign
"
usa - National Scale (contiguous US) weather data set for 1980 - 2010,"
this set doesn't get to 2010 but does start 20 years prior (1960), and its gridded:
http://www.columbia.edu/~ws2162/dailyData.html
"
Where can I find data sets that have no API?,"
Another option would be to look at the datasets registered to a US government agency on Data.gov and then contract that list against known APIs from that agency.  
So - for instance, filter for datasets from the US Department of Transportation on Data.gov and compare that list against the APIs listed for the Department of Transportation on this page (Cmd/Ctrl+F for 'Federal Aviation Administration' and start there).  
Disclaimer: I am Sr. API Strategist at the General Services Administration. 
"
data request - AIS (Automatic Identification System) or The Long-Range Identification and Tracking (LRIT),"
Some time ago I was searching for shipping graphs to try playing with routing, but I found a little.
Here's some links:
- http://geocommons.com/maps/109850 (contains a density on the arc)
 http://gizmodo.com/see-the-global-shipping-revolution-in-these-beautiful-o-1556851187 (links to a dataset I couldn't process)
I wonder if there are complete dumps somewhere...
"
data request - 2014 Ebola outbreak dataset,"
It sounds like you're looking for a line listing. There are no case data available for this outbreak, but like Skram said, I collect and maintain data from various sources on github. Sierra Leone and Liberia release some case data on the province/county level; Liberia's data is quite good. The WHO used to include town names for Guinea, but it has since stopped doing that. Another thing to be aware of is that the only dates for this outbreak are report dates. We have nothing on onset dates, hospitalization dates, death dates, etc. My contact info is on my github, feel free to write if you want to chat more. -Caitlin
"
"data request - Any dataset containing the price/charge that patients ""actually"" pay for their health care service?","
You are looking for the variables in MEPS (the Medical Expenditure Panel Survey from AHRQ) with the text slf in them.  There are both utilization-level files as well as a consolidated person-level file available every year.
http://www.asdfree.com/search/label/medical%20expenditure%20panel%20survey%20%28meps%29
"
data request - Large list of quotes,"
Have you seen WikiQuote by the Wikimedia Foundation?
There is an API endpoint at http://en.wikiquote.org/w/api.php which uses the standard MediaWiki API for there are API clients in many different languages.

EDIT: Two WikiQuote API-specific links: https://stackoverflow.com/questions/13762688/wiki-quotes-api and http://bwgz57.wordpress.com/2013/02/14/in-search-of-quotes/
"
"standards - Does an Authoritative Definition of ""Dataset"" Exist?","
No, there isn't.
The most in-depth analysis that I'm aware of is the 2011 paper by Renear, Sacchi and Wickett, ""Definitions of dataset in the scientific and technical literature"", in which they decided that there were four basic concepts that reoccurred (grouping, content, relatedness, purpose), but weren't completely consistent when dealing with different communities.
There have been other attempts to deal with the term 'data', which has significantly different meaning in the hard sciences, information science, and commputer science communities.  See for instance, Ballsun-Stanton's 2010 paper, ""Asking about data: Experimental philosophy of Information Technology"".
...
As it's been so difficult to get fixed definitions, a few years ago, Todd King and I attempted to collect terms that we saw as problematic when attempting to talk about data systems with folks from other science disciplines.  As it's been stable for more than a year now, we should probably try to publish it.  (I wanted to provide cross walks to terms in OAIS and other vocabularies, so I'm the one holding it up).  We specifically gave up on trying to define 'dataset' :

Please note that the terms dataset, data product and data series are not defined here. Although all refer to a grouping of data granules, the terms are used inconsistently across disciplines; in solar physics, a dataset is a collection of data products while in earth sciences, a data product is either a collection of similar datasets or a classification of datasets. These terms should be avoided, or clearly defined when used.

There's also an effort going on right now through the Research Data Alliance as part of their Data Foundation and Terminology Workgroup to come up with an overall data model ... I haven't been following their work as much as I should (and I still need to look at the draft they sent me a couple of weeks ago), but they should be releasing something for their plenary meeting at the end of the month.
"
companies - Open company data for Germany?,"
Aswath Damodaran, Professor at Stern School of Business at New York University, has been compiling information on major corporations since 1998. His EU dataset contains data on 6,000 EU public corporations, including those in Germany.
http://pages.stern.nyu.edu/~adamodar/New_Home_Page/data.html
Quandl has free datasets on current and historical stock prices for companies listed on the Frankfurt Stock Exchange:
https://www.quandl.com/FSE
"
parsing - Parse Wiktionary Data Dump XML Into MySQL Database,"
I found some hints for the schema here https://meta.wikimedia.org/wiki/Help:Export#Export_format
To read manually the XML, try using a viewer like:

http://www.readfileonline.com/
http://www.swiftgear.com/ltfviewer/features.html (windows)
head -n [numberoflines] dump.xml (gnu/linux terminal)

"
data request - Open replacement for cfbstats.com NCAA football CSV's,"
the wayback machine is your friend
http://web.archive.org/web/20140128204311/http://www.cfbstats.com/blog/college-football-data/ 
EDIT:
this data is now hosted on open data se's datahub.io account:
http://datahub.io/dataset/college-football-statistics-2005-2013
"
data request - College student suicide dataset,"
Here are two half-answers that hopefully can lead to a full one.
It seems a data set doesn't exist, with the exception of @Skram's comment about the WISQARS database.

But there are many journal papers that have done studies, and by browsing them you can find lists. It's unstructured, but you can structure the data with the fields you provided in your question.
To start, the SPRC has a page on college prevalence (LINK), with links to pages of research studies. Their white paper (PDF) talks about the Big Ten study being the most comprehensive. 

The Big Ten Student Suicide Study (Silverman et al., 1997), undertaken from 1980
  to 1990 to determine the suicide rate on Big Ten campuses, was the most comprehensive
  report on the incidence of suicides in undergraduate and graduate school populations
  by age, gender, and race. The study collected demographic and correlational data on
  261 suicides of registered students at 12 Midwestern campuses.


Silverman, M., Meyer. P., Sloane, F., Raffel, M., & Pratt, D. (1997). The Big Ten
student suicide study. Suicide and Life Threatening Behavior, 27, 285–303.

As expected, that paper is behind a firewall. But I think this is the kind of data that will require some digging. Emailing authors and asking if they are willing to share the data would be a good start.

Another option would be to read news stories, for example, with the NYTimes API. Suicides reported in the NYTimes would be either high-profile stories or local New York / New Jersey colleges. Perhaps a network of news APIs could 'grep' stories from across the country.
"
data request - Amount of people in each job U.S,"
You're looking for data from the Bureau of Labor Statistics. Probably the Current Employment Statistics, which provide a breakdown of all employees by NAICS industry/sector.
"
best practice - Extending North American Industry Classification System (NAICS),"
I don't know of any existing standards (de facto or not) for extending NAICS, but what I would do is start with the NAICS Index File, which ties over 19,000 industry names to the standard 1,000 or so NAICS codes. Take the list of index entries for each NAICS, give each one a 2-digit sequential ID, and tack this onto the standard NAICS to make it 8-digit (assuming no more than 100 index entries for any given NAICS).
To keep with the spirit of NAICS, underground or illicit activities should be coded using similar standard codes, or under new codes in the hierarchy close to legitimate businesses that use similar inputs or production processes. E.g., ""herbal"" grow operations would go in ""11: Crop Production"". Smuggling operations would be in ""48-49: Transportation and Warehousing"". You may also want to look at a humorous anecdote involving a BLS recommendation for the SIC coding of legalized prostitution in Nevada in the 1970s.
"
federal - Is ISO 19115 (Geographic Metadata) a closed standard?,"
Short answer: because ISO makes money selling the technical specifications. It's not closed the standard per se, but the specification (so if you're writing software you'd need the document.
Searching on the net I found the first edition of the standard ftp://podaac.jpl.nasa.gov/misc/outgoing/ed/pre_2013/GHRSST_metadata/ISO%2019115%20.pdf
Also from NOAA some info http://www.ncddc.noaa.gov/metadata-standards/
"
data request - Street gang dataset,"
bureau of justice stats, but i'm not sure how detailed (to the level you want) this will be:
http://www.bjs.gov/index.cfm?ty=tp&tid=36
not datasets but you can find stuff:
http://www.fbi.gov/about-us/ten-years-after-the-fbi-since-9-11/just-the-facts-1/violent-gang-initiatives
http://www.nationalgangcenter.gov/
possible winner:
http://www.nationalgangcenter.gov/About/Related-Web-Sites
criminal stats archive has alot too:
http://www.icpsr.umich.edu/icpsrweb/NACJD/studies?archive=NACJD&q=gang&permit[0]=AVAILABLE&x=0&y=0
"
metadata - Where can I find a taxonomy of open data sites?,"
First Caveat, I am a co-founder of opengeocode.org
We have a few resources that might help you:

Catalog of Open Data sites around the world
Specification for open data vocabulary
Detailed specification on representing open data
Papers related to Open Data

"
Searching for a Mergers and Acquisitions (M&A) panel data set.,"
The effects of mergers: an international comparison (Gugler, Mueller, Yurtoglu, Zulehner) indicates that its ""principal source of data"" is the Global Mergers and Acquisitions database of Thompson (sic) Financial Securities Data (TFSD). Their ""data description"" (section 4) doesn't describe substantial follow-on work.
Starting from that reference, it appears that the Thomson database is the most highly regarded. (See ""Sources for M&A Information"" from the University of Chicago library. U of C library also refers to MergerMarket, which has free league table ""data"" (in PDF form) from 2011 on their site. The PDFs are computer generated, so would probably lend themselves to data extraction with a tool like Tabula.
Other papers I found described their methodology as something which sounded more labor-intensive, but perhaps if you contact authors, you might get more information about what they did and if they'll share it.
"
metadata - Open Data about Open Data,"
I'm not really sure that a StackExchange site is the best way to organize the kind of information you're looking for. I actually think a Wikipedia category is more appropriate and, indeed, one already exists. The CC-licensed data subcategory looks particularly useful and could be expanded.
"
openfda - Is it possible to receive complaint count in monthly format?,"
Here is a small Python snippet that takes your URL and produces a output that you can load into Excel.
import requests
from collections import defaultdict
monthly = defaultdict(int)

url = 'https://api.fda.gov/drug/event.json?search=receivedate:[20110101+TO+20150101]+AND+brand_name:lantus&count=receivedate'
r = requests.get(url)
r = r.json()
data = r.get(u'results',u'')

for item in data:
    count = item.get(u'count',u'')
    ddate = item.get(u'time',u'')
    if ddate != u'' and count != u'':
        dkey = ''.join(ddate[0:4])+'-'+''.join(ddate[4:6])
        monthly[dkey] += int(count)

for item in monthly:
    print item, monthly.get(item)

Gives as an output:
2011-08 569
2011-09 505
2011-02 432
2011-03 462
2011-01 439
2011-06 560
....

Which is in an Excel-importable format.
"
data request - Solfège (do re mi fa sol la si do) sung by human voice,"
If you know how to make midi files, then you can use a vocal sound font (like choir sound fonts) to play that midi file. You can find a collection of choir sound fonts here.
Here are the steps:

Make a midi file with the software of your choice (I don't exactly know how to do that, so can't recommend any, but maybe you can try Aria Maestosa)
Download a choir sound font. Or you can just Google for it.
Install VLC media player and then install the font you downloaded. An instruction can be found here.
Play the midi file.

"
Data over artisan bakery sales and earnings,"
I haven't seen the detailed data you are looking for. But, there are several datasets of farmer's markets in the US. Some of them contain contact information and general categories of goods sold.
The USDA American Marketing Service (AMS) maintains a dataset of 8,200 farmers markets in the US: http://search.ams.usda.gov/farmersmarkets/ 
I have my own version of this dataset converted into a Linked CSV format: http://www.opengeocode.org/cude1.1/USDA/AMS/index.php
The Federation of New York also has a published list of farmer's markets and details in the State of New York. This can be found at:
http://www.nyfarmersmarket.com/farmers-market-profiles/markets/markets.html
CalPoly also has a research dataset (available to the public) on one years worth of sales/goods from a bakery chain in 4 states:
https://wiki.csc.calpoly.edu/datasets/wiki/ExtendedBakery
The UC Irvine Machine Learning Repository also keeps a restaurant related research dataset covering 130 restaurants:
https://archive.ics.uci.edu/ml/datasets/Restaurant+%26+consumer+data#
"
"medical - Where is the ""product problem"" field in the openFDA API?","
This is a good feature request.  We are accepting these request via Github (https://github.com/FDA/openfda/issues) and we will keep you posted there.
"
Open Data for Quiz Game,"
I suggest you take a look at the following Q&As:

https://stackoverflow.com/questions/11067191/public-domain-trivia-database-for-game
http://ask.metafilter.com/16200/Where-to-download-Public-Domain-general-knowledge-trivia-question-bank

However, I don't think you're going to find a data set for you to download and import into your game. One idea would be to do some NLP on open-ish sources like Wikipedia to extract facts and turn them into questions and answers.
"
education - Open Source GRE Data,"
Anki is a flashcard software.
It has a large collection of shared decks, curated by the Anki community.  
In particular, Anki has many decks of GRE content:
https://ankiweb.net/shared/decks/GRE
That list contains a few false positive, but when you filter them out you can see that there are about 120 decks about GRE:

极品GRE红宝书    
Verbal Workout GRE 4th edition (The Princeton Review)   
GRE Words (with examples, antonyms and notes)   
要你命3000（GRE核心词汇考法精析）    
極品GRE紅寶書(繁體中文)+聲音檔  
極品GRE紅寶書(繁體中文)  
High Frequency GRE words with Bengali Meaning   
GRE Words List  
GRE Vocabulary in Sentences     
極品GRE紅寶書(繁體中文)+聲音檔 2014.05 新版   
新GRE核心词汇考法精析（再要你命3000）  
GRE Vocabulary  
GRE&GMAT阅读难句    
Barron's GRE    
GRE词汇精选     
GRE词以类记     
GRE Vocabulary-High frequency words with example sentences  
GRE Tara    
GRE Wordlist    
G.GRE精选分频(剔除四六级):5星(1)+4星(13)+3星(78)+2星(1129)   
I.GRE精选分频(剔除四六级):0星(2449)   
H.GRE精选分频(剔除四六级):1星(2959)   
GRE High-Frequency Words    
GRE for sis 4   
GRE for sis 1   
GRE Words List wj   
GRE Words   
Anthony's GRE Flash Cards   
Kaplan's GRE Word Groups    
GRE Psychology Physiological Psychology     
GRE for sis 2   
GRE Vocab   
GRE for sis 3   
GRE Psychology Sensation and Perception     
GRE High Frequency  
GRE Deck 1 sept 7   
GRE for sis 7   
GRE Vocabulary  
GRE     
GRE红宝书（安卓版修改）       
GRE for sis 5   
GRE Master Word List - 5000+ Words  
GRE1    
GRE Psych Deck 9/24/2013    
WCG's GRE Words     
GREFANG     
GRE sentences part 3    
GRE Psychology Cognitive Psychology     
GRE Vocabulary 2    
math GRE    
GRE Words with Similar Meanings     
GRE Vocab   
GRE High-Frequency Vocabulary   
GRE Vocabulary  
GRE Antonyms    
Classical Mechanics - GRE   
GRE vocabulary  
GRE Red Bible - Freq 3+     
GRE Psychology Social Psychology    
GRE words   
GRE Vocab   
GRE Parts of Speech     
GRE 490 - all words mattf   
GRE Verbal 1100+Some Magazines  
Kaplan GRE 500 Vocab    
AneesHussain GRE Wordlist Vocabulary    
GRE (math verval)   
Optics and Wave Phenomena - GRE     
1736 Words for GRE  
Electricity and Magnetism - GRE     
Barron's 799 Essential Words for the GRE    
GRE Gifty   
Kaplans GRE words   
GRE 1100    
GRE Psychology Research Design  
GRE红宝书2012  
GRE Vocabulary for Verbal Section   
Barrons GRE 4813 wordlist pron, EN, RU  
GRE Word Groups     
Princeton Review GRE Hit Parade     
High Freq GRE   
GRE May Vocab   
GRE(set1 of 300)    
GRE Roots (Graduate Record Examination Roots)   
Angela's Top 200ish Vocab for the New GRE Verbal    
GRE High Frequency Words    
GRE words (with examples)   
GRE(Set2 of 180)    
GRE words part 2    
GRE High-Frequency Words (K)    
GRE word list 1     
GRE vocabulary  
GRE- Flashcards     
GRE Vocabulary 2014     
GRE words list 800 with 300 high frequency tags     
GRE vocab 1     
GRE Psychology Learning and Ethology    
GRE Vocabulary List     
GRE words part 3    
GRE-Barron's    
GRE Psychology Developmental Psychology     
GRE Duy Anh     
GRE-Personal Practice   
GRE for sis 6   
Kaplan GRE words    
My GRE Wordlist     
Physics GRE     
GRE Literature  
GRE 3978 words with my own sentences added  
Thermodynamics and Stat Mech - GRE  
Self Input GRE Words    
Psychology GRE  
My GRE  
GRE Psychology Personality and Abnormal Psychology  
GRE红宝书带音标   

Each deck is a collection of questions and answers.
Quality and focus varies from one to the other.
"
"Open data for chemical substances, structures and products?","
You may try to download Structures, Sequences and Ligand free of charge from:

Protein Data Bank (RCSB) by specifying PDB IDs which you looking for (e.g. 2bg9),
PubChem by NCBI (BioAssays, Compounds and Substances),
ChemSpider (free chemical structure database providing fast access to over 28 million structures, properties and associated information),
Wikipedia (Chemical_substances) by downloading the whole database or specific category (e.g. Chemical substances, Chemical compounds, etc.) by using Special:Export into XML format.

Also check out few free chemistry softwares which could potentially contain some databases:

Yenka Chemistry (before Crocodile Chemistry) - complete virtual laboratory which offer free home licenses for personal, non-commercial, non-academic use only.
Gamess and Quantum ESPRESSO - Open-Source computer codes for electronic-structure calculations and materials modeling at the nanoscale

See also:

Which Chemical Database Software Are Available?
Program that simulates basic reactions in organic chemistry

"
data request - List of vocabulary (kanjis) for the JLPT1 exam,"
Have you tried the AJet resources (Japanese Language Proficiency Test Resources)?
"
data request - Open database of enterprise software prices,"

A source at the US federal government level is GSA Advantage! which allows schedule holders (government contractors) to list products and services that government agencies can purchase much like an online shopping cart.
This is by no means clean or normalized data and runs into the issues described in the comment thread above about discounts/sales.

Another one at the US federal government level is USASpending.gov which has it's own data issues but is supposed to be comprehensive. This will show you prices actually paid under contract/grant/etc. rather than list prices. Unfortunately, usually only a top-line number is available and transparency of the actual contract terms is not what many wish they were. Here's a sample search for FileNet where you can filter to see only purchases directly from FileNet Corporation versus other resellers/service providers.


"
"data request - Free icons of country flags, reusable in both open source and commercial products","
There is a Google Internationalization project called Region Flags that structures the Wikipedia data.

This package is a collection of flags for BCP 47 region codes. Most people think of these as country flags, but there are a few codes / flags that do not correspond to countries. The flags are in SVG and PNG format and named by their BCP 47 region code, which for countries is the same as ISO 3166-2 country code.

The license is based on the Wikipedia license, which are flag-dependent

The flags are downloaded from Wikipedia. When Wikipedia flags were copyrighted, we worked we Wikipedia editors to either relicense them, or drew / sourced and uploaded new public-domain versions.

How to get data:

To download missing flags, run download-wp.py.
To update to latest flags from Wikipedia, delete the html, svg, and png directories, then run make-aliases.sh followed by download-wp.py.

Data formats are PNG and SVG, where SVG can be easily and safely be used at any resolution.

"
data request - Sports results datasets,"
the olympics site seems to have all of the data you seek:
http://www.olympic.org/athletics
but it looks like the opposite of open...  
the guardian's data store has some of what you seek:
http://www.theguardian.com/sport/series/london-2012-olympics-data
2012 open data
http://www.theguardian.com/sport/datablog/interactive/2012/aug/03/london-2012-results-open-data
more guardian data, in google drive spreadsheet, THIS actually looks close to what you seek:: https://docs.google.com/spreadsheet/ccc?key=0AonYZs4MzlZbdHlfd0F1QlAxYjgtOW53ZXNOZ0JzNVE#gid=0
all records for 2012
https://docs.google.com/spreadsheet/ccc?key=0AonYZs4MzlZbdFNaMTRsVDNiV1RZaWNGdmJDU1RSSGc#gid=0
some gis data for you:
http://geocommons.com/overlays/16680 
wikipedia looks like an optimal choice for data selection; but (i think) you're going to have to work for it:
picked a medal sport: pentathalon; to vague; drill pentathalon down to a random year:
http://web.archive.org/web/20091103091009/http://en.wikipedia.org/wiki/Modern_pentathlon_at_the_2008_Summer_Olympics
still not the detail wanted...but if you click on the details link in the medalists table, you'll find the information you seek:
https://en.wikipedia.org/wiki/Modern_pentathlon_at_the_2008_Summer_Olympics_%E2%80%93_Men%27s
note: i only did this for one medal sport, so i'm not sure if its the same throughout, but knowing wikipedia, i'm sure its the same throughout ;).
offhand, you should get a list off wikipedia off events with links, and use a scraper to ping them all, appending the appropriate details info onto each url (""-Mens, -Womens"", etc); i'm willing to bet the table with the detailed info has an id attribute, which i would then nail down to in the scraper, scooping up it and its contents...  
you can ping nbc's olympics site via wayback machine for more data, at least going back to athens 2004; wayback had links even older, although i'm not sure if they're the same format
http://www.nbcolympics.com/index.html
http://web.archive.org/web/20040804003859/http://www.nbcolympics.com/index.html
http://web.archive.org/web/20060504132456/http://www.nbcolympics.com/index.html
http://web.archive.org/web/20140103124519/http://www.2008.nbcolympics.com/modernpentathlon/resultsandschedules/index.html
just to show a few
BONUS:
datavis of olympic data howto:
http://datavisualization.ch/inside/how-we-visualized-112-years-of-olympic-games/
olympic datavis example gallery:
http://www.visualizing.org/galleries/peoples-choice-visualizing-london-2012-olympic-games
"
licensing - Is it possible to use CKAN for commercial use?,"
Yes.
CKAN is licensed under Affero GNU GPL v3.0. From the preamble to the license:
When we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things.
Sources: http://ckan.org/developers/about-ckan/, http://www.gnu.org/licenses/agpl-3.0.html, http://www.gnu.org/licenses/gpl-faq.html#GPLCommercially
"
data request - Global births for past 10k years?,"
This author (Scott Manning) reviews various sources for historical earth population
http://www.scottmanning.com/content/year-by-year-world-population-estimates/
At this link, he has an excel file showing various sources for estimating back to 10000 BC.
https://spreadsheets.google.com/pub?key=pbb0aoD3hdM-HgD-knTGXIA
"
Daily european weather data of past few years for non commercial use,"
In the past I've used the Weather Underground API, which allows for 500 calls per day. See my answer here for some details - LINK.
In their TOS they say you can even use for commercial use (but read the small print!)

WUL licenses to you a limited, worldwide, non-exclusive, non-transferable, revocable license for your personal or commercial use.

My python 2.7 code snippet (shared here). Because there are only 500 calls allowed for free per day, I retrieved one year per day:
import requests
def get_precip(gooddate):
    urlstart = 'http://api.wunderground.com/api/INSERT_KEY_HERE/history_'
    urlend = '/q/Switzerland/Zurich.json'

    url = urlstart + str(gooddate) + urlend
    data = requests.get(url).json()
    for summary in data['history']['dailysummary']:
        print ','.join((gooddate,summary['date']['year'],summary['date']['mon'],summary['date']['mday'],summary['precipm'], summary['maxtempm'], summary['meantempm'],summary['mintempm']))

if __name__ == ""__main__"":
    from datetime import date
    from dateutil.rrule import rrule, DAILY

    a = date(2013, 1, 1)
    b = date(2013, 12, 31)

    for dt in rrule(DAILY, dtstart=a, until=b):
        get_precip(dt.strftime(""%Y%m%d""))

"
data request - In what programming language(s) is software X written?,"
Wikidata has a property called programming language that should provide the data you are looking for.
Examples: Skype, Tomcat
You can also have a look at a list of available items that contain the property ""programming language"".
"
releasing data - Rating system for websites,"
The Better Business Bureau has a listing of ""Safe, Secure and Approved Places to Visit Online"", but I have no idea how they license their data as their 'terms of use' link redirects to their front page.  
(and the cynic in me thinks that companies paying to be included, which is BBB's economic model, is a conflict of interest (as their 'customer' are the businesses, to de-list a company is to lose a customer)
"
data request - Flashcards to remember all Ingress glyphs,"
I've just created an anki deck based on the ingress.tv glyphs. It is available for download as an apkg here.
https://www.dropbox.com/s/745sda96j070fyg/ingress.glyphs.apkg?dl=0
One of the differences is my deck contains known 2-5 string sequences as well (well, as shown on ingress.tv). Note that the deck you have found is based on glyphtionary - I previously used this as well but found it had some minor errors (I believe they have been corrected now)now.
"
csv - Where to upload open data? (no online data editing needed),"
Well my comment received a number of up votes which I take as a signal of quality and I am posting the links here so they are more visible to future visitors:

opendata.socrata.com - you can upload a number of different file types here, create visualizations, link to them, and take advantage of a very mature set of APIs for data consumption and publishing
datahub.io - like opendata.socrata.com, datahub.io is an open data portal but it runs CKAN which is free and open source unlike Socrata. CKAN is modular and you can write plugins for it but in my experience, out of the box, Socrata has better visualization and mapping tools
github.com - You mention SourceForge and I would recommend taking a look at github instead. You will have limited analytics compared to the first two sites which are built specifically for what you are looking for but Github has a lot of advantages like the ability for folks to fork your code and they do render CSV and GeoJSON documents nicely now. Also take a look at this blog post by OKFN (folks behind CKAN) for some suggestions on how to use Git (and Github) for Data
quandl.com and modeanalytics.com - these sites seem promising but I have not had a chance to work with them as much and they do not appear to be as feature complete or widely adopted

"
best practice - Hackathons and opendata,"
One resource I have sent many people to (and they always find value in it) is Socrata's* ""hackathon in a box"" guide: http://hackathon-in-a-box.org/guide/
The Open Knowledge Foundation also has a guide which I have not read as closely at http://blog.okfn.org/2012/10/26/hackathons-the-how-to-guide/.
And finally, please at least skim ChallengePost's blog post titled ""Hackathons ≠ developer exploitation. A what-not-to-do guide for good hackathon organizers."" 

*Full disclosure: the company I co-own is a partner of Socrata and they have sponsored hackathons we have helped organize.
"
metadata - Data set for identifying columns as categorical or numerical,"
Two things:

Existing functionality: The two main open data portal software platforms I have worked with, Socrata and CKAN, have this functionality built in. Socrata is closed source while CKAN is open source.

The add-on part of CKAN that handles this is called DataPusher (https://github.com/ckan/datapusher) and it ""pushes"" data from readable files like CSV to the DataStore extension. Documentation at http://docs.ckan.org/projects/datapusher/en/latest/. The code related to choosing column types is at https://github.com/ckan/datapusher/blob/6175c69e9ab7013272949e6e604fc45d21fce853/datapusher/jobs.py#L282 which uses the messytables python library which can be found at http://messytables.readthedocs.org/en/latest/

Check out data.gov, datahub.io, and others for a lot of open data to ""train"" from. Notably:

For Socrata API endpoints, such as https://open.whitehouse.gov/Government/2010-Report-to-Congress-on-White-House-Staff/rcp4-3y7g you can get column metadata at URLs such as https://open.whitehouse.gov/views/rcp4-3y7g/columns.json. Be careful though, this is no longer documented on dev.socrata.com from what I can tell and might be deprecated/unsupported
For CKAN, you need to find the resource ID of a datastore-backed resource, and you can create a URL such as http://54.204.10.90/api/3/action/datastore_search?id=7c0608e3-fb4d-4fb3-928c-5351e5b9b122 and it will show you the metadata of the columns as well


Hope this helps!
"
data request - Dataset with a multivariate time series of circular and linear variables,"
There is an area of statistics called functional data analysis.  If you look through the documentation for these procedures (R and matlab) or the books/short courses, you should be able to find a dataset that meets your needs.

http://faculty.bscb.cornell.edu/~hooker/ShortCourseHandout.pdf
http://www.psych.mcgill.ca/misc/fda/
http://cran.r-project.org/web/packages/fda/index.html
http://www.amazon.com/Functional-Data-Analysis-Springer-Statistics/dp/038740080X/ref=sr_1_1?ie=UTF8&qid=1421760835&sr=8-1&keywords=functional+data+analysis&pebp=1421760797440&peasin=038740080X

"
industry - open data in maintenance and repair,"
I found this link:
Predictive Maintainance and Sensor Data Analytics

Data from a semi-conductor manufacturing process
Key facts: Data Structure: The data consists of 2 files the dataset file SECOM consisting of 1567 examples each with 591 features a 1567 x 591 matrix and a labels file containing the classifications and date time stamp for each example.

"
usa - U.S. federal government data fetchable via SPARQL?,"
your best bet is to search specific data sites that you want to get the data from...i went to healthdata.gov and found this guy:
http://healthdata.gov/cqld
although their query point throws a 404
OKFN also runs a sparql service to show available endpoints, though you'll have to pick through to find us ones:
http://sparqles.okfn.org/availability 
this slideshare set tells you how to do healthcare.gov's data:
http://www.slideshare.net/george.thomas.name/clinical-quality-linked-data-on-healthdatagov
"
openfda - Is there an easy way to access medical device approvals beyond recently-approved devices?,"
I see you added the openfda tag to this question but this doesnt seem to be data that is available from openFDA. The data regarding devices that openFDA has is enforcement reports (https://open.fda.gov/device/enforcement/) and adverse events.
If you are determined to get this data from openFDA, you could.. sorta kinda. You can gather data about devices that have had at least one adverse event by getting a list of manufacturers by going to https://api.fda.gov/device/event.json?&count=device.manufacturer_d_name.exact&limit=1000 and the doing queries for all events related to a manufacturer (https://api.fda.gov/device/event.json?&search=device.manufacturer_d_name.exact:KENDALL). This is probably not a good idea because you will have to de-duplicate events from devices but it is an option for using currently open openFDA data for this case.
Moving on from openFDA to the general info on FDA.gov including the link you posted, you are looking for data older than 2008 or are you in need of a way to search those data (in which case a scraper will likely need to be created)?

EDIT: You might be interested in this news story from Modern Healthcare: FDA urged to encourage development of medical-device registries 

EDIT 2: My colleague found this, http://www.fda.gov/MedicalDevices/ProductsandMedicalProcedures/DeviceApprovalsandClearances/510kClearances/ucm089428.htm, on FDA.gov. It is 510(k) data going back to 1976.
"
data request - Finding new and future song releases before they are found by Google Search,"
I guess Amazon gets new song releases information directly from the labels, which they can do thanks their their reseller position. So it is probably not open data.
If open data existed, Google would know it and crawl it often, and would not be far behind.
AZLyrics probably does not have any relationship with the labels, they just crowdsource their data. Their data is not open either.
So, my guess would be that unfortunately there is no open data resource for song releases which has info before Google Search. I hope to be proven wrong.
"
How to get my data accepted in the Linked Open Data Cloud Diagram?,"
essentially you need to:

validate your dataset with ckan lod validator  
publish dataset
add it to ckan ""so that it appears in the next version of the lod cloud diagram""  

Reference: http://datahub.io/group/about/lodcloud
"
geospatial - How does Google Maps get traffic info?,"
Google uses crowdsourcing techniques for the collection of the traffic data, as they explain in this blog post. This basically means that everybody who uses Google Maps on their mobile phone automatically reports anonymous traffic data back to Google.
"
transportation - Car Sales & Car Theft (Stolen Automobiles) Data,"

For the USA, the National Highway Traffic Safety Administration (NHTSA) has general information and fact sheets about theft as well as a search tool where you can get theft rates by year, production rates by year, look by manufacturer, and more. Unfortunately, it looks like data is only up to 2011 but it does go back to 1983.
For more USA data and aggregated info, check out the National Insurance Crime Bureau (see the 'Theft and Fraud Awareness' menu) which does seem to possibly be the source for the Honda Accord data point. 
Interpol has some international data

"
usa - How can you get nationwide data of a particular type from the US Census website?,"
TLDR: Census data is complicated.

The simplest actual answer to your question: If you type in ""united states"" on the front page of American FactFinder, you can get to the ""Community Facts"" page for the US, which has direct links to several kinds of data at the national level.

Probably way more than you really wanted to know:
More generally, Census data is a very simple phrase behind which lies a ton of variety, nuance and detail. I've spent several years now working on various projects to make Census data easier to use, and it's harder than you would guess. Even a straightforward concept like ""population"" has multiple answers from the US Census: every ten years the Decennial census comes up with one count; annually, the American Community Survey (ACS) estimates the population using a different methodology, and every month, the Population Estimates program produces its own numbers. Oh, and also monthly the Census Bureau and the Bureau of Labor Statistics update the Current Population Survey (CPS) which has its own total population number.

Is there no straightforward to say ""I want data X about region Y, grouped by subregion Z, give it to me now, please and thank you""? Importantly, Y may be ""the entire United States"", and ideally, X may include multiple data points.

data X
For the American Community Survey alone, there are several hundred table variants, and several of the variants come in two forms, and some of them come in about 20 forms because they break down figures according to race or hispanic status. (In total, there are over 1400 tables in the ACS). Because the raw data can't be released for privacy protections, the Census needs to presumptively tabulate all the different kinds of cross-referenced data people might want. (And some of the data you may want is collected in the CPS or other programs.)
region Y
the Census Bureau data is tabulated into ""summary levels"" which represent different classes of geography. The ACS Glossary has a summary level list has 188 different kinds of value for Y, with hundreds of thousands of actual values. And there are more summary levels than on that list. The Missouri Census Data Center (MCDC) has delved into summary levels and tried to compile a master list which has 220 of them.
These lists are complicated for technical reasons (mostly to do with the next section) but even if you think in terms of simple shapes-on-a-map, there are about 30 basic geographies.
Also, for the US and for states, there's a separate concept (""geographic components"") to do with ""give me the total of data X for just the people living in ""urban areas"", or ""not in a metropolitan or micropolitan statistical area"". There are about 20 of these components.
grouped by subregion Z
only some Census geographies are by definition properly contained by specific other geographies. Cities (sumlevel 160, formally known as ""places"") cross census tract and county boundaries, adding a bit of complexity to one commonly desired grouping. And in some parts of the country, locally prevailing government systems make ""places"" a less appropriate way to break things up: things are different in many ways in New England, especially.
Oh, and ""the census year from which I want the data."" Census maps change every ten years, so data is not always comparable across Census releases. How you do your comparisons when that happens depends sometimes on whether you're looking at statistical areas like census tracts, or legal areas like states and counties. And the questions change too, not just the maps. In 2000, the Census Bureau allowed people to select more than one race, complicating comparison of new data with data collected before then.
Third Party Projects
A lot of people have worked on third-party projects to make Census data easier to use. I invite you to check out a project I've worked on called Census Reporter. It's limited to the American Community Survey for the foreseeable future.  I'm proud of what we've done, and we get great feedback, but I know for a fact we have a long way to go to make it as easy to find tables as you (and I!) want.
Besides Census Reporter, there's a commercial project called Social Explorer which has done a lot of work to make the data more directly cross-comparable. You may be able to access it through your local library.
There are some other great projects which also take on these challenges, but which tend to be aimed at researchers who are more deeply invested. For example, the aforementioned MCDC, NHGIS, IPUMS, Census.IRE.org (which I also helped to make), and I'm sure more that I'm forgetting.
"
releasing data - Where should I host/contribute a mapping from Stack Overflow tags to Wikipedia articles?,"
The best is to host your mapping directly on Wikidata. The property is https://www.wikidata.org/wiki/Property:P1482
For instance, https://www.wikidata.org/wiki/Q859221 represents Java Swing, and has https://stackoverflow.com/tags/swing as a Stack Exchange tag property:

[...]

License: public domain
"
usa - Are there freely available equivalents to the HUD crosswalk data (zip code to county/census mapping) that go back farther in time?,"
although zctas and zip codes are not perfect equivalents, they are pretty close.
you can generate any conceivable crosswalk with census 2010, 2000, or 1990 geographies here:
http://mcdc.missouri.edu/websas/geocorr12.html
http://mcdc.missouri.edu/websas/geocorr2k.html
http://mcdc.missouri.edu/websas/geocorr90.shtml
"
openfda - Wildcard searches,"
You cannot do a two-way table where you get a list of adverse events by drug using the current openFDA API. 
What you can do is find most of* this information in a two step process which can of course be automated:

Get a list of drugs by generic name: https://api.fda.gov/drug/event.json?search=&count=patient.drug.openfda.generic_name.exact&limit=1000
Get a list of adverse events that have shown up in event reports that contained that generic drug. For example, ASPIRIN: https://api.fda.gov/drug/event.json?search=patient.drug.openfda.generic_name.exact:ASPIRIN&count=patient.reaction.reactionmeddrapt.exact&limit=1000

* ""most of"" because these count queries will return only the top 1000 results
Hope this helps!
"
usa - Fiber (dark or lit) maps and data,"
BroadbandMap.gov, by the FCC, has a raster map of fiber to the end user at http://www.broadbandmap.gov/technology/fiber-to-the-end-user
Unfortunately it does not seem to have the data available as anything other than a tile service.
Edit 1: You might consider requesting the data from a FCC Data Officer
Edit 2: Also, have you seen http://www.telecomramblings.com/network-maps/usa-fiber-backbone-map-resources/ which at least tries to link to each carrier/provider's maps which could then be used to create a dataset from?
"
medical - Where can I find open data on healthcare quality indicators?,"
As you know, there are a number of different quality improvement initiatives at all different levels of government and the healthcare ecosystem. Here are some of my favorite sources for hospital level data which has good coverage across CMS certified hospitals in the US:

CMS Hospital Compare - http://www.medicare.gov/hospitalcompare/Data/Measures-Displayed.html - this page shows the different measures data is available for. You can download the data from data.medicare.gov here and/or through data.cms.gov by searching through the data sets. You can programmatically access all the great data on data.cms.gov through Socrata APIs which are detailed at http://dev.socrata.com.
I personally think hospital patient experience (HCAHPS) data should be considered in conjunction with other quality data and so I suggest checking out the HCAPS data which can be accessed at https://data.medicare.gov/Hospital-Compare/HCAHPS-Hospital/dgck-syfz
CMS Physician Compare - https://data.medicare.gov/data/physician-compare - similar to CMS Hospital Compare but with less measures. Mainly PQRS data which is at https://data.medicare.gov/Physician-Compare/Physician-Quality-Reporting-System-PQRS-Group-Prac/ddiw-pgap
ACO Quality Data - https://data.medicare.gov/Physician-Compare/Accountable-Care-Organization-ACO-Quality-Data/ytf2-4ept - this is sort of under the umbrella of the CMS Physician Compare data sets but I think it's worth noting separately too

"
programming - R packages with open data in them,"
Here is a human-friendly list of all 731 datasets in R, as well as CSV download links.
"
licensing - Suggested formats for open data documents,"
Ideally, like @Andrew - OpenGeoCode mentions, you would release it in multiple formats.
I would really suggest you look into organizing it into a e-book written in Markdown and hosted on Github. There are several advantages to this such as a built-in change log, being able to let people to (publicly) fork your document(s) and share their changes with the public, give you a workflow to bring in suggested changes (pull requests), and much more. 
An example of this can be seen at https://github.com/truevault/hipaa-compliance-developers-guide. 
There are a number of ways to convert your Markdown to PDF, HTML, etc. (1, 2). You can also google for tools to convert your existing ODF/Word/etc documents to Markdown.
"
data request - Reusable pictures of Chinese factories in the 1930~1940s,"
Here are some of my findings.. no one source, lots of looking all over the net:

Search for ""factory"" on http://www.asu.edu/lib/archives/smedforeign.htm 

""Chinese factory workers have spiritual  faces--faces filled with suffering.""
  1930s Photographer: Agnes Smedley Agnes Smedley Collection
  MSS-122 Vol. 39

http://commons.wikimedia.org/wiki/File:China_Motor_Corporation_top_crew.jpg

Top managers, chief engineers, and American consultants in front of
  the China Motor Corporation. China Motor Corporation was the first
  Chinese factory of manufacturing jet engines. It was established with
  American assistance. World War II, in Kuichow (current Guizhou),
  China.

1950s and not a photograph but: https://www.flickr.com/photos/chinesepostersnet/5099136754

Emulation in the Patriotic production campaign  Designer: Huang Jun
  ca. 1950


"
data request - List of relase dates and prices of Adobe products?,"
You can get release dates of versions/iterations at http://en.wikipedia.org/wiki/Adobe_Creative_Suite but I think you'll need to use a combination of Google and The Wayback Machine by archive.org to find pricing.
Also see http://www.computerworld.com/article/2517120/enterprise-applications/adobe-creative-suite--the-history.html
"
json - Any Open (Structured) Datasets for the World Factbook (Public Domain Country Profiles Published by the CIA)?,"
You can use factbook for data. A simple google search can get you raw json or csv. 
Officially you can use their tool : 
https://github.com/factbook/factbook
or download data directly from CIA website :
https://www.cia.gov/library/publications/download/
Previous answer : 
The links are dead - looks like data has been removed without any notice.

Check this out. https://github.com/factbook/factbook.json
it has all data in json - I recommend using nosql database for importing. 
They also offer sql dump https://github.com/factbook/factbook.sql
CSV with facts. https://github.com/factbook/factbook.csv

"
data request - Open resources about cosmetics and beauty/body products,"
Not exactly what you are looking for but any cosmetic containing sunscreen is in the Structured Product Label dataset at FDA (http://labels.fda.gov/), which is part of the openFDA effort (https://open.fda.gov/drug/label/).
"
Is there a vocabulary for linking weather data?,"
We (met.no) plan on publishing a JSON-LD vocabulary for climate/weather data, when we release our public portal later this year. We'll be publishing JSON-LD ourselves as our primary data format.
To the best of my knowledge, there isn't anything else stable out there yet suitable for this kind of use. The closest you get, AFAIK, is http://codes.wmo.int/
"
licensing - Can I simply copy a CC-BY-SA 3.0 work as CC-BY-SA 4.0?,"
I believe that a copy is also a kind of adaptation (opposite is not true, obviously).
And yes you can take a CC-BY-SA 3.0 work and publish your ""adaptation"" as CC-BY-SA 4.0:
The CC-BY-SA 3.0 license says:

You may Distribute or Publicly Perform an Adaptation only under the terms of: (i) this License; (ii) a later version of this License with the same License Elements as this License; (iii) a Creative Commons jurisdiction license (either this or a later license version) that contains the same License Elements as this License (e.g., Attribution-ShareAlike 3.0 US)); (iv) a Creative Commons Compatible License.

and:

""Creative Commons Compatible License"" means a license that is listed at http://creativecommons.org/compatiblelicenses that [...]

I turn, http://creativecommons.org/compatiblelicenses says in its BY-SA Version 3.0 paragraph:

Your contributions to adaptations of BY-SA 3.0 materials may only be licensed under: BY-SA 3.0, or a later version of the BY-SA license. [...]

CC-BY-SA 4.0 is a later version of CC-BY-SA 3.0, so you can redistribute.
"
linked data - What triplestore that can handle the largest number of triples?,"
The W3C hosts a list of large triple stores with documented deployments and numbers of triples.
The top contenders with more than 10B triples currently are:

AllegroGraph (1Trillion+)
Stardog (50B)
Oracle Spatial and Graph with Oracle Database (48B+)
OpenLink Virtuoso v6.1 (15.4B+) 
Ontotext GraphDB (formerly BigOWLIM) (12B+)
Garlik 4store (15B)
Bigdata (12.7B)

"
rdf - What Linked Data serialization format to choose for our (now CSV) open data?,"
In addition to @enridaga's great answer, let me offer a few additional thoughts.
RDFa is (only) useful if you already have HTML content that you want to enrich with semantic data.
RDF/XML is very well readable for machines, but not so much for humans.
N-Triples, Turtle or N3 are currently pretty much the default formats for Linked Data dumps. For example, have a look at the recently released official Wikidata RDF exports. N-Triples are preferable for large files because they can be easily processed line by line.
JSON-LD is the new kid on the block with lots of potential and already some uptake. Its biggest advantage: Even programmers that have never heard of the Semantic Web can use it immediately.
As @enridaga mentioned, once you have your data in one of these formats, it can be converted automatically to any of the other formats. You can give it a quick try with the online RDF Translator.
If you only want to provide a simple dump of your data for the Semantic Web crowd, go with (gzipped) N-Triples. If your users prefer a different format, they can easily convert it themselves.
If you also want your data to be instantly accessible to regular programmers, make it JSON-LD.
"
"usa - Are zip-code-level IRS income tax data available for every year, in machine-readable fomat?","
According to @MichaelA (in the comments), zip code level IRS income machine-readable income data is not available for every year, and the IRS has no plans to do so/isn't very receptive to the idea  
edit:
You can get all the data based on county/city (not zip) for the years 1989-2010 here:
http://www.irs.gov/uac/SOI-Tax-Stats-County-Data
So not the exact format you asked for, but its something to work with and also covers the years you are missing and then some.
I took this data and scraped out the information for Virginia, which you can view here:
http://data.openva.com/dataset
"
film - Is there an API for the Oscars/Academy Awards that lists past winners as well as current nominees?,"
Check out Wolfram Alpha API for things like:
http://www.wolframalpha.com/input/?i=academy+awards+1998

This is the free (public) interactive form interface. You can get a download as raw data with a Pro Subscriber (paid) license.
"
data request - Amazon ASIN and Category,"
You can use the Amazon Product API to find the category breakdown on Amazon and corresponding products 4 sale.
This link explains how items for sale are organized under the API:
http://docs.aws.amazon.com/AWSECommerceService/latest/DG/CHAP_OrganizationofItemsforSaleonAmazon.html 
This is the general link to the API:
http://docs.aws.amazon.com/AWSECommerceService/latest/GSG/Welcome.html
"
medical - Imaging Cost Data and Procedure Costs,"
Medicare has a public dataset that may provide the information you are looking for:

Medicare Provider Utilization and Payment Data: Physician and Other
  Supplier 
As part of the Obama Administration’s efforts to make our healthcare
  system more transparent, affordable, and accountable, the Centers for
  Medicare & Medicaid Services (CMS) has prepared a public data set, the
  Medicare Provider Utilization and Payment Data: Physician and Other
  Supplier Public Use File (Physician and Other Supplier PUF), with
  information on services and procedures provided to Medicare
  beneficiaries by physicians and other healthcare professionals.  The
  Physician and Other Supplier PUF contains information on utilization,
  payment (allowed amount and Medicare payment), and submitted charges
  organized by National Provider Identifier (NPI), Healthcare Common
  Procedure Coding System (HCPCS) code, and place of service. This PUF
  is based on information from CMS’s National Claims History Standard
  Analytic Files. The data in the Physician and Other Supplier PUF
  covers calendar year 2012 and contains 100% final-action
  physician/supplier Part B non-institutional line items for the
  Medicare fee-for-service population.

http://www.cms.gov/Research-Statistics-Data-and-Systems/Statistics-Trends-and-Reports/Medicare-Provider-Charge-Data/Physician-and-Other-Supplier.html
"
data.gov - Looking for openFDA datasets which gives JSON data,"
They way openFDA stands, it is an API which returns JSON for a search. I too have asked for bulk downloads.
The information you seem to be looking for is available by modifying the following API call:
https://api.fda.gov/drug/event.json?search=patient.drug.openfda.brand_name:advil&count=patient.reaction.reactionmeddrapt.exact&limit=1000
If you are looking to do this with an offline data set (aka not need to query the API each time), you have a few options:

Cache the results for a week or two
Go through a list of all drugs and download the results and store that in a database of your own
Completely bypass openFDA API and go to the source data at http://www.fda.gov/Drugs/GuidanceComplianceRegulatoryInformation/Surveillance/AdverseDrugEffects/ucm082193.htm 

"
"Open data sets about software development: code quality, defect rate, programming languages?","
Here are a few sources to take a look through:

Most comprehensive and closest to what I think you're looking for is a project by the acronym of PROMISE (PRedictOr Models In Software Engineering). Go to http://promisedata.googlecode.com/svn/trunk/defect/ to go straight to their data which is organized by code base and into CSVs. For example, tomcat.csv. It looks like the project also has a number of tools to help analyze and create models from the data.
Eclipse Bug Data! is, as it sounds, a project to mine bug and version datas from the Eclipse project. 
NASA also publishes ""cleaned versions of publicly available NASA MDP software defects data sets"" over at http://nasa-softwaredefectdatasets.wikispaces.com/

EDIT: there are also a number of academic papers on this topic that you might want to skim to see if they reference specific data sets or sources.
"
Where to find spatio-temporal data?,"
You could look for climate data on the open data portals around Italy, Regione Liguria has it for meteorogical measurements (http://www.cartografiarl.regione.liguria.it/SiraQualMeteo/Fruizione.asp) but doesn't have an API.
Other suggestions from https://groups.google.com/forum/#!forum/spaghettiopendata :

http://toolserver.openstreetmap.it/carburantiMiSE/ (fuel data)
http://www2.arpalombardia.it/sites/QAria/_layouts/15/QAria/IDati.aspx?v=1 (air pollution)

"
Open Data formats used,"
I don't know about worldwide, but for Europe you can find statistics on data formats on the ENGAGE portal.

"
Data Ferret (Census.gov) will not load,"
There are two parts to this question that I can address. First, you'll need to make sure that javascript is enabled.
Second, if you're after CPS data you have three separate resources. You have the option of using the FTP site which allows you to download the raw data. A better resource is from the National Bureau of Economic Research which offers the raw data, as well import statements for SAS, Stata, and SPSS for the basic monthly CPS and CPS supplements. The third resource is iPUMS for the CPS, it allows you to do things that are similar to dataferret, although it does require free registration with a simple approval process.
"
government - how does add123.com get their data?,"
From add123.com NMVTIS page:

The National Motor Vehicle Title Information System (NMVTIS) is a federal database containing automobile information from states, insurance carriers and the salvage industry. The vehicle history information is available to the public, and can provide valuable information about a potential purchase.

The problem with cars is that they can be easily moved around, so being able to find registration history from the Texas DMV will only give you details for Texas cars, and not cars that were flooded in Lousiana, or something.
An alteranative to add123.com would be to contact each State's DMV. Here is the PDF list of contact info - LINK.
"
data request - Birth dates vs. Due dates,"
Allen Downey's free online book Think Stats uses data from the National Survey of Family Growth to consider the question ""Do First Babies Arrive Late?""
Section 1.3 of the book explains some use of the data. There is a newer version of the dataset (2006-2010) on the NSFG site.
"
usa - Errors with running Ruby Government Data SDK Sample,"
This issue was caused by both a bug and a missing update in the Ruby SDK.  It has been fixed now.  You can find the updated Ruby SDK at the Ruby_DOLDataSDK Github repo.
Also, the sample app code you linked has been updated to remove the API_SECRET parameter. The ""shared secret"" is no longer necessary. So the updated version of the sample program will now work with latest version of the Ruby SDK.
"
"county - Sourcing or creating (using PHP), a lists of all countries and their relevant ISO codes?","

http://peric.github.io/GetCountries/ seems to have everything you want except the language code however it is in SQL format which could be fairly easily converted.
http://cran.r-project.org/web/packages/ISOcodes/ISOcodes.pdf also seems to have all  your data but is in R format

"
standards - Data showing what JavaScript operations are usable in what browser,"
The caniuse.com project includes compatabilities for many browsers versus CSS, HTML, JS API, SVG, and Other categories.

""Can I use"" provides up-to-date browser support tables for support of front-end web technologies on desktop and mobile web browsers.

License is CC BY-NC 3.0.
Raw data is on GitHub.
"
machine learning - Regression problem data suitable for ML library unit test,"
The ""elemapi"" data set may be promising (or maybe it will help refine the question of what you're looking for). I don't see an explicit license, but it's used widely and is from a public source, so hopefully there is a friendly license somewhere.
UCLA-affiliated course use:

In this chapter, and in subsequent chapters, we will be using a data
  file that was created by randomly sampling 400 elementary schools from
  the California Department of Education's API 2000 dataset.  This data
  file contains a measure of school academic performance as well as
  other attributes of the elementary schools, such as, class size,
  enrollment, poverty, etc.

"
"data request - Sample datasets with known outliers for IQR, Q-test and Z-test math tests","
My advice would be to look at the documentation for the relevant statistical procedures in popular statistics software, specifically in R or SAS.  Statistical function documentation usually has sample datasets and examples of the usages.
http://www.rdocumentation.org/
http://support.sas.com/documentation/onlinedoc/stat/
"
data request - Looking for a database for cellular tower and antenna locations in the united states,"
I have not looked into these datasets. The FCC (fcc.gov) has datasets on antenna registrations and the locations of cell towers:
http://wireless.fcc.gov/geographic/index.htm?job=licensing_database_extracts
archive.org link: https://web.archive.org/web/20141015140708/http://wireless.fcc.gov:80/geographic/index.htm?job=licensing_database_extracts
"
api - Is it the right query on what I am looking for in openFDA?,"
Yes, that API query looks correct but note the following:

You can get a count of the adverse event reactions through the API by adding &count=.. to the end of your URL like this: https://api.fda.gov/drug/event.json?search=patient.drug.openfda.brand_name:advil+AND+patient.patientonsetage:64&count=patient.reaction.reactionmeddrapt.exact
You should be aware that 45% (1,734,133 of the 3,814,280) adverse events on openFDA as of right now do not have any age set. You can see this at https://api.fda.gov/drug/event.json?search=missing:patient.patientonsetage for openFDA in general and at https://api.fda.gov/drug/event.json?search=missing:patient.patientonsetage%20AND%20patient.drug.openfda.brand_name:advil for  your advil query
Age is not always (but usually is) denoted in years by openFDA

"
rdf - How to express that a schema.org/Restaurant is located in a given Wikidata geographical entity?,"
What you can do is to create another entity that represents Abbeville, set it as schema:location (or maybe schema:containedIn?) of the restaurant and then use schema:sameAs to link to that Wikidata URI:
<rdf:Description rdf:nodeID='Nb925d432-69b1-42e0-8063-d914e3504dde'>
    <rdf:type rdf:resource='http://schema.org/Restaurant'/>
    <schema:name>Chez Mel</schema:name>
    <schema:address>63-65 rue Saint-Vulfran</schema:address>
    <schema:telephone>+33 3 22 19 48 64</schema:telephone>
    <schema:description>Hearty and family-friendly restaurant.</schema:description>
    <schema:location>
        <rdf:Description rdf:nodeID='NsomeotherGUID'>
            <rdf:type rdf:resource='http://schema.org/City'/>
            <schema:sameAs rdf:resource='https://www.wikidata.org/wiki/Q28520'/>
        </rdf:Description>
    </schema:location>
</rdf:Description>

"
data request - Mapping between Wikidata and Geonames,"
Approximately since October 2014 there exists GeoNames ID external identifier on Wikidata. The current number of entities with this identifier is about 1 500 000.
You could retrive this identifier value for Antananarivo using the following SPARQL query:
SELECT ?geoNamesID WHERE
{
   ?place wdt:P1566 ?geoNamesID 
   VALUES (?place) {(wd:Q3915)}
}

Try it.
"
data request - Dataset of languages and where they are spoken (sub-national),"
The CIA Word Factbook has a field listing for languages spoken per country and percentage of population that speaks it.
https://www.cia.gov/library/publications/the-world-factbook/fields/2098.html
For finer grain, you can Google 'language spoken home dataset'. This will find census datasets related to the distribution of languages spoken. Below are some examples:
United States (Census) by state:
https://www.census.gov/hhes/socdemo/language/data/other/detailed-lang-tables.xls
City of Chicago, IL by neighborhood:
https://data.cityofchicago.org/Health-Human-Services/Census-Data-Languages-spoken-in-Chicago-2008-2012/a2fk-ec6q
City of Cambridge, MA by neighborhood:
https://data.cambridgema.gov/Neighborhood-Census-Data/2007-2011-Language-Spoken-at-Home-by-Neighborhood/sba5-5kgg
Madison County, NY
http://cnyvitals.org/madison/demographics/language-spoken-at-home/
State of Hawaii:
https://data.hawaii.gov/Culture-and-Recreation/English-ability-by-language-spoken-at-home/i9hq-hna6
Canada (from their Census) by Province, Electoral District:
http://data.gc.ca/data/en/dataset/81a2bd6e-622f-4f17-84c1-215216485992
Queensland, AU by statistical area:
https://data.qld.gov.au/dataset/language-spoken-home-sa4-qld/resource/9fa1fc3a-ab09-4c99-a60f-f95c59269492
Greater London by borough:
http://publicdata.eu/dataset/first-language-spoken-at-home-borough0e333
UNESCO Atlas of the World's Languages in Danger:
http://www.unesco.org/culture/languages-atlas/
"
data request - Mapping between Wikivoyage article names and their Wikidata identifier,"
The dump that you want is wb_items_per_site.sql.gz. Though it contains mapping between article titles and wikidata ids for all wikis, so it's relatively big (~1.2 GB compressed).
And it's an SQL dump, so you can either import it into a MySQL database and query that, or you can try to parse it based on the unspecified (but reasonably stable) format.
Since you're interested in English Wikivoyage, look for enwikivoyage in the  ips_site_id column.
"
openfda - help diagnose the query result? or a possible bug?,"
The problem was a minor one with query syntax: You're using equals signs = when you should be using colons :.
For example, adverse_event_flag=Y should be adverse_event_flag:Y.
The new query returns only 23 results instead of about 1300.
The openFDA API basics page has information about the correct syntax for these queries.
"
Are there free APIs for searching news articles that I can use to collect trend data in news coverage?,"
Here's some more news APIs you can look into (that are free or cheap - but may have limits on usage):
USA Today API - http://developer.usatoday.com/docs/read/Breaking_News
The Guardian - http://www.theguardian.com/open-platform
The New York Times API - http://developer.nytimes.com/
"
geospatial - Expressing restaurant information in RDF/XML,"
How you can improve it depends very much on how you want to use it, and who you want it to be useful for. 'Valid' RDF is fine, but may not be useful depending on your intended application! If you want it to be helpful to open/linked types, publishing it using other 'accepted' vocabularies is good (although schema.org is the best catch-all, it is not always as expressive as you want):

FOAF (Friend of a Friend)
Dublin Core
W3 Geo Ontology
DBpedia/Freebase

This also allows any internal applications you have to retrieve extra data from other sources (much as you have done with your wikidata links).
Another thing I'd suggest is that you consider how you're publishing the data, and have a look at the W3C Linked Data Platform principles. The two major points here are:

How you name your RDF resources. Try to give them a URI (in your rdf:resource) that means something rather than just a UUID - this is usually something affiliated with an organisation, or ideally re-using a URI for the same resource if it exists in DBpedia or Freebase.
Publish your data in friendlier formats (not just RDF/XML) unless you have a reason to only concentrate on RDF/XML. It is a horrible format; Turtle is better in every way and easier to work in (but you should provide both, as well as N-triples). 

You also need to work out where to publish it to (if you want it out in the open) - this varies depending on what you want to do...
"
data.gov - How can I access FDA drug and medical device recall data from 1970 to 2014?,"
The process isn't as nice as the API at openFDA, but you can always file a Freedom of Information Act request:  http://www.fda.gov/RegulatoryInformation/FOI/HowtoMakeaFOIARequest/
"
finance - Where to find household financial data for my research,"
if you can use united states residents, you are looking for the survey of income and program participation.  this microdata set contains monthly income about persons, families, and households.  enjoy!
http://www.asdfree.com/search/label/survey%20of%20income%20and%20program%20participation%20%28sipp%29
"
data request - Database of all Ingress sentences,"
akiraak has compiled such a database:
https://gist.github.com/akiraak/b7c112e46b79dacfabf1
It contains 383 sentences.
"
data request - Wickes UK stores' postcodes,"
To be exact, there are 231 stores listed in the dropdown. I was able to extract (fairly simple) the list of stores and corresponding store IDs. I've included the list in CSV format for your convenience. Each store detailed is listed with the following URL sequence:
http://www.wickes.co.uk/store/""STORE_ID""
It should be fairly straight forward to write a program to read in the CSV data and step through each store page and extract out the detailed information (e.g. postcodes). When you accomplish that, please add the info to the CSV data and post in github for the benefit of others.
8276, AINTREE
8475, ALTON
8321, ANDOVER
8360, ASHBY DE LA ZOUCH 
8090, ASHFORD 
8418, ASHTON GATE 
8316, AYLESBURY EXTRA 
8291, BAGULEY 
8277, BANBURY 
8297, BARKING EXTRA 
8247, BARNSLEY 
8256, BARNSTAPLE 
8332, BASINGSTOKE 
8722, BATH KITCHENS & BATHROOMS 
8729, BATTERSEA KITCHENS & BATHROOMS 
8727, BECKENHAM K&B 
8264, BEDFORD 
8454, BEVERLEY 
8319, BEXHILL 
8318, BICESTER 
8298, BIRKENHEAD 
8457, BISHOPS STORTFORD 
8281, BLACKHEATH 
8296, BLACKPOOL 
8279, BLETCHLEY 
8467, BOREHAMWOOD 
8355, BOSTON 
8246, BOURNEMOUTH 
8312, BRACKNELL 
8055, BRADFORD 
8438, BRADFORD IDLE 
8381, BRAINTREE 
8409, BRENTWOOD 
8271, BRIDGEND 
8255, BRIDGWATER 
8230, BRIGHTON 
8077, BRISTOL 
8344, BROADSTAIRS 
8364, BULWELL 
8448, BURGESS HILL 
8404, BURTON SOUTH 
8284, BURY 
8347, CAERPHILLY 
8472, CAMBRIDGE 
8208, CANNING TOWN 
8261, CANNOCK 
8422, CARDIFF PENARTH 
8280, CARDIFF WEST 
8253, CARLISLE 
8083, CATFORD 
8372, CHADWELL HEATH 
8466, CHARLTON
8233, CHATHAM 
8232, CHELMSFORD 
8074, CHELTENHAM 
8456, CHESHAM 
8262, CHESTER 
8283, CHESTERFIELD 
8413, CHILWELL 
8349, CHIPPENHAM 
8720, CHISWICK KITCHENS & BATHROOMS 
8431, CHORLEY 
8715, CHRISTCHURCH KITCHENS & BATHROOMS 
8724, CLARKSTON KITCHENS & BATHROOMS 
8236, CLIFTON 
8287, COLCHESTER 
8075, COVENTRY 
8717, CRAWLEY KITCHENS & BATHROOMS 
8250, CREWE 
8098, CRIBBS 
8072, CRICKLEWOOD 
8215, CROYDON 
8730, CROYDON KITCHENS & BATHROOMS 
8259, DARLINGTON 
8093, DARTFORD 
8079, DERBY 
8066, DEWSBURY 
8239, DONCASTER 
8240, DORKING 
8252, DUDLEY 
8365, DUMFRIES 
8258, DUNDEE 
8062, DUNSTABLE 
8414, EALING 
8382, EAST GRINSTEAD 
8265, EASTBOURNE 
8354, EDINBURGH 
8322, EDMONTON EXTRA 
8309, EPSOM 
8310, ERITH 
8336, EXETER 
8228, FAREHAM 
8052, FARNBOROUGH 
8479, FOLKESTONE 
8352, GLASGOW 
8366, GLOSSOP 
8377, GLOUCESTER 
8443, GRANTHAM 
8410, GRAVESEND 
8425, GRIMSBY 
8385, GUILDFORD 
8451, HAILSHAM 
8436, HALESOWEN 
8303, HALIFAX 
8235, HALL GREEN 
8216, HANDSWORTH 
8237, HANGER LANE 
8073, HANWELL 
8260, HANWORTH 
8337, HARLOW EXTRA 
8731, HARROW KITCHENS & BATHROOMS 
8338, HAVANT EXTRA 
8353, HAVERFORDWEST 
8222, HAYES 
8474, HEDGE END 
8096, HEMEL HEMPSTEAD 
8473, HENDON 
8367, HEREFORD 
8453, HERTFORD 
8313, HIGH WYCOMBE 
8458, HINCKLEY 
8263, HUDDERSFIELD
8053, HULL 
8464, HUNTINGDON 
8295, INVERNESS 
8350, IPSWICH 
8326, KETTERING EXTRA 
8428, KINGS LYNN 
8266, KINGSTON
8251, LANCASTER 
8094, LEICESTER 
8270, LETCHWORTH 
8359, LICHFIELD 
8061, LINCOLN 
8444, LITTLEHAMPTON 
8278, LOUGHBOROUGH 
8317, LOWESTOFT 
8415, MACCLESFIELD 
8314, MAIDSTONE EXTRA 
8449, MALDON 
8368, MANSFIELD
8214, MERTON 
8429, MILTON KEYNES 
8225, MINWORTH 
8728, MUSWELL HILL K&B 
8348, NEWCASTLE EXTRA 
8290, NEWPORT 
8293, NORTHAMPTON 
8218, NORWICH 
8257, NOTTINGHAM 
8071, NOTTINGHAM CENTRAL 
8452, NUNEATON 
8374, OLDHAM 
8726, ORPINGTON KITCHENS & BATHROOMS 
8234, OXFORD 
8721, PAIGNTON KITCHENS & BATHROOMS 
8369, PENRITH 
8334, PERRY BARR EXTRA 
8248, PERTH 
8209, PETERBOROUGH 
8370, PLUMSTEAD 
8292, PLYMOUTH 
8267, PONTEFRACT 
8273, PRESTON 
8088, PUDSEY 
8082, RAYLEIGH 
8371, READING 
8220, REDDITCH 
8065, ROTHERHAM 
8078, RUISLIP 
8460, RUSHDEN 
8430, SALISBURY 
8064, SCUNTHORPE 
8411, SEVENOAKS 
8097, SHEFFIELD CENTRAL 
8468, SHEFFIELD CRYSTAL PEAKS 
8412, SHEFFIELD NORTH 
8268, SHREWSBURY 
8320, SITTINGBOURNE 
8340, SLOUGH 
8423, SOUTH GOSFORTH 
8441, SOUTH SHIELDS 
8328, SOUTHAMPTON EXTRA 
8719, SOUTHPORT KITCHENS & BATHROOMS 
8238, ST ALBANS 
8299, STAFFORD
8416, STEVENAGE 
8417, STIRCHLEY 
8282, STIRLING 
8081, STOCKPORT 
8226, STOCKTON 
8231, STOKE 
8254, SUNDERLAND 
8067, SUTTON (ASHFIELD) 
8482, SUTTON (LONDON) 
8323, SWANSEA EXTRA 
8342, SWINDON EXTRA 
8331, TAMWORTH
8311, TAUNTON EXTRA
8343, TELFORD 
8357, THETFORD 
8091, TOTTENHAM 
8275, TROWBRIDGE 
8407, TRURO 
8723, TUNBRIDGE WELLS KITCHENS & BATHROOMS 
8455, TUNSTALL 
8405, TWICKENHAM
8330, UXBRIDGE 
8327, WAKEFIELD 
8229, WALLSEND 
8243, WALTHAM CROSS 
8301, WARRINGTON 
8718, WARWICK KITCHENS & BATHROOMS 
8227, WATERLOOVILLE 
8245, WATFORD 
8345, WEMBLEY 
8716, WEST HAMPSTEAD KITCHENS & BATHROOMS 
8461, WEST WICKHAM 
8329, WESTON SUPER MARE 
8289, WIGAN 
8059, WIMBLEDON 
8465, WINNERSH 
8459, WINSFORD 
8080, WOKING 
8437, WOLVERHAMPTON 
8406, WORCESTER 
8434, WORKSOP 
8221, WORTHING 
8308, WREXHAM 
8274, YEOVIL 
8471, YORK
I took a look at the HTML code at the store page. It is pretty easy to identify the store name, address and postcode from the javascript code for placing a pin on the Google Map. You get the extra benefit that it has the lat/lon coordinates as well. The snippet looks like:
        var storelatitude = '51.157364';<br/>
        var storelongitude = '-0.956997'; <br/>
        var storename = 'ALTON';<br/>
        var storeaddressline1 = 'WICKES UNIT 1, ALTON RETAIL PARK';<br/>
        var storeaddressline2 = 'MILL LANE';<br/>
        var storeaddresstown = 'ALTON';<br/>
        var storeaddresspostalCode = 'GU34 2QS';<br/>
        var storeaddresscountryname = 'United Kingdom';<br/>

"
data request - List of common foodstuffs/meals?,"
The 2002 UK food nutrition study (McCance & Widdowson's Composition of Foods) lists 3000+ food items along with dietary information. The link below provides this info in an Excel spreadsheet (which can be converted to CSV) at the bottom of the page.
http://tna.europarchive.org/20110116113217/http://www.food.gov.uk/science/dietarysurveys/dietsurveys/
"
"""The API response was an error"" in OpenFDA search query","
When I run the query, using an ordinary web browser:
https://api.fda.gov/device/event.json?search=date_received:[20120101+TO+20141231]+AND+device.device_report_product_code:LLZ+AND+product_problem_flag:Y+AND+event_type:""Death""&limit=1
The API returns:

{
  ""error"": {
    ""code"": ""NOT_FOUND"",
    ""message"": ""No matches found!""
  }
}

I believe the message the original poster was seeing is on the interactive query explorer on the openFDA website. It manually reprocesses no-results queries into the ""bummer"" error message. I'll log a bug to handle ""no results found"" cases.
In this case, neither the API nor the query are wrong. There are no results for the time period specified.
"
data request - Is there a free list of English word phonetics?,"
The cmudict provides phonetic spellings of a sizable number of American English words.  The CELEX database is a similar project; you can select which data you want and download wordlists at WebCelex.  Part-of-speech data (word class) is also available in CELEX.  The CELEX download interface is somewhat frustrating, but you should only need to use it right once to be able to download what you want.
"
"licensing - Which licence and format should I use for the Norwegian language ""data"" I am creating?","
andrew's option for cc0 is spot on, if that is your case.
as far as formats go, json is ideal in my opinion, especially for web. but csv is just fine. if you want to join team open, go ahead and convert it to open data format (odf) too, which helps encourage others to use .odf and open office.
"
openfda - what is adverse_event_flag used for?,"
Reporters fill out a paper form when submitting adverse events to FDA; they may not check the right boxes in all cases.
"
api - How do we relate Adverse Reactions and Drug in Open FDA?,"
There is no way to make the connection you seek using the data in the openFDA drug adverse events API. Reports include all suspected adverse reactions, and all known drugs the patient was taking. While the classification of drugs as suspect or concomitant may be helpful, it is not always present, and it is often impossible to know which drug caused a specific reaction, or whether a drug caused the reaction at all. A suspected adverse reaction may actually be part of a course of illness.
The documentation at https://open.fda.gov/drug/event/#adverse-event-reports clearly states:

A report may list several drug products, as well as several patient reactions. When a report lists multiple drugs and multiple reactions, there is no way to conclude from the data therein that a given drug is responsible for a given reaction.

"
usa - Querying OSHA Data Returning Unexpected Results,"
Those datasets were originally set up to support an app challenge targeted specifically at the hospitality, retail, and restaurant industries.  The Enforcement Data site's search isn't as limited.  We've recently lifted that limitation for Wage & Hour's data.  I will be reaching out to the database owner to see if we can do the same with OSHA's.
"
usa - BEA - how to get employment data by industry?,"
In July I wrote to BEA with a similar question--getting regional employment and wages by industry. They confirmed that these tables (specifically, I wanted SQ6N, SQ7N, and SA27N) aren't yet available via API. ""Later this year, I hope,"" they said.
In the meantime, I've been getting data from BEA's interactive tool and bulk download page (http://www.bea.gov/regional/downloadzip.cfm) and loading it to our internal API.
"
government - Irrigating with recycled water: Permissible levels of Na Cl EC SAR BOD etc per water analysis of the water source,"
The Food and Agriculture Organization (FAO) of the UN has numerous datasets and papers relating to crop production. This paper 'Water Quality in Agriculture"" discusses recommendations in a wide variety of aspects of irrigation. - 
http://www.fao.org/docrep/003/t0234e/T0234E01.htm
The FAO's AQUASTAT datasets cover information on water resources per country:
http://www.fao.org/nr/water/aquastat/main/index.stm
Within these datasets are data related to waste water treatment for crop production use.
http://www.fao.org/nr/water/aquastat/wastewater/index.stm
"
data request - Dataset about Japanese companies,"
Aswath Damodaran, Professor of Finance at the Stern School of Business at New York University, has been compiling corporate data on corporations worldwide into (FREE) datasets and providing them online since 1998.
You can find this information on Japanese firms (3258 companies), as well as other countries at this page:
http://pages.stern.nyu.edu/~adamodar/New_Home_Page/data.html
The Japan Statistics Bureau publishes annual aggregated data on R&D expenditures by industry sector. The information is available online starting with 2004:
http://www.stat.go.jp/english/data/kagaku/index.htm
"
data request - Dataset of multiple judges or examiners giving scores,"
It may be hard to find a dataset that matches your exact criteria, but there are some promising open datasets with essay scoring.


Kaggle Automated Essay Scoring, (link to data, requires registration)


The data will contain ASCII formatted text for each essay followed by one or more human scores, and (where necessary) a final resolved human score.
Where it is relevant, you are provided with more than one human score, so that you may evaluate the reliability of the human scorers

You can find code for benchmarks here.


International Corpus of Learner English, (link to data)


Data available on this page include annotated organization scores for 1,003 essays from the International Corpus of Learner English (ICLE).

"
data.gov - Gasoline station dataset,"
There are several sources you could compile information from. 
City data portals that list gas station locations:
New Orleans: https://data.nola.gov/Administrative-Data/NOLA-Gas-Stations-Map/ic3z-bztr
Washington, DC: https://opendata.socrata.com/dataset/Gas-Stations-in-Washington-DC/tk2x-chx8
State data portals that list alternative fuel stations:
Oregon (Clear Fuel): https://data.oregon.gov/Recreation/Local-clear-Gas-stations/if4z-s7kp
Maryland (Alternative Fuels): https://data.maryland.gov/Energy-and-Environment/Public-Electric-Vehicle-Charging-Stations-and-Alte/7yut-5ayv
The Department of Energy has a dataset that lists the location of 19,000 alternative fuel stations:
http://www.afdc.energy.gov/data_download
And you could do a dump from openstreetmap for locations of fuel stations that have been crowdsourced. This link should give you info on how to do that.
http://wiki.openstreetmap.org/wiki/Tag:amenity%3Dfuel
"
data request - Electricity and (space) heating load curves for a city,"
GreenButtonData.org (open source) is compiling this type of information from its users. They currently claim that upto 60 million households can access their monitoring/analysis system via partnerships with utility companies. 
They have a REST API, but I do not know how much aggregated data is available yet. They have though given some datasets (or samples), including hourly loads, to openEI (crowd sourced project for energy usage/information - http://en.openei.org/wiki/Main_Page)
32 day hourly sample - http://en.openei.org/datasets/node/901
UPDATE: 
Below is the dataset for monthly energy usage and CO2 emissions for Boston's municipal buildings.
https://data.cityofboston.gov/dataset/Municipal-Energy-Data/bcnb-bux2
Below is the datast for annual energy and water usage and CO2 emissions for NYC's buildings > 50K square feet.
https://data.cityofnewyork.us/Environment/Energy-and-Water-Data-Disclosure-for-Local-Law-84-/5gde-fmj3
"
city - Data on demographics at the neighborhood level in Oakland,"
Oakland has an open data portal. This dataset contains the outlines (Shapefile or KML) for the neighborhoods.
https://data.oaklandnet.com/Property/Oakland-Neighborhoods/7zky-kcq9
Unfortunately it has very little data related to what you are looking for. Where it does, the datasets lack Lat/Lon coordinates. One would need to bulk geocode the datasets and check which neighborhood (e.g., KML polygons) the location fits into.
You can get the KML files for Census Tracts and Block Groups from the US Census at this link:
https://www.census.gov/geo/maps-data/data/tiger-kml.html
Combining the two you should be able to map any demographic resource based on Census Tracts/Blocks to the Oakland neighborhoods. If you do go through this effort, I would suggest making the mapping of the census tract/blocks mapping to neighborhoods publicly available in Github.
"
data request - Enron Email Dataset in MySQL,"
After some additional search, I found the worked link among of lots, which point to 404 pages.
The worked ones are:
— https://s3.amazonaws.com/rjurney_public_web/images/enron.mysql.5.5.20.sql.gz
— http://www.ahschulz.de/pub/R/data/enron-mysqldump_v5.sql.gz
Detailed information about these datasets:
— http://hortonworks.com/blog/the-data-lifecycle-part-one-avroizing-the-enron-emails/
— http://www.ahschulz.de/enron-email-data/
P.S. Don't know why, but for me the download process was very slow.
"
Image sensor response data,"
This PDF contains a graph showing the quantum efficiency across all wavelengths from 380 nm to 1050 nm:

Notes:

Data is for the MT9M034 1/3-Inch CMOS Digital Image Sensor
It is a graph, so values have to be sampled from it, and precision is not great
It is not open data

"
data request - Database of bicycle stations in Minato (Tokyo),"
The service's map shows a map of stations: http://docomo-cycle.jp/minato/map/
Reading the JavaScript source code leads to a JSON file which contains the data with location names in Japanese:
http://docomo-cycle.jp/minato/system/data/portnavi.json
The location names in English are found by prepending 'en' in front of portnavi.json:
http://docomo-cycle.jp/minato/system/data/enportnavi.json
Each participating ward uses the same URL syntax. Replace 'minato' with the other ward(s) name for their information.
The wards are:
chiyoda
yokohama
sendai
This data is not always up-to-date, though, some stations are missing.
"
machine learning - Data for web security/database security issues,"
CVE is a freely usable database of security issues, many of them related to web/database.
The CVE database can be downloaded at https://cve.mitre.org/data/downloads/index.html as XML, CSV, and other formats.
A typical entry includes:

Name
Description
Status
Phase
References

"
"data request - Fairtrade open dataset, UK","
The UK Department for Environment, Food and Rural Affairs publishes a weekly price index for commodities:
https://www.gov.uk/government/statistical-data-sets/commodity-prices
And here is the weekly dataset for weekly wholesale prices for vegetables and fruits:
https://www.gov.uk/government/statistics/wholesale-fruit-and-vegetable-prices
The European Commission keeps data related to cost of goods vs. cost of labor for countries in the EU in the EuroStat database.
http://epp.eurostat.ec.europa.eu/portal/page/portal/statistics/themes
The National Accounts (with GDP) will provides indexes on current prices relative to GDP.
http://epp.eurostat.ec.europa.eu/portal/page/portal/product_details/dataset?p_product_code=NAMA_GDP_C
The Agriculture section contains data related to farm labor costs, income, price and production.
http://epp.eurostat.ec.europa.eu/portal/page/portal/agriculture/data/database
Beow is a link of the [historical] annual difference of price of consumables and builder's wages in England from mid 1200s and 1885. It is an aggregation from a number of compiled acacdemic sources. The chart shows that as government (and corresponding taxes) evolved in the development of England that the gap increases each year.
http://esfdb.websites.bta.com/table.aspx?resourceid=11484
"
data request - Federal and State Road Information,"
For the national highway system you will find all the locations/polylines in the National Highway Planning Network dataset at:
http://www.rita.dot.gov/bts/sites/rita.dot.gov.bts/files/publications/national_transportation_atlas_database/2014/polyline
You will also find a girth of geocoded data related to the national transportation system (hubs, railways, airports, etc) under the Bureau of Transportation Statistics - Geospatial Information
http://www.rita.dot.gov/bts/sites/rita.dot.gov.bts/files/subject_areas/geographic_information_services/index.html
As far as state routes, I think you will still have to get that data from each state's transportation or geographic survey sites. Here's a few:
Kansas: http://gisinventory.net/GISI-5224-Road-and-Street-Centerlines---Highways---This-dataset-is-a-single-centerline-highway-network-representation-of-the-10000-miles-Kansas-State-Highway-System-(Interstate-U.S.-and-Kansas-routes)..html
Minnesota: http://www.dot.state.mn.us/maps/gdma/gis-data.html
New Jersey: http://www.state.nj.us/transportation/gis/metadatafaq.shtm
New York: https://gis.ny.gov/gisdata/inventories/member.cfm?OrganizationID=539
Utah: http://gis.utah.gov/data/sgid-transportation/roads-system/
If you end up compiling all the state data together, I recommend placing it in github for the benefit of all.
UPDATE: I had forgotten that I had a web page with a collection of state transportation sites (lol):
http://www.opengeocode.org/opendata/traffic.php
One may also find other transportation related data portals at the crowdsourced catalog of open data portals (I am a co-founder of) by selecting the category Transportation:
http://www.opengeocode.org/opendata
"
data request - Open datasets for affiliation networks between African farmers?,"
The UN Food & Agriculture Organization (FAO) collects and publishes a lot of data on Africa related to farming. 
Here's the entry point into the FAO's Statistics Database (FAOSTAT):
http://faostat.fao.org/
Here's the link to the World Bank's datasets on Farming in Rural Areas of the World:
http://data.worldbank.org/topic/agriculture-and-rural-development
The CGIAR/CCAFS program at Harvard University collects and publishes data surveys related to Agriculture in Africa:
https://thedata.harvard.edu/dvn/dv/CCAFSbaseline
Here's an example of where Columbia University has used the FAO data:
http://iridl.ldeo.columbia.edu/maproom/Agriculture/Farming_Systems/Africa/
Here's an example of where Australian Centre for International Agricultural Research has used FAO data on farming in Africa
http://aciar.gov.au/files/node/14087/mapping_farming_systems_in_africa_21_june_2012_16871.ppt
Here's a dataset from 2012 covering 35 million hectacres of farm land in 66 countries that has been bought up by foreign firms:
http://www.grain.org/article/entries/4479-grain-releases-data-set-with-over-400-global-land-grabs
FOR AFFILATIONS:
NATIONAL AFRICAN FARMERS’ UNION (NAFU)
African Farmers Association of South Africa (AFASA) - http://www.afasa.za.org/
East Africa Farmer's Federation - http://eaffu.org/eaffu/
"
usa - Flood Plain API,"
You can also emulate/reverse engineer the form at https://www.floodsmart.gov/floodsmart/pages/landing_pages/landing0000_1.jsp using the following curl command. This isn't an API, per se, but it does get you the information you're asking for programatically.
curl 'https://www.floodsmart.gov/floodsmart/oneStepFloodRiskAddressSearch.action' -H 'Content-Type: application/x-www-form-urlencoded' --data 'nav_address=2100+Clarendon+Blvd&nav_city=Arlington&nav_state=VA&nav_zipCode=22201&nav_residential=Y&x=23&y=10'
You'll then want to look for <div class=""hide-span flood-risk-profile-header clearfix""> in the resulting HTML.
You can do this from your browser using a tool called hurl.it and following the screenshot below.

"
data request - Infectiousness vs. deadliness for various diseases,"
Both the US CDC and the World Health Organization have databases for statistics on infectious diseases. The entry points to the online databases are:
CDC http://wonder.cdc.gov/datasets.html
WHO http://apps.who.int/globalatlas/DataQuery/default.asp
http://who.int/research/en/
This is the EU Commission's portal on Communicable Diseases. I don't think you will find statistical data here, but lots of links to other resources:
EU Commission http://ec.europa.eu/health/communicable_diseases
"
data.gov - How can I find the missing information from openfda?,"
For example,
https://api.fda.gov/drug/event.json?search=missing:patient.patientsex+AND+patient.drug.openfda.brand_name:advil
and also please add up total reports=male(total) + female(total) + unknown(total) + missing(total). If you do it you might get the right answer.
"
api - Is it possible to get the exact (particular) information from openFDA?,"
At this time it's not currently possible to have the API return only the meta section. You can, as others have pointed out, restrict the API to return only one record along with the meta section. Let us know if this proves to be a problem for your use case.
"
"finance - Where does Allrecipes.com get its ""On Sale"" data?","
After taking a look at the source code of the widget on a sample recipe page (selected at random by me), it appears that they are using http://corp.groceryserver.com/ as a data provider

For example, the recipe at http://allrecipes.com/Recipe/Sherry-Braised-Beef-Short-Ribs/Detail.aspx makes an AJAX request to http://allrecipes.groceryserver.com/groceryserver/service/w5fDoHvC... which returns JSON about the sales. Note: I tried to base64 decode that string and some variations but didn't get very far. 
"
data request - A list of cities of each country,"
My open data project (I am a co-founder) has a free list of all the cities in the world, along with their area centroid (lat/lng), as a CSV file. It is compiled from the USGS/GNIS (US) and NGA/GNS (non-US) databases. 
http://www.opengeocode.org/download.php#cities
As an alternate source, the United Nations Statistical Division publishes an annual yearbook on world statistics. Table 8 has the population of cities > 100,000
http://unstats.un.org/unsd/demographic/products/dyb/dyb2012/Table08.xls 
We have a version of it converted to ur Linked CSV format/vocabulary:
http://www.opengeocode.org/cude1.1/UN/UNSD/dyb2012-pop100k.zip
METADATA (dyb2012-pop2k)

(Empty)
ISO 3166-1 alpha-2 country code (e.g., US => United States)
National Geospatial Intelligence Agency (NGA) Geographic Name Server (GNS) Feature Code (e.g., P = Populated Place Type Feature)
NGA/GNS Feature Designation Code (e.g. PPL = Populated Place (incorporated))
Extended Feature Description (e.g., city, capital)
Total Area in Square Kilometers
ISO 639-1 language code for language that name field is in (e.g., lc = local language native to the country)
Language Script for name fields (e.g., latin, arabic, chinese)
Short Name (Gazetteer) for City
Year of Population Statistics
Total Population (e.g., within city proper)
Urban Population (e.g., within agglomerated area of city)
Total Male Population
Total Female Population

"
data request - Historical forward exchange rate between $ and yen.,"
If you're just looking for FOREX data, http://ratedata.gaincapital.com/ has a ton of it. FOREX assumes you will be taking delivery in 2 days (or 1 day in some cases).
I read your question as ""if I want to buy yen at today's price, but delivered (and paid for) in 6 months, how much will it cost?"". Is my understanding correct?
If so, you're talking about currency futures, which involve rollover rates.
If US banks are paying a higher interest rate than Japanese banks, there is a holding cost for the person who has the yen, even if the exchange rate remains the same. So, you have to adjust today's exchange rate for the interest the yen-holder will lose (compared to the dollar holder) in 6 months.
"
data request - A DB of banks for each country,"
The FDIC has a data download of all finanically insured banks and their branch offices in the US. The data does not have though website and phone number. It does have name, address, institution type, deposits.
https://www2.fdic.gov/IDASP/warp_download_all.asp
The FDIC dataset on Data.gov has some additional fields, including website (but not phone):
http://catalog.data.gov/dataset/fdic-institution-directory-id-insured-insitution-download-file/resource/df80f510-8c30-421d-8f83-f90f0ebf887b
"
Where can I find examples of open data being used in business?,"
The answers to the question on open data stories might also be useful to you.  I've summarized some of those and added some more (courtesy of Anastasios Ventouris, pattern-recognition, Charles Worthington, Alisha Green, Taal, tobip, fgregg, Diabolus, Rebecca Williams)

The Open Data 500 is a collection of 500 companies that have built a business model on open data (both government and non-government data). Note that the Open Data 500 is expanding to include other countries as well.
Highlights on Data.gov show companies, civil society groups, non-profits, and citizens using open government data 
Data.gov.uk has economic and civil society case studies 
McKinsey report on Open Data: Unlocking innovation and performance with liquid information has many examples in education, transportation, consumer products, electricity, oil and gas, health care, and consumer finance
Code For America's e-book Beyond Transparency 
Data for Good, a platform for sharing data-driven projects for social good
Open Knowledge Foundation's community stories 
Sunlight Foundation's Data Deep Dives 
Dan Nguyen's Small Data Journalism Readings articles for his data journalism class at NYU

"
data request - Where can I get standard iOS icon collection in PDF format?,"
Using a Advanced Google Image Search, you can search based on file type (SVG, not PDF) and also usage rights. If you want a common theme for all buttons, then you can use this tool to find single websites that host similarly designed images that meet your criteria.
Here is an example of the Resize Buttons with SVG. You'll have to filter by license for your use.


To find PDF files, you can't use the Image Search tool but you can use Google Advanced Search, and specify PDF as the file type.

To create or edit vector files (i.e. PDF or SVG), there is a very professional open source program called Inkscape (similar functionality to Adobe Illustrator). With this tool, you can also easily make your own buttons in vector format. You can also easily convert SVG to PDF, using tools described here. 
"
geospatial - Airport / airline data from all over the world,"
A quick google search found me the data page of OpenFlights.org. This page has a free (donation request) CSV file with 8000+ airports: LINK.
OpenFlights.org points to OurAirports.com, which provides extensive CSV downloads with data being in the public domain. See their data page.

Regarding contact info for airport management, the FAA provides this info for US airports: LINK. Use this form to filter and then scroll down to the download section:

Here is a direct link to the Airport Facilities Excel file (6.5 MB). Screenshot below. Lots of contact info when you scroll right.

There is also a 'download data' link but it requires registration.
"
data request - Global calendar of Open Source-related events,"

Hackathon Watch - not specific to open source or open data though
European PSI / Open Data Events - European, not global
Lanyard's Open Data and Open Source lists (can be saved to your calendar, see the right column)
...

(please edit if you want to add a calendar)
"
api - Drug time series produces unexpected results,"
The short answer is that when you do the following API call, you see that bextra shows up in the patient.drug.medicinalproduct instead of patient.drug.brand_name.
https://api.fda.gov/drug/event.json?search=receivedate:[20050101+TO+20050430]+AND+bextra&limit=100
So I suggest doing the following:
https://api.fda.gov/drug/event.json?search=receivedate:[20050101+TO+20050430]+AND+(medicinalproduct:bextra%20OR%20brand_name:bextra)&count=receivedate
You can read up on why this is at https://github.com/FDA/openfda/issues/22
"
usa - Opening up Federal Trade Commission data on Safe Harbor,"
I checked out the webpage. There is a 'Export to Excel' link on the page. I tried it. It downloads a file called 'OrganizationList.xls'. But you cannot open it in Excel. I checked the contents of the file and it is an HTML table. To view the file, change the file suffix to '.html'. 
As far as extracting the data, I would use a convert HTML table to CSV tool. Below is one site that I tried with this file and it appeared to do a correct conversion:
http://www.convertcsv.com/html-table-to-csv.htm
"
How to get access to half-open subscription-only data?,"
I can't speak for other libraries, but at the university library where I work at in the United States...
If we had a CD copy of this, you would need to get an account with us (which would cost $25 and be restricted to residents of the state) to be able to check it out.
If we had online access to this, you'd need to come by in person to one of our libraries and use one of the guest computers. Typically we allow guests access for an hour a day but for folks doing research, we'd gladly extend that time if their research intent was made clear. Folks not affiliated with the university can't get off-site access to our electronic subscriptions per our contracts with the vendors.
"
data request - Hospital originated infections and mishaps,"
The data referenced appears to be an aggregation of Hospital Acquired Infections which are part of the CMS Hospital Compare family of data sets.
The various measures included in the dataset are listed at  http://www.medicare.gov/hospitalcompare/Data/Measures-Displayed.html 
There are a number of ways to access the raw data which are listed at http://www.medicare.gov/hospitalcompare/Resources/Download-Data.html including zipped archives of CSVs, APIs at data.medicare.gov, and more.
If you're going to visit one line.. go to https://data.medicare.gov/Hospital-Compare/Healthcare-Associated-Infections-Hospital/77hc-ibv8
"
api - How can I get accurate data re Adverse Events?,"
Actually, Andrew from OpenGeoCode's answer isn't quite correct; it is a much wider query than expected. I'll explain why.
First, to the OP: This is exactly what is necessary to answer your original question—a pasted link to the queries you used to arrive at your results! Otherwise there's no way to reproduce the queries.

https://api.fda.gov/drug/event.json?search=patient.drug.medicinalproduct.exact:EthinylEstradiol+Norelgestromin+patient.drug.medicinalproduct.exact:Ethinyl%20Estradiol+Norelgestromin&count
This returns about 83k results. Why so many? Here's what's going on.
The API is going to search for:

patient.drug.medicinalproduct.exact:EthinylEstradiol
All records where medicinalproduct contains exactly ""EthinylEstradiol"" 0 results
+Norelgestromin
Because Norelgestromin is ""loose"" in the query—it's not prefixed by a field to search in—the API will search in every field, in every record, for the word Norelgestromin ~14k results.
+patient.drug.medicinalproduct.exact:Ethinyl
Because there's a space between Ethinyl and Estradiol, but these are not grouped with quotation marks to indicate an exact phrase match is desired, the API searches for all records where medicinalproduct contains exactly ""Ethinyl"" 0 results
%20Estradiol
Again, this is ""loose"" in the query so the API does a big keyword search across all fields for the word Estradiol, wherever it may be found. ~83k results
+Norelgestromin
Again, this is ""loose"" in the query so the API does the same broad keyword search it did before. No new records are returned.
&count
This does nothing now. count needs a parameter—a field to count. i.e. count=patient.drug.medicinalproduct.exact will return a list of the top medicinalproduct entries across the matching records; it's a good way to ballpark your query logic and see whether you're casting too wide a net.


Here are different, more specific queries.

This first one is a much more precise intersection of two ingredients, but may not capture all the misspellings or combinations that are possible.
https://api.fda.gov/drug/event.json?search=patient.drug.medicinalproduct:(%22EthinylEstradiol%22+AND+%22Norelgestromin%22)+patient.drug.medicinalproduct:(%22Ethinyl%20Estradiol%22+AND+%22Norelgestromin%22)
In this query (about 30 results) we're looking for any records where the medicinalproduct contained BOTH ""Ethinyl Estradiol"" AND ""Norelgestromin"" or contained BOTH ""EthinylEstradiol"" AND ""Norelgestromin"".
Here's a more compact version of the same query, using more parentheses to group all the things we want to search for within medicinalproduct:
https://api.fda.gov/drug/event.json?search=patient.drug.medicinalproduct:((%22EthinylEstradiol%22+AND+%22Norelgestromin%22)+(%22Ethinyl%20Estradiol%22+AND+%22Norelgestromin%22))&limit=25
Note that the + is just a ""space"" and to the API, it's an implicit OR. In other words, if you just write a bunch of words with spaces between them, the API is going to try to match the biggest number of records that contains any of those words. You can use quotation marks and the keyword AND to be more specific.
This second one is more like what I think Andrew from OpenGeoCode was going for—a union of records that match ANY of ""Ethinyl Estradiol"" OR ""EthinylEstradiol"" OR ""Norelgestromin"". It returns about 7k records.
https://api.fda.gov/drug/event.json?search=patient.drug.medicinalproduct:(%22EthinylEstradiol%22+%22Ethinyl%20Estradiol%22+%22Norelgestromin%22)&limit=25
Remember, the parentheses are grouping all the things we want to look for WITHIN medicinalproduct, and the + between them is an implicit OR.

"
How to download all datasets from a CKAN instance using the API?,"
This is not supported by the CKAN API, so you would need to script it.
"
data request - Dataset of sentences translated into many languages,"
Tatoeba.org is exactly that: http://tatoeba.org
Their data is human-edited and used by dozens of products/websites including electronic dictionaries, so it is of reasonable quality.
It has 471468 English sentences and 179 languages.
(not all sentences are translated to all languages though, far from that)
The structure is not a simple 1-1-1, a sentence can have several translations.
For your example, see http://tatoeba.org/eng/sentences/show/406004 :

The whole data is open (Creative Commons CC-BY) and downloadable at http://tatoeba.org/eng/downloads in various formats.
Disclaimer: I am a member of Tatoeba (I mentored a GSoC that created a webapp to enrich your Anki decks using Tatoeba)
"
Data request: NBA data to practice statistical programming,"
Reputation too low to comment (could someone please change this into one?).
For older data: http://www.databasesports.com/
"
data request - Video game dataset,"
Giant Bomb (API info) is probably going to be your best bet. They have a very large database of games, and they keep track of details like release date, genre, platforms, publishers, franchises, characters, locations, etc. However, they restrict their API to non-commercial use and 200 requests/hour.
Another one to check out is TheGamesDB (API info). Their database doesn't support some of the more advanced stuff Giant Bomb has like locations, characters, etc, but they are much less restrictive with their data. Unfortunately, their API is known for having a fair bit of downtime -- I wouldn't recommend relying on them for a user-facing app.
There's also IGDB (API info), which is also free and emphasizes that they allow commercial use.
"
"data request - IMDb users (reviews, watchlist, and ratings) dataset","
Users-related data is not currently released.
One option could be screen-scraping, but it is forbidden by the usage conditions:

Robots and Screen Scraping: You may not use data mining, robots, screen scraping, or similar data gathering and extraction tools on this site, except with our express written consent as noted below. 

"
usa - Basic Financial Data for Non-Governmental Organizations (NGOs),"
Selected financial information on all entities that file using form 990 with the IRS is public. A variety of datasets can be found in the IRS Statistics of Income (SOI). I would start at this page:
http://www.irs.gov/uac/SOI-Tax-Stats-IRS-Tax-Exempt-Organization-Population-Data
"
data request - Is there a publicly available database of all Apple products?,"
Apple provides this information in on their Trademarks site, which includes all Apple product names and descriptions (an easy scrape off the page)
You reference MacTracker, but that does provide what you need (latest database). The terms are relatively lenient for use.
"
geospatial - R package for geographic regions,"
The best place to find classes of R packages is with a task view:
http://cran.r-project.org/web/views/
Within task views, the Spatial view is going to have a large number of options:
http://cran.r-project.org/web/views/Spatial.html
Part of the complexity is that there are different levels of granularity or zoom levels.  You probably are looking for something that has polygons for nations and polygons for states/provinces within nations.  That would make the ""maps"" and ""mapsdata"" packages most relevant,
[I don't have enough reputation to post links to those]
The best way to learn R tools is with vignettes, so a search for ""R vignette maps mapdata"" is a great way to start.
Mapping data is a big ball of wax, because many different disciplines have very different needs for their data, hence you have tools from the GIS community, spatial statistics, geology, political science, etc.  The ""Applied Spatial Data Analysis with R"" is a nice overview of working with spatial data in R.
"
data request - List of all restaurants in a city (EU)?,"
Here is my obligatory pointer to overpass turbo, a ""graphic"" way to explore the data quality for certain POIs (points of interest) in the OpenStreetMap database.
This examplary query for amenity=restaurant in Rome (patience: the query takes several seconds to execute) yields (probably incomplete) 543 locations with varying metadata.

How-to: For a given POI, find out its most canonical way of tagging through the page Map Features on the OSM Wiki. Then use the ""Wizard"" button on overpass turbo to generate the query. Browse the map to a suitable test location and hit execute. The map then can be navigated and the POIs inspected for data quality. 
"
"data.gov - Is there any open data FDA, or other, regarding U.S. Pharmacopoeia standards?","
I found a definition from the NIH site related to humans and other animals. (PDF link)

Pharmaceutical-grade compound: A pharmaceutical-grade compound (PGC) is defined as any active or inactive drug, biologic or reagent, for which a chemical purity standard has been established by a recognized national or regional pharmacopeia (e.g., the U.S. Pharmacopeia (USP), British Pharmacopeia (BP), National Formulary (NF), European Pharmacopoeia (EP), Japanese Pharmacopeia (JP), etc.). These standards are used by manufacturers to help ensure the products are of the appropriate chemical purity and quality, in the appropriate solution or compound, to ensure stability, safety, and efficacy. The Food and Drug Administration (FDA) maintains a database listing of FDA approved commercial formulations for both FDA approved human drugs (the Orange Book) and veterinary drugs (the Green Book).

"
data request - Sample dataset: files of a typical company,"
I think there are too many diverse filetypes to expect a good set. For example, wikipedia has many listed at List of File Formats, with this disclaimer:

This is an incomplete list that may never be able to satisfy particular standards for completeness. 

But, if you make a list of possible extensions (see here from a list of lists, and here for A-E), you can pass them all in a big loop to a search engine with a ""filetype:abc"" argument (and download the results).
The problem is that I don't know of a search engine that has robust filetype searching. I checked both Google and DuckDuckGo and neither found actual files for the few I checked.
So, this isn't really an answer, but perhaps someone knows a search engine for robust filetype searching. Hopefully that search engine also has a way to filter based on license.
"
data request - Where to download the Forvo database?,"
I guess because it's part of Anki, it's somewhat related, but I just stumbled on this
A half-automatic Forvo Downloader


You select the field you want the audio to go into and press an editor button or keyboard shortcut.

The addon will open the relevant forvo.com page in your web browser.

You decide on one or more pronunciations and download them the regular way in your web browser.

The addon will automatically pick up the files from your web browser's downloads directory and insert them into the field.



Source code: https://github.com/yunidatsu/anki-forvodl
"
data request - Credit card metadata database,"
if you need a free bin database, you can get it here:
https://getcreditcardonline.com/free-bin-database-download/
Data in CSV, JSON, SQL, TXT, Excel formats, etc.
"
data request - Taxation systems across countries,"
This EU documentation shows various Value Added Tax rates for countries in Europe:
http://ec.europa.eu/taxation_customs/resources/documents/taxation/vat/how_vat_works/rates/vat_rates_en.pdf
"
Why does a report listing a generic drug (drospirenone and ethinyl estradiol) have multiple brand names listed in the drug’s openfda section?,"
What's in a drug name?
It turns out that drug names are a complicated business. Drugs are known by a generic name (usually the name of the active ingredient)—like IBUPROFEN, and often multiple brand names—like ADVIL, or MOTRIN; even IBUPROFEN itself can be a brand name, like NIGHTTIME IBUPROFEN. Drugs may also have multiple ingredients, so the generic name may describe all of those ingredients—like ACETAMINOPHEN/CODEINE.
It’s important to note the difference between two “drug name” fields in openFDA’s drug adverse events API—medicinalproduct and openfda.brand_name.
medicinalproduct
This is imported from the original FDA adverse drug event report. It notes the drug name as reported, and even for a single “drug” there can be wide variation among records. For instance:

Generic or brand. Sometimes reporters write ACETAMINOPHEN, and sometimes TYLENOL.
Format. For the same drug, you might see ACETAMINOPHEN/CODEINE or CODEINE/ACETAMINOPHEN or ACETAMINOPHEN + CODEINE or CODEINE WITH ACETAMINOPHEN or ACETAMINOPHEN AND CODEINE.
Misspellings. IBUPROFEN appears in about 40,000 reports but even IBUPROPHEN (a misspelling) appears in almost 60! It’s not possible to know a priori all the ways that humans might have misspelled drug names when reporting.

Because of all this variation, you can imagine that it might be difficult to find ALL the records for a particular drug. It’s not even easy to capture all the variation in correctly spelled drug names, never mind the misspelled ones.
openfda annotations, which are added to the original record (like a sticky note), help by listing other known names and codes for the drug that was written in medicinalproduct. If the openFDA system was able to match a correctly spelled drug product name, it’ll add an openfda section for each drug; this section sits alongside medicinalproduct, and doesn't replace it. This is called “harmonization” in openFDA.
This section makes it easier to search for reports by those names and identifiers—no matter what name someone used in the original report. However, when you search in openfda fields, you’re distinctly NOT searching for the name used in the original report. You’re casting a wider net.
openfda.brand_name
This is a list of brand names that a drug may be known by. If you search in medicinalproduct for IBUPROFEN, a record may be returned with a long openfda.brand_name list including dozens of possible names that IBUPROFEN is marketed under, including:
""PAIN RELIEF ANTI INFLAMMATORY"", ""ADVIL"", ""HEALTH SENSE INFANTS IBUPROFEN ORAL SUSPENSION"", ""DOVER ADDAPRIN"", ""HEALTHY ACCENTS IBUPROFEN CHILDRENS"", ""PROFEN IB"", ""SHOPRITE IBUPROPHEN"", ""JUNIOR STRENGTH ADVIL"", ""PEDIACARE CHILDRENS"", ""GOOD NEIGHBOR PHARMACY IBUPROFEN CHILDRENS"" …

This list isn’t doesn’t necessarily tell you what drug brand the patient was taking, but it gives you an idea of all the products you might want to search for if you want a complete count of adverse events for a particular drug (if you think of the drug as its ingredient, i.e. its generic name).
So, now it’s possible to answer the question…
1. Why are there so many brand names listed in patient.drug.openfda.brand_name, when the original report listed DROSPIRENONE AND ETHINYL ESTRADIOL?
DROSPIRENONE AND ETHINYL ESTRADIOL is the generic name of the drug YAZ. Although the original report listed DROSPIRENONE AND ETHINYL ESTRADIOL as the medicinal product being taken, the openfda section shows possible brand names (and other information) for the the drug. This particular drug, a combination of two hormones, is marketed under many brand names, all with the same ingredients (but sometimes in different dosage forms and strengths, meant to be taken on different schedules). Those brand names include, but are not limited to:
`GIANVI, SYEDA, YAZ, OCELLA, LORYNA, YASMIN, VESTURA, ZARAH`

This Mayo Clinic article lists _even more names that this drug (DROSPIRENONE AND ETHINYL ESTRADIOL) goes by. What’s important to understand is that the openfda annotation is additional information that can be used to learn more about the drug listed in the report—it does not necessarily list the name of the drug the patient was taking.
2. Why does a search in patient.drug.openfda.brand_namefor one of the brand names—YAZ—include reports listing a generic drug (DROSPIRENONE AND ETHINYL ESTRADIOL)?
Searching in openfda.brand_name tells the API to look in the annotated list of possible brand names for a drug, not in the original report’s medicinalproduct field. The results include all the records that have an openfda.brand_name list that contains YAZ.
Here’s why a record would have YAZ in openfda.brand_name:

The patient.drug.medicinalproduct contained YAZ.
The patient.drug.medicinalproduct contained DROSPIRENONE AND ETHINYL ESTRADIOL, which is marketed under many brand names, including YAZ.

https://api.fda.gov/drug/event.json?search=safetyreportid:""4990905-5""
For this report, the original medicinalproduct contains DROSPIRENONE AND ETHINYL ESTRADIOL—the generic name of YAZ. So, openFDA matched the generic name and made an openfda section that lists the known possible brand names for DROSPIRENONE AND ETHINYL ESTRADIOL—including YAZ. That’s why it showed up in a search for YAZ in patient.drug.openfda.brand_name.
"
data request - Food expiration database,"
I found a link to the Food Bank of Alabama, which has a 26 page PDF with a table of food expiration dates (pdf link). Here is another similar PDF, but even less machine readable. And here is yet another one (PDF).

It's not too machine readable, but you may be able to copy individual pages and paste them into a spreadsheet tool.
If this data source is useful for you, I can also help get it to be machine readable using a tool like pdftotext.
"
Why does the number of results change when I search for YAZ in the field patient.drug.medicinalproduct vs. patient.drug.openfda.brand_name?,"
patient.drug.medicinalproduct and patient.drug.openfda.brand_name are not the same.
First, patient.drug is a list of multiple drugs from the adverse event report. For each drug, there is always a medicinalproduct (name), and sometimes an openfda section.

medicinalproduct is the drug name as written in the original adverse event report (FAERS record), imported into the openFDA database. It could be a brand name, or a generic name. It might even be misspelled. (cf. IBUPROPHEN which has ~59 results, instead of IBUPROFEN which has ~40k results.)
brand_name is part of the openfda section. It’s an annotation, like a “sticky note,” added to each item in patient.drug. It has additional possible names and codes that can be used to identify the drug in medicinalproduct. It’s only added if the drug name in medicinalproduct was spelled correctly, and the name matched in certain other FDA and NLM datasets; openFDA calls the process of adding this section “harmonization.”

Why do the searches have different result counts? Four ways to search, with explanations.
In this case, YAZ is one of the many brand names for a certain generic drug, DROSPIRENONE AND ETHINYL ESTRADIOL. In fact, many brands contain the same ingredients, but in different quantities and forms: GIANVI, SYEDA, YAZ, OCELLA, LORYNA, YASMIN, VESTURA, ZARAH, etc. And this Mayo Clinic article lists even more brand names that contain the same generic drug. Here’s how different searches work, and what the result counts mean. (The diagram may help and is followed by specific examples.)

1. Find exact matches on the drug name (brand or generic) in the original report.
Criterion: The original report listed at least one drug with the exact name YAZ.
Use patient.drug.medicinalproduct.exact.
https://api.fda.gov/drug/event.json?search=receivedate:[20040101+TO+20120930]+AND+patient.drug.medicinalproduct.exact:""YAZ""
Results: 19,946.
2. Find “fuzzy” matches on drug name (brand or generic) in the original report.
Criterion: The original report listed at least one drug with a name like YAZ, YAZ (24), YAZ 28, etc.
Use the field patient.drug.medicinalproduct.
https://api.fda.gov/drug/event.json?search=receivedate:[20040101+TO+20120930]+AND+patient.drug.medicinalproduct:""YAZ""
Results: 20,033. As expected, this is larger than the exact match, because it included more possible names.
3. Find reports for a drug, when you know its brand name—whether the original report used the brand name or generic name.
Criterion: The report has an openfda annotation with the brand name YAZ.
Use the field openfda.brand_name.
This will match when:

The original report listed at least one drug with a name like YAZ, YAZ (24), YAZ 28, etc. (in the medicinalproduct field).
The original report listed at least one drug with the generic name DROSPIRENONE AND ETHINYL ESTRADIOL (in the medicinalproduct field).

https://api.fda.gov/drug/event.json?search=receivedate:[20040101+TO+20120930]+AND+patient.drug.openfda.brand_name:""YAZ""
Results: 22,028. As expected, this is even larger than the previous search, because it included even more possible names.
4. Find reports for a drug, when you know its generic name—whether the original report used the generic name or any of the known brand names.
For drug adverse events, it’s often important to find all the reports for a drug—like IBUPROFEN, no matter what brand name was reported (ADVIL, MOTRIN, etc.). However, it’s not always appropriate—and may not be for a hormone therapy for contraception like DROSPIRENONE AND ETHINYL ESTRADIOL, where the different brand names may represent drug products with different strengths and dosage schedules.
Criterion: The report has an openfda annotation with the generic name DROSPIRENONE AND ETHINYL ESTRADIOL.
Use the field openfda.generic_name.
This will match when:

The original report listed at least one drug with a name like YAZ, YAZ (24), YAZ 28, or YASMIN, or OCELLA, or any other brand names that the generic drug is marketed as.
The original report listed at least one drug with the generic name DROSPIRENONE AND ETHINYL ESTRADIOL.

https://api.fda.gov/drug/event.json?search=receivedate:[20040101+TO+20120930]+AND+patient.drug.openfda.generic_name:""DROSPIRENONE+AND+ETHINYL+ESTRADIOL""
Results: 30,459. As expected, this is the largest result set, since it includes the largest possible set of drug names.
(The result counts above are valid as of the openFDA “last updated date” of 2014-08-06. In the future, when this date changes, the data may be slightly different.)
"
data request - Where can I find the source code for Liferay Sync?,"
Here is a github https://github.com/liferay/liferay-plugins for liferay plugins. May be, it's here.
"
economics - Open alternatives to the IMF data,"
The UN has several sources of datasets relating to government expenditures and trade that are free to use for commercial and non-commercial usage. Generally, the terms require attribution. 
Note, not all UN datasets are free to use. 
UN Data - data.un.org
Terms of Use - http://data.un.org/Host.aspx?Content=UNdataUse
All data and metadata provided on UNdata’s website are available free of charge and may be copied freely, duplicated and further distributed provided that UNdata is cited as the reference.
World Bank - data.worldbank.org/
Terms of Use - http://data.worldbank.org/summary-terms-of-use
You are free to copy, distribute, adapt, display or include the data in other products for commercial and noncommercial purposes at no cost subject to certain limitations summarized below.
You must include attribution for the data you use in the manner indicated in the metadata included with the data.
"
"data request - Open database of industrial companies and products, with bill of materials","
The National (US) Renewable Energy Laboratory provide open-source LCI (Life Cycle Inventories) for products, which contain inputs to and outputs from production of products.
From the ""on the project"" page: 

The U.S. Life Cycle Inventory (LCI) Database is a publicly available database that allows users to objectively review and compare analysis results that are based on similar data collection and analysis methods.

You can get to the site at: This link
See an example of an inventory of Ethylene, at plant here
"
data request - Information about congresspeople,"
The data you are looking for is available at openstates.org. This is a project of the Sunlight Foundation. They have an API and a bulk download page (JSON and CSV) at : http://openstates.org/downloads/
The about page on their website:
Open States is a collection of tools that make it possible for citizens to track what is happening in their state's capitol by aggregating information from all 50 states, Washington, D.C., and Puerto Rico.
Using the site is simple: enter a U.S. address or select a state to start to research bills, review voting records, contact elected officials and more. Check out this Sunlight Academy tutorial to see how Open States can help citizens, journalists and activists learn more about their state government.
Open States is a project of the Sunlight Foundation. Thank you to the Rita Allen Foundation, Minnesota Historical Society and Open Society Foundations for their generous support.
"
data request - Retrieve affiliations from research paper,"
Many of the bibliographic databases offer APIs, but they might not be 100% open.  Typically, you can't get access to the ones that charge for access unless your institution has a subscription to the service.
I know that link-only answers are bad, but the problem is that policies change over time, and the folks at MIT libraries has a rather long list of bibiometric APIs and info about how open they are, which I assume they'd maintain/update over time:

http://libguides.mit.edu/apis

For your purposes, I'd avoid the subject specific ones (pubmed, arXiv) or publisher specific ones, and look to JSTOR Data for Research or Web of Science (the one run by Thomspon-Reuters). 
As you're interested in tracking by people, I'd like to recommend ORCID, but it requires your faculty registering with them and acknowledging their papers, so they don't mis-attribute papers because of similar names.
"
data request - Country specific information for travellers using opendata api/dataset,"
The Department of State's API covers security issues relating to travel, relations with countries - but not travel visa/passport requirements.
http://www.state.gov/developer/
The also have a humanitarian information unit that publishes maps and datasets about risk areas of the world: such as conflict in Syria and Ebola in West Africa.
https://hiu.state.gov
This is the State Department's XML feed on travel warnings:
http://cadatacatalog.state.gov/storage/f/2013-11-24T21%3A00%3A58.223Z/tws.xml
"
usa - Data set of US Congress in-session or recess,"
Keeping in mind the caveats raised in comments about what counts as being ""in-session,"" any day that at least one chamber of Congress meets will result in an entry in the Congressional Record, which you can find at the Library of Congress. See, as an example, August when neither chamber typically meets much at all.
You should be able to scrape that page to obtain a complete calendar of session dates since 1989.
"
Looking for historical data for WWII era navy fleet makeups,"
There's a surprisingly comprehensive list of ships, naval equipment, and vessels for WWII at the World War II Database.  You can search by type of vessel (destroyer) or by country (Germany).  There is also detailed information about each vessel mentioned (such as the U.S. Casabianca submarine).
"
data request - List of competitions at Tokyo 2020 Olympics,"
The official source for Olympic data is http://odf.olympictech.org/
This site does not yet have any references to the Tokyo olympics. 
"
data request - Dataset with informations about donations,"
You can find aggregated data on charitable donations from the IRS's Statistics of Income (SOI). Here are some links to look around:
http://www.irs.gov/uac/Tax-Stats-2
http://www.irs.gov/uac/SOI-Tax-Stats-Statistics-of-Income
In the UK, the National Council for Voluntary Organisations (NCVO) and the Charities Aid Foundation (CAF), has been conducting and compiling survey results on charitable contributions by individuals. The data goes back to 2004:
http://data.ncvo.org.uk/datastore/datasets/dataset-6-uk-giving-survey/
"
medical - Where would I find Step or Pedometer Data?,"
There are human activity datasets available for research. Some of the better known ones are:
Fordham University, NY
WISDM dataset (walking, jogging, standing, sitting, etc) - no gender/age
The WISDM (Wireless Sensor Data Mining) Lab is concerned with collecting the sensor data from smart phones and other modern mobile devices (e.g., tablet computers, music players, etc.) and mining this sensor data for useful knowledge. Currently our efforts are mainly focused on the acclerometer and GPS sensor data from these devices, but in the future we will mine the audio sensors (microphones), image sensors (cameras), light sensors, proximity sensors, temperature sensors, pressure sensors, direction sensors (compasses) and various other sensors that reside on these devices.
http://www.cis.fordham.edu/wisdm/dataset.php
University of Michigan
Collective Activity Dataset 
This page describe a Collective Activity Dataset. This dataset contains 5 different collective activities : crossing, walking, waiting, talking, and queueing and 44 short video sequences some of which were recorded by consumer hand-held digital camera with varying view point. 
http://wwweb.eecs.umich.edu/vision/activity-dataset.html
Here's something else that is fairly new. It is a FitBit activity dataset and analysis. Though I think it is collected from one individual:
https://rpubs.com/dmaurath/24643
"
api - Does data stored in Mendeley qualify as open data?,"
Data
Mendeley does not seem to grant me any license to reuse the content (in particular the academic papers uploaded by other users), so by default the data must be considered closed: We are not allowed to redistribute it. Even if an API allows us to retrieve it:

You may not use our Services to [...] download, use or re-use any Academic Papers without authorization.

Software
Mendeley's software is clearly not FOSS, as seen in the Terms:

you undertake not to copy, rent, lease, sub-license, loan, translate, merge, adapt, vary or modify the whole or any part of our Software

The question about the data itself is independent of the software itself being open source or not. You can run MediaWiki (open source software) to run a private server and store non-open data on it. On the opposite, you could run IIS (proprietary software) to host open data.
"
releasing data - How to embed licenses within SVG?,"
This is not RDFa, but rather embedded RDF/XML, which is allowed by the SVG Tiny 1.2 Specification.
There are the following errors:

<cc:license>

instead of  
<cc:License>

in lines 26, 29 and 30, 33. The former is a property, the latter is a class.
<cc:attributionName rdf:resource=""Laurent Notarianni and LittleMap.org"" />
<cc:attributionURL rdf:resource=""LittleMap.org"" /> 

instead of  
<cc:attributionName/>Laurent Notarianni and LittleMap.org</cc:attributionName>  
<cc:attributionURL rdf:resource=""http://littlemap.org"" />  


Literals are not IRIs.

<cc:license rdf:about=""http://opendatacommons.org/licenses/odbl/1.0/"">
   <cc:legalcode rdf:resource=""http://opendatacommons.org/licenses/odbl/1.0/"">
      <dcq:hasversion>1.0</dcq:hasversion>
    </cc:legalcode>
</cc:license>

instead of
<cc:License rdf:about=""http://opendatacommons.org/licenses/odbl/1.0/"" >
   <cc:legalcode rdf:resource=""http://opendatacommons.org/licenses/odbl/1.0/index.html"" />
   <dcq:isVersionOf rdf:resource=""http://opendatacommons.org/licenses/odbl/"" />
</cc:License>

In short, hasVersion is intended to be used with non-literal values (IRIs and blank nodes).
Next, if a license has a version, it doesn't mean that this version is applicable.

Final version:
<svg xmlns=""http://www.w3.org/2000/svg"" version=""1.2"" baseProfile=""tiny"">
<metadata xmlns:rdf=""http://www.w3.org/1999/02/22-rdf-syntax-ns#""
          xmlns:dcterms=""http://purl.org/dc/terms/"" 
          xmlns:dc11=""http://purl.org/dc/elements/1.1/""
          xmlns:cc=""http://creativecommons.org/ns#"" 
          xmlns:geo=""http://www.w3.org/2003/01/geo/wgs84_pos#"" >
  <rdf:RDF>
    <cc:Work rdf:about="""">
      <dc11:format>image/svg+xml</dc11:format>
      <dc11:type    rdf:resource=""http://purl.org/dc/dcmitype/StillImage"" />
      <cc:license rdf:resource=""http://creativecommons.org/licenses/by-sa/4.0/"" />
      <cc:license rdf:resource=""http://opendatacommons.org/licenses/odbl/1.0/""  />
      <cc:attributionName>Laurent Notarianni and LittleMap.org</cc:attributionName>
      <cc:attributionURL  rdf:resource=""http://LittleMap.org"" />
    </cc:Work>
    <cc:License    rdf:about=""http://creativecommons.org/licenses/by-sa/4.0/"">
      <cc:permits  rdf:resource=""http://creativecommons.org/ns#Reproduction"" />
      <cc:permits  rdf:resource=""http://creativecommons.org/ns#Distribution"" />
      <cc:permits  rdf:resource=""http://creativecommons.org/ns#DerivativeWorks"" />
      <cc:requires rdf:resource=""http://creativecommons.org/ns#Notice"" />
      <cc:requires rdf:resource=""http://creativecommons.org/ns#Attribution"" />
      <cc:requires rdf:resource=""http://creativecommons.org/ns#ShareAlike"" />
    </cc:License>
    <cc:License rdf:about=""http://opendatacommons.org/licenses/odbl/1.0/"" >
      <cc:legalcode rdf:resource=""http://opendatacommons.org/licenses/odbl/1.0/index.html"" />
      <dcterms:isVersionOf rdf:resource=""http://opendatacommons.org/licenses/odbl/"" />
    </cc:License>
    <geo:Point>
      <geo:lat rdf:datatype=""http://www.w3.org/2001/XMLSchema#decimal"">55.701</geo:lat>
      <geo:long rdf:datatype=""http://www.w3.org/2001/XMLSchema#decimal"">12.552</geo:long>
    </geo:Point>
  </rdf:RDF>
</metadata>
</svg>

RDFa 1.1 Distiller will produce the following Turtle:
<file:///cygdrive/c/users/.../Desktop/map.svg> a cc:Work ;
    cc:attributionName ""Laurent Notarianni and LittleMap.org"" ;
    cc:attributionURL <http://LittleMap.org> ;
    cc:license <http://creativecommons.org/licenses/by-sa/4.0/>,
               <http://opendatacommons.org/licenses/odbl/1.0/> ;
    dc11:format ""image/svg+xml"" ;
    dc11:type <http://purl.org/dc/dcmitype/StillImage> .

<http://creativecommons.org/licenses/by-sa/4.0/> a cc:License ;
    cc:permits cc:DerivativeWorks,
        cc:Distribution,
        cc:Reproduction ;
    cc:requires cc:Attribution,
        cc:Notice,
        cc:ShareAlike .

<http://opendatacommons.org/licenses/odbl/1.0/> a cc:License ;
    cc:legalcode <http://opendatacommons.org/licenses/odbl/1.0/index.html> ;
    dcterms:isVersionOf <http://opendatacommons.org/licenses/odbl/> .

More info: RDF 1.1 XML Syntax
"
data request - Emu population in Australia from 1900,"
There are 9 different types of emu whose populations range from common to extinct.  The Grey Emu is prevalent, while the Kangaroo Island Emu is extinct.  
If you want general populations of emu in Australia, try Birdata, which shows populations of Australian birds by location and time, and allows you to download custom datasets.  Here's the link for emus.  Many other sites simply state that the emu is not endangered.
For a truly interesting read on the Great Emu War, check out this Scientific American article.
"
usa - Data Licenses for US Government Data not in data.gov,"
Each of the datasets on Data.gov describes the license used (see the upper left items on the dataset page). The intent for data provided by the U.S. Government (whether it is on Data.gov or not) is to have an open license, as defined by Project Open Data. The license field in the Data.gov metadata schema is defined as well.
In most cases, the license is ""public"", meaning that it is in the public domain and open for free and unrestricted use. If the license is ""License not specified"", you can assume that it is in the public domain. See for example, the Campus Security Data.  Agencies that have a more restrictive license must provide that restriction in the metadata. For example, NOAA's National Mosaic of Weather Data explicitly notes it uses the Creative Commons license and cites a disclaimer.
The intent of U.S. Government in publishing data on Data.gov is for individuals and organizations to freely use that data for transparency, civic good, insight, analysis, and economic development.  
Specific to your questions:

As above, there is a general expectation that these data are in the public domain, with any restrictions noted.
The classification system for licenses is open and noted at Project Open Data.
""Initially, SOI’s ZIP code data were not produced on an annual basis. Therefore, some years are missing. However, beginning with the 2004 data, the SOI Division began producing annual updates to the data and we are committed to continuing this trend into the future.  In similar fashion, SOI began producing machine-readable formats of the data beginning with the 2007 data and has produced these formats for all years thereafter. Given our current resources, there are no plans to produce ZIP code data for the missing years or to create machine-readable versions of the data prior to 2007.  We regret the inconvenience this causes. The Statistics of Income ZIP code data are available on the Tax Stats section of this IRS web page."" (via Diane Austin, IRS)
Violations can be reported by following the FEC use rules, specifically here (via Paul Clark at the FEC)
Information is anonymized by the publishing agency before being posted to Data.gov, in accordance with federal regulations for government officials to protect personally identifiable information (see a summary here).

Data.gov's own data policy notes: ""License: U.S. Federal data available through Data.gov is offered free and without restriction. Data and content created by government employees within the scope of their employment are not subject to domestic copyright protection under 17 U.S.C. § 105. Non-federal (city, county, and state) data available through Data.gov may have a different licensing method as noted under “Show more” at the bottom of the dataset page. Non-federal data can be identified by name of the publisher and the diagonal banner that shows up on the search results and data set pages. Federal data will have a banner noting “Federal” and non-federal banners will note “University”, “Multiple Sources”, “State”, etc.""
(Disclaimer: I have worked on the Data.gov project.)
"
"Healthcare.gov's API Finder app returns ""File not found""","
On the command line, give a try to the following successful request (or see/edit it in runscope):
$ curl -XPOST https://api.finder.healthcare.gov/v3.0/getIFPPlanQuotes -d '<?xml version=""1.0"" encoding=""UTF-8""?>
<p:PlanQuoteRequest xmlns:p=""http://hios.cms.org/api"" xmlns:p1=""http://hios.cms.org/api-types"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://hios.cms.org/api hios-api-11.0.xsd "">
  <p:Enrollees>
    <p1:DateOfBirth>1959-03-03</p1:DateOfBirth>
    <p1:Gender>Male</p1:Gender>
    <p1:TobaccoLastUsedMonths>2</p1:TobaccoLastUsedMonths>
    <p1:Relation>SELF</p1:Relation>
    <p1:InHouseholdIndicator>true</p1:InHouseholdIndicator>
  </p:Enrollees>
  <p:Enrollees>
    <p1:DateOfBirth>1982-03-03</p1:DateOfBirth>
    <p1:Gender>Female</p1:Gender>
    <p1:TobaccoLastUsedMonths>2</p1:TobaccoLastUsedMonths>
    <p1:Relation>SPOUSE</p1:Relation>
    <p1:InHouseholdIndicator>true</p1:InHouseholdIndicator>
  </p:Enrollees>
  <p:Location>
    <p1:ZipCode>22901</p1:ZipCode>
    <p1:County>
      <p1:FipsCode>51003</p1:FipsCode>
      <p1:CountyName>ALBEMARLE</p1:CountyName>
      <p1:StateCode>VA</p1:StateCode>
    </p1:County>
  </p:Location>
  <p:InsuranceEffectiveDate>2014-10-01</p:InsuranceEffectiveDate>
  <p:Market>Individual</p:Market>
  <p:IsFilterAnalysisRequiredIndicator>false</p:IsFilterAnalysisRequiredIndicator>
  <p:PaginationInformation>
    <p1:PageNumber>1</p1:PageNumber>
    <p1:PageSize>20</p1:PageSize>
  </p:PaginationInformation>
  <p:SortOrder>
    <p1:SortField>BASE RATE</p1:SortField>
    <p1:SortDirection>ASC</p1:SortDirection>
  </p:SortOrder>

</p:PlanQuoteRequest>'

FWIW, I was able to reproduce the 503 ""File not found"" by POSTing the same request but with trailing characters after the url like https://api.finder.healthcare.gov/v3.0/getIFPPlanQuotesXYZ
"
"licensing - No-attribution open license, applicable to documentation","
Creative Commons ShareAlike 1.0 Generic (CC SA 1.0) should match all of your requirements. However, CC SA 1.0 has been retired by Creative Commons in 2004 due to inadequate demand, and they recommend using one of their current licenses instead.
"
Where I can find data to help build family trees?,"
There are a lot of paid services that provide this type of information, such as Ancestry.com.  They are unlikely to offer access to their highly curated content.
However, the Church of Latter Day Saints at FamilySearch does provide a free service for others to use.  That site provides developer resources specifically for people like you, and encourages others to build apps using APIs and accessing their data.  
Data formats are described in more detail here.
Good luck!
"
Music lyrics timing data,"
(Not a very practical answer, but maybe can get the ball rolling...)
If you can create a list Youtube links to the songs or albums you want, you can download the captions (subtitles). These have a timestamp along with the lyrics. 
With YouTube you can search for videos with captions. Unfortunately, I found that many are listed with captions but actually don't have any meaningful text.

Here is an example of one video with good captions (link).

You can see the transcript by going to ""More"" and then ""Transcript"".


One tool to do download captions is called Google2SRT.

What is Google2SRT? Google2SRT is a tool that can download ""not embedded"" subtitles (Closed Captions - CC) from YouTube and former Google Video and convert them to a standard format (SubRip - SRT) supported by most video players. 

There are some online tools, too, such as keepsubs.com and yousub.net (although questionable how reliable).
Also, a Firefox add-on.

You could also use OpenSubtitles.org to download subtitles soundtracks or concerts.
"
economics - Looking for datasets with sigmoidal relationships,"
In technology forecasting, the market penetration of a new technology often follows a sigmoidal curve. Hence, with one variable being the time, the other variable being the market share of

ebooks
LED
organic food

could form examples. There is a classic paper by Fisher and Pry on this, however behind a paywall.
"
data.gov - Is there a way to get a drug's experimental name on open FDA?,"
My only guess is that you could find the previous version of that setid (it looks like that it is currently on version 30), and perhaps the name you are look for would be in there. The OpenFDA API is only ever going to have the latest version. 
The DailyMed site has an API for pulling SPL history, so you know the date that it changed its name, then you can determine which version to track down. 
Hope that helps.
The API call would be: http://dailymed.nlm.nih.gov/dailymed/services/v2/spls/8bc6397e-4bd8-4d37-a007-a327e4da34d9/history.json
The response looks like: 
```
{
    ""metadata"": {
    ...
    },
    ""data"": {
        ""history"": [{
            ""spl_version"": 31,
            ""published_date"": ""Mar 05, 2015""
        }, {
            ""spl_version"": 30,
            ""published_date"": ""Aug 12, 2013""
        }, {
            ""spl_version"": 29,
            ""published_date"": ""Mar 13, 2013""
        }, 
          ...
           {
            ""spl_version"": 12,
            ""published_date"": ""Sep 18, 2009""
        }, {
            ""spl_version"": 11,
            ""published_date"": ""Jul 23, 2009""
        }, {
            ""spl_version"": 1,
            ""published_date"": ""Nov 14, 2008""
        }],
        ""spl"": {
            ""setid"": ""8bc6397e-4bd8-4d37-a007-a327e4da34d9"",
            ""title"": ""ERBITUX (CETUXIMAB) SOLUTION [IMCLONE LLC]""
        }
    }
}

```
"
releasing data - Interest of double licenses CC-BY-SA + ODbL for SVG maps,"
Let's give this a try then.
1. To allow people inserting our data into OSM, do we require to use ODbL?
Not necessarily. This is what OSM has to say on the topic:

We are only interested in 'free' data. We must be able to release the data with our OpenStreetMap License. Obviously we are allowed to use public domain data sources, of which there are quite a few, but beyond that, it gets more complicated.

So as long as your license is either very liberal (e.g. CC0) or compatible with the ODbL, it should be fine. Which brings us to your next question.
2. Is CC-BY-SA enough for OSM compatibility?
According to this discussion on the OSM wiki, I'm going to say no. The main problem seems to be the attribution requirement, although I don't quite understand why. However, when you license your data under both CC-BY-SA as well as ODbL, there should be no problem.
3. Else, what could be the advantage of CC-BY-SA in our case?
As you pointed out yourself, Creative Commons licenses are currently the best-known open licenses. A lot of time has gone into refining them over time, and since version 4.0 they are perfectly suited to cover data and not just content.
4. Does CC-BY-SA prevent OSM data to be inserted within our maps?
This is where things start to get interesting.
OSM state on their copyright page:

If you alter or build upon our data, you may distribute the result only under the same licence.

However, the ODbL states:

4.4 Share alike.
a. Any Derivative Database that You Publicly Use must be only under the terms of:
i. This License;
ii. A later version of this License similar in spirit to this License; or
iii. A compatible license.

So as long as CC-BY-SA 4.0 is deemed a ""compatible license"", all is well.
5. Is it correct to license XML file (SVG) under a Database license?
Well, I guess it depends. If your SVG is a creative work (such as a painting or drawing), then no. If it is just simple geometric shapes, then yes. 
What about SVG file generated from OSM data?
If they are generated automatically, there's probably no creative process involved, so it should be fine to assume that a database license always fits.
This last question is actually a great example of one of the big advantages of CC-BY-SA 4.0: You simply don't have to care if your content is a database or a creative work, because the license covers both.

Disclaimer: I am not a lawyer, this is just my own personal assessment. I'm happy about any input and feedback :)
"
"business - Transactional data over multiple years (Customer ID, Date, Price)","
Kaggle once conducted a competition with 22GB of real transaction data:

http://www.kaggle.com/c/acquire-valued-shoppers-challenge/data (registration required)

But look after the terms of use (only for the purposes of competition) and so on.
"
data request - Looking for datasets of tumor or cancer growth,"
I asked on Twitter and got this response: https://twitter.com/sachsmc/status/532600589033951234
Looks like there is some data at http://dtp.nci.nih.gov/index.html, but he said it's hard to navigate
"
data request - FIFA 2014 tweets dataset for academic project,"
Getting access to historical Twitter data may end up coming with a price.  Here are some options:

I've had great luck using Topsy in looking at a wide variety of tweets ranging from disease vectors in Africa to sentiment analysis. Here's the link for 86K #FIFA tweets for the last 30 days.  You can expand to ""all time"", search by language, and look at influencers.
Use the Twitter API to get the data you can for free.  Good developer resources are available.
The most comprehensive historical archive may be via Gnip, but unfortunately it is not free and it's unclear what the actual costs are.

Good luck!
"
data.gov - Missing medical device recall information? OpenFDA not finding recalls from FDA database,"
Couple things possibly at play but this is not a complete answer. I noticed that there are reports form 2003 on the FDA site you are referencing and ones from October 2014. Both sets (and only a handful) are not found in the API.
A litte deeper digging I queried Monoject and could not find any of the recalls associated with them in the API return - http://www.researchae.com/recalls?reporttype=device&from_date=2000-01-01&to_date=2014-11-30&search=Monoject
Not sure the reason for this but did notice some missing ones as well.
"
data request - Where to download decimals of Pi?,"
UPDATE: Unfortunately this website is not available anymore, see philshem's comments below for a potential? workaround.

This Japanese site has downloadable chunks up to 13x10^12:
Web site: http://piworld.calico.jp/estart.html
There are 130000 files (pi-0001.txt-pi-130000.txt), each chunk is a ZIP file about 55MB (so it's around 7.15TB in total, 15.34TB uncompressed).
No registration, agreement or anything needed.
"
data request - London rail and tube station locations,"
Alex, I looked through the TFL APIs, documentation and datasets. The station list (KML) format indicates it has locations for light rail (DLR), tube and overground. But as you observed there are no entries for overground stations. I also did not find anything on National Rail in the documentation.
I did find some additional resources. Below is the National Rail's list of stations in London (which you would need to scrape):
http://www.nationalrail.co.uk/css/OfficialNationalRailmaplarge.pdf
The Guardian maintains its own downloadable dataset of all rail stations in Great Britian. .
http://www.theguardian.com/news/datablog/2011/may/19/train-stations-listed-rail#data
https://docs.google.com/spreadsheet/ccc?key=0AonYZs4MzlZbcktheEZFeF84U1J4dFFvckI5X0VBcEE#gid=7
It appears the Guardian reversed engineered the dataset from the Office of Rail Regulation' station usage statistics.
http://orr.gov.uk/statistics/published-stats/station-usage-estimates
The UK data.gov has a dataset labeled: National Public Transport Access nodes (NAPTAN). 

NaPTAN is a GB national system for uniquely identifying all the points
  of access to public transport in GB. It is a core component of the GB
  national transport information infrastructure and is used by a number
  of other UK standards and information systems. Every GB station, coach
  terminus, airport, ferry terminal, bus stop, etc., is allocated at
  least one identifier.

http://data.gov.uk/dataset/naptan
"
data request - Adverse Events for Avastin - vision issues,"
This API call should help you get started with all adverse event (AE) reports involving Avastin:
https://api.fda.gov/drug/event.json?search=brand_name:avastin&limit=100

but be sure to read through the comprehensive docs at https://open.fda.gov/drug/event/reference/ to see if you want to be searching for a subset of this data set which is where Avastin was the suspect drug. I am not 100% sure that would be possible through the API since you want to limit your search to a nested object.
To search for only AE reports where a certain AE related to vision was reported, you'll want to look through the list of all AEs from the above API call (https://api.fda.gov/drug/event.json?search=brand_name:avastin&count=patient.reaction.reactionmeddrapt.exact) and then filter based on those.
You can also use a more user-friendly tool that Brian Norris and I built at http://researchae.com. Here's a hyperlink to only Avastin AE reports.
Hope this helps and good luck!
"
usa - Source of ABA Numbers for US banks,"
The source for this data is going to be accuity, that takes care of the routing numbers for the ABA, 
http://www.aba.com/Products/Pages/PS98_Routing.aspx
To fast forward, it looks like you can download a version from the federal reserve board here:
http://www.fededirectory.frb.org/download.cfm
but it states:

I understand that the terms of use prohibit selling, relicensing, or otherwise using information in the directory for commercial gain

You can buy an official copy from accuity for $495, ""ABA Key to Routing Numbers""
http://store.accuitysolutions.com/order.html
Up until last year, the ABA was claiming copyright to this list of 9 digit numbers, so I don't know if there is a legally open version of the dataset around.
http://www.gregthatcher.com/Financial/LawyerProblems.aspx
"
data request - Database of inaccuracies in mainstream media,"
It's just a subset of ""mainstream media"", but you can use the NYTimes API to search for ""Correction: "".
They also maintain a page with recent corrections, which point to the amended article.
Their format looks like this, so it should be easy to find with the API.


You'll probably have to create the ""database"". This NYTimes data would plug into the data model and some fields you'd have to generate (i.e. tags)
"
"licensing - How do I license a work with CC license version X ""or later""?","
CC BY-SA 4.0 states that ""The Adapter’s License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-SA Compatible License."" Similar clauses can be found since CC 2.0.
"
HUD Section 8 100% Median Income Data,"
As Andrew has suggested, you could just use ACS data here. Make sure to download as a .csv to get the FIPS State-County codes.
"
usa - Understanding the population sample represented in CDC WONDER database,"
I would go with the population denominator of 878,964. According to the Census Bureau, the 20 to 24 year old 2011 population in Illinois was 881,738.
As to your second issue, just remember that it only makes sense to combine them when you are looking for rate of cases among the population. If you want to reference it as the rate of chlamydia among the population, you would have to keep it separate from the calculation of gonorrhea because some people might have both. The best practice would be to only reference them as separate rates. Ignore whatever CDC wonder is doing with the sum total at the bottom. It is probably a programming oversight.
Let us know if you're looking for any more information.
"
data request - Dataset of historical economic forecasts,"
The World Bank provides an excellent overview and summary, as well as detailed current and historical data. 
Additionally:

You can get information from individual government open data portals (scroll to bottom), but it's inconsistently formatted and may not be available from all governments.
Oxford Economics provides country economic forecasts.  There's summary information here, and an archive of free open data here. (Note that there is a fee to get access to all their data, although you can get a free trial.)
The International Monetary Fund has a summary of GDP and detailed data.

"
"linked data - ""DBpedia as Tables"" not having all the properties","
According to DBpedia as Tables, DBpedia uses the following data to create the tables:

Each instance is described by its URI, an English label and a short abstract, the mapping-based infobox data describing the instance (extracted from the English edition of Wikipedia), geo-coordinates, and external links.

DBpedia basically provides two different types of data: the Infobox Dataset (everything beginning with dbpprop) and the Mapping-based Dataset (everything beginning with dbpedia-owl). If you're interested, you can read more about the difference.
For these tables, only the mapping-based data (dbpedia-owl:…) is used. That's why you won't find any raw infobox properties starting with dbprop — however, you can download the raw data if you're really interested.
Quick note from DBpedia about the raw infobox properties though:

Information that has been extracted from Wikipedia infoboxes. Note that this data is in the less clean /property/ namespace. The Mapping-based Properties (/ontology/ namespace) should always be preferred over this data.

"
usa - Looking for US County elevation data,"
Have you tried the Area Health Resource File (AHRF)? It has elevation data and then some for this type of analysis.
"
usa - License for data product of music lyrics?,"
This from the U.S. Copyright Office might be a better reference on U.S. copyright law in relation to musical lyrics.  They provide an excellent plain-language description of the copyright rules for those wanting to register a copyright.
There is also a description for use that may allow what you are trying to do: ""Under the fair use doctrine of the U.S. copyright statute, it is permissible to use limited portions of a work including quotes, for purposes such as commentary, criticism, news reporting, and scholarly reports. There are no legal rules permitting the use of a specific number of words, a certain number of musical notes, or percentage of a work. Whether a particular use qualifies as fair use depends on all the circumstances. See FL 102, Fair Use, and Circular 21, Reproductions of Copyrighted Works by Educators and Librarians.""
"
data request - List of all passenger airlines with regular flights to anywhere in New Zealand,"
There are many commercial sites that will provide this info, but for a CC license I'd use the data from the NZ government site: transport.govt.nz.

The license is CC 3.0 BY NZ (details).
"
data request - Pictures of taxis in Karachi,"
With Google you can search for images and choose the general license terms. For specific companies you'd have to do a specific search.
Here is a search with ""Reuse with modification""

Here is one example of results, from Wikipedia.

"
data request - I'm looking for an Oracle Express Edition 11g testing database,"
Based on this stackexchange question, I found the page:
Creating the Sample Database in Oracle 11g Release 2 (Other instructions)

Also, if you are using XE, there is a demo DB ready to play with.
"
releasing data - What are some OpenData torrents to seed?,"
Community wiki to collect data sets available to download/seed on bit torrent


DNS Census, the DNS registration dataset snapshot taken in 2013 (compressed ~15GB and uncompressed 157GB).


The DNS Census 2013 is an attempt to provide a public dataset of registered domains and DNS records. It was inspired by the Internet Census 2012 which showed that releasing data anonymously via BitTorrent is a good thing to do. The dataset contains about 2.5 billion DNS records gathered in the years 2012-2013.
All data is compressed using xz/LZMA2.
DNS records are written into CSV files. There is one file for each DNS record type (A/AAAA/CNAME/DNAME/MX/NS/SOA/TXT). The records are sorted lexicographically by hostname and by time.



NYC taxi trip data - 2013 Trip Data (11.0GB) and 2013 Fare Data (7.7GB)


Fare data looks like this, showing medallion, hack_license, vendor_id, pickup date/time, payment type, fare, tip amount (look at all those zeros!), tolls, and total.
Trip data (the good stuff!) looks like this.  Each file has about 14 million rows, and each row contains medallion, hack license, vendor id, rate code, store and forward flag, pickup date/time dropoff date/time, passenger count, trip time in seconds, trip distance, and latitude/longitude coordinates for the pickup and dropoff locations. 



Natural Earth data BitTorrent mirror (description,  4.81 GB)


Natural Earth is a public domain map dataset available at 1:10m, 1:50m, and 1:110m scales. Featuring tightly integrated vector and raster data, with Natural Earth you can make a variety of visually pleasing, well-crafted maps with cartography or GIS software.



UK Government expenditure (multiple files, more information)


The Combined Online Information System (COINS) is the database for UK Government expenditure. The data is used to produce expenditure data in the Budget report; Supply Estimates; Public Expenditure Statistical Analyses (PESA); Whole of Government Accounts (WGA); the monthly Public Sector Finance Releases. It is also used by the ONS for other National Statistics releases. 



Academic torrents (multiple files, browse datasets or collections)


This service is designed to facilitate storage of all the data used in research, including datasets as well as publications. There are many advantages of using bittorrent technology to disseminate this work. 



911datasets - (3,254 GB) (3 TB) in 256,673 files (as of 31 Dec 2014)


Crowdsourcing 9/11 information distribution.
The idea behind 911datasets.org is to make raw information about 9/11 available to a broad audience, and to provide a way to ask useful questions about the data. 
Most of the material was obtained using the Freedom Of Information Act (FOIA) process. 

Link to all torrents

+ Wikileaks data storage - wlstorage.net
You can either download everything, or select files by ""project"", as folders or single files: torrents (1827 in total at time of posting).
More details here



2012 Internet Census (568 GB torrent)


While playing around with the Nmap Scripting Engine (NSE) we discovered an amazing number of open embedded devices on the Internet. Many of them are based on Linux and allow login to standard BusyBox with empty or default credentials. We used these devices to build a distributed port scanner to scan all IPv4 addresses. These scans include service probes for the most common ports, ICMP ping, reverse DNS and SYN scans. We analyzed some of the data to get an estimation of the IP address usage.

Full data download 
Hilbert Browser tool
Image gallery
(Taken from this answer)


Sci-Hub Bulk Access


Sci-Hub is a paywall-bypassing website that uses ""shared"" user credentials to provide PDF or HTML scientific papers. The website itself doesn't store any papers. Answer from this open data stack exchange question.

Collection of more than 1 million books.
Collection of more than 50 million scientific papers.

All data gathered during our research is released into the public domain for further study. 



Geocities archive (641 GB)


This is a collection of Geocities data downloaded by a bunch of people who
    call themselves ARCHIVE TEAM, who began scraping the Yahoo! Geocities site
    during a six month period in 2009, before Yahoo! shut down geocities.com 
    on October 26th, 2009. This collection is compressed in a UNIX filesystem
    with both 7zip archives and tape archives (gtar).



Wikimedia data dump (over 23 TB total, but smaller torrents available)


This is an unofficial listing of Wikimedia data dump torrents, dumps of Wikimedia site content distributed using BitTorrent...
This includes both dumps already being distributed at dumps.wikimedia.org and dumps created and distributed solely by others.
BitTorrent is not officially used to distribute Wikimedia dumps; this article lists user-created torrents. Please protect your computer and verify the md5sum for any file downloaded from these unofficial mirrors.

In particular, Wikidata

The GHTorrent project 

GHTorrent monitors the Github public event time line. For each event, it retrieves its contents and their dependencies, exhaustively. It then stores the raw JSON responses to a MongoDB database, while also extracting their structure in a MySQL database.
Currently (Jan 2015), MongoDB stores around 4TB of JSON data (compressed), while MySQL more than 1.5 billion rows of extracted metadata. A large part of the activity of 2012, 2013, 2014 and 2015 has been retrieved, while we are also going backwards to retrieve the full recorded history of important projects.

Since 2015, the dumps are daily as mysql format
Downloads: http://ghtorrent.org/downloads.html

"
data request - Cost of living dataset,"
I found an API from Numbeo (documentation) that claims

8,282,934 prices in 11,072 cities entered by 706,591 users
(information updated 2022-10-18, source)

Here is their page related to the Cost of Living.
And an overview of the data for Zurich.
Their Terms of Service state that the license is CC BY-SA 3.0 and GNU Free Document License (GFDL).
What I can't find is how to get an API key (or quotas), although I think you can get one once you register as a user.
.
"
economics - Historical monthly farm/agricultural data 1950 to the present,"
The ""FOOD AND AGRICULTURE ORGANIZATION OF THE UNITED NATIONS - Statistics Division"" (link) provides historical data for commodity prices and agricultural production.
You can download for a single country or for all countries. Some data sets go back to at least the 1960s. You'll have to investigate which data sets may be best suited to your purpose.
Data formats are typically zipped CSV.

"
licensing - What are the legal uses of the data from livingwage.mit.edu?,"
This site helps to calculate the cost of living in different parts of the United States.  There's a caveat about the fidelity and comprehensiveness of the data (""Consider the results a minimum cost threshold that serves as a benchmark, but only that."").
For licensing, the site references that it is part of the Living Wage Project. However, in looking through all those sites, there are no references to data use and licensing.
To verify the licensing, you can contact the author of the page or the project team. (I'm in contact now with the author to clarify the data use rules.)
"
Data.gov: Is the API Version 3 down completely?,"
Rebecca, from the Data.gov team here. Following up on this, to our knowledge the Data.gov CKAN API hasn't been down. Can you provide more details on the issues you were having? 
If helpful in constructing your query parameters, here is documentation on the CKAN API: http://docs.ckan.org/en/latest/api/index.html#making-an-api-request
Also, we are working to provide additional documentation specific to the Data.gov CKAN API, you can follow that progress or weigh in here: https://github.com/GSA/data.gov/issues/180
I hope that helps! 
"
best practice - Is it okay to download all datasets from a government open data portal?,"
Bad manners have nothing to do with it. As a taxpayer, it's your data, and they're making it available for you to download. Your country also benefits from having backups of their dataset floating around out there. 
You could take the public service angle of it to the next level by uploading a copy of the full archive to the Internet Archive using their S3-compatible interface: https://archive.org/help/abouts3.txt
If you think their system is creaky and might strain at giving you the full dataset, or you think there's a more efficient way for them to give you a full dump -- then you can always email or call and ask them. 
But do it after you've got your first complete copy downloaded, just in case.
"
data request - Datasets of detailed statistics from MMORPGs,"
Great question! There are several MMORPG games that provide this type of data, and APIs to others from which you can extract the data.  Check out the following:

World of Warcraft APIs on Github with everything from pets to professions extending over players, guilds, and regions
League of Legends API (includes game data and assets, champions, items, runes, masteries, summoner spells, and profile icons)
Eve Online data access explained (market and universe data at a regional level)
GuildWars 2 API covering recipes, items, and commerce
And another 179 game APIs via the Programmable Web

Several of these require registering but seem to be free. Some, like Eve Online, restrict secondary commercial use, but encourage developers, researchers, and others to use the data for co-development or non-commercial use.
"
data request - The media word frequency (n-grams) dataset,"
If the dataset you want isn't already available, you can create it with some basic programming. Creating an N-gram tool is pretty straightforward, and the volumes of data are not so large if you limit yourself to one topic or a handful of media sources.

Collecting
For this type of project, you are looking to cast a wide net and collect as much data as possible, in order to get a strong signal. Errors in data collection will be lost in the noise (not enough frequency to stand out).
For the data collection, one idea is to use RSS feeds, which come usually in XML format and are easy to parse. For example, for Yahoo News - Politics. You can also use the URL in each short RSS item and then scrape the page.
Another option is the Google News RSS feed or Bing News Developer (details). Here is the URL to an XML output from Google News RSS feed for a particular query.
https://news.google.com/news/feeds?q=apple&output=rss

I like python and I can suggest HTML scraping tools such as lxml, beautifulsoup, scrapy, etc.
You might also look for some news aggregator tools that provide news articles from various sources in a simple text format. 

Parsing
After download as much text as possible, it's time to parse. Python's zip module is ready to go (my source):
input_list = ['all', 'this', 'happened', 'more', 'or', 'less']
def find_ngrams(input_list, n):
  return zip(*[input_list[i:] for i in range(n)])

By running with n=2, the response is this:
[('all', 'this'), ('this', 'happened'), ('happened', 'more'), ('more', 'or'), ('or', 'less')]

In reality, you need to create input_list by splitting on all punctuation and white space. For example,
import re
text = 'all. this, happened more : or ; less'
print re.split('\W+', text)

would return an array of words from a large piece of text, without punctuation (warning - the '_' character isn't considered punctuation)
There are also some pre-built N-gram tools from NLTK (one, two)

Storage
If the parsing takes some time because of large raw data volumes, I'd write to an the raw N-grams to an intermediate file (probably just CSV).  Perhaps you also want to store the source relating the N-gram to the source document (something Google provide), as well as the date (or just year).
Then with some processing you can convert arrays and similar files (like CSV) to a format similar to Google N-grams. To do so, one option is to use the defaultdict library from python's Collections module. To do so, a form of each N-gram would be the key to the dictionary and the integer count would be the value.
from collections import defaultdict
data = defaultdict(int)
for item in find_ngrams(input_list, 2): # using the function from above
    data[item] += 1
print data

would return a count for each N-gram (not so exciting in this case).
 {('or', 'less'): 1, ('all', 'this'): 1, ('more', 'or'): 1, ('happened', 'more'): 1, ('this', 'happened'): 1})


Further

Consider tagging your words with a part of speech (POS) with tools like NLTK.

"
tool request - Sentiment search engine,"
The first source of raw interactions that fit your needs that comes to mind is Twitter.
NCSU's Tweet Sentiment Visualization
http://www.csc.ncsu.edu/faculty/healey/tweet_viz/tweet_app/ seems pretty good in that it allows you to enter keywords and get a graph of recent tweets graphed on axes (pleasant/unpleasant and active/inactive). It also has some pretty extensive documentation about how to interpret and how the system arrives at the results. Clicking on a data item shows the raw tweet:

Lists of tools

http://matei.org/ithink/2012/02/08/a-list-of-twitter-sentiment-analysis-tools/ has a list of other Twitter Sentiment Analysis Tools
http://www.khwrites.com/online-sentiment-analysis-social-media-monitoring-tools/ has a list of free Online Sentiment Analysis: Social Media Monitoring Tools


Another thought would be to ingest tweets on your own and use one of many Sentiment APIs to arrive at your own results.
"
data request - Educational attainment in the private and public sector,"
this is easy if you use the public-use microdata.  
the public/private-sector workforce variable in the current population survey (cps) is a_clswkr and the educational attainment variable is a_hga
open up cps or acs, subset by public vs private-firm worker, and calculate educational attainment rates.
cps and acs are both representative at the state-level.  acs also has pumas, which are sub-state areas.
for international comparisons, look at the codebook for piaac
good luck!
"
data request - Phones dataset for speech recognition (not telephone number),"

You can find US and British english language phones at university linguistic departments such as Berkeley, Ca. Though, what is useful for speech recognition are the features in statistical acoustic models that are extracted from a large amount of annotated audio recordings. These acoustic model features involve hidden markov models and Mel-Frequency Cepstral coefficients. (MFCC tutorial by James Lyons below)  Interpreting and understanding these feature files is complex.
The CMU Sphinx speech recognition project provides different acoustic models such as AN4 and RM1.  The distribution of some CMU audio databases such as RM1 are restricted. The AN4 database includes the audio recordings and utilizes 34 phones.  

Berkeley Linguistics Phonetics
(click on phone, then left click for audio, right click for spectrogram)
https://corpus.linguistics.berkeley.edu/acip/ 
CMU Sphinx acoustic models
http://www.speech.cs.cmu.edu/databases/an4/index.html 
CMU Sphinx dictionary
https://github.com/cmusphinx/cmudict
cmudict.dict
  yes Y EH1 S
  no N OW1
cmudict.phones  (sample of 39 phones vs 34 phones for AN4)
  Y       semivowel
  EH      vowel
  S       fricative
  N       nasal
  OW      vowel
cmudict.symbols
  Y
  EH1
  OW1  

Voxforge provides open source corpus, audio recordings and acoustic models for multiple speech recognition engines in multiple languages.  CMU Sphinx, ISIP, Julius, and HTK
http://www.voxforge.org/home
https://github.com/julius-speech/julius
https://www.isip.piconepress.com/projects/speech/
http://htk.eng.cam.ac.uk/
James Lyons provides an excellent Mel Frequency Cepstral Coefficient (MFCC) tutorial along with python code.
http://www.practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/
https://github.com/jameslyons/python_speech_features
Adam Coates of Baidu Research and Stanford gave an excellent presentation about improving the acoustic model.
http://cs.stanford.edu/~acoates/ba_dls_speech2016.pdf
https://github.com/baidu-research/ba-dls-deepspeech
http://www.openslr.org/12/ (corpus, audio recordings, acoustic models)

"
data request - Return on investment in international development projects,"
great question. 
The first thing to note is that the IEG database that you're referencing is only one of several that the World Bank provides. You might want to also check out the Bank's projects portal: http://www.worldbank.org/projects. In particular, check out specific country pages for a lot of good details, as well as many project documents (in pdf) for most projects. 
In addition, you might want to check out this platform that tracks many types of development financing (aid, private foundations, remittance flows, etc.): aiddata.org
More and more organizations in international development are also providing data on their own projects. A few examples:

DfID (UK): http://devtracker.dfid.gov.uk/
AfDB's project portfolio
AsDB: adb[dot]org[forwardslash]projects

Many countries are now also publishing their own aid information. Just a handful here:

Haiti's MGAE
Nepal aid management system
Timor aid management system
Bangladesh aid information system
Afghanistan DAD system

More and more organizations and countries are reporting actual deliverables from these investments, but that information is more scattered and difficult to synthesize. RoI itself is hard to measure, as there's no clear indication of how you might track that.
"
finance - Stock market historical data,"
There are many good resources described in the comments to your question (1 & 2). One that combines many different types of financial products and has some open access is Quandl.

Data browser tool

API documentation



Quandl provides a single easy-to-use API for stock prices and fundamentals. Coverage includes end-of-day prices, harmonized fundamentals, key financial ratios, earnings estimates, analyst ratings, price targets, indexes and more.

The API comes with 3 levels (1 open/free, 2 paid). The open level is based on community maintained data (details).

End of day stock prices, dividends and splits for 3,000 US companies, curated by the Quandl community and released into the public domain.
History to 2004.


In addition to Quandl, I've also had a good experience using the Markitondemand API (documentation). Their interactivechart endpoint can give you historical data.

If you use programming languages like Python or R, you can automatically integrate historical data via the public feeds (i.e yahoo, google, quandl).
Python Quandl module
import Quandl
mydata = Quandl.get(""NSE/OIL"", authtoken=""your token here"")

R Quandl example
library(tseries) # Loading tseries library
mtgoxusd <- read.csv('http://www.quandl.com/api/v1/datasets/BITCOIN/MTGOXUSD.csv?&trim_start=2010-07-17&trim_end=2013-07-08&sort_order=desc', colClasses=c('Date'='Date'))

"
"rdf - Is there any work, api, publication, or example of providing a SPARQL query engine ontop of an OSLC web service? - Stack Overflow","
There has been some work by some, such as IBM, to build a SPARQL index by leveraging the Tracked Resource Specification (TRS) to keep the index current.  Though, it could be possible to leverage a similar model and using OSLC's query capability to build such an index and keep it current.  As you say, it depends a lot on what tool(s) you are trying to connect to and what support they expose.  At the simplest level, you could just treat like Linked Data and crawl the links and leveraging HTTP caching controls to minimize requests.
"
data.gov - Plans and documentation to tidy RAW data in Maude?,"
I haven't seen any ""plans"" per se from FDA but have noticed that they have split up the MAUDE database into separate components. There is a MASTERLIST but the PATIENT_NARRATIVE has been split off into another set of files.
I have been poring over MAUDE in efforts to extract ""significant unanticipated adverse effects"". Such events trigger strict reporting and distribution timelines. However, when a clinical trial is conducted under an Investigational Device Exemption (IDE) (as many are), the FDA doesn't seem as interested in ensuring regulatory compliance. 
Perhaps the FDA is not interested because such delays can trigger statutory fines for each additional day of violation. 
"
data request - Database of food sharing places for the whole world,"
I suspect this type of dataset will have to be constructed from many local sources. Particularly since the 'meaning and access' may vary on culture and environment/conflict.
But for the US/Canada, I would suggest searching on 'food banks' and 'public food pantries'.
USA
Some of the major cities with open data portals have such datasets. Including:
Seattle: https://data.seattle.gov/Community/Food-Banks/ryz5-i54h
Boston: https://data.cityofboston.gov/Health/Food-Pantries-with-Local-Sourcing-Map-View/7ygz-72yc
Westchester County, NY: http://giswww.westchestergov.com/Metadata/wcpantry.htm
Here are agencies that keep online list of their food bank locations:
Second Harvest of Missouri : http://www.ourcommunityfoodbank.org/index.cfm/pageid/207/fuseaction/user.alphaSelect/m/0
Oregon Food Bank: http://www.oregonfoodbank.org/Our-Work/Regional-Food-Bank-Network
Feeding America keeps: http://www.feedingamerica.org/find-your-local-foodbank/?_ga=1.223258523.1626167282.1417548401
Food Bank of South Jersey: http://www.foodbanksj.org/FindHelp.html
Food Bank of Alaska: http://www.foodbankofalaska.org/viewPage.php?ID=8
Here's someone's site attempting to (crowd source) collect comparable information across the US:
http://www.foodpantries.org/
"
Looking for audio data set for English words,"
Lingua Libre has a lot of pronunciation recordings, including in English, by many speakers:

https://lingualibre.org
https://commons.wikimedia.org/wiki/Category:Lingua_Libre_pronunciation-eng
https://lingualibre.org/datasets/

Unfortunately I have not found an easy way to find out what words have the most pronunciations. You might have to download the dataset and write a small script to find them.
Content is available under Creative Commons 4.0 Attribution-ShareAlike.

[Easy to use but not open data] Forvo is a crowdsourced effort to create sound files for every word of every language.
The great thing is that a word in a language can have more than 1 sound file.
In fact, you will often find sound files created by males and females from various regions with different accents and voices.
Pronunciations are ranked by clarity, so you can focus only on clear sound files, or on the contrary include difficult-to-understand sound files, depending on your goal.

bread: 10 pronunciations from USA, UK, Malaysia
anything: 30 pronunciations

This page contains the words that have most pronunciations:
http://www.forvo.com/languages-pronunciations/en/by-popularity/
Unfortunately the data can be reused but not for commercial purposes.
"
"Open data on public opinion polls (Gallup, PEW, and others)","
All Pew Research Center data is available for free use. But, you have to manually download each dataset.
Data for almost all other public opinion polling is stored at the Roper Center. These data are publicly searchable but not free to download.
"
Is there an open data feed of storm tracks in the Asia-Pacific region?,"
Yes, there is!  The real-time datasets related to hurricane, cyclone, and typhoon tracks is at the National Hurricane Center.
Note that people use the terms hurricanes, cyclones, and typhoons interchangeably (as noted by the U.S. National Oceanic and Atmospheric Administration):
Hurricanes, cyclones, and typhoons are all the same weather phenomenon; we just use different names for these storms in different places. In the Atlantic and Northeast Pacific, the term “hurricane” is used. The same type of disturbance in the Northwest Pacific is called a “typhoon” and “cyclones” occur in the South Pacific and Indian Ocean.

Hurricane track data is provided by NOAA's National Weather Service, and you can find historic tracks for the Pacific and Atlantic.
"
data.gov - How does one link two selected elements on two tables within openFDA?,"
Try adding .exact to the end of drugdosagetext 
Also ... this does not count on the dosage but does get to a query of the former parameters - http://www.researchae.com/drugevent?from_date=2004-01-01&to_date=2015-01-31&from_age=&to_age=&search=venlafaxine+hydrochloride&country=GLOBALLY&patientsex=&manufacturername=&drugbrandname=&druggenericname=&medicinalproduct=&reactionmeddrapt=hypertension&drugclass=&drugindication=&indsubmit=&productndc=&safetyreportid=#stats
You can find the code here where you could simply append one of the ""count"" calls with the desired dosage field in the API. https://github.com/GeekNurse/ResearchAE-Open-Source/blob/master/app.rb#L1024
Helpful?
"
The Data.gov organization_list CKAN API is not working,"
Thanks to Philip Ashlock for providing the following answer:

These are both known, but unfortunately unresolved issues. You can
  track their status on github at
  https://github.com/GSA/data.gov/issues/294 and 
  https://github.com/GSA/data.gov/issues/492. You can still filter by
  type using the web UI though, eg
  https://catalog.data.gov/dataset?organization_type=Federal+Government

"
data request - Where can I find open etymology databases?,"
One option is the XML data dump from Wiktionary (link to a recent one).

A complete copy of all Wikimedia wikis, in the form of wikitext source and metadata embedded in XML. A number of raw database tables in SQL form are also available. 
      These snapshots are provided at the very least monthly and usually twice a month.

There is also an API and file download based on the Wiktionary data, but I don't know how comprehensive: http://www1.icsi.berkeley.edu/~demelo/etymwn/

The Etymological Wordnet project provides information about how words in different languages are etymologically related. The information is for the most part mined from Wiktionary. The semi-structured data is turned into a machine-readable etymological database that also incorporates some additional manually added etymological relationships. 

"
tool request - Looking for web scraper that works on YouTube channel video catalogs,"
TubeKit might be of interest to you:

TubeKit is a toolkit for creating YouTube crawlers. It allows one to build one's own crawler that can crawl YouTube based on a set of seed queries and collect up to 16 different attributes.

The tool is open source (licensed under CC BY-NC-SA*) and has been developed for research purposes.
* Creative Commons advise against using one of their licenses for software — but that's really beside the point in the context of this answer ;)
"
data request - API giving ship positions worldwide,"
I've used MarineTraffic.com for finding details about yachts and ships I've seen in ports. It's a really cool website.

They also have an API, but, unfortunately, there is no free access.

There is an API option from FleetMon that is mostly-free:

The FleetMon Public API lets software developers create great software apps that are able to display ship positions and master data, port calls, weather conditions, ports and much more. Developers get a powerful tool to connect their software to the FleetMon.com Vessel Database, enabling them to develop own services as well as integrate with existing IT systems and logistics solutions.

This would be a better option, although I don't know how complete their DB is.

"
data request - Database of Ingress portals,"
Disclaimer: The Ingress Terms of Service disallows ""extracting, scraping, or indexing"" game data. It's possible to get banned if you use this data.
I had the same question and found an answer. A helpful person called Lanced has done the hard work of extracting portal data from the game and provides a public API, available at https://lanched.ru/PortalGet/ However, the author warns not to download the whole database.
There are essentially three ways to use this API:

The most basic API takes minimum and maximum bounds in lat/lon coordinates as input and provides GUID, coordinates and title for 1000 portals at a time. You can make several requests (using the offset= parameter) to retrieve all portals in a larger area.
A more detailed request (using telegram=true), which also returns portal image URL and street address, but is limited to 50 portals per request.
With the search API you can search for portal names and addresses. This one also returns images and addresses.

Examples
Example 1:
https://lanched.ru/PortalGet/getPortals.php?swlat=59.47106&swlng=24.8862&nelat=59.4719&nelng=24.88749&offset=0
{
  ""nextOffset"": -1,
  ""portalData"": [
    {
      ""guid"": ""4c95b3ae447d4605a8421e21ace654c4.16"",
      ""lat"": 59.47143,
      ""lng"": 24.887374,
      ""name"": ""Tallinna Teletorn""
    },
    ...
  ]
}

Example 2: https://lanched.ru/PortalGet/getPortals.php?swlat=59.47106&swlng=24.8862&nelat=59.4719&nelng=24.88749&offset=0&telegram=true
{
  ""nextOffset"": -1,
  ""portalData"": [
    {
      ""guid"": ""4c95b3ae447d4605a8421e21ace654c4.16"",
      ""lat"": 59.47143,
      ""lng"": 24.887374,
      ""name"": ""Tallinna Teletorn"",
      ""image"": ""http:\/\/lh3.ggpht.com\/GG_O7UHlFcqw6tlGvKPxU9jKZ9y-9oBiheIibRtQUP5Q-hZykSnDVrjzL5v_2SlgQmRxxirIAze79lDuDq1jbg"",
      ""address"": ""Kloostrimetsa tee 58a, 11913 Tallinn, Estonia""
    },
    ...
  ]
}

Example 3: https://lanched.ru/PortalGet/searchPortals.php?query=teletorn
[
  {
    ""guid"": ""4c95b3ae447d4605a8421e21ace654c4.16"",
    ""lat"": 59.47143,
    ""lng"": 24.887374,
    ""image"": ""http:\/\/lh3.ggpht.com\/GG_O7UHlFcqw6tlGvKPxU9jKZ9y-9oBiheIibRtQUP5Q-hZykSnDVrjzL5v_2SlgQmRxxirIAze79lDuDq1jbg"",
    ""name"": ""Tallinna Teletorn"",
    ""address"": ""Kloostrimetsa tee 58a, 11913 Tallinn, Estonia""
  },
  ...
]

"
Look for sets of data of historical dates,"
You can find datasets like this from Universities (particularly history depts.). Here's a few:
University of Pittsburg: http://www.dataverse.pitt.edu/external/datasets.php
University of North Texas: http://www.paulhensel.org/icowcol.html
University of Wisconsin-Madison: http://www.sage.wisc.edu/download/crop1700/hist_croplands.html
Uppsalla University: http://www.pcr.uu.se/research/ucdp/datasets/ucdp_prio_armed_conflict_dataset/
"
data request - Where can I find the locations of factories in the U.S. and Japan?,"
In the United States, you might start by looking at water permits. Most large factories have to get permits for their waste and storm water runoff.
Here are discharge permits for Illinois. http://dataservices.epa.illinois.gov/dmrdata/dmrsearch.aspx
Storm water permits: Many industrial facilities have to get a permit for the pollution caused by water running off a location. Here's data for Illinois
"
data request - Coordinates of all embassies and consulates,"
For a wider listing of diplomatic entities, consider using Open Street Maps. 
For example, you can use the Tag: amenity=embassy. This data source includes the following dimensions:
 ""lat""
 ""lon""
  ""tags"": {
    ""addr:city""
    ""addr:country""
    ""addr:housenumber""
    ""addr:postcode""
    ""addr:street""
    ""amenity""
    ""contact:email""
    ""contact:fax""
    ""contact:phone""
    ""country""
    ""name""
    ""name:de""
    ""name:en""
    ""name:fr""
    ""opening_hours""
    ""website""}

With 5,600 objects including consulates, missions, embassies (as of Dec 2014).

To test it out, you can use the overpass-turbo API.

And the corresponding JSON data:

"
healthcare finder api - Is 2015 data available for small group plans?,"
Yes, 2015 data is certainly available. Here is a sample request, that returns 309 plans for the Fairfax, VA area as of 12/08/2014:
curl 'https://api.finder.healthcare.gov/v3.0/getSMGPlanQuotes' -H 'Content-Type: application/xml' --data-binary $'<?xml version=""1.0"" encoding=""UTF-8""?>\n<p:PlanQuoteRequest xmlns:p=""http://hios.cms.org/api"" xmlns:p1=""http://hios.cms.org/api-types"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://hios.cms.org/api hios-api-11.0.xsd "">\n  <p:Enrollees>\n    <p1:DateOfBirth>1984-01-01</p1:DateOfBirth>\n    <p1:Gender>Male</p1:Gender>\n    \n    <p1:Relation>SELF</p1:Relation>\n    <p1:InHouseholdIndicator>true</p1:InHouseholdIndicator>\n  </p:Enrollees>\n  \n\n \n    \n\n  <p:Location>\n    <p1:ZipCode>22031</p1:ZipCode>\n     <p1:County>\n         <p1:FipsCode>51059</p1:FipsCode>\n         <p1:CountyName>FAIRFAX</p1:CountyName>\n         <p1:StateCode>VA</p1:StateCode>\n      </p1:County>\n  </p:Location>\n  <p:InsuranceEffectiveDate>2015-01-01</p:InsuranceEffectiveDate>\n  <p:Market>SmallGroup</p:Market>\n  <p:IsFilterAnalysisRequiredIndicator>true</p:IsFilterAnalysisRequiredIndicator>\n  \n  <p:PaginationInformation>\n    <p1:PageNumber>1</p1:PageNumber>\n    <p1:PageSize>10</p1:PageSize>\n  </p:PaginationInformation>\n\n  <p:SortOrder>\n            <p1:SortField>BASE RATE</p1:SortField>\n            <p1:SortDirection>ASC</p1:SortDirection>\n  </p:SortOrder>\n  <p:Filter>\n      \n      \n      \n      \n      \n      \n     \n      \n    \n</p:Filter>\n  \n</p:PlanQuoteRequest>\n' --compressed

"
economics - Global customs and trade import/export data on the shipping container level,"
I'm clear you're looking for open data and NOT vendors, but I've found some of these vendors do give attribution to their sources and thus aid your search from their marketing materials:

Panjiva
Import Genius
Datamyne
Goodwill
Fleetmon
Marine Traffic
Vessel Finder
Container Ship
Maritime Connector
Marine Vessel Traffic
Air Nav Systems
CMA
Marine Vessel Traffic (more)

"
data request - A table mapping from US county or ZIP to Nielsen Designated Market Area (DMA),"
A 2011 court decision found that Nielsen's DMA maps are protectable by copyright. This article from Bloomberg goes into the details. Here is the formal opinion
Thus, technically, DMA shapes are not freely available and must be licensed from Nielsen.
"
data.gov - Accessing datasets from HUD and VA data portal sites,"
Trainingday,
I believe the master list tht fhamap.com refers to of all HUD owned, managed or approved properties is derived from their dataset of physical inspections of those properties. This and related datasets can be found here:
http://www.huduser.org/portal/datasets/pis.html
I've used these datasets myself. I also have a version of these datasets converted to CSV format using our linked data vocabulary.
http://www.opengeocode.org/cude1.2/HUD/PHA/index.php
It was as clear what you were looking for as a 'master list' from the VA. But the VA website has an open data portal and API. You can download a complete (national) list of VA hospitals and homeless services for Vets.
http://www.va.gov/data/
"
data.gov - FedBizopps Raw Data - API,"
Full disclosure: I am a GSA employee and the Tech Lead for FBOpen, a website and API for search and discovery of federal business opportunities.
There is indeed both bulk data and an fbo.gov API available, although I can only offer experience with the former. There are two different versions of the bulk FTP downloads, weekly files and nightly files. These two are quite different. The weekly XML is proper XML and straightforward to parse. The nightly XML is XML in name and appearance only, but there is a special parser grammar available for it.
https://github.com/presidential-innovation-fellows/fbo-parser
Depending on your needs, you can also use our FBOpen API. Since we rely on the nightly files, we will generally be about a day, sometimes two, behind fbo.gov. You can find more information at our Github repo, and please offer feedback and ask any questions by opening an Issue.
"
usa - Looking for Arrests Data at a state level across years,"
here is complete documented R code to work with all of the microdata that you are looking for
http://www.asdfree.com/search/label/national%20incident-based%20reporting%20system%20%28nibrs%29
"
data request - Official APIs for postal stamps releases by French postal service La Poste,"
The Universal Postal Union (UPU) keeps a database of issued stamps starting in 2002 (it does not go earlier). For France, you can find the stamps issued 2002-2014 here:
http://www.wnsstamps.post/en/
Copyright information from that website:

© Copyright UPU - WADP
  The stamp designs are the property of their respective issuing postal authority.
  The issuing postal authorities have allowed the reproduction of the stamps displayed on this website.

"
data request - Where can I download legend for Corine land cover 2006 classes?,"
I believe I found what you are looking for by googling. This PDF is labeled the CLC 2006 Legend.
http://sia.eionet.europa.eu/CLC2006/CLC_Legeng.pdf
They also have the legend as an image on this page:
http://www.eea.europa.eu/data-and-maps/figures/corine-land-cover-2006-by-country/legend
I also found the spreadsheet version for 1990.
http://www.eea.europa.eu/data-and-maps/data/corine-land-cover-clc1990-250-m-version-8-2005/corine-land-cover-1990-classes-and-rgb-color-codes/clc1990legend.xls
As a courtesy, I compared the 2000 XLS spreadsheet to the 2006 PDF. The data is identical.
"
data request - Crime Statistics on Abductions by Gender,"
This publication has a lot detailed statistics, including breakdown by state/city on abduction/kidnappings. But I did not see a breakdown by gender.
http://www.insideprison.com/Crime_Rates_Detailed_splash.asp?crime=100&crimeName=Kidnaping/Abduction
The FBI UCR tables do have a breakdown of arrests by gender and category, though they do not breakout Kidnapping/Abductions in this set of tables. I suspect they are merged into other categories. You may need to do some researching on the site.
http://www.fbi.gov/about-us/cjis/ucr/crime-in-the-u.s/2013/crime-in-the-u.s.-2013/persons-arrested/persons-arrested
"
crime - US arrest data in 2010 by race and county,"
This link is to the FBI's UCR data on crime statistics (2010) broken down by race and gender
http://www.fbi.gov/about-us/cjis/ucr/crime-in-the-u.s/2010/crime-in-the-u.s.-2010/persons-arrested/persons-arrested
This link shows a subset of the tables where the data is broken down by cities and counties.
http://www.fbi.gov/about-us/cjis/ucr/crime-in-the-u.s/2010/crime-in-the-u.s.-2010/persons-arrested/browse-by/cities-and-counties-grouped-by-size-population-group
This is the home page for all UCR datasets:
http://www.fbi.gov/stats-services/crimestats
"
government - Is there any open data for Emergency Room waiting time?,"
There is no one source that I know of for realtime wait times though some larger networks do have apps or mobile sites that you might look into scraping though of course that has many disadvantages.
In terms of historical data, ProPublica has some great data and analysis at https://projects.propublica.org/emergency/
I believe the sole source of the ProPublica ER wait time site is CMS Hospital Compare data which can be found at https://data.medicare.gov/Hospital-Compare/Timely-and-Effective-Care-Hospital/yv7e-xc69
"
usa - ACS PUMS Data Dictionary Codes in .csv?,"
There are a plethora of XLS converters and scripts out there, but the easiest way IMO is to use google drive in this case. I uploaded the first of the dictionaries, 2011-2013 3-year Code Lists, took about 20 seconds. Now you can edit it in the browser or download as CSV. you can do the same for the text files. check it out - LINK
"
data request - Collecting Canadian postal address information,"
You can use metadata from the Google i18n Internationalization project (link). For example, the address completion metadata from the libaddressinput package (C++ and Java tools) used for Android (Java) and Chromium OS (C++).
The raw data is stored here: https://i18napis.appspot.com/address
For Canada, it looks like this:
    {
   ""lang"":""en"",
   ""upper"":""ACNOSZ"",
   ""zipex"":""H3Z 2Y7,V8X 3X4,T0L 1K0,T0H 1A0,K1A 0B1"",
   ""posturl"":""http://www.canadapost.ca/cpotools/apps/fpc/personal/findByCity?execution=e2s1"",
   ""zip"":""[ABCEGHJKLMNPRSTVXY]\\d[ABCEGHJ-NPRSTV-Z][ ]?\\d[ABCEGHJ-NPRSTV-Z]\\d"",
   ""fmt"":""%N%n%O%n%A%n%C %S %Z"",
   ""require"":""ACSZ"",
   ""name"":""CANADA"",
   ""languages"":""en~fr"",
   ""sub_keys"":""AB~BC~MB~NB~NL~NT~NS~NU~ON~PE~QC~SK~YT"",
   ""key"":""CA"",
   ""id"":""data/CA"",
   ""sub_names"":""Alberta~British Columbia~Manitoba~New Brunswick~Newfoundland and Labrador~Northwest Territories~Nova Scotia~Nunavut~Ontario~Prince Edward Island~Quebec~Saskatchewan~Yukon"",
   ""sub_zips"":""T~V~R~E~A~X0E|X0G|X1A~B~X0A|X0B|X0C~K|L|M|N|P~C~G|H|J|K1A~S|R8A~Y""
}

(link)
And the full list for Canadian addresses:
data/CA
data/CA--fr
data/CA/AB
data/CA/AB--fr
data/CA/BC
data/CA/BC--fr
data/CA/MB
data/CA/MB--fr
data/CA/NB
data/CA/NB--fr
data/CA/NL
data/CA/NL--fr
data/CA/NS
data/CA/NS--fr
data/CA/NT
data/CA/NT--fr
data/CA/NU
data/CA/NU--fr
data/CA/ON
data/CA/ON--fr
data/CA/PE
data/CA/PE--fr
data/CA/QC
data/CA/QC--fr
data/CA/SK
data/CA/SK--fr
data/CA/YT
data/CA/YT--fr 

You then use the individual sub-keys (i.e. QC in french) to download the address formats for that region.
{
   ""lang"":""fr"",
   ""name"":""Québec"",
   ""zip"":""G|H|J|K1A"",
   ""key"":""QC"",
   ""id"":""data/CA/QC--fr""
}

"
data request - Which Governments publish their Ledgers and Accounts?,"
OpenSpending
This website collects datasets related to public spending by country:
https://openspending.org/

Other resources
You should check Open Budget Survey's top 10's national websites for the data.
Among all-in-one place resources, see enigma.io. Here are some datasets:

United States 2014
United Kingdom 2014


Country-specific: Russia
Russia does well in opening its budget to the public: 10th place in Open Budget Survey.
You can find user-friendly interfaces for:
Budget
http://budget4me.ru/ (currently under construction)
Contracts
For all government units and SOEs. API and dataset access forms:
http://clearspending.ru/ (available in English)
"
Weather warning data,"
Here are some US thoughts:
For US warnings/watches, you probably want to go the the storm prediction center,
http://www.spc.noaa.gov/wcm/
and you may want to look at the weather related shapefiles
http://www.nws.noaa.gov/geodata/
and the current alert system probably has an archive, but I didn't see it on first glance:
http://alerts.weather.gov/
I would suggest digging into the census tools related to disaster management.
http://onthemap.ces.census.gov/em/
The ""on the map"" product is a mashup of (1) weather data and (2) population/business data so that the impact of weather on population/businesses can be assessed.
It is only actual federally designated areas, I do not know if it has warnings/watches.
"
"healthcare finder api - I'm trying to make a POST request to HealthcareFinder API v3.0, but I can not. Returns this error","
Make sure you include the Content-Type header and the XML for the request in the POST body.
When I try their example it seems to work fine and the HTTP header includes Access-Control-Allow-Origin: * which should work for CORS. 

"
data request - Open dataset for the number of pets per country,"
Commercial Source:
You can get what you need for the US from this publication. But it is copyrighted material and expensive (~$300).
The all-new 2012 edition of the U.S. Pet Ownership and Demographics Sourcebook is the largest, most statistically accurate and complete survey of the pet owning public and pet population demographics. Drawn from a national survey of over 50,000 households, the survey results are presented alongside the results from similar surveys dating back to 1987, illustrating long-term trends.
https://www.avma.org/KB/Resources/Statistics/Pages/Market-research-statistics-US-Pet-Ownership-Demographics-Sourcebook.aspx
Free (but Coarse):
The US Census publishes some very coarse information on pet ownership, but if you look where the data comes from it is the same source (American Veterinary Medical Association)
https://www.census.gov/compendia/statab/2012/tables/12s1241.pdf
The Human Society has some coarse data as well:
http://www.humanesociety.org/issues/pet_overpopulation/facts/pet_ownership_statistics.html
The BLS has some coarse data as well:
http://www.bls.gov/opub/btn/volume-2/pdf/spending-on-pets.pdf
Public Datasets:
Austin, TX - animal intake:
https://data.austintexas.gov/dataset/2011-Animal-Intake-Report-Cats-and-Dogs-Exclusivel/wrwk-skv6
Colorado - animal intake:
https://data.colorado.gov/Agriculture/2011-PACFA-Shelter-Intake-Statistics/qv2q-ek3a
Weatherford, TX - animal intake:
http://www.weatherfordtx.gov/index.aspx?NID=1469
Adelaide, Australia - dog registration:
http://data.sa.gov.au/dataset?tags=dog+ownership&tags=city
New Zealand - Dog Control Statistics
https://data.govt.nz/dataset/show/3282
"
data request - Where can I get the standard iOS 8 bar icons in a vector graphics format?,"
Austin Andrews has built (and collated from elsewhere, when licensing permits) a collection of icons that follow the material design. These are also available from his Github repository in a variety of formats, including SVG.
To use the actual Apple glyphs within an iOS app, then you can refer to the documentation for UIBarButtonSystemItem in UIBarButtonItem Class Reference
"
tool request - Vocab to describe Image Data / Pictures,"
Are you referring to Focal Point?

Focal Point: Pure HTML/CSS Adaptive Images Framework
A small set of CSS classnames to help keep images cropped on the focal point for responsive designs. Using only HTML/CSS, web authors can specify an image's focal point, which stays as the image's primary focus as the image scales on responsive webpages.

If this is not what you are looking for, they also link to related resources that might be of interest to you.
"
data format - Applying filters to headers in a huge CSV file,"
The csvkit python library is great for transforming big csv datasets in the style of a unix command line tool (like sed). It has many small utilities that do one thing well each so you can compose them in helpful ways. In your case, csvcut can extract certain columns from a csv. 
From their docs:
Extract columns named “TOTAL” and “State Name” (in that order): 

$ csvcut -c TOTAL,""State Name"" examples/realdata/FY09_EDU_Recipients_by_State.csv

Good luck!
"
data request - Dump of WikiLeaks,"
October the 18th 2016: https://file.wikileaks.org/file/ was made publicly visible and file dates and timestamps changed to 1984. 
"
data.gov - DOL Wage and Hour data: Counting backwages under Fair Labor Standards Act,"
There's a data catalog here: http://ogesdw.dol.gov/views/data_catalogs.php (and go to the Wage and Hour Compliance Action Data page). According to the dictionary, flsa_bw_atp_amt is the ""BW Agreed to under FLSA (Fair Labor Standards Act)"".
But it sounds like your question is more about how to get real qualitative information about the data fields. If that is the case I would suggest you try to speak with someone from the department. The WHD's website provides contact information for several offices - I'd give them a call. I've found talking to a real live person to be infinitely helpful in figuring out what exactly I'm looking at.
"
data.gov - Gov data may be used only for statistical purposes?,"
My interpretation is the data can only be used in aggregate (counts and statistics). You cannot use it to identify an individual person. For example, linking a public event (e.g. car accident) with public health data (e.g. Medicare ambulatory event) to gain an individual's medical information. 
http://www.bloomberg.com/infographics/2013-06-05/reidentifying-anonymous-medical-records.html
"
data request - Database of all Coca Cola (and its franchises) bottling plants locations worldwide,"
The data is not apparently available as a downloadable dataset, but there are some documents and visualizations available.

A great visualization of all the Coca Cola facilities (and an associated KML file -- thanks @philshem)
Coca-Cola's description of the facilities in the network
Overall performance of the facilities in the annual Sustainability Report
Overview of global facilities

"
"where can I find gov datasets (from data.gov, bea etc) in MySQL format?","
you can search data.gov, this search returns over 300:
http://catalog.data.gov/dataset?q=MySQL&sort=score+desc%2C+name+asc
and this search within datasets hosted by data.gov reveals 12:
http://www.data.gov/search-results?group=site&q=MySQL 
"
data request - Yule's disturbed pendulum time series example,"
Since you are looking for only a pendulum, and not a generic time series with random noise (like @blairchristian posted), then I would consider simulating the data.
Here is an example of a pendulum code, although you don't need the visualization part. It's a pretty common coding assignment (not as common as double pendulum it seems), so you can probably find your favorite code.
To add the random perturbations, you just need to determine which dimensions are allowed (i.e. if the pendulum is like the Foucault Pendulum, the perhaps the peas can only add random force in the horizontal plane of motion.)
just for fun, consider using quasi-random perturbations that have burst or breakout behavior, like

stock market volume deviation from the mean of that day of the week (example, Apple stock over last 10 years)
live data, i.e. from the twitter public stream, that perturbs the pendulum based on the occurrence of tweets or aggregate data from tweets


"
data request - Energy Consumption of individual buildings,"
i think open ei has this in their building energy data book:
http://en.openei.org/doe-opendata/dataset/buildings-energy-data-book
here's a section from that which is a survey of building energy consumption and extends past 2011:
http://www.eia.gov/consumption/commercial/data/2012/
lastly, try the data hub for the energy performance of buildings for the eu:
http://www.buildingsdata.eu/ 
"
data request - Dictionary containing keywords used on the Web,"
You wrote:

I considered crawling twitter hashtags, or facebook hashtags or google trends, but the thing is people use many useless keywords and hashtags nowadays like #I_ate_pizza #I_love_bieber and they don't search for keywords, the majority search What gift to buy for my girlfriend

I don't really know Facebook, but regarding the Twitter trends:

underscores aren't often used in hashtags
trends don't have to be a hashtag. Here is an example of my local trends:


The Twitter API has several methods that can be used to collect trends (for example), or to create your own (public stream). What is missing is the ability to search for tweets from the complete set (relevance, not completeness).
Also consider LinkedIn API and the many other web APIs.
If you use Google Trends, then the data they provide is stripped of the non-trending words. For example, the search:
What gift to buy for my girlfriend

would come back as part of a trend on:
girlfriend

(maybe on Valentine's day). None of the stop words are included (otherwise, 'the' would be always trending).
Google Trends doesn't have an API, but you can follow instructions from here to hack it. The raw data is in JSON form here:
http://hawttrends.appspot.com/api/terms/

and using this curl command you can change date and place:
curl --data ""ajax=1&geo=US&date=201310"" http://www.google.com/trends/topcharts/category

This request would give you Google trends from the US in October 2013. Here is an unformatted JSON file for the above curl request - LINK. Here is the same data in a JSON formatted web tool - LINK.
"
extracting - Monthly data in Google Trends,"
note: this answer is based on another answer, with some modifications for your question.
Google Trends doesn't have an API, but you can follow instructions from this great blog post to hack it. The raw data is in JSON form here:
http://hawttrends.appspot.com/api/terms/

and using this curl command you can change date and region:
curl --data ""ajax=1&geo=US&date=201310"" http://www.google.com/trends/topcharts/category

This request would give you Google trends from the US in October 2013.
Output:

Unformatted JSON file for the above curl request - LINK. 
Same data in a JSON formatted web tool - LINK.

If you aren't comfortable programming a tool to parse this JSON output, you can use something like OpenRefine as a front-end. With this kind of tool, I think you can convert to CSV. There are also some webtools to convert JSON to CSV (example). Note that the JSON format is for data-interchange and requires understanding the structure for proper transfer to a tabular format (like CSV). Data is stored as key/value and the values can be dictionaries, lists, or any other data-type.
"
medical - Do you know where I can find health data sets?,"
I would recommend taking a look at the Medicare Expenditure Panel Survey (MEPS), a nationally representative survey on the health status of individuals that includes patient discharge information. You could also try looking at the National Hospital Discharge Survey (NHDS), which is a selection of patient discharge data from randomly sampled hospitals.
As a health researcher, you'll definitely want to become acquainted with the ICD 9 diagnosis and procedure codes available by fiscal year here. For example, ischemic heart disease is ""410**"" or ""411**"". Since you're new to the field, I suggest you seek some in person advice about how to use these two completely different data setups. And it would be worthwhile to become familiar with ICD codes. 
"
data request - Where can i find the surface area and the number of premises for all UK districts?,"
Disclaimer: I want to say I am not an expert at using UK Ordnance Survey data.
As Nils pointed out, page 177 in this wifi/broadband coverage report for 2014 makes references to the number of premises (buildings). The report refers to the data source as coming from the UK Ordnance Survey. I looked at the sample datasets, and they do contain building addresses per city/county equivalent.
There are a number of licensing options. Some are free. As far as understanding the licensing of this dataset, I would start here:
https://www.ordnancesurvey.co.uk/business-and-government/licensing/using-creating-data-with-os-products/index.html
On the Office of National Statistics (ONS) I searched for 'land area'. The resulting list was fairly long. I did find some CSV/Excel datasets that contained land area on at least a county equivalent.
http://www.ons.gov.uk/ons/datasets-and-tables/index.html?pageSize=50&sortBy=none&sortDirection=none&newquery=land+area&content-type=Reference+table&content-type=Dataset
"
How do I get a full list of datasets available on Data.Gov using the CKAN API?,"
For now the Data.gov CKAN API redirects package_list to package_search and for package_search the relevant Solr parameters to limit your query are rows and start.
For example, http://catalog.data.gov/api/3/action/package_search?rows=1000&start=0
And then page through. 
If you or anyone has additional feedback on what Data.gov CKAN API documentation would be helpful or would like to help contribute there is a relevant issue open on GitHub. 
"
Historical weather data,"
For international and historical data, and for a modest number of requests per day, I personally recommend the Wunderground API. Once you register, you can get 500 free requests per day.
The URL for historical data will look like this:
http://api.wunderground.com/api/Your_Key/history_YYYYMMDD/q/CA/San_Francisco.json

I've posted a sample code (python 2.7) that you can use (and improve!) - LINK. I would run this code every day, just changing the year (currently it's set for 2013). The output of the code is a CSV file, but you can store the JSON and/or parse as needed.
"
data request - Fantasy Soccer (English Premier League) datasets,"
I like what you are working on!  Looks like about the same time I was adding a Github project/repo to start sharing R code for analysis we were motivated to try out.  I've not added much code to Github yet but hope to in the near future with data for examples.
Another site to look at - you have more more expertise inputting data of the sites than I do, is the primary EPL site and fantasy game.  My older son and I have been playing that one for many years and like the mix of data made available.  Much more than other sites.  We also manually compile some data on specific areas of interest such as shots (an associated goalie data).  Every good analytics article you read on the web or in print always gives a few more questions.
"
api - Public domain paintings database,"
Check out the Walters Art Museum Collections API:
https://github.com/WaltersArtMuseum/walters-api
Might give you access to what you are looking for...
"
geospatial - Where can I download photoperiod (daylength) data?,"
Turns out there is a function daylength in the geosphere package in R that calculates day length for any latitude and date.
"
What is the Date Format for the Healthcare Finder API?,"
Short answer: ""Year-Month-Day"", or CCYY-MM-DD
Longer answer: The Healthcare Finder API uses XML Schema to define the API schema..
So a line like <xs:element name=""InsuranceEffectiveDate"" type=""xs:date""/> indicates that you should format the value according to XML Schema. This is a reference page for the date type 
(the reference to xsd:date on the linked page instead of xs:date as specified in the API schema document is a minor style choice, and not a cause for concern.)
"
business - Where can I find project risk management data?,"
Some project risk management data can be found within the following resources:

Enterprise Risk Management Initiative's Surveys and Benchmarking Data (free)
TM Forum's Business Benchmarking Database (commercial)
International Software Benchmarking Standards Group (ISBSG) Data Portal or, alternatively, ISBSG Industry Data Sets (commercial; industry focus: software / IT project data)

NOTES: 1) I don't think that Project Management Institute (PMP) has project risk management data, as @Joe suggested. At least, I haven't been able to find it. 2) Obviously, there exists other industry-focused project risk management data, similar to the one referenced above, focused on the software / IT industry.
"
data request - ACS 2013 5 Year METRO areas in KML,"
As some comments to your question note, it's not entirely clear what you're looking for.
If you're looking for the CBSA geometries as KML, your best route would probably be to get the shapefiles from the Census and then convert to KML. (If you don't have tools to convert, googling shapefile to kml will turn up several online options.)
Shapefiles for metropolitan divisions are also available
Are you using ""city"" as synonymous with ""metro area""? Or are you looking for the shapes of cities in given metro areas? The Census Bureau offers a cross reference of principal cities of CBSAs as an Excel file. It would be fairly labor intensive to retrieve the city boundaries for each of those (~1250 cities).
Are you trying to compare data between CBSAs, NECTAs, and Metropolitan divisions? Proceed with caution. Metropolitan divisions are a subset of CBSAs. NECTAs are geographic agglomerations similar to CBSAs, but a NECTA is defined as a list of cities/towns while a CBSA is defined as a list of counties. 
"
data request - Where can I find an ontology for “project”?,"
DOAP - Description of a Project. still alive, now on github
https://github.com/edumbill/doap/wiki
"
data request - Historical Mobile App Rankings/Downloads and Prices,"
I have shared the statistics of my 2 million+ downloads app here:
https://github.com/nicolas-raoul/google-play-statistics
Data available: Installs/uninstalls/ratings/crashes data by country/language/device/version/date/etc.
It is a free app for Android.
"
data.gov - Does anyone know of a U.S. city that has crime data with location?,"
The Open Knowledge Foundation keeps a list of Cities that publish crime datasets. Most datasets have location data. In some cases the location is in State Plane coordinates instead of Lat/Lon. They list 51 cities.
http://us-city.census.okfn.org/dataset/crime-stats
Below is a blog posting I posted a year ago on methods for doing crime analysis:
http://www.opengeocode.org/articles/crime%20analysis.txt
Below is a PPT presentation I've given in the Portland, OR area. Starting at slide 31, we show our method and results for doing location based crime analysis in Portland, OR for correlations with public transit stops and alcohol establishments.
http://www.opengeocode.org/articles/Open%20Data.pptx
"
data request - Database of wheelchair-accessible places,"
NPR has been developing a crowdsourced database of accessible playgrounds in the US. The data is downloadable as CSV or JSON. The license for the data isn't clearly stated, but the language suggests that they want it to be freely used.
"
releasing data - Best place to publish my Android app's Google Play statistics?,"
make a github repo, save your data there:
central location
versioning
social coding!!
you can upload virtually any format, including csv ->
bonus! github displays csvs in the browser!
also manually editing csvs in the browser on github is actually pretty straightforward.  
you could also just upload them to google drive/docs and share/publish the data there...
"
data request - Open downloadable recipe database?,"
hrecipe (and microformats in general) are the bees knees and lucky for you are widely employed across the web; here's a list of sites actively publishing hrecipes in the wild; you can scrape and parse as you please!
http://www.eat-vegan.rocks/
http://funcook.com/
http://www.therecipedepository.com
http://sabores.sapo.pt/
http://www.epicurious.com/
http://www.williams-sonoma.com/
http://foodnetwork.com/
http://www.plantoeat.com/recipe_book
http://www.essen-und-trinken.de
http://itsripe.com/recipes/
this list was lifted from the hrecipes specification on the microformats wiki ->
http://microformats.org/wiki/hrecipe 
EDIT:
Auntie's Recipes Repository 
"
data request - Public database of book titles?,"
Open library has a goal of one page for every book, and has the data you seek.
It's run by the internet archive.
"
What does it take to get an app featured on data.gov?,"
On the Contact page at Data.gov you can select an option in the form to Submit an Application.
Requirements for the apps listed are also posted. Those apps must:

""Use open government data from the United States
Be accessible, vetted, and available
Be, for the majority, free and do not require registration to use""

Many of the apps have come out of businesses using open government data (see examples at the Open Data 500), city and Federal hack-a-thons, or challenges issued by the Federal government.  But, anyone is welcome to submit their app or service to Data.gov.
"
data request - Japan software export statistics,"
I am not sure of how Japan handles exports but the US Dept of Commerce Bureau of Industry and Security (BIS) has a list of 10 Commerce Control List Categories and  these are broken into 5 Product Groups 
It looks like software is a Product Group and this means it would need to be aggregated across categories.
The staff at BIS may be your best bet on connecting with someone that can lead to the data.
If you find out anything, please post back here.
"
europe - Demographic data for Belgium or the Netherlands,"
Note that population for postal and political regions are not often provided in the same public dataset.

An option for Belgium:

2013 population data from the government site - LINK (see right side of page)

Excel file for ""population by commune"" - LINK
unfortunately, uses ""code INS"" and not ""postal code""

can join population data with postal data from BE post office, on commune name - LINK (see Excel links on right side of page)

Excel download, sorted by postal code - LINK



(It's just a bunch of Googling. If it's useful, I can do the same for the Netherlands.)
"
usa - Police precinct jurisdiction data for joining UCR with census,"
After asking, I found via Googling a series from the National Archive of Criminal Justice Data (NACJD) called the Law Enforcement Agency Identifiers Crosswalk.
Here is its description from the NACJD website:

The crosswalk file is designed to provide geographic and other identification information for each record included in either the Federal Bureau of Investigation's Uniform Crime Reporting (UCR) Program files or in the Bureau of Justice Statistics' Census of State and Local Law Enforcement Agencies (CSLLEA). The main variables each record contains are the alpha state code, county name, place name, government agency name, police agency name, government identification number, Federal Information Processing Standards (FIPS) state, county, and place codes, and Originating Agency Identifier (ORI) code. These variables allow a researcher to take agency-level data, combine it with Bureau of the Census and BJS data, and perform place-level and government-level analyses.

And an introductory technical report from 1998, when the first dataset was published: http://www.bjs.gov/content/pub/pdf/lucrdod.pdf
"
data request - Pollution DataSet,"
The WHO provides global data for many cities. It doesn't seem to break down the overall air pollution into molecular components, though.

Website with details
Ambient (outdoor) air pollution database, by country and city (Excel file)

Many individual cities or regions provide air quality data. A short example, although basically every city or region will have some level of data or map available:

Ontario, which can be filtered on only Toronto - http://www.airqualityontario.com/history/
London

You may also find good data from the EPA (US) website:

Overview of data sets
There are several resources that have varying levels of details


Note that air pollution and greenhouse gases are different molecules and at different levels of the atmosphere. Air pollutants are often measured with PM (particular matter) sizes (wikipedia) 

The composition of particulate matter that generally causes visual effects such as smog consists of sulfur dioxide, nitrogen oxides, carbon monoxide, mineral dust, organic matter, and elemental carbon also known as black carbon or soot. 

If you are looking for only greenhouse gases, you may not find data with such accurate geolocation.
"
api - Getting more than 100 records from OpenFDA in MS Excel,"
I believe the limit is now 1000 with an API key. Try getting and adding an API key and changing the limit to 1000
"
licensing - How I can get list of licenses used in Data.gov?,"
The licenses for the data accessible through Data.gov vary, sometimes based on the source (Federal government, local government, or university) or on previous agreements that led to the data collection or publishing.  
In general, the licenses are governed by Project Open Data and will reflect any issue with reuse and redistribution.  Some of the licenses used are standardized, while others may be particular to a specific dataset.
On each dataset's page, there is a ""license"" field in the metadata that should answer your question about the use or reuse of that dataset.  For example, NOAA provides a dataset of earthquakes for the last 4100 years! Look at the dataset page, scroll to the bottom and expand the metadata (click ""Show More""). The license field notes, ""Produced by the NOAA National Geophysical Data Center. Not subject to copyright protection within the United States.""
If there is not license noted, the default is that the data is in the public domain and available to use freely and without restriction (""the state of belonging or being available to the public as a whole, and therefore not subject to copyright"").
"
data request - Historical OFAC SDN lists,"
The Archive of Changes to the SDN and NS-ISA Lists looks complete to me.
There's no XML. At a glance the "".txt"" files seem formulaic. They might not be difficult to transform using a programming language.
If there is more information that you are seeking, it may be necessary to submit a Freedom of Information Act request to the Office of Foreign Asset Control, which is among the Departmental Offices in the Department of the Treasury.
"
data request - Database of shrines in Japan,"
The Association of Shinto Shrines I would expect to have a database of shrine locations - but they do not have it online. Perhaps you can contact them. Here is their link:
http://www.jinjahoncho.or.jp/en/

This is a dataset of ancient Shinto shrines (warning: it is in Japanese):
http://21coe.kokugakuin.ac.jp/db/jinja/index_e.html
For each shrine, the available information is:

Name of the shrine
Commandery (Japanese territorial subdivision system used centuries ago)

"
data request - Android device name->maker mapping,"
This list from Google has devices that support Google Play, and are grouped by manufacturer. The list is updated regularly. Unfortunately, it's a PDF - LINK.

One option to make the data more machine readable is to convert to TXT with pdftotext - raw text
pdftotext -layout devices.pdf

"
Cosmetic Names (INCI nomenclature) and URLs to OpenFDA,"
In which API endpoint are you searching? Drug adverse events? Drug labeling? You should look at the particular API endpoint you're searching to see what fields are available, and do some quick searches for the kinds of products or records you're interested in to see what kinds of data they have and in what fields.
For instance, there are a good number of cosmetics products in the drug product labeling API:
https://api.fda.gov/drug/label | Docs at https://open.fda.gov/drug/label/reference/
These products tend to list ingredients in the active_ingredient and inactive_ingredient fields. In this API, these fields contain ingredients as reported to FDA by the product manufacturer. I don't know if the manufacturers use INCI standard terms when describing ingredients.
Here's a search that may be of relevance:
https://api.fda.gov/drug/label.json?search=inactive_ingredient:(Ammonium+AND+Methacrylate+AND+Copolymer)
In this case, the API returns ~90 product labels where the inactive_ingredient field contains ALL of those words, in any order, in any combination.
I did a quick search and didn't find the word nitrophenol in any records in that endpoint. https://api.fda.gov/drug/label.json?search=inactive_ingredient:(nitrophenol)
With respect to using UNII, you should know that the product labeling endpoint in particular only provides UNII codes for active ingredients, not inactive ingredients.
HTH.
"
Where can I find data about online dating websites,"
I find it unlikely that a dating website would share a dataset, although OKCupid Trends was one of the first good data blogs (and I'm glad they are back posting after being silent since 2011).
There was a Pew research study from 2013 - Online Dating

This data set contains questions about online dating, technology and existing relationships, and non-internet users.

(requires entering some info to download data)

Sample: n=2,252 national adults, age 18 and older, including 1,127 cell phone interviews
Interviewing dates: 04.17.2013 – 05.19.2013

The survey is quite long and the data is available in crosstab, csv, or spss formats. Included are questions and answers about internet usage, where people meet each other, usage of dating websites and apps, demographic information.
"
data request - Are there ID crosswalks between NPI and OSHPD for California Providers?,"
I approached this issue by extracting the business address of the OSHPD and matching it to the NPI dataset. I only needed to do it for a handful of Clinics but I imagine the error rate would be relatively low and you would be able to fix/ignore whatever didn't match up, depending on your purpose of course.
The NPI Core Dataset is a little big for excel (2 GB). I attempted to use Tableau, it could manage with a lot of glitching. I would recommend working with these datasets in SQL to make this connection.
"
data format - Most reusable way to publish a list as JSON,"
Your sample data as well as the provided list of country codes are valid JavaScript, but they are not JSON. To make them valid JSON, put all strings in double quotes:
[ 
  {""name"": ""Afghanistan"", ""code"": ""AF""}, 
  {""name"": ""Albania"", ""code"": ""AL""}
]

This answer on StackOverflow gives a nice overview of the differences between JavaScript and JSON.
Regarding your initial question, your sample data would look like this:
[
    {""product1"": ""price1""},
    {""product2"": ""price2""},
    {""product3"": ""price3""},
    {""product4"": ""price4""}
]

In most cases1 it's probably better to make your prices actual numbers instead of strings that just look like numbers:
[
    {""product1"": 123.45},
    {""product2"": 234.56},
    {""product3"": 345.67},
    {""product4"": 456.78}
]

If you think about adding more properties to your products in the future, you should probably use this instead:
[
    {
        ""name"": ""product1"",
        ""price"": 123.45
    },
    {
        ""name"": ""product2"",
        ""price"": 234.56
    },
    {
        ""name"": ""product3"",
        ""price"": 345.67
    },
    {
        ""name"": ""product4"",
        ""price"": 456.78
    }
]


1 As Walter Tross pointed out in the comments, there are edge cases when rounding errors can occur. For example, if you have a product that costs 456789.99, it might happen that the computer program that reads your JSON will actually calculate the price as 456790.00. There is currently no simple solution for this problem. If you think this might be relevant for you, you might want to read up on some related discussions and the general problem of the precision of floating point numbers.
"
No data dump and low daily-cap API: Can it be called Open Data?,"
To quote from the Open Definition v2.0:

1.2 Access
The work shall be available as a whole and at no more than a reasonable one-time reproduction cost, preferably downloadable via the Internet without charge. […]

So no, what you describe is not Open Data according to the Open Definition.
"
data request - Mapping counties to zip codes,"
You might want to check out the HUD zip code-county crosswalk (screenshot below).

My gut feeling is that this feature would be in the US census TIGER product line
https://www.census.gov/geo/maps-data/data/tiger.html
maybe here:
https://www.census.gov/geo/maps-data/data/relationship.html
Please let us know what you find most useful.
"
data request - Longman English Dictionary Database,"
This particular dictionary's legal notice says that:

Users are not entitled to [...] transmit [..] it on any other website without the express permission of Pearson.
You must not use data mining, robots, scraping or similar data gathering or extraction methods on any part of this Site without our express prior written consent (my emphasis)

So either contact them, or use a more open dictionary.
See this list of open English dictionaries.
"
Where I can get NMEA data dump of GLONASS or GPS simulator?,"
You can get NMEA logs from this website: FreeNMEA
Actually, it does not provide real logs (until you uploaded them), but you can generate ""synthetic"" (random) logs for testing. e.g. you can generate logs with GSV sentences only.
Use this tool without registration: NMEA emitter for generating random logs.
If you need to generate logs for GPS/GLONASS you need to log into website and you'll be able to select Talker IDs for your logs.
"
tool request - What forums / boards do you use to answer open data questions?,"
This one obviously. I also use http://reddit.com/r/opendata, and more recently the Open Knowledge Foundation's forums (http://discuss.okfn.org/) but that's for a project specific to them so I can't vouch for it as being good for general discussions or questions.
"
geospatial - Data on forest cover (land use) in The Gambia from 2010 or more recent,"
Have a look at the global Forest Change cover From Hansen et al. They used Landsat 7 images from 2000 to 2013 to assess forest cover changes. Tiles can be downloaded from their website under creative common license.
"
data request - Free English Dictionary,"
The English Language & Usage stackexchange site has a question with answers related to your question

What's the largest open-source dictionary that includes brief definitions of each word?

Source
The popular answer is WordNet from Princeton. You can either browse or download the full data set, although it's about 10 years old.
The license allows commercial use.

Here is an example of the page for Data:
http://wordnetweb.princeton.edu/perl/webwn?s=data

And the definitions

S: (n) data, information (a collection of facts from which conclusions may be drawn) ""statistical data""
S: (n) datum, data point (an item of factual information derived from measurement or research)

"
openfda - FDA Open Data sample URL for date range,"
You might want to try hurl.it for testing your calls: https://www.hurl.it/  it's really useful.
I looked at the API reference:
https://open.fda.gov/api/reference/
which has directions, but all of the info there is geared towards drugs, not food.
This URL has food recall info:
https://open.fda.gov/food/enforcement/
The examples are pretty good.  Does that page help, or did you have a more specific question?  Just remove the limit from the first example,
https://api.fda.gov/food/enforcement.json?search=report_date:[20040101+TO+20131231]
and that should be your call.  You may not need an API key if you are below the limit.  Otherwise, you would probably add it in:
https://api.fda.gov/food/enforcement.json?api_key=yourAPIKeyHere&search=report_date:[20040101+TO+20131231]
"
africa - Where can I find data on African higher education institutions?,"
The World Bank data is useful, but not comprehensive. You might also want to keep an eye on data posted to openAFRICA.net for examination & graduation results, as well as data about education facilities, resources, and staffing from some of the countries not currently covered by the World Bank datasets.
There seems to be ~218 education related datasets on openAFRICA. Here's a link: http://africaopendata.org/dataset?q=education
"
transportation - Where can I find data about traffic flow for European cities?,"
The UK Department for Transport (DFT) provides a traffic data set for the city of London, for 2000 to 2013.

Main page

London

About

Metadata PDF


There are two main data sets:

AADF - Annual average daily flow


AADF figures give the number of vehicles that will drive on that stretch of road on an average day of the year. For information on how AADFs are calculated, see the guidance on the Traffic Statistics pages on GOV.UK.
AADF figures are presented as: Units = vehicles per day


Traffic - Annual volume of traffic


Traffic figures give the total volume of traffic on the stretch of road for the whole year, and are calculated by multiplying the AADF by the corresponding length of road and by the number of days in the year (i.e. one vehicle travelling one mile each day for a year would equal 365 vehicle miles).
Traffic figures are presented as: Units = thousand vehicle miles

"
metadata - Custom Categories in CKAN,"
The filters on the search page sidebar can be configured implementing the IFacets interface from your own extension (you'll need to write some code for that).
For instance, these ones are defined like this:
https://github.com/okfn/ckanext-tsbsatellites/blob/master/ckanext/tsbsatellites/plugin.py#L236:L249
"
How can I add Wikidata properties of item a on the Wikipedia page of item b?,"
According to v0.4 of the Wikidata inclusion syntax, this feature has not been implemented yet. The progress of this feature is available in the Wikimedia bug tracking system.
"
data request - Graph of Landsat Downloads?,"
Apparently, here:

Source: http://www.usgs.gov/blogs/features/files/2015/01/free-and-open-data-policy-graph-no-footnote.jpg
Here is the original post, noting cost savings from Landsat. http://landsat.gsfc.nasa.gov/?p=9654
"
releasing data - API for datapackages?,"
CKAN apparently, although I haven't done this myself CKAN is a good solution: http://okfnlabs.org/blog/2014/09/11/data-api-for-data-packages-with-dpm-and-ckan.html 
"
data request - Extreme weather dataset for all countries from 2008 to 2011,"
Addressing partly your question:
The NOAA has a Significant Earthquake database that can be downloaded in CSV format. The dataset also indicates if the earthquake event was linked to a tsunami.
"
usa - federal guidelines for reporting expenses and donations,"
The first thing to consider is what data you have and what format is it in. 
Whatever it is, you can first make it available in a bulk download. Providing all the data and updating it on a schedule is a vast improvement that doesn't require a lot of overhead.
Then, take stock of what kind of format the data is in. If you have the data in PDFs, focus on a way to get that information into a data base. If the records are being kept in excel, that is at least structured but you would want to upgrade to a database as well. 
If the information is in a database, then making an API is not too complicated. I would suggest a json API. Making the field names clear and readable can go a long way. Having good documentation is very important- even the best schema is frustrating if there is poor documentation. 
The level of detail that you can give is dependent on the underlying detail in your data. 
"
data request - Household income by ZIP+4,"
First, note that ZIP codes are not geographies (see this other answer).
Also, as Kotebiya notes in a comment, there aren't pseudo-geographies for ZIP+4 and if there were, they would probably represent too small a group of people to allow data sharing without concern for individual privacy.
There may be commercial services that approximate this level of detail, but I am extremely doubtful that any public agency makes it available.
"
programming - Open datasets for data mining R,"
R itself has a datasets package. Check out the R Datasets Package

Description: Base R datasets
Details: This package contains a variety of datasets. For a complete list, use library(help = ""datasets"").

(details)

Update: Thanks to @PatrickHoefler for pointing out the full list of easily imported R datasets.
"
data request - Query medicine descriptions API,"
A few thoughts:

For a simple and down to the point description, Brian's suggestion of MedlinePlus' Web Services is a good start however if you do a search such as http://wsearch.nlm.nih.gov/ws/query?db=healthTopics&term=Levodopa you'll notice that it returns generic information about Parkinkson's disease and not this more appropriate page at http://www.nlm.nih.gov/medlineplus/druginfo/meds/a601068.html. I believe the reason for this is that the drug information at the link above is licensed information and is therefore not shown in the MedlinePlus API, but is shown on the website with the appropriate warnings.
As Brian also mentioned, DailyMed is an authoritative source, also from NLM which gets drug SPL (structured product labels) from the pharma industry and/or FDA; however this might return too much information for you -- see http://dailymed.nlm.nih.gov/dailymed/drugInfo.cfm?setid=6c1f7cd4-de56-45c1-a734-5e77b4aeb6f7, for the top result for ""LEVODOPA AND CARBIDOPA"", for example. You can get this data as XML and extract just the ""INDICATIONS AND USAGE"" information from the drug SPL, though in my experience they don't all have the same sections, which can make things quite complicated for on-the-fly querying.
Similar to bullet #2, similar SPL/""labeling"" information is available from openFDA, a beta initiative by FDA at https://open.fda.gov. Sometimes, querying openFDA will be easier than DailyMed. For example, you can do this (https://api.fda.gov/drug/label.json?search=levodopa+AND+carbidopa) API call on openFDA and then look at the first result's indications_and_usage field which reads as follows:

INDICATIONS & USAGE section Carbidopa and levodopa extended release tablets are indicated in the treatment of the symptoms of idiopathic Parkinson’s disease (paralysis agitans), postencephalitic parkinsonism, and symptomatic parkinsonism which may follow injury to the nervous system by carbon monoxide intoxication and/or manganese intoxication.

Just note that ""openFDA is a beta research project and not for clinical use"" and DailyMed might be able to be trusted more depending on your use case.

Sincerely hope this helps!
"
licensing - Google translate english monolingual definitions,"
If you are using the Google Translate API then you are allowed commercial use. But there is no free quota for the API (terms of service, pricing). If you are a paying customer, you can use the translations for commercial use (with attribution if you use them directly).
I suspect you are not talking about the paid API but instead either scraping yourself or using a package like goslate. These tools are against the Terms of Google so you may find that they aren't very stable (goslate works by faking a user-agent). Building a commercial app based on these data sources is probably not a good idea. See the comments here for more discussion.
(if you want simple, single word translations with a flexible license, check out using Anki decks, for example)
"
data request - Where can I find prices of tickets sold by airlines?,"
The US Bureau of Transportation Statistics provides various stats and datasets for historical airline ticket prices. 

General overview, datasets and statistics
Airfare data

From the Route Fares page:

Route fares

If you go through the PDF report (2012 to Q1 2014), you can find the corresponding raw data (CSV/Excel) for each table.

"
data request - Where can I find a dataset that represents the level of education or/and social class of people and their parents,"
This does not sound like panal data to me. It is a very classic problem in sociology and all that is required is to have parental education or class and the respondent's education or class. A simple survey like the General Social Survey (http://www3.norc.org/GSS+Website/) in the USA or the ALLBUS (http://www.gesis.org/en/allbus/allbus-home/) in Germany is sufficient, and a lot easier to work with. 
"
data request - Advertising Corpus for SVM,"
After long hours of searching, I could not find one. The short answer was to crawl many different websites, separate the data, and use multi-nomial bayes or a radial basis function depending on how awful the results were to generate my own corpus via cluster/grouping with the manually classified data forming a kernel.
"
Forward slashes in OpenFDA queries invariably lead to errors,"
Carbocation couple things that may be of use to you. HEre is an open source site that allows you to searach in various ways. It might help you with what you are looking for - for example http://www.researchae.com/drugevent?from_date=2004-01-01&to_date=2015-01-31&from_age=&to_age=&search=&country=GLOBALLY&patientsex=&manufacturername=&drugbrandname=&druggenericname=&medicinalproduct=&reactionmeddrapt=&drugclass=Hepatitis+C+Virus+Protease+Inhibitor&drugindication=&indsubmit=&productndc=&safetyreportid=
Also the code is open source so the exact API calls are included in the app.rb file which you can see here - https://github.com/GeekNurse/ResearchAE-Open-Source/blob/master/app.rb
"
data request - Datasets of Oil&Gas/electrical industry machinery for fault detection systems,"
I've found an interesting project With tons of data available. It's a real data benchmark executed over an industrial valve. This is the website. 
Industrial Actuator Real Data Benchmark Study.
"
data request - Average gasoline price for all countries,"
Try looking at Numbeo. It is a crowd source project on user contributed real-time data. The cost-of-living section has data on gasoline prices.
http://www.numbeo.com/cost-of-living/
Creative Commons Attribution-Sharealike 3.0 Unported License (CC-BY-SA) and GNU Free Documentation License (GFDL)
"
data request - Wind energy vs. wind speed,"
The wind power production data will likely be proprietary because of energy trading (production companies don't want to freely announce how profitable they are.)
There is an online tool to calculate power production for all sorts of turbine types. You can choose a power curve related to a specific turbine type, or manually set one.

Swiss Wind Power Data Website



With the power calculator you can estimate the power production for a site for different turbine types.
A turbine availabiliy of 100% is assumed (no losses due to down time, icing, transformer losses, park effects etc.).
No guarantees can be given for the obtained results.



Another option would be to find research data for specific turbines, but that isn't real production data.
"
"openfda - I am using open FDA for the first time, How can I get API key?","
You can query the openFDA API without a key, but for more requests, you can freely request one.
The screenshots below come from this Reference Page.

To get a key, you register with your email address on this screen

"
education - Data sets for teaching,"
It's hard to know your exact audience, but there is a great blog post with (by now) more than 100 interesting data sets - 100+ Interesting Data Sets for Statistics
""Data science"" is pretty far-ranging, so you could do many different types of analysis:

IT-related data mining - for example, Firefox browser data (alternative download site)
Image processing, recognition, mining - for example, 10,000 annotated images of cats
Language processing - for example, Google N-grams (downloadable raw data)
Text mining - for example, Enron Email dataset

"
government - successful open data advocacy website,"
Public advocacy is effective when it shows decision makers the examples they'd like. But it's not always clear what they'd like. The best bet is to demonstrate them good examples widely used across other locations.
Here are some of them:
GitHub and Government
The most comprehensive collection of public software projects already implemented in the world.
Many of these projects are data-driven, so you can understand what data adds value.
Data.gov Search
You can sort 130,000+ datasets by popularity and find out what're the most demanded data.
For example, this is the list of popular data related to transportation on the city level.
Data.gov Applications
The list of apps that use public data. Though it's difficult to estimate the impact of each particular app, you may ask your stakeholders what they like.
Open Knowledge Foundation (OKFN)
A major organization closest to your requirements. It has branches in various countries, so you can see what many of them do.

OKFN projects on GitHub. Very specific ones: you can use them as examples.
OKFN Labs. Experimental stuff.

OpenDataHandbook.org
A few good examples from their value stories section:
http://opendatahandbook.org/value-stories/en/
Reports by consulting companies
McKinsey has a very good reputation among top decision makers and its reports get right to the point: how much money the client can make out of the subject. In this case:

McKinsey report on open data
McKinsey report on big data

The World Bank also wrote on open data, but their report is a way too general.
"
data request - Needed: Dataset for Outlier / Anomaly Detection,"
Try looking at the data here !
https://datamarket.com/data/list/?q=
Looots of time series. I've found some nice anomaly sets in there. Specfically, time series.

Are you a student ? If so, you can request Yahoo's outlier dataset !

Have you checked out the datasets at quandl? They aren't made for outlier detection but you can definitely find anomalies if you just spend a little time looking.

You might also appreciate this project: Numenta Anomaly Benchmark (NAB)
This is taken directly from the readme in NAB/tree/master/data.
NAB Data Corpus
Data are ordered, timestamped, single-valued metrics. All data files contain anomalies, unless otherwise noted.
Real data

realAWSCloudwatch/
AWS server metrics as collected by the AmazonCloudwatch service. Example metrics include CPU Utilization, Network Bytes In, and Disk Read Bytes.
realAdExchange/
Online advertisement clicking rates, where the metrics are cost-per-click (CPC) and cost per thousand impressions (CPM). One of the files is normal, without anomalies.
realKnownCause/
This is data for which we know the anomaly causes; no hand labeling.

ambient_temperature_system_failure.csv: The ambient temperature in an office
setting.
cpu_utilization_asg_misconfiguration.csv: From Amazon Web Services (AWS)
monitoring CPU usage – i.e. average CPU usage across a given cluster. When
usage is high, AWS spins up a new machine, and uses fewer machines when usage
is low.
ec2_request_latency_system_failure.csv: CPU usage data from a server in
Amazon's East Coast datacenter. The dataset ends with complete system failure
resulting from a documented failure of AWS API servers. There's an interesting
story behind this data in the Numenta
blog
machine_temperature_system_failure.csv: Temperature sensor data of an
internal component of a large, industrial mahcine. The first anomaly is a
planned shutdown of the machine. The second anomaly is difficult to detect and
directly led to the third anomaly, a catastrophic failure of the machine.
nyc_taxi.csv: Number of NYC taxi passengers, where the five anomalies occur
during the NYC marathon, Thanksgiving, Christmas, New Years day, and a snow
storm. The raw data is from the NYC Taxi and Limousine Commission.
The data file included here consists of aggregating the total number of
taxi passengers into 30 minute buckets.
rogue_agent_key_hold.csv: Timing the key holds for several users of a
computer, where the anomalies represent a change in the user.
rogue_agent_key_updown.csv: Timing the key strokes for several users of a
computer, where the anomalies represent a change in the user.

realRogueAgent/
This data represents computer usage patterns for different users, where an
anomaly may occur with a rogue user of the computer.
realTraffic/
Real time traffic data from the Twin Cities Metro area in Minnesota, collected
by the
Minnesota Department of Transportation.
Included metrics include occupancy, speed, and travel time from specific
sensors.
realTweets/
A collection of Twitter mentions of large publicly-traded companies
such as Google and IBM. The metric value represents the number of mentions
for a given ticker symbol every 5 minutes.

Artificial data

artificialNoAnomaly/
Artifically-generated data without any anomalies.
artificialWithAnomaly/
Artifically-generated data with varying types of anomalies.

"
What are the practical differences between WikiData's infrastructure and SemanticMediawiki?,"
It really depends on what you're trying to achieve.
This is what the Semantic Mediawiki project has to say on the relationship between Semantic MediaWiki and Wikidata:

The software that powers Wikidata is a set of MediaWiki extensions
  collectively known as Wikibase, and though Wikibase has similarities
  to Semantic MediaWiki, it is a distinct set of software. However, some
  of SMW's backend code has been spun off into a separate library,
  called ""DataValues"", that is used by both SMW and Wikibase as a
  framework for storing data.
There is the potential that Wikibase and Semantic MediaWiki will
  compete against one another as software, with some wikis choosing to
  use Wikibase instead of SMW as their data storage system. This seems
  doubtful, however: the Wikibase user interface is geared for a highly
  multilingual, highly general knowledge base like Wikipedia. Wikis with
  a specific focus and only one or a handful of languages would be
  better off with the greater structure and simplicity of Semantic
  MediaWiki.

They also have a dedicated page explaining some of the differences between SMW and Wikidata:

The main use case of Wikidata (a centralised, multi-lingual site that
  serves as a data repository) is different from that of SMW (a
  data-enhanced MediaWiki), and this leads to a number of differences.
Central to a wikidata statement is that its factual claim is supported
  by reference(s) (source of the claim). For example, when SMW makes a
  claim about the population of Berlin it would only be annotated with
  Berlin has a population of 3,5 Mio where Wikidata would make an
  extended statement describing it as Berlin's population being 3,5 Mio
  as of 2011 according to the German statistical office.

"
open definition - Data Classification,"
The state of Washington adopted a standard for data classification that has worked pretty well for us:
 (1) Category 1 - Public Information
Public information is information that can be or currently is released to the public.  It does not need protection from unauthorized disclosure, but does need integrity and availability protection controls.
(2) Category 2 - Sensitive Information
Sensitive information may not be specifically protected from disclosure by law and is for official use only. Sensitive information is generally not released to the public unless specifically requested.
(3) Category 3 - Confidential Information
Confidential information is information that is specifically protected from disclosure by law.  It may include but is not limited to:
    a. Personal information about individuals, regardless of how that information is obtained.
    b. Information concerning employee personnel records.
    c. Information regarding IT infrastructure and security of computer and telecommunications systems.
(4) Category 4 - Confidential Information Requiring Special Handling
Confidential information requiring special handling is information that is specifically protected from disclosure by law and for which:
    a. Especially strict handling requirements are dictated, such as by statutes, regulations, or agreements.
    b. Serious consequences could arise from unauthorized disclosure, such as threats to health and safety, or legal sanctions.

https://ocio.wa.gov//policies/141-securing-information-technology-assets/14110-securing-information-technology-assets 
"
usa - H-1 (Work visa) data by year by state,"
The Department of Homeland Security has a table with breakdown by State, but not by Visa type. Probably you can find similar data sets for previous years.

Webpage
Excel file


Persons Obtaining Lawful Permanent Resident Status by State or Territory of Residence and Region and Country of Birth: Fiscal Year 2013
  (XLS, 175 KB)

Also from DHS is a breakdown by State over year, but without country of origin - Excel, 2008 data.

In general, you can search the data.gov portal for ""immigration"" and more search terms.
http://catalog.data.gov/dataset?q=immigration

Doing so returns my 359 data sets aggregated from multiple government sources. Scan through those results and perhaps you'll find exactly what you are looking for.
"
Healthcare Finder PlanSummaryBenefitURL Error,"
This is an old question, but there was an issue with the service around that time. I can load the URL you provided and get back an SBC.
In Chrome on Android I do get a message asking me to set device security for certificate, but hitting cancel will still download the PDF.
"
data request - Is there a way to extract names and other public information from Facebook?,"
The Facebook Graph-API contains some functionality that you can piece together to do what you asked.
Step 1: Use the User-Friends endpoint to get a list of user-ids. You can use your own user-id as a starting point.
Step 2: Use the User endpoint to get the following information of public profiles:

Provides access to a subset of items that are part of a person's public profile. A person's public profile refers to the following properties on the user object by default:
id, name, first_name, last_name,link,gender,locale,timezone,updated_time,verified

Step 3: Use the user-ids that are in your target region to make additional requests for their friend's lists (step 1), which you can use to get more IDs, more locales, and on and on...
"
data request - API or dataset with listings (property for sales) in an area?,"
A couple of suggestions:
Zillow has an api, which can give you price data from postings and estimates of home values in different areas.
3taps api allows you to search craigslist data, including real estate listings.
Hope that helps!
"
Need a judicial data set,"
Australian laws and cases (all state and federal jurisdictions) are online at: austlii.edu.au.
See for instance High Court (the highest) 2014 cases at:
http://www.austlii.edu.au/au/cases/cth/HCA/2014/
Federal (we call them Commonwealth; Cth) laws starting with letter 'M': 
http://www.austlii.edu.au/au/legis/cth/consol_act/toc-M.html
etc etc etc... 
"
data request - Civilian Labor Force Demographics,"
This might suit your needs.

SEX BY AGE BY EMPLOYMENT STATUS FOR THE POPULATION 16 YEARS AND OVER
  Universe: Population 16 years and over  more information
  2009-2013 American Community Survey 5-Year Estimates 

Just generate the proportions on your own.
"
"demographics - Looking for longitudinal population data by age and sex, any country","
You could try a few of the following resources that I know of:
The U.S. Census Bureau's International Data Base
The U.N. Population Division also has a good resource, though it is available only in XLS format.
"
data request - Where can I find the music lyrics dataset?,"
Song lyrics are going to be mostly copyrighted, so distributing them as a dataset would probably not be legally possible.
To collect lyrics you can scrape one of the many lyrics websites. Here are some steps for how to do so with azlyrics.com

Find a page that lists all artists: 

http://www.azlyrics.com/a.html
Notice that artists with a number are listed under http://www.azlyrics.com/19.html

Find URL to one artist:

http://www.azlyrics.com/a/amywinehouse.html

Find URL to one song:

http://www.azlyrics.com/lyrics/amywinehouse/fuckmepumps.html


Of course, you can't do this manually. But with some simple code, it's not hard to find URLs on a page, then go to those pages and find new URLs. Because you know the structure of the website, specifically where lists of artists are stored in a page called a.html, it's straightforward.

Here's a more technical description. Each artist page has a section of HTML like this:
<div class=""artists fr"">

<a href=""a/anathema.html"">ANATHEMA</a><br />
<a href=""a/anberlin.html"">ANBERLIN</a><br />
<a href=""b/bonnieanderson.html"">ANDERSON, BONNIE</a><br />
<a href=""a/andersoneast.html"">ANDERSON EAST</a><br />
<a href=""j/johnanderson.html"">ANDERSON, JOHN</a><br />
<a href=""k/keithanderson.html"">ANDERSON, KEITH</a><br />
<a href=""a/andreabocelli.html"">ANDREA BOCELLI</a><br />
<a href=""a/andreacorr.html"">ANDREA CORR</a><br />
<a href=""a/andreasbourani.html"">ANDREAS BOURANI</a><br />
...
</div>

Each song has HTML structure like this:
<b>""Fuck Me Pumps""</b><br />
<div style=""margin-left:10px;margin-right:10px;"">
<!-- start of lyrics -->
When you walk in the bar,<br />
And you dressed like a star,<br />
Rockin' your F me pumps.<br />

That <!-- start of lyrics --> will be a great help to scraping!
Here is an outline of a python code to get your started:
import requests
# step 1 - get artist pages
for page in ['abcdefghijklmnopqrstuvwxyz']: # also add '19'
    url = 'http://www.azlyrics.com/' + page + '.html' # construct url of artists page
    html = requests.get(url) # download a.html
    # parse HTML for list of URLs to artists

# step 2 - make unique list of URLs to artists

# step 3 - loop over artist URLs to get list of URLs to songs
# go to song HTML page and parse into text.

For parsing HTML, BeautifulSoup is commonly used.
Here's a python scraping tutorial.
"
Subquery in wikidata,"
Ok, I found out how to do this. You can use curly braces to further filter on your qualifiers. So the query to get all people that are Bundesminister and where the Bundesminister has a start but no end time is:
CLAIM[39:248352]{CLAIM[580] AND NOCLAIM[582]}

demo link
I found out by reading: http://magnusmanske.de/wordpress/?p=178
And specifically by the example query for the winner of the Royal Medal in 1853.
"
"data request - I need a list of medications, diagnoses, and medical terms for a police report auto publication project","
The NLM's UMLS Metathesaurus http://www.nlm.nih.gov/research/umls/knowledge_sources/metathesaurus/ is a possibility but, there is some non-trivial technical overhead to using it.  You can, however, get it at no cost.  You might check out FDA's NDC database file.  Perhaps that will help you.  http://www.fda.gov/Drugs/InformationOnDrugs/ucm142438.htm  You can also find a small list of drug ingredient names in the documentation for prescription medications collected by the National Health and Nutrition Examination Survey.  As eluded to above, your request is quite broad so it is difficult to give you more concrete options.
"
openfda - Is there any better way to get sum number of reactions,"
Don't use limit, use date boundaries instead:
https://api.fda.gov/drug/event?&search=penicilin+AND+[2004-12-31+TO+2005-01-01]&count=patient.reaction.reactionmeddrapt.exact
https://open.fda.gov/api/reference/#dates-and-ranges
"
openfda - Any public accessible web service to get MedDRA description?,"
Part of the difficulty is that Meddra is a licensed product.
http://www.meddra.org/subscription/subscription-rate
I am not aware of any place that offers the data online, since it is probably against the license agreement. At the top of the list is a minimal charge for non-commercial use, which might be a good option if you are just doing research.
"
usa - Are there good examples of open read-write APIs in Federal government?,"
The Regulations.gov API answer is a good example! It's also an example of government dogfooding its own APIs. 
As I am sure you are aware, the We the People petition site has a write API as well: http://www.whitehouse.gov/blog/2014/10/23/new-we-people-write-api-and-what-it-means-you
For more information on Federal Government APIs, see this Hub of Federal API information & check out the APIs listed on Data.gov.
"
usa - Is the web performance of U.S. government websites open data?,"
The US Government's Digital Analytics Program (DAP) has been working on unified Web analytics for federal agencies for a couple of years
Recently they launched a nice web site that displays the top data:
https://analytics.usa.gov/ 
Philadelphia's
http://analytics.phila.gov/
"
data request - Any available Travel Dataset,"
The University of Illinois has an archived dataset of tripadviser reviews for up thru 2008.
http://times.cs.uiuc.edu/~wang296/Data/
TripAdvisor also has an API. I've never used it so I don't know the terms of use:
http://api-portal.anypoint.mulesoft.com/tripadvisor/api/tripadvisor-api
The UK government publishes annual travel surverys, starting from 2007. This data is at the region level, how many visitors, how many miles traveled, etc.
http://data.gov.uk/dataset/national_travel_survey
"
data request - how to get semantic ontology of words related to technology,"
The best place to start looking is Linked Open Vocabularies. As of February 2015 they have indexed more than 450 vocabularies and ontologies and provide a convenient search feature.
"
Visitors to Data.gov by country of origin?,"
Thanks for filing the GitHub issue too Mitchell. 
For background on this:

Visitor stats by country and top states were retired with the new
  data.gov theme launch about an year ago, earlier the process used to
  be manual where we were taking the data from google analytics and
  feeding the data to a visualization.
Ideally we would need to integrate with google analytics api directly
  and build on these visualization / metrics.

We're working on a sustainable way to display this information. For others on the Stack Exchange you can follow along here: https://github.com/GSA/data.gov/issues/570
"
data.gov - Searching for obesity data at the ZIP or County level by age for US 2011-1013?,"
The BRFSS identifies respondents in most U.S. counties. However, the coverage is not exhaustive. But there exists small area estimations that usually use BRFSS data as the reference.
"
data request - Public Website statistics,"
u.s. city open data census tracks analytics offered by cities
http://us-city.census.okfn.org/ 
EDIT:
the epa lists analytics of some sort, and has over a decades worth here:
http://www.epa.gov/reports/objects/emfjulte/ 
EDIT 2:
Pennsylvania Spatial Data Access (PASDA) is PA's gis clearinghouse and offers great stats datasets covering almost a decade.  here's an example:
http://www.pasda.psu.edu/about/publications/PASDAStatistics_20140101-20141231.pdf
you can snag them here:
http://www.pasda.psu.edu/about/publications.asp 
EDIT 3:
usgs landsat recently published some data regarding their data use; the post is more about the economic benefits of open data, however there's a really cool chart about data usage stats over time. hat tip to @dan for answering his question here:
Graph of Landsat Downloads?
you can read the report and view the chart here:
http://landsat.gsfc.nasa.gov/?p=9654 
EDIT 4:
A.C.I.S. SEARCH TOOL - Animal Care Information System Search Tool shares some of its visitors statistics, which were much higher than i thought they would be, based off the tools design and functionality. but there's roughly 14000 days of data here:
via:
https://acissearch.aphis.usda.gov/LPASearch/faces/CustomerSearch.jspx;jsessionid=7f00000130d77a076f42228f4753ad1492029f4879f5.e38Obx8Sb3yQby0Qa3mQe0 
EDIT 5:
analytics.usa.gov is a dashboard for analytics across dotgov sites:
https://analytics.usa.gov/ 
Philadelphia's site:
http://analytics.phila.gov/
"
data request - People of public interest living in Germany,"
There is a category called ""Deutscher"" (German) in the German Wikipedia that is accessible with the help of the German DBpedia and contains more than 191000 pages. You could query DBpedia for pages that have a link to this category with the wikiPageWikiLink relation and filter for living people by looking up if their DBpedia page contains a death date or not, using deathDate.  
"
programming - Scraping columns from a PDF,"
The pdftotext library (man page) that comes standard with most linux distros and can be installed on Windows contains a -layout flag that preserves table structure.
pdftotext -layout input.pdf output.txt

After that, you can easily parse with any language into your desired JSON structure.
There is a python wrapper for pdftotext, but as far as I know, it only works on linux. For my application on Windows, I used a system call to pdftotext.
(There are also some python libraries for reading PDFs, but I found that pdftotext with the -layout option works best for multi-page PDF files with tables.)
"
data request - Aerial crop image datasets,"
I have a few set of aerial images taken by an X8+ Drone with a redEdge camera, maybe they will help you. Contact me. 
"
data request - Where can I find accurate US county boundaries?,"
You might find these cartographic boundary shapefiles for U.S. Counties useful: http://www.census.gov/geo/maps-data/data/cbf/cbf_counties.html
"
programming - Where can software developers interested in open government data be reached?,"
i'll tell you where i lurk:
google groups:
open gov jobs
https://groups.google.com/forum/#!forum/opengovjobs
civic data alliance
https://groups.google.com/forum/#!forum/civicdataalliance
uk gov data developers
https://groups.google.com/forum/#!forum/uk-government-data-developers
linked data api discuss
https://groups.google.com/forum/#!forum/linked-data-api-discuss
code for northern virginia
https://groups.google.com/forum/#!forum/nova-brigade
geo dc
https://groups.google.com/forum/#!forum/geonerds-dc
code for dc
https://groups.google.com/forum/#!forum/dc-cfa-brigade
code for fort lauderdale
https://groups.google.com/forum/#!forum/code-for-ftl
school of open
https://groups.google.com/forum/#!forum/school-of-open
okfn task force
https://groups.google.com/forum/#!forum/okfn-task-force
open data engagement
https://groups.google.com/forum/#!forum/open-data-engagement
global open data initiative
https://groups.google.com/forum/#!forum/global-open-data-initiative
open civic data
https://groups.google.com/forum/#!forum/open-civic-data
us open gov
https://groups.google.com/forum/#!forum/us-open-government
foia machine
https://groups.google.com/forum/#!forum/foia-machine
open data day
https://groups.google.com/forum/#!forum/open-data-day 
your facebook group  
reddit:
http://www.reddit.com/r/opendata
http://www.reddit.com/r/opendata_pt
http://www.reddit.com/r/opengov
there's more on reddit if you feel like digging  
code for america lets authors post on their tumblr, you could submit a post there  
medium seems to be the cool kid blog of the day, you could double down and post it there as well.  
code for philly has a linkedin group  
every avenue sunlight foundation and okfn offer you: there's a slew of groups and mailing lists  
code for hampton roads and code for atlanta have mailing lists, i'm betting most brigades do...  
not to mention github.....man there's a ton of groups there. 
i know of a few flickr groups but they're most gov-related and you have to be in gov to join...so i couldn't join.
EDIT:
i also keep all of these twitter lists:
https://twitter.com/jalbertbowdenii/lists/civhax
https://twitter.com/jalbertbowdenii/lists/openglam
https://twitter.com/jalbertbowdenii/lists/dotgov
https://twitter.com/jalbertbowdenii/lists/open-acc-sci-edu-glam-kno
https://twitter.com/jalbertbowdenii/lists/open-data-journo-crypto
https://twitter.com/jalbertbowdenii/lists/open-science-stem
https://twitter.com/jalbertbowdenii/lists/data 
nicar data journalists mailing list:
http://www.ire.org/resource-center/listservs/subscribe-nicar-l/ 
knight-mozilla open news:
http://opennews.org/ 
"
geospatial - Ordnance Survey BoundaryLine Data pre 2014,"
MySociety.Org has been caching Ordinance Survey Boundary Line data since 2010:
This is the mySociety cache of OS OpenData, first released 1st April 2010, and other related similarly-licensed data, as allowed under the licences (please read!). Thanks to Ordnance Survey, data.gov.uk, ONS, and everyone else involved in releasing this data! :) 
http://parlvid.mysociety.org/os/
"
geospatial - Data for water features by state/province,"
There are variety of sources both for the US and Worldwide. Here are a few that I am familiar with:
http://nhd.usgs.gov/ (vector data)
The National Hydrography Dataset (NHD) and Watershed Boundary Dataset (WBD) are used to portray surface water on The National Map. The NHD represents the drainage network with features such as rivers, streams, canals, lakes, ponds, coastline, dams, and streamgages. The WBD represents drainage basins as enclosed areas in eight different size categories. Both datasets represent the real world at a nominal scale of 1:24,000-scale, which means that one inch of The National Map data equals 2,000 feet on the ground. To maintain mapping clarity not all water features are represented and those that are use a moderate level of detail. 
The USGS geographic name dataset contains the names of geographic features in the United States. This includes both natural and manmade water features (e.g., canal, dam, beach, channel, lake)
The GNIS contains information about physical and cultural geographic features of all types in the United States, associated areas, and Antarctica, current and historical, but not including roads and highways. The database holds the Federally recognized name of each feature and defines the feature location by state, county, USGS topographic map, and geographic coordinates. Other attributes include names or spellings other than the official name, feature designations, feature classification, historical and descriptive information, and for some categories the geometric boundaries.
http://geonames.usgs.gov/domestic/
The National Geospatial Intelligence Agency (NGA) provides the same data for the countries of the world.
The database is the official repository of foreign place-name decisions approved by the BGN. Geographic Area of Coverage: Worldwide excluding the United States and Antarctica. For names in the U.S. and Antarctica, please visit the United States Geological Survey (USGS) Geographic Names Information System (GNIS) web site.
http://geonames.nga.mil/gns/html/index.html
At my website, I have both the USGS and NGA datasets converted to Linked CSV format:
http://www.opengeocode.org/cude1.2/NGA/GNS/index.php
http://www.opengeocode.org/cude1.2/USGS/GNIS/index.php
"
data request - how to get drug database for free?,"
It's hard to say without some more specific information, but RxNorm is probably the best place to start:
(2nd section is download/API access)
http://www.nlm.nih.gov/research/umls/rxnorm/
"
usa - Looking for a source of data for these specific United States Demographics for People and Businesses,"
You can get census data on population by single year on a county basis here:
https://www.census.gov/popest/data/datasets.html
I can't recall seeing it published down at the tract/block level (more of what the asker is asking) broken down by single year. You can find it in the 10 year increments as was noted in the question.
"
data request - Obtaining personal mail corpus,"
Here is a dataset of emails of a governor in Florida. It contains all email communications this governor made from 1999 to 2007. It contains about 280K emails. It was made public for political reasons I believe.
"
geospatial - Download of big gis data,"
Have a look at Geolife Trajectories dataset from MS Research. http://research.microsoft.com/en-us/downloads/b16d359d-d164-469e-9fd4-daa38f2b2e13/
Its open and free as well.
"
data request - How can I get aggregate dollar values out of Zillow?,"
You need to check the zillow terms of use to make sure your use is appropriate: 
http://www.zillow.com/wikipages/Privacy-and-Terms-of-Use/
This is not an open data set at this time.  However the API may satisfy your needs:
http://www.zillow.com/howto/api/APIOverview.htm
"
usa - School Quality Data at the County Level,"
The Department of Education maintains rating data on schools in a large number of categories. You can build your own tables here:
http://eddataexpress.ed.gov/state-tables-main.cfm
The US Dept. of Ed also has related search tools here:
http://nces.ed.gov/datatools/
The link below will allow you to very a variety of aggregated data at state, county, school district and school level. Note, only a minimal amount of data is aggregated at county level.
http://nces.ed.gov/ccd/elsi/expressTables.aspx
For those that want to fully customize their results (datasets), use this tool:
http://nces.ed.gov/ccd/elsi/tableGenerator.aspx
"
data request - Dataset for temporal community detection,"
My thought would be to look at the bioinformatics world, specifically looking at metabolic networks in developmental biology or something like that.
http://www.devbio.biology.gatech.edu/?page_id=11223
Things like phage lambda are the simplest networks (see book, ""A Genetic Switch""), then you can go to bacteria (E. coli), eukaryotes (yeast, S. cerivisiae), slime molds (dicty), then multicellular. There is a lot of network information out there but it may have some overhead.
This question may be very similar:
Examples or datasets of evolving networks
"
food - Where can I find data on eating behaviors/habits?,"
The UK has done a family food survey for a number of years. It breaks down food categories and percentage of consumption. Has national, region, urban, rural, by age, by income, etc.
https://www.gov.uk/government/statistical-data-sets/family-food-datasets
I've not review this USDA dataset, but the description seems to be what you are looking for:
USDA's National Household Food Acquisition and Purchase Survey (FoodAPS) is the first nationally representative survey of American households to collect unique and comprehensive data about household food purchases and acquisitions
http://www.ers.usda.gov/data-products/foodaps-national-household-food-acquisition-and-purchase-survey.aspx
"
geospatial - LiDAR data for Palawan (Philippines),"
The Philippine government with the aid of the UN has been doing LiDAR mapping of the major river basins. Here's a press release:
It appears the effort is part of the Philippines DREAM project (for disaster risk/assessment):
https://www.dream.upd.edu.ph/
https://www.facebook.com/DREAMLIDAR
The University of the Philippines Los Banos has a page for LiDAR mapping of the Palawan province.
http://phil-lidar.uplb.edu.ph/index.php/study-sites/palawan
fyi> I see you posted the same question on GIS stackexchange.
"
geospatial - Installing GeoJSON preview plugin on CKAN,"
Just to be clear, you still need to do the steps on the ""Install the extension"" section to enable the plugin.
As an aside, we will move the geospatial previews to a separate CKAN extension to avoid this issue of having to install the ckanext-spatial requirements. I'll add a note here when it's done, or keep an eye on the ckan-dev mailing list.
"
data request - Seeking global list of registered businesses,"
Probably the closest thing you'll find to this is OpenCorporates. They claim to have data on over 60,000,000 companies.
They don't have a download, but there is an API. It has a mixed usage license depending on whether you are contributing open data back to the community.
"
Historical flight track and aircraft type data,"
Give this dataset a shot.  It's maybe a 7/10 for your needs?
http://stat-computing.org/dataexpo/2009/
""The data consists of flight arrival and departure details for all commercial flights within the USA, from October 1987 to April 2008. This is a large dataset: there are nearly 120 million records in total, and takes up 1.6 gigabytes of space compressed and 12 gigabytes when uncompressed. To make sure that you're not overwhelmed by the size of the data, we've provide two brief introductions to some useful tools: linux command line tools and sqlite, a simple sql database.""
You might find something here to join that dataset with to make what you're looking for:
http://www.rita.dot.gov/bts/data_and_statistics/index.html
and this might not be covered above:
http://openflights.org/data.html
"
analysis - 10 million user name and passwords. How do I analyze?,"
What are your goals? You may need to upgrade your toolset.  Bash command line tools like cut, sort, uniq are very useful for this kind of thing.
https://stackoverflow.com/questions/6712437/find-duplicate-lines-in-a-file-and-count-how-many-time-each-line-was-duplicated
If you want to analyze digit frequency, transitions between letters (n-grams), common variations in passwords, then a toolkit like R's natural language processing libraries will get you far:
http://cran.r-project.org/web/views/NaturalLanguageProcessing.html
If you change the dataset around and look at variations like LLLNL (letter-letter-number-letter), you could probably gain great insight into the variation inherent in commonly used passwords.
"
data request - Is a spoken digit dataset available?,"
https://github.com/Jakobovski/free-spoken-digit-dataset is a free spoken digit dataset (FSDD).
As an added bonus it comes with a few useful python utility functions.
I created this dataset because I had the same problem. Please contribute to increase the dataset's size.
"
data.gov - American Indian reservation boundaries,"
My guess is that the census tiger products are going to have what you're looking for.
http://www2.census.gov/geo/pdfs/maps-data/data/tiger/tgrshp2016/TGRSHP2016_TechDoc_Ch2.pdf
""The legal entities included in these shapefiles are:

American Indian off-reservation trust lands
American Indian reservations (both federally and state-recognized)
American Indian tribal subdivisions (within legal American Indian areas) 
...""

Main tiger site:
https://www.census.gov/geo/maps-data/data/tiger-line.html
"
data request - Google search by file size,"
I suspect this is not supported. There's no evidence of it in the ""advanced search"", nor in the guide to search operators (which advises against worrying about memorizing all of the operators because you can use the advanced search page.)
Also, the advanced search help page lists filters and doesn't include file size.
"
data request - Largest US City Budgets,"
I've not seen that data compiled yet into a single dataset. The Open Knowledge Foundation has a list of cities which have their budget data online:
http://us-city.census.okfn.org/dataset/budget
"
data request - Where can I find a geographical database?,"
The US National Geospatial Intelligence Agency (NGA) has what you are looking for all countries in the world. The datasets (one per country) have country, administrative level divisions (e.g., states/counties), incorporated and unincorporated places, both in native language/script and in English.
https://www.nga.mil/ProductsServices/GeographicNames/Pages/default.aspx
I also have converted all the country datasets into Linked CSV format:
http://www.opengeocode.org/cude1.2/NGA/GNS/index.php
The United Nations Multilingual Terminology Database has administrative divisions/city names in the 6 UN languages. I don't think they have a direct dump, so you may need to scrap it:
http://unterm.un.org/dgaacs/unterm.nsf/
The US/NGA also took over maintaining the GEC (Geopolitical Codes):
http://earth-info.nga.mil/gns/html/gazetteers2.html
Here's my archive of the updates in the original FIPS/XLS format:
http://www.opengeocode.org/cude1.2/NGA/GEC/index.php
http://www.opengeocode.com/
Per Deer Hunter's comments, here are links to the US Census Gazetteer files:
https://www.census.gov/geo/maps-data/data/gazetteer.html
Here's my version converted to linked CSV:
http://www.opengeocode.org/cude1.2/US%20Census/index.php
There is also the USGS Geographic Name Information System (GNIS) database for domestic and Antarctic names:
http://geonames.usgs.gov/domestic/download_data.htm
Here's my version converted to linked CSV:
http://www.opengeocode.org/cude1.2/USGS/GNIS/index.php
The Canadian (2011) census would also have geographic name information. Here's my version converted to linked CSV:
http://www.opengeocode.org/cude1.2/Canada/Census/index.php
"
data request - Police Misconduct dataset?,"
Unfortunately, from PoliceMisconduct.net's own FAQ:

Why do this?
Simply because nobody else does. Only a small fraction of the 17,000 law enforcement agencies actually track their own misconduct in a semi-public manner, and even when they do, the data they provide is generic and does not specify what misconduct occurred, who did it, and what the end result was.

This FiveThirtyEight.com article by Reuben Fischer-Baum goes into more detail about the scarcity of this data:

As is the case with police shooting statistics, comprehensive numbers on accusations of police misconduct are hard to come by. There is no national reporting requirement for such accusations; in fact, many places have laws to purposefully keep the details of misconduct investigations out of the public eye.

For the article, Fischer-Baum settles for the NPMRP/PoliceMisconduct.net data.
"
Will the Sunlight's TransparencyData state campaign finance contributions every be updated again?,"
With Labs no longer in existence, the answer is no, Influence Explorer will never be updated.
However, the ProPublica's FEC Itemizer comes highly recommended as the best source for most of the data that lived in Influence Explorer. 
"
data request - car parking Dataset in US,"
If you're not looking for real time data, you might want to search on the term 'parking study', which tends to look at the availability of parking in various areas.  
I would assume that cities that have gone to dynamic pricing aka 'active parking management' would need to be collecting the data to set their pricing ... so Seattle and San Francisco would be prime candidates ... unless you're looking for more typical parking data.
"
data request - Free reusable aerial photography of the whole world,"
Sounds like a case forOpenAerialMap, which after a hiatus, looks like it might get a reboot. Wish I could think of something that was active. They already have a functional beta:
OpenAerialMap
The OSM wiki also has some notes on aerial imagery.
"
openfda - Big Data Neophyte Trying To Work With FDA Data -- Any kind souls out there willing to help?,"
I would suggest filing a FOIA request with FDA for the complete dataset.
Barring that, web scraping is a common strategy for extracting data from web sites. It might be a stretch for a ""neophyte,"" but it's something you'll find a ton of guidance on from a Google search. (Perhaps too much, but I think getting into the details of scraping are out-of-scope for this site, and definitely for this question.)  
Note that a simple Google search for how to scrape web pages includes a number of hits which talk about how to do it without writing your own code.
"
asia - Demographic data for asian countries,"
if you want something comparable to the united states decennial census, then you want
https://international.ipums.org/international/
"
data request - How do I find out if a given movie is available in Germany?,"
The IMDB data dump contains theater release dates (warning, 46MB file):
ftp://ftp.fu-berlin.de/pub/misc/movies/database/release-dates.list.gz

An example:

Unfortunately this is only cinema releases, and won't include TV showings or DVD releases. To get TV releases, the data source will probably be TV-specific.
"
"economics - Open data set for fair trade commodities in developing countries (any: bananas, sugar, wine/grapes)","
You can find commodity trade information (by category) between countries in the UN ComTrade database
http://comtrade.un.org/
The United Nations Commodity Trade Statistics Database (UN Comtrade) stores more than 1 billion trade data records from 1962. Over 140 reporter countries provide the United Nations Statistics Division with their annual international trade statistics detailed by commodities and partner countries. These data are subsequently transformed into the United Nations Statistics Division standard format with consistent coding and valuation using the UN/OECD CoprA internal processing system.
The WTO also has datasets on trade between countries:
http://stat.wto.org/StatisticalProgram/WSDBStatProgramTechNotes.aspx?Language=E
The Food Agriculture Organization (FAO) of United Nations' statistical database (FAOSTAT) contains price data on wheat, tea, cereals, rice and sugar per country going back to 2002.
UPDATE: I answered a related question awhile back ago on datasets about commodities.
Where can I get historic prices for a commodity?
"
data request - where in the uk may i find a register or map of properties and their energy use,"
This is not a direct answer, but you could try to extrapolate the data using openstreetmap. As far as I know, the data contains also information about the usage of the building (housing, industrial). Based on the size and the usage you could estimate the energy use of the building. How it is done for the heat demand is described e.g. in this IEEE paper.
"
api - Any uses of Hydra Core Vocabulary?,"
If you mean Hydra-based Web APIs that closely resemble current JSON-based APIs, than I guess the answer is that no ""significant site or project"" has published anything production ready yet. Quite of a few people and a handful of startups mentioned that they are working on it. If, on the other hand, you include Triple Pattern Fragment servers, which are based on the Hydra Core vocabulary, then there are actually quite a few significant projects that serve their data using Hydra. There's even a post claiming there are more than 600,000 of them :-) You can also find a short list of prominent projects on http://linkeddatafragments.org/data/.
Disclaimer: I'm the chair of the Hydra W3C Community Group and one of the core designers of JSON-LD.
"
data request - Mapping Business Hours,"
This CSV dataset contains opening-closing hours for thousands of US hotels/restaurants/bars/museums/etc:
https://github.com/baturin/wikivoyage-listings/
Business type, name and location are included as well.
You will need to filter out:

Businesses that have no documented opening-closing hours
Businesses that are not in the USA (there are columns for city name and latitude/longitude, you might use that)

Data license: Creative Commons Attribution-ShareAlike 3.0
"
"data.gov - Trying to find definitions for ""Location"" in Farmers Market datasets","
Below is an excerpt from Establishing Land Use Protections for Farmers' Markets, published by Public Health Law and Policy and funded by the California Department of Public Health. It is an old publication. It doesn't give definitions, but it does provide some examples relevant to your question.

Identify potential farmers’ market sites on public property, including parks, schools, colleges and universities, and other institutions; on private property, including hospitals and commercial centers; and, where feasible, on streets using street closures.

I believe ""other institutions"" would include the grounds of government buildings.  You might want to take a look at your state's legislative resources page for more information.
"
research - Ebbinghaus 1885 memory experiment data from credible source,"
Given the amount of time, the original data are probably no longer extant. But there is some hope, because the inheritance of Hermann Ebbinghaus was donated to the Adolf-Würth Zentrum in Würzburg, see
http://www.awz.uni-wuerzburg.de/en/news/news/single/artikel/schenkungs/
Note that you need some skills to work with this: German language, reading Old German Handwriting, maybe more ... if the desired data are part of the inheritance at all.
"
data request - Daily electricity usage dataset,"
The European countries in the Entsoe-E provide hourly data aggregated on the country level.
Please see this answer for more details.

European Network of Transmission System Operators for Electricity (ENTSOE) provides Consumption and Production data for individual countries (and Exchange data between countries). (Link to Data Portal.)
Link to ENTSOE Consumption data


"
data request - SQL queries dataset,"
Not exactly SQL but SPARQL:
NL-SPARQL: A Dialog-System Challenge Set for Converting Natural Language to Structured Queries

NL-SPARQL is a data set of natural language (NL) utterances to a conversational system in the movies domain and corresponding queries to Freebase in SPARQL. This dataset was collected via Crowdsourcing as described below.
The data set is split into two sets: training (3,338 examples) and test (1,084 examples) set NL utterance and SPARQL query pairs.

Two examples:

NL utterance:
How many movies has Alfred Hitchcock directed?
SPARQL Query:
 SELECT (COUNT(?movie) AS ?count) WHERE {?movie
http://rdf.freebase.com/ns/type.object.type
http://rdf.freebase.com/ns/film.film. ?movie
http://rdf.freebase.com/ns/film.film.directed_by ?director.
?director http://rdf.freebase.com/ns/type.object.name ""Alfred
Hitchcock""@en.}
NL utterance:
Show me movies in Chinese.
SPARQL Query:
SELECT ?movie WHERE { ?movie
http://rdf.freebase.com/ns/type.object.type
http://rdf.freebase.com/ns/film.film. ?movie
http://rdf.freebase.com/ns/film.film.language ?lang. ?lang
http://rdf.freebase.com/ns/type.object.name ""Chinese language""@en.}

"
data request - Dataset of entry/exit times of vehicles in/out of a parking lot,"
If the lot of interest is a paid lot, then you might be able to get this information from the owner.
Information from a government run lot might be possible with a Fredom of Information Act, FOIA, request. 
"
Posting about data sets - Open Data Meta Stack Exchange,"
Go ahead and post a ""question"", and then answer it yourself. For example, ""Where can I find outbreak data... related to ... ?""  
Here is a recent question that was self-answered: Graph of Landsat Downloads
The meta-stackexchange site has an interesting discussion.
"
data request - swimming velocity for whales or fishes,"
There are a number of research papers/study on this subject.
This 1991 paper covers a lot of historical as well as data current at the time.
Fish swimming stride by stride: speed limits and endurance, Videller, 1991
http://www.researchgate.net/profile/John_Videler/publication/226033193_Fish_swimming_stride_by_stride_speed_limits_and_endurance/links/0fcfd506672ef8c64f000000.pdf
Here's a 2012 paper on sharks in Greenland
The slowest fish: Swim speed and tail-beat frequency of Greenland sharks, YY Watanabe, 2012
http://www.researchgate.net/profile/Christian_Lydersen/publication/258980090_The_slowest_fish_Swim_speed_and_tail-beat_frequency_of_Greenland_sharks/links/0deec52988c6c8cd91000000.pdf
Here's another paper with data, circa 1990
http://pubs.iclarm.net/Naga/FB_1363.pdf
And here's another paper source:
Morphological predictors of swimming speed: a case study of pre-settlement juvenile coral reef fishes, R Fisher, ‎2007 
http://jeb.biologists.org/content/210/14/2436.long
"
data request - open database photos of human faces with age,"
You'll have to go through the list yourself to find the dataset that includes age of the face, or other demographics.


There is a long list of face databases at the Face Recognition Home Page. Searching for "" age "" on the page.


When benchmarking an algorithm it is recommendable to use a standard test data set for researchers to be able to directly compare the results. While there are many databases in use currently, the choice of an appropriate database to be used should be made based on the task given (aging, expressions, lighting etc). Another way is to choose the data set specific to the property to be tested (e.g. how algorithm behaves when given images with lighting changes or images with different facial expressions). If, on the other hand, an algorithm needs to be trained with more images per class (like LDA), Yale face database is probably more appropriate than FERET.



Results of searching ""facial recognition"" on datahub.io - LINK

"
social media - How do people link twitter handles to e-mail accounts,"
Depending upon a Twitter user's Twitter configuration you can locate their Twitter profile searching with their email address or telephone number. See more here: https://support.twitter.com/articles/20170001.
I'm unaware of a quick way to locate an email address with ONLY a Twitter ID.
I suppose someone with a list of valid email addresses could bang away at Twitter and build their own Db of email addresses to Twitter IDs.
If there is a match on an email I think it would be moderately to very reliable. The absence of a match, however, would very unreliably indicate the absence of a corresponding Twitter profile.
"
weather - How can I get temperature data for each Country (Annual),"
This is very coarse data from the worldbank. It shows historical average temperature per country:
http://data.worldbank.org/data-catalog/cckp_historical_data
Using their Climate API you can get slightly more detailed information and by per year. I believe they have data going back to 1961.
http://data.worldbank.org/developers/climate-data-api
Berkeley Earth provides aggregated data on world temperature averages going back to 1750, and country/city data going back to 1960.
http://berkeleyearth.org/data
Then there is NASA's global temperatures database collected from 500 stations, some going back to 1720. I haven't used this tool yet, so I'm not sure how to navigate it.
http://gcmd.gsfc.nasa.gov/KeywordSearch/Metadata.do?Portal=GCMD&KeywordPath=&EntryId=Rimfrost&MetadataView=Data&MetadataType=0&lbnode=mdlb3
Then there is NOAA. It's datasets, while primarily US, includes datasets for data collecting from weather stations around the world:
http://www.ncdc.noaa.gov/data-access
"
"data request - Open, big time-series dataset (ideally web traffic)","
I publish the Google Play Store statistics of one of my apps:
http://datahub.io/dataset/google-play-statistics/
See for instance the installs per device type. It is fragmented by day, with metrics such as installs/uninstalls/upgrades.
It is less than 100K rows, though, as data is pre-aggregated.
"
language - Data set of news articles and scientific journals,"
As far as I know, one of the best data sources for NLP has been the ENRON emails. While this is not a ""news"" source, it would certainly provide some raw data for you to play around with.
Another source is PubMed. While this doesn't provide you with the actual article information, it does provide you with a lot of abstracts for scientific writing. There have even been papers published in PubMed about using this topic.
Finally, there's good old Wikipedia, which allows you to download their entire archived content.
Hope this helps!
"
programming - Public web API to get list of apps in App Store?,"
There is an unofficial API for the Google Play Android Market: android-market-api


You can browse market with any carrier or locale you want. 
Search for apps using keywords or package name. 
Retrieve an app info using an app ID. 
Retrieve comments using an app ID. 
Get PNG screenshots and icon 


Command line usage:
java -jar androidmarketapi-X.Y.jar myemail mypassword myquery 

My source. Check out the other answers, too.
"
API for in-season food info based on location,"
It doesn't appear to be offered as data, but it would be exceedingly easy to scrape this Eat Local resource from NRDC. There's a page for each state which lists foods by month.
For each state they have ""learn more"" links, which probably also lead to their data sources (few of which appear to be structured data.)
"
data request - ATM locations in UK and/or Ireland,"
While the data is only as good as the community contributes, OpenStreetMap (OSM) has some ATM data.
In OSM, points can be tagged with ""amenity=atm"". You can extract these nodes using these steps:

Download OSM data. This page has a ""British isles"" extract, among others. This page explains more generally how this kind of data is packaged.
Download and extract the osmosis command-line java tool
Run a command such as ./osmosis --read-pbf british-isles-latest.osm.pbf --tf accept-nodes amenity=atm --write-xml british-atms.osm

Osmosis also supports writing to databases instead of XML.
The details available for each node vary widely, but then, you get what you pay for, and as you noted, there are a number of commercial sources for this data. 
"
economics - Looking for data on 3D Printer sales trends worldwide,"
These data are compiled in the annual Wohlers Report, which is US $495.
See https://wohlersassociates.com/2016report.htm.
There is no free source for these sales figures if you need a comprehensive dataset. I would call or email Wohler's Associates and ask if there is a research/academic edition available; it's a small company and friendly to work with.
"
What needs to be done to be 4 or 5-star open data?,"
The current situation of your data

Data tables in PDF (example from your forestry statistics page) are a great example of 1-star data: The data is on the web, but it's pretty much unusable for computer programs without some heavy-duty data extraction. After all, PDF is where data goes to die. My advice: Just don't do it.
Your example Excel file is actually a good start. It's 2-star data, since it's still in a proprietary file format, but it's structured and machine-readable. It could easily be turned into 3-star data (""Save as CSV""). In other words, the difference between 2-star and 3-star data is mainly ideological and much less technical.

Getting to 3-star data is not a problem. Turning it into 4-star and 5-star data is where things start to get interesting.
The structure of your example data

Your data has three dimensions: County (expressed as number, code, and abbreviation), land use class, and year.
For each combination of these three dimensions, there is one data value, which is the area in hectare.

Expressing two dimensions in tabular form — such as an Excel spreadsheet — is trivial: On dimension (e.g. county) goes on the vertical axis, the other dimension (e.g. land use class) on the horizontal axis. However once the dimensionality is larger than 2, it becomes tricky to represent the data in two dimensions (e.g. in a table on a computer screen).
There are standard solutions to deal with multi-dimensional data, such das Pivot Tables and OLAP Cubes. In the context of Linked Data, the RDF Data Cube Vocabulary is the current best practice for expressing multi-dimensional data.
What needs to be done to move to 4- or 5-star data?
4-star data means that you ""use open standards from W3C"" (such as RDF) and/or ""use URIs to denote things"" so that people can point at your stuff. 5-star data means that you link your data to other people's data.
What does that mean in case of your statistical data? Well, basically it means that you would have to turn your statistical data into RDF Data Cubes. With the current state of tool support, this still requires a lot of effort and expertise. While RDF Data Cubes are a great thing in theory, so far they are not really relevant in practice because, outside of the Semantic Web research community, not that many people use this technology.
Other things you could do
If you do not want to go for RDF Data Cubes — and unless you have strong reasons in favor, I would not recommend it — there are other things you could do to make your data more machine-readable:
Publish your data as (compressed) flat CSV tables
This makes it trivial to

import the data into a spreadsheet application and create a pivot table
import the data into OpenRefine and perform advanced operations
turn the data into RDF Data Cubes, if someone wants to do it and already has the necessary skills

You could even add another column containing URIs representing the counties (e.g. https://www.wikidata.org/entity/Q104926 for Uppsala County). This would fulfil the requirements for 5-star data (although we skipped the requirements for 4-star data).
Keep an eye on the Metadata Vocabulary for Tabular Data
So far it's only a W3C Working Draft, but it sounds promising:

Validation, conversion, display and search of tabular data on the web requires additional metadata that describes how the data should be interpreted. This document defines a vocabulary for metadata that annotates tabular data. This can be used to provide metadata at various levels, from collections of data and how they relate to each other down to individual cells within a table.

"
geospatial - uk postcode outcode border data,"
Take a look at this site:
UK Postcodes
It has the CSV data for all the boundaries. You can also download a KML file at each level to display in Google Earth, or QGIS where you can do more analysis. 
Unfortunately you may need to individually download each Area to get detail for the next level down. For example, select CH from the list and then click 'Download the individual postcode data in KMl format' to get data of the next level down (CH1, CH2 etc).
"
data request - Looking for large multidimensional datasets,"
You can search through the UCI Machine Learning Repository by the # of Attributes.
For example, the Arrhythmia dataset has 279 attributes.
"
data request - Datasets in which people make probability estimates,"
the health and retirement study asks what's the probability you think you'll be in a nursing home in the next few years.
http://www.asdfree.com/search/label/health%20and%20retirement%20study%20%28hrs%29
you would also be able to calculate how many of these probability guesses came true.  hrs is a longitudinal survey, so they follow americans aged 50+ until they die - they also follow people if they move into a nursing home.  you could, for example, you could use the ""Probability to live 75+"" question at the point in the panel when each respondent turns 65 to calculate what share made it vs what share said they would.  the link i provided has usage examples.  look at the RAND codebooks for ""reported probability"" and be careful about how you're using the survey weights.
"
data request - Is there an API or global database for sports events that is used by famous sports websites?,"
You will find that most sports data is collected by commercial concerns who control licensing rather carefully, especially for current data. (See, for example, this from Sports-Reference.com about why they can't provide their bulk data.)
Most references to sports data that I've found (e.g. DatabaseSports.com) provide web access but not bulk downloads. Other resources frequently seem out of date. (But try googling for ""sports data,"" you'll find lots of leads to follow beyond what I list here.)
Sean Lahman's index of sports data looks pretty carefully maintained and oriented towards bulk data. There's a book, Analyzing Baseball Data with R, which uses Lahman's baseball data as a source.
Tableau Public has a page which collects links to sports data sources for over a dozen sports but a little clicking doesn't show any that have bulk download. (You're advised to check further for the sports which interest you.)
There is a book, Sports Data Mining, which has an entire chapter devoted to data sources. The book is expensive, and was published in 2010 so it's data references may be obsolete… but perhaps you can find it in a library.
"
data request - Missing private sector credit in IFS for Euro-area countries 1998--1999,"
I'm pretty sure it's too late to help you, but maybe others will face the same problem. For 1998 you can find data in the IMFs ""International Financial Statistics Yearbook(s)"" (if you are at a university your library might have them), but even there data for 1999 is missing. I'm not sure yet why this is the case, but I try to find out, since I want to know too. I think in the end the only possibility to fill these gaps will be interpolation.
It's also a bit strange that the gaps in the online IFS data are longer than those in the (printed) IFS Yearbook(s). Unfortunately there are no hints in the yearbooks why those gaps exist. There are notes for many countries, but they only explain that the establishment of the euro area changed the reporting, no word on the gap in 1999.
"
data request - Weather Web Service by Postal Code?,"
The Weather API from Wunderground allows 500 free requests per day (with registration) and has a geolookup endpoint for finding the nearest weather station.

Once you have the nearest weather station, you can use other endpoints like forecast and history.
The full list of endpoints is here.
"
data request - Database of Greyhound bus stops,"
i think this is a good start, but i can't find who authored it. i would take the kml from this map and upload it to geojson.io where you can fiddle and convert it more.
https://www.google.com/maps/d/viewer?msa=0&mid=zzToj9iDb7Q0.kZhGp83_3Et8 
edit:
the wayback machine has what you want albeit a year old. i only checked texas, so maybe i'm wrong. go here and then swap out state two letter digits.
http://web.archive.org/web/20131212034714/http://www.greyhound.com/en/locations/locations.aspx?state=tx 
canada locations by province:
http://web.archive.org/web/20130817065113/http://greyhound.ca/en/locations/states.aspx 
also there's this but you'll have to filter out mexico:
http://www.routefriend.com/stations/greyhound
"
data format - HIOS Plan Year 2016 XSD Schema,"
These files are now posted on the SERFF site: 2016 QHP Templates.
"
HealthCare Finder API Links Broken?,"
Yes this looks like a bug on the site. The APIs themselves seem to be working fine, but the links to the documentation are broken. If you are simply interested in the XSD then you should be able to download it from https://finder.healthcare.gov/api/finder_api_v3.0.xsd
If you are looking for a working example, try the following curl request: 
curl 'https://api.finder.healthcare.gov/v3.0/getIFPPlanBenefits' -H 'Content-Type: application/xml' --data-binary $'<?xml version=""1.0"" encoding=""UTF-8""?> \n<p:PlanBenefitRequest xmlns:p=""http://hios.cms.org/api"" xmlns:p1=""http://hios.cms.org/api-types"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://hios.cms.org/api hios-api-11.0.xsd ""> \n  <p:Enrollees>\n    <p1:DateOfBirth>1990-01-01</p1:DateOfBirth>\n    <p1:Gender>Male</p1:Gender>\n    \n    <p1:Relation>SELF</p1:Relation>\n    <p1:InHouseholdIndicator>true</p1:InHouseholdIndicator>\n  </p:Enrollees>\n  <p:Location>\n    <p1:ZipCode>48001</p1:ZipCode>\n     <p1:County>\n         <p1:FipsCode>26147</p1:FipsCode>\n         <p1:CountyName>SAINT CLAIR</p1:CountyName>\n         <p1:StateCode>MI</p1:StateCode>\n      </p1:County>\n  </p:Location>\n  <p:InsuranceEffectiveDate>2015-01-01</p:InsuranceEffectiveDate>\n  <p:Market>Individual</p:Market>\n  <p:PlanIds>\n      <p:PlanId>59830MI0010009</p:PlanId>\n</p:PlanIds>\n   </p:PlanBenefitRequest>' --compressed

"
data request - finding online finance datasets,"
Here's a link to datasets (Excel spreadsheets) from a NYU professor whom has been keeping corporate finance data on major corporations in US, Canada, UK and Australia for 20 years.
http://pages.stern.nyu.edu/~adamodar/New_Home_Page/data.html
"
usa - Where to get older digital OCR'd data sets of unsummarized US Census data?,"
you're talking about historical public use microdata, right?  you want
https://usa.ipums.org/usa/sampdesc.shtml
"
usa - Average height of males and females for each of the 50 US states: Where can I download the data?,"
The BRFSS would be a good data set to use for this question. Download the data here.
For example, for the 2013 data, I recommend getting acquainted with the codebook and then downloading the raw data. It will be an ASCII fixed column width raw file, so refer to the column positions document for transferring to the analysis package of your choice.
"
data request - Open database of ingredient names?,"
I think USDA.gov's NDB (full name: USDA National Nutrient Database for Standard Reference) would get you what you need.
Download links and data metadata is available on the ""About the Database"" file at http://www.ars.usda.gov/Services/docs.htm?docid=8964. If you're looking solely for a list of food items, the latest Food Descriptions file is at http://www.ars.usda.gov/SP2UserFiles/Place/80400525/Data/SR27/asc/FOOD_DES.txt
There are many sites such as nutritionix.com, etc. which have APIs and have added additional commercial off the shelf ingredients like packaged goods, etc. too though so you might look into a more refined/complete collection as well. See http://www.nutritionix.com/api for information about their API.
"
"data request - How would I find on iMDb, all male actors with movies debuts between ages 20 and 30?","
Have you taken a look at http://www.imdb.com/interfaces which includes a link to FTP mirrors where you can get data files that can presumably be joined in a RDBMS or other type of database system. Files you probably want to investigate are actors.list.gz, movies.list.gz, and release-dates.list.gz
Hope this helps!
"
data request - Open database of filename extensions,"
In today's modern world we think less about the file extensions (suffix) and classify files by their 'Media Type' (formerly MIME type). IANA is the official repository for registering media types. The most complete list ( as of March 5, 2015) is here:
http://www.iana.org/assignments/media-types/media-types.xhtml
This third-party page which lists MIME types with their typical extensions and has links to more information for each one. If there are media types not listed on that page, you can visit the links in the ""template"" column from the IANA registration page and find file extension information.
"
data request - Novel Blurb Corpus,"
Blurbs, or short descriptive material to promote a book, may be impossible to legally share do to individual copyrights of the authors or publishers.
The Goodreads API has many endpoints, and they include this note:

Book cover images, descriptions, and other data from third party sources might be excluded, because we do not have a license to distribute these data via our API. 

In contrast, the book metadata can be shared (ISBN, author, publisher, etc). See, for example, the Book-Crossing Dataset.
So, in order to get a big data set of blurbs, you'd have to contact (large) publishers and ask for access. I noticed you have university affiliation, so you should mention that it is for non-commercial purposes.
"
data request - Map of Orange County (CA) buildings,"
As someone who actually has a fair bit of knowledge into the technological progress Orange County, California has made in the way of GIS, I can say that the data more than likely still does not exist for the whole county. Major cities such as Anaheim, Santa Ana, Irvine, Newport Beach may have building footprint shapefiles (I know for a fact that Anaheim and Irvine do). But many of the smaller cities have not done the work from the last time I can recall having the most relevant information (2012).
Orange County has only recently taken up the cause of digitizing its infrastructure to data/GIS. They originally hired the utility companies to create the parcel shapefile layout for the county and that was as recent as 2009. The parcel shapefiles have been publicly available for Orange County, CA since 2012 and the most recent data which has sufficient attribute data is for 2014.
"
Variable/multidimensional json record structure OpenFDA,"
For any of the /drug endpoints, I would use the opendfda.substance_name.exact annotated field. It will not be on all records, it is matched in a variety of ways against each dataset, so we took an approach of being conservative in the matching for the initial phases of this project; however, when it is there, it will have all sorts of wonderful information. 
In the case /drug/event.json, the openfda record is under patient.drug.openfda.substance_name.exact. If this fields is not fully populated (which is might not be at this point), you can always fall back on the patient.drug.medicinalproduct.exact field (which is part of the original adverse event submission). An example would be:
https://api.fda.gov/drug/event.json?search=patient.drug.medicinalproduct.exact:RETINOL
or
https://api.fda.gov/drug/event.json?search=patient.drug.openfda.substance_name.exact:RETINOL
As you can see, there are only two hits against the openfda.substance_name field in this case, which is a good sign that there is not good coverage in the annotation process, which means that you stick with the medicinalproduct field in this case. 
A note about 'exact'. In this case, it is best to use because substance names can have multiple words in them (exact does phrase style searching); however, exact also is case-sensitive and most substance names in the SPL are uppercase.
"
data request - CPU instruction set,"
Only a single architecture, but maybe helpful nevertheless:
The educational MMIX architecture by Donald Knuth has a full list of instructions on their current website, including opcodes, signature and timing information (e.g. ADDU = 1 cycle, MULU = 10 cycles). 
However, the list of instructions is not available as a machine-readable format, but might be parsable with a bit of RegExp matching of HTML lists and tables. This compact table of all opcodes might facilitate that task.
"
data request - Datasets that input GPS and output historical time-series,"
There Daily Global Weather Measurements, 1929-2009 (NCDC, GSOD) dataset contains:

A collection of daily weather measurements (temperature, wind speed, humidity, pressure, &c.) from 9000+ weather stations around the world.
Global summary of day data for 18 surface meteorological elements are derived from the synoptic/hourly observations contained in USAF DATSAV3 Surface data and Federal Climate Complex Integrated Surface Data (ISD). Historical data are generally available for 1929 to the present, with data from 1973 to the present being the most complete.

The data is global, including ocean measurements, will contain frequent daily measurements as well as latitude and longitude coordinates.

One common way to get this dataset is through the hosting by Amazon Web Services, and the original details are found at the NOAA site.

Caveat: the license says

This data set can only be used within the United States.

So perhaps your 1 year of free tier AWS instance can be set up in the US.
"
data request - Simulated dataset agent based gps,"
To simulate GPS tracks, consider using the Optimal Roadtrip code from Randy Olson.
Steps (taken directly from ipython notebook):

Construct a list of road trip waypoints
Gather the distance traveled on the shortest route between all waypoints (using Google Maps directions)
Use a genetic algorithm to optimize the order to visit the waypoints in 
Visualize your road trip on a Google map (not important for your application)


"
Historic data on number of houses and number of households in UK?,"
Did you find the Live tables on Household Characteristics page? Table 801: Tenure trend is an Excel spreadsheet which has the total number of households dating back to 1939 (not every year) and for 1918 it has percentages of owners and renters but not absolute numbers. (""Absolute"", as the data is presented in thousands of households anyway.)
Files on the Live tables on House Building have data on the number of dwellings, although the one I checked only dates back to 1969. These files are in ODS format.
"
"usa - Where can I get NY small businesses data, including revenue, number of employees etc?","
There is a New York State Directory of Minority/Women-owned Business Enterprises. You can search using a form on that page, or download the entire directory as CSV or Excel.
It has the following columns:

Company Name
DBA Name
Owner First
Owner Last
Physical Address
City
State
Zip
Mailing Address
City
State
Zip
Phone
Fax
Email
Agency
Certification Type
Capability
Work Districts/Regions
Industry
Business Size
General Location
Location

"
real estate - Looking for home sale data,"
nationwide, the biggest and best data set you're going to get is
http://asdfree.com/home-mortgage-disclosure-act-hmda.html
but if you can use a survey instead, start with
http://asdfree.com/american-housing-survey-ahs.html
and check other surveys on asdfree.com that have household-level statistics like acs, nychvs, and maybe scf
this is also worth a read
How to construct a database with the underlying real estate data displayed by Redfin, Zillow, or Trulia?
"
Where can I find publicly available data about internet usage?,"
Even though it is not open data, I can suggest to use also Statista. In this platform (I'm a free member), I found some data related to Internet usage in China.
Internet users in China 2005 - 2014.
I hope this one help you. There's also more information about Internet usage in the World Bank Database or the US Census Office.
"
food - How can I download the Product Open Data database?,"
The Wayback Machine is always your best friend; Here's the data from 2014-02
http://web.archive.org/web/20140209011312/http://product-open-data.com/download/
OPD Product Browser Web Repository
There's also open food facts
"
data request - Global map of protected areas by IUCN category,"
I believe you will find what you are looking for at the Global Landcover Facility (though a little dated - 2007).
http://staff.glcf.umd.edu/sns/branch/htdocs.sns/data/wdpa/
As a member of the IUCN, the GLCF provides the World Database on Protected Areas for free to the world. This data set contains GIS layers of protected areas that were produced by the World Conservation Union (IUCN) and the United Nations Environment Programme (UNEP)'s World Conservation Monitoring Centre (WCMC). The 2007 data set includes protected areas recognized at the international and national levels. Data are provided as point or polygon shapefiles.
"
What is the status of OKFN's Open Product Data project?,"
We're actually looking for a new maintainer for the project.
The new version is running on Django and the source code is here.
"
best practice - Searching for Open Data Dataset That is No Longer Online,"
The Memento Web and the Wayback Machine are two possible solutions:
Wayback Machine
The Wayback Machine by the Internet Archive is your best friend for all things that were once online, and even some things that still are, if you want to compare changes.
From Wikipedia:  

The Wayback Machine is a digital archive of the World Wide Web and other information on the Internet created by the Internet Archive, a non-profit organization, based in San Francisco, California. It was set up by Brewster Kahle and Bruce Gilliat, and is maintained with content from Alexa Internet. The service enables users to see archived versions of web pages across time, which the Archive calls a ""three dimensional index.""

If you have a dead link, simply paste it into the search input element of Wayback Machine.
Alternatively, you can type https://web.archive.org/web/*/DEAD_LINK'S_URL directly into the address bar of your favorite browser.
Memento Web
Los Alamos National Lab has been offering the Memento Web project which unifies search across the archives.

Main form to use for online search
Memento plugin for Chrome
Memento extension for Firefox

Other
Some search engines may provide you with a cached version of the page where the dataset resided (possibly, the dataset files as well).
If the steps above fail, you may consider

looking for papers which used the dataset (at arXiv or in academic journals),
contacting their authors,
looking at various bulletin boards for those who might have downloaded the dataset and contacting them.

You can also have a look at http://academictorrents.com to see if your dataset is present there.
"
data request - Where can I find massive and high dimensional survival datasets,"
If you're using the ""survival"" statistical package on R, you should also be familiar with the free datasets that Rstudio comes packaged with.
Check out this webpage that lists all datasets that come preinstalled. Searching for ""survival"" should yield quite a few different tables that will be readily available to download.
While many of them only contain only a few dimensions, some provide more granularity - but not sure if this meets your requirements. Do you have a more thorough list of requirements (N number of dimensions... X number of rows) that you can specify?
"
"releasing data - Where to host a public KML/GPX/OSM map file? (preview, map links, statistics, conversion)","
geojson.io supports kml. it also allows you to import to github or gist.github
edit: it doesn't have all of your requirements but the integration into github should allow you to do most except for the gpx
edit:
end results in gists: all of these .geojson maps were rendered via kml files from geojson.io
https://gist.github.com/jalbertbowden 
step by step:
download this .zip file containing points for a history map of Hampton, VA:
https://ckannet-storage.commondatastorage.googleapis.com/2015-03-05T21:52:24.450Z/locations-kmz.zip
extract Locations.kml out of the .zip.
go to geojsion.io:
http://geojson.io/
go to open, file, and upload the kml file, which gives us:
http://geojson.io/#map=12/37.0532/-76.3602
you are previewing the data. you can edit it. you can redownload the kml. you can download geojson/topojson/csv/shp as well.
this also provides you with a bloc.ks url for sharing.
click on sharing:
http://bl.ocks.org/d/6dd95a7f0d7d39b1e41a
more preview version but for an audience.
click on save. you have all the format options i've already listed, as well as github/gist. 
clicking on gist gives you:
https://gist.github.com/jalbertbowden/006bf26bf7e5da3200fd
optionally you can include it in a github repo as well. clicking on github will pull up your github repos and give you a menu from where to save.  
"
Consumption data of gasoline by month and city available?,"
a think tank in chicago has done some of this legwork for you, but the price is from 2007; pretty sure you can change it to reflect new prices. and while this is only going to give you a cost estimate, i imagine dividing the cost by price should give you the number of gallons. not sure if you can enter a city, but you can search by zip/address so you could also compile them into cities/localities:
http://abogo.cnt.org/ 
there's also the urban mobility report, which should give you exactly what you want:
http://mobility.tamu.edu/ums/
"
data request - Searching for list(s) of babynames containing huge (10k+) amounts of unique names,"
The best source of international human given (first) names comes from a German computer magazine. The text file has nearly 50k names that are classified by likely gender, and how popular in each country. It's carefully curated and has a friendly license (GNU Free Documentation License 1.2).
The file can be downloaded here : ftp://ftp.heise.de/pub/ct/listings/0717-182.zip (name_dict.txt contains the data).
Archive Link: https://web.archive.org/web/20200414235453/ftp://ftp.heise.de/pub/ct/listings/0717-182.zip
Instead of parsing this file, you can use the python port SexMachine (really) - package and github repo. I'm sure other languages have their own ports. There is also a windows executable (details).
(my reference)

For US baby names, you can use the Social Security Admin's download (overview) and link to data. This data can be national or on the state level, and going back to the late 19th century.

To safeguard privacy, we restrict our list of names to those with at least 5 occurrences.

You'll also find ports of this data to various languages.
"
tool request - What product is the Australian government Meteor made using?,"
Based on http://meteor.aihw.gov.au/content/index.phtml/itemId/236643, which states Synop Pty Ltd was awarded the contract to develop METeOR and developed the system by customising their XML-based content management system., I would say it is not free / open source software.
You could contact Synop, or you could ask AIHW if they can host some of your data: http://meteor.aihw.gov.au/content/index.phtml/itemId/276559
"
What are best practices for where to answer agency specific Open Data Policy implementation questions?,"
this is a ""depends"" question, but i'll lean to always having the answers on your site. you are the authority. you are releasing the data. you are in control.
bringing them here could help alert people to the data and the questions, as well as help crowd source solutions or answers you are seeking.
there's a saying called ""own your data"" and i believe this falls under it as well. 
"
data request - Alcohol consumption and reaction time,"
cdc is probably your best bet. start here:
http://www.cdc.gov/alcohol/fact-sheets/alcohol-use.htm
"
What standards exist for accessibility of open data?,"
I'm going to assume that the data is in some graphical format -- if it's numerical data, then the visualization would be a function of the software that's using it.
For images, there isn't anything that I'm aware of that you can do for blind people.  I've heard of devices that allow a blind person to trace a pen-like device over a surface, and it'll vibrate depending on what it's pointing at.  I've also seen specialty braille books by Noreen Grice that take a few images and apply textures to them.  (one of my co-workers was involved with her 'Touch the Sun' book)
If you're serving images via HTML, you can use the 'longdesc' attribute to provide a textual description of the image, with more specificity than would be appropriate for an 'alt' attribute.
Another consideration for images are the color-blind.  'Rainbow' style color tables are particularly bad not only for the color blind, but have also been shown to influence people in unintended ways:

How The Rainbow Color Map Misleads
Dear NASA: No More Rainbow Color Scales, Please
The Rainbow color map
Rainbow Colormaps – What are they good for? Absolutely nothing!

The last one links to a number of other resources, including alternate color tables and scholarly articles on the issue.  (note that AGU's EOS had a front-page article on this topic years ago, and did absolutely nothing in their editorial process, within a couple of issues they had front-page articles with the problem).
"
NOAA QCLCD weather data - inconsistencies in hourly data?,"
So I emailed the NOAA, and they responded pretty quickly with clarification (props to them!)
Q: My understanding of the ""WeatherType"" is that it is an ""abbreviated 3-hourly weather observations"" (from the QCLCD summary at https://data.noaa.gov/dataset/quality-controlled-local-climatological-data-qclcd-publication ). Is it more of a 3-hour summarized outlook of the weather, or a spot determination of rain/snow/haze etc?""
A: No, it's hourly, not 3-hourly. It's a spot determination at the time of observation.
As for the HourlyPrecip column, the measurements are only taken hourly, so there will only be measurements at 9.51, 10.51, 11.51, etc. 
"
data request - What's the train frequency at all stations in the UK?,"
You can find the complete timetable in PDF through Network Rail.
You might be able to scrape the required information from it.
Otherwise, perhaps you can contact Network Rail directly to see if the information you're asking for is available.  In Sweden, tydal.nu has had success in scraping information from a similar Swedish PDF timetable, so perhaps the people there have some code they are willing to share.
"
weather - Hourly data on whether it is snowing for a particular location (NYC),"
The live weather feeds for weather stations, reporting to the NWS, in the State of New York can be found here at the link below. Each station has its own update interval. The XML feed is really HTML, so you will have to parse it as a HTML table.
http://w1.weather.gov/xml/current_obs/seek.php?state=ny&Find=Find
"
data request - Average Job Salary by Title/Location/Etc. (US or Intl),"
I recommend Occupational Employment Statistics. Includes approximately 800 job categories/titles (Standard Occupational Classification). Reports estimated employment, average wages and 10th/25th/50th/75th/90th wage percentiles.
The upside is that it's pretty easy to download or hit the API. The downside is that there are quite a few ""holes"" in the data, either due to confidentiality issues (e.g., one large company in an area employing the majority of, say, Aerospace Engineers can lead to those values being blanked out) or insufficient sample size.
But for free / publicly available, geographically detailed stats on occupations in the US, it's pretty much the only game in town.
XLS files for metro and multi-county nonmetro areas here: http://www.bls.gov/oes/tables.htm.
For API access, it's similar for most BLS data. Example:
http://api.bls.gov/publicAPI/v2/timeseries/data/OEUM001018000000029114103

The last component is the BLS ""seriesID"". Breaking it down:

OE: dataset ID
U: not seasonally adjusted
M: area type is metro
0010180: area code for metro area of Abilene, TX
000000: industry code ""total"", all types of businesses
291141: occupation code for Registered Nurses
03: statistic code - get the mean hourly wage

For additional info on codes, see http://download.bls.gov/pub/time.series/oe/ (esp. the ""oe.area"" and ""oe.occupation"" files). It's a little misleading because this dataset is not actually a timeseries; only the latest year (currently 2014) is available at any given time.
"
releasing data - What's a good file format for sharing 3D models of cities?,"
In answer to the original question, you should look at CityGML – this is a standardised format that's being heavily used by cities, particularly in Europe. It handles model definition, textures, various feature types (buildings, bridges, street furniture, all sorts) and has been built to keep the data about buildings and other objects intact (eg. which part is the roof and which is the wall). In fact it has various levels of detail that can be used depending on your need, so you could even go as far as modelling the full interior of a building down to the fixtures and fittings. 
Here's an example CityGML building at LOD2 (not the worst, not the best):

That building is actually within a new open building library that I'm creating with the guys at Mapzen that uses ViziCities (I'm the creator of ViziCities). We hope to release it publicly very soon.
Failing that – collada or OBJ or fairly well used by cities for sharing buildings.
"
government - What's a good file format for presidencies of countries?,"
The following has the data you want, but in an HTML table format. You will need to parse it out of the tables.
https://www.cia.gov/library/publications/the-world-factbook/fields/2077.html
"
Related Books API,"
Project Gutenberg has what's called a Bookshelf, which are categories.
Also, for each title, there is a link called ""Also downloaded""
So for book:
http://www.gutenberg.org/ebooks/20194

the related titles would be:
http://www.gutenberg.org/ebooks/20194/also/


Some resources

Project Gutenberg Catalog and Offline Catalogs
Sample Index (to get the book ID for the URL)
Bulk download
Python package

"
Interested in the history of the Washington Data Processing Center,"
The Washington Data Processing Center was located in the basement and subbasement of the second wing of the South Building of the USDA between 12th and 14th streets along Independence Avenue, SW, Washington, DC. 
The USDA South building was built in stages between 1930-1936 using the designs of Architect Louis A. Simon.
"
weather - Are there open historic cloud cover data files available?,"
For the satellite era, the best you can get is likely reanalysis data:

A meteorological reanalysis is a meteorological data assimilation project which aims to assimilate historical observational data spanning an extended period, using a single consistent assimilation (or ""analysis"") scheme throughout.

The two most widely used reanalysis are the ones from ECMWF and NCEP.  Of those, the former is European and semi-free, whereas the latter is fully free.  After making an account, you can download data through this page at UCAR.  Hourly data includes fields like Cloud Amount/Frequency, and Cloud Liquid Water/Ice.
Normal users of this data are atmospheric and climate scientists, so you might need to be quite careful to see you are interpreting the data correctly.
For before the satellite data, there exists no global record of cloud cover.
"
licensing - Using Google Maps Data,"
Yes, it is possible, using the Google Places API.
Just for completeness sake: This is most definitely not open data. For an open alternative, have a look at Phil's answer about Open Street Maps.
If your usage complies with Google's terms depends on what exactly you plan to do with the data. I guess section 10.1.3 of the Google Maps/Google Earth APIs Terms of Service will be most relevant for you:

(a) No Unauthorized Copying, Modification, Creation of Derivative Works, or Display of the Content. You must not copy, translate, modify, or create a derivative work (including creating or contributing to a database) of, or publicly display any Content or any part thereof except as explicitly permitted under these Terms. For example, the following are prohibited: (i) creating server-side modification of map tiles; (ii) stitching multiple static map images together to display a map that is larger than permitted in the Maps APIs Documentation; (iii) creating mailing lists or telemarketing lists based on the Content; or (iv) exporting, writing, or saving the Content to a third party's location-based platform or service.
(b) No Pre-Fetching, Caching, or Storage of Content. You must not pre-fetch, cache, or store any Content, except that you may store: (i) limited amounts of Content for the purpose of improving the performance of your Maps API Implementation if you do so temporarily (and in no event for more than 30 calendar days), securely, and in a manner that does not permit use of the Content outside of the Service; and (ii) any content identifier or key that the Maps APIs Documentation specifically permits you to store. For example, you must not use the Content to create an independent database of ""places"" or other local listings information.
(c) No Mass Downloads or Bulk Feeds of Content. You must not use the Service in a manner that gives you or any other person access to mass downloads or bulk feeds of any Content, including but not limited to numerical latitude or longitude coordinates, imagery, visible map data, or places data (including business listings). For example, you are not permitted to offer a batch geocoding service that uses Content contained in the Maps API(s).

"
data request - List of all NANP area codes and central office / exchanges and their geographic location?,"
http://www.nationalnanpa.com/nanp1/npa_report.csv with definitions found in http://www.nationalnanpa.com/area_codes/AreaCodeDatabaseDefinitions.xls
Look through nationalnanpa.com's sitemap has some valuable resources.
http://cnac.ca/data/COCodeStatus_ALL.zip gives you Canadian codes.
"
Public datasets containing less-boring and more detailed data,"
The go-to for random, interesting datasets comes from this article:
100+ Interesting Data Sets for Statistics
Here are some quotes from the article based on your question:

How do gender and mental illness affect crime? This data set was collected explicitly with that question in mind.
There’s a lot of data from a series of online personality tests available here.
Who receives H1-B visas?
List of the most frequently searched-for data (google)

etc
"
api - Is there any way to get a package's resource's field names/datatypes with downloading the entire resource?,"
If the site in question has enabled it, the CKAN's Datastore allows you to query tabular data directly via additional API functions. In your case you would probably use the datastore_info function to get the column names and data types for a given resource.
Here's an example of getting that information for some weather data (using the excellent HTTPie command line tool):
$ http post https://transparenz.karlsruhe.de/api/3/action/datastore_info id=117ae307-902a-4bf1-9c43-8b1aa9e8c2bd
HTTP/1.1 200 OK
Cache-Control: no-cache
Connection: keep-alive
Content-Length: 642
Content-Type: application/json;charset=utf-8
Date: Fri, 21 Jul 2017 11:25:19 GMT
Pragma: no-cache
Server: nginx/1.11.2
Strict-Transport-Security: max-age=31536000
Vary: X-Forwarded-Proto,X-Forwarded-Port

{
    ""help"": ""https://transparenz.karlsruhe.de/api/3/action/help_show?name=datastore_info"", 
    ""result"": {
        ""meta"": {
            ""count"": 3550
        }, 
        ""schema"": {
            ""Apparent temperature (Degree Celcius)"": ""number"", 
            ""Atmospheric pressure (hPa; not normalized to sea level)"": ""number"", 
            ""Average wind speed (km/h; 10min window)"": ""number"", 
            ""Date and time (YYYY-MM-DD HH:MM:SS)"": ""date"", 
            ""Dew point (Degree Celcius)"": ""number"", 
            ""Global irradiance (W/m^2)"": ""number"", 
            ""Humidity (%)"": ""number"", 
            ""Maximum wind speed (km/h; 10min window)"": ""number"", 
            ""Precipitation (mm)"": ""number"", 
            ""Temperature (Degree Celcius)"": ""number"", 
            ""Wind direction (Degree)"": ""number""
        }
    }, 
    ""success"": true
}

You can check whether a CKAN instance is running the Datastore extension via the status_show API function.
"
data request - List of abbreviations and acronyms,"
I found a github repository that scrapes the wiki list of roughly 6000 acronyms
https://github.com/krishnakt031990/Crawl-Wiki-For-Acronyms
raw acronyms file
and then I wrote this python script to read that code's output and clean it up, as well as put it into a valid CSV (gist)
#!/usr/bin/env python
# coding=utf-8

ignore_list = ('Search-Navigation','Tools-What links','Top-','Contents','Magyar')
with open('AcronymsFile.csv','r') as inp:
    data = inp.read().split('\n')

with open('clean_AcronymsFile.csv','w') as out:
    out.write('acronym'+'\t'+'definition'+'\n')

    for line in data:
        if not line.startswith(ignore_list):
            tmp = line.split('-')
            acronym = tmp[0].strip()
            definition = '-'.join([x.strip() for x in tmp[1:]])

            out.write(acronym+'\t'+definition+'\n')

writes this CSV output (tab separated): gist (still needs some cleaning)
sample header:
acronym definition
0D  Zero-dimensional
1AM Air mechanic 1st class
1D  One-dimensional
2AM Air mechanic 2nd class


"
"medical - EEG data, specifically for alzheimer's?","
There are two datasets from the UCI Machine Learning Repository related to EEG, but not specific to Alzheimer's patients.

The EEG dataset


This data arises from a large study to examine EEG correlates of genetic predisposition to alcoholism.
It contains measurements from 64 electrodes placed on subject's scalps which were sampled at 256 Hz (3.9-msec epoch) for 1 second.



There is also an EEG Eye State related to video analysis of open/closed eyes and EEG measurement.


All data is from one continuous EEG measurement with the Emotiv EEG Neuroheadset. The duration of the measurement was 117 seconds. The eye state was detected via a camera during the EEG measurement and added later manually to the file after analysing the video frames. '1' indicates the eye-closed and '0' the eye-open state. All values are in chronological order with the first measured value at the top of the data.

"
usa - why doesn't federal reserve services website have all routing numbers?,"
There is a GitHub Repo FedACHdir that contains what I think is the same text file and also the specific routing number you are looking for:
https://raw.githubusercontent.com/gatepay/FedACHdir/master/FedACHdir.txt

"
data request - Public transport maps for all cities,"
The OpenStreetMap.org website has a ""Transport Map"" layer:

It's not great. In particular, tram stops are far too prominent (for the city of Melbourne anyway).
You could conceivably make a better style, using openstreetmap data, and a tool like Mapbox Studio.
You didn't really make it clear whether you want a geographically accurate or a stylised map (like a subwap map). The latter can only be made by hand, as far as I'm aware.
"
state - USAID DDL - program codes,"
Our apologies for the delay.  We are updating our settings here and will strive to respond within 48 hours to future inquiries.  Per Project Open Data, the individual metadata fields correspond to the specific ""data asset"" (or dataset) which you are describing.
A single USAID award could generate data corresponding to every single program code.  Therefore using the program code to refer to an entire award would be of little value.
"
usa - Open data version of the Foreign Assistance Program Inventory?,"
We have requested that a machine-readable (CSV) version of the Standardized Program Structure and Definitions be posted to this website.  Will circle back when we have confirmation of posting. 
"
us census - How can I get a full list of US Zipcodes with their associated names/CSAs/MSAs/lats/longs?,"
Update: I recently found this data at GeoNames.org, among many related data sets. The file includes city, county, state, latitude and longitude. It would be a separate exercise to roll it up to MSA or CSA.
Direct link to US Postal codes ZIP file

For many people, the best way to get this data is to buy it. There's a cottage industry of data services that have been providing this to small businesses for years, the costs are pretty low, and the length of the rest of this answer verifies that it's involved, although certainly something you can do if you have some coding and data skills.
If you'd rather create this dataset than buy it, here are things to know:
The Census Bureau states: 

The USPS ZIP Codes identify the individual post office or metropolitan area delivery station associated with mailing addresses. USPS ZIP Codes are not areal features but a collection of mail delivery routes.

If the distinction matters, much more information can be found on this page The key things to know: ZCTAs overlap city boundaries and certainly at least a few exist outside of any city boundary. Also, if you're starting from mailing addresses, you will probably have some ZIP codes which aren't in the ZCTA dataset. And, the USPS may have changed ZIP codes since the last time the Census Bureau produced ZCTA definitions. (Some commercial data vendors promise to keep up with those changes as another value-add.)
So anyway, this means that naming is not straightforward. The Census Bureau just uses the 5-digit reference ZIP Code as a label. If you want to assign them names based on the nearest town, etc, you have some work to do.
ZCTAs are areas, not points, but when you ask for the latitude and longitude, you're probably interested in the centroid. You can find this in the Census Bureau's ZIP code tabulation area (ZCTA) gazetteer file, which can be downloaded here. (You want the INTPTLAT and INTPTLONG columns.)
Once you have a ZCTA centroid, you can locate it in a CSA or MSA. GIS software makes this fairly straightforward, with shapefiles provided by the Census's TIGER program.
But your question is further complicated by this: to the Census Bureau, CSA and MSA are different (and overlapping) things. 
The bureau doesn't use the term ""MSA"" but rather combine ""metropolitan statistical areas"" and ""micropolitan statistical areas"" into a set called ""core based statistical areas"" or CBSAs. 
To the Census, CSAs are ""Combined Statistical Areas"" and they are composed of some groups of CBSAs that are economically related. Not all CBSAs are part of any CSA. On Wikipedia, there's a page of ""primary statistical areas"" that articulates which CBSAs aren't in any CSA, but I haven't seen a GIS shapefile that matches that list. But maybe you can get by with just the CBSAs.
"
data request - List of European Hospitals and Health Clinics,"
There might not be a database covering all countries of Europe (for instance the non-democratic country Belarus does not publish much information). So if you need more data that OpenStreetMap has, you will probably need to go country-by-country. Below is the data for France.
The French government has created a website showing the list of all 4307 hospitals and clinics in France:
http://www.ScopeSante.fr
Click on ""Voir la liste des établissements"" to get the full list.

You will need to scrape that. Each hospital name has a link to its detailed information page:

I have just sent them a message asking for a dump of the data. They replied to me saying that all of their data comes from the FINESS database, which is also maintained by the French government, and available online ""for private use"" and unfortunately does not have dump downloads, so that would mean scraping that:

"
data request - Where can I find State tax income rates from 1995 to present?,"
The taxfoundation.org provides this information going back to 2000. In addition to their interactive form, they have a downloadable Excel spreadsheet.
http://taxfoundation.org/article/state-individual-income-tax-rates
"
Exclusion lists when crawling web directories,"
Using --reject-regex ""\?.=.;.=."" seems to work for me to avoid the extra listings provided by Apache. The full command-line I use to get all the files from an Apache directory listing is:
wget --recursive --no-parent --timestamping --no-directories --reject-regex ""\?.=.;.=."" http://example.com/some/dir/

"
data request - Database of free WiFi hotspots,"
I don't think the coverage is very complete, but Open Street Maps has a key called internet_access.
With this key you can use tags like wlan to find open WLAN.
To find free WLAN, add the internet_access:fee=no option
"
sql - How to get a list of all strengths & forms for a drug from RxNorm?,"
Kind of stumbled upon this, while unlikely to help the OP seeing as this was asked years ago, hopefully someone will find this useful. I have not downloaded the full dataset but you can fetch the desired data here. I believe the rate limit is 20/second/IP address.
example call:
https://clinicaltables.nlm.nih.gov/api/rxterms/v3/search?sf=DISPLAY_NAME&ef=DISPLAY_NAME,STRENGTHS_AND_FORMS,RXCUIS,DISPLAY_NAME_SYNONYM&terms={your search term here}
this should get you all names, aliases, strengths and forms along with rxcuis.
"
data request - Electric utility boundary lines in US,"
The US Energy Information Administration has a few geographic resources (like the US Energy Mapping System and the State Energy Data System but there's no sign of a collection of electric utility service areas.
On a state-by-state basis, there are a variety of resources available. This might provide a roadmap for motivated parties to request shapefiles and assemble the dataset. I'm going to make this answer a 'community wiki' in case other people want to flesh it out with more states or hunt further for GIS files on states where PDFs were easier to find..

Arkansas (map, shp, gdb)
California (map, shp)  
Connecticut (gdb, shp) Utility Infrastructure and Pipelines
Illinois (unofficial PDF map)
Indiana (map, shp)
Iowa (pdf map)  
Kentucky
Massachusetts (map, shp; town-level detail only)
Minnesota (.shp, .gdb, .kmz, OGC GeoPackage)
New Hampshire (pdf map)
North Carolina (pdf map of co-ops)
Vermont (.shp)
Wisconsin (pdf map)

US (National/Federal)
 - Bureau of Transportation Statistics Geospatial Information
Canada
Atlas of Canada Energy GIS
"
calendar - Free data on public holidays?,"
There are many attempts to make a public holiday calendar, and one of the best ones is a python module called workalender (my source).
But the main problem with a global holiday calendar is:

Please take note that some calendars are not 100% accurate. The most common example is the Islamic calendar, where some computed holidays are not exactly on the same official day decided by religious authorities, and this may vary country by country. Whenever it's possible, try to adjust your results with the official data provided by the adequate authorities.

"
machine learning - Any data set available for Twitter tweets classification?,"
The Streaming APIs
Overview
The Streaming APIs give developers low latency access to Twitter’s global stream of Tweet data. A proper implementation of a streaming client will be pushed messages indicating Tweets and other events have occurred, without any of the overhead associated with polling a REST endpoint.
If your intention is to conduct singular searches, read user profile information, or post Tweets, consider using the REST APIs instead.
Please refer the link: https://dev.twitter.com/streaming/public
"
geospatial - Open Data about the internet,"
There is a good source of ""unofficial"" internet data from the 2012 Internet Census.

While playing around with the Nmap Scripting Engine (NSE) we discovered an amazing number of open embedded devices on the Internet. Many of them are based on Linux and allow login to standard BusyBox with empty or default credentials. We used these devices to build a distributed port scanner to scan all IPv4 addresses. These scans include service probes for the most common ports, ICMP ping, reverse DNS and SYN scans. We analyzed some of the data to get an estimation of the IP address usage.
All data gathered during our research is released into the public domain for further study.


Hilbert Browser tool

Full data download (568 GB torrent)

Image gallery




A similar project is the DNS Census 2013

The DNS Census 2013 is an attempt to provide a public dataset of registered domains and DNS records.
The dataset contains about 2.5 billion DNS records gathered in the years 2012-2013.

(answer from this site)
"
api - Airline check for availability data,"
Just thought this might help anyone... https://www.airport-data.com/api/doc.php also provides API. you might have to sign up but its free. data is not real-time, so not to be used for flight travel
"
data request - 1945 (spring) daily weather Germany,"
For German daily data, start here at the Deutschewettersdienst website.
ftp://ftp-cdc.dwd.de/pub/CDC/observations_germany/climate/daily/kl/historical/
ftp://ftp-cdc.dwd.de/climate_environment/CDC/observations_germany/climate/daily/kl/historical/
Open the file KL_Tageswerte_Beschreibung_Stationen.txt to find a station with data for the location and time period that you want.  For Bonn, it looks like station ID 599 ""Bonn-Friesdorf"" covers 1932 through 1999;  for Cologne, station ID 2665 Koln-Botanischer Garten covers 1936 thru 1984.
Then, find the zipfile for that station, e.g. tageswerte_00599_19321001_19990301_hist.zip
or tageswerte_02665_19360101_19841231_hist.zip
Within the zipfile will be a large textfile named something like produkt_klima_Tageswerte_19321001_19990301_00599.txt which contains daily records of temperature (avg, high, low, air pressure, precipitation, etc.)  It's raw data, so you'll have to find the column headings at the top, then read down the rows until you find the date that you want.
The only problem is that, for fairly obvious reasons, the stations stop recording and have data gaps beginning in 1944 or 1945, resuming again in late 1945 and 1946.  However, I have found two Berlin stations - 417 Berlin-Lichterfelde(Sud) and 402 Berlin-Dahlem (LFAG) - that continue recording up to March 31, 1945 and April 24, 1945, respectively.
Makes you think.
"
"data request - Pictures of all airplanes, organized by tail number","
Most commercial aricraft, lots of military and some general aviation aircraft.
Airliners.net
"
data request - The spread of cable tv,"
there are a few, certainly not all, dates in this pdf about the history of television. one example: Cable television is introduced in Pennsylvania as a means of bringing television to rural areas. (1948)  
not a complete answer, but some information in here for you:
http://tarlton.law.utexas.edu/exhibits/mason_&_associates/documents/timeline.pdf
"
linked data - Are there any dataset for psychology,"
The Thesaurus of Psychological Index Terms, published by one of your employer's (?) customers, is known now as Psychology Ontology.
This ontology is not even a taxonomy, but rather a flat list of classes.
BioPortal provides mappings between this ontology and many other ontologies.  
Additionally, BioPortal contains other ontologies published by APA:

The OntoPsychia ontology seems very interesting, but is not available for download.

AberOWL repository provides these links:

MFOEM - Emotion Ontology
MF - Mental Functioning Ontology
MFOMD - MFO Mental Disease Ontology
NBO - Neuro Behavior Ontology

"
Linked Geospatial Data in WFS,"
You can use HTTP-URI values from controlled vocabularies in the attributes of your WMS datasets, as properties in your WFS feature types and WCS coverages.  You can additionally add links in your GetCapabilities response documents through metadata urls, and data urls...  And finally you can make links in your metadata documents for your services and datasets.
"
Does there exist a set of data that measures internet traffic that is specific to image downloading and uploading?,"
I think you're unlikely to find this as true ""open data,"" since there are substantial privacy concerns to internet traffic monitoring, and the volume of data involved to do proper analysis is considerable.
That said, there is an organization, the Center for Applied Internet Data Analysis (CAIDA), which makes datasets available to researchers under oversight. A brief look at their data overview doesn't reveal any sets which are obviously content-oriented, but they may be able to advise.
Beyond getting the data yourself, you may be able to find a citation in media or academic publishing which substantiates your hypothesis. This GigaOm story, ""How the core of the internet has changed from data to content"" might lead you down the right path.
Ultimately, the amount of still image data looks like it will be a drop in the bucket, if the Cisco study covered in this Recode article is correct: they claim that video traffic is currently 78% of internet traffic, and headed for 84% by 2018. If you still want to get numbers about image data, perhaps digging deeper into the Cisco study will yield results.
"
"data request - Seeking geographical database that, given coordinate boundaries, returns all the mountain peaks within the boundary, their coordinates, and elevation","
I think this is another task for OSM.

Extract data within Polygon or by bounding box
Filter based on tag Peak

See some other questions about using OSM as a datasource, including the Overpass API, by searching here or Google
"
legal - Legality of using data against terms,"
here's more of an opinion, not an answer:
i'm free software all the way, and i wouldn't give a care to what their terms say, i'd do whatever i pleased with it, until they killed the service.  
in reality, corporations will destroy you, unless you have an army of lawyers too. look how broken the patent system is, or the hacker that exposed a public url and went to prison. odds are you won't get nailed, but if you do, it'll probably be severe. remember, hackers are treated like terrorists in america.  
it really pains me to say that utilizing an rss feed makes one a hacker, but the media will spin it like that, no doubt.
"
data request - Machine-readable way to determine if plant species is native to Australia,"
G'day Steve,
Yep APNI (Australian Plant Name Index) list Australian Native and Naturalized plants as well as some weed species.
APNI includes some services for looking up plant names.
APC the Australian Plant Census is a list of the currently accepted names as recognised by CHAH the Council of Heads of Australian Herbaria. (http://www.anbg.gov.au/chah/)
To find a name in APNI you can use https://biodiversity.org.au/nsl/services/apni
This search will display an APNI format output of pretty much all we know about the name and synonyms of the name (not about the plant, just the names references, authors )
A search like this one https://biodiversity.org.au/nsl/services/search?product=apni&tree.id=3029293&name=Hakea+actites&max=100&display=apni&search=true will give you the output for Hakea actites. But your search may return mutiple matches. The match is against the 'full name' of the plant, so it includes the Author.
The link icon in that output is a permalink to a name object which is consumable e.g. https://biodiversity.org.au/boa/name/apni/163125 You can ask for that link in JSON, XML, or RDF using the request mime type. To see it in your browser though you'll need to add .json to the end, e.g. https://biodiversity.org.au/boa/name/apni/163125.json
You'll notice the links on the name object include https://biodiversity.org.au/boa/Hakea%20actites%20W.R.Barker which uses the full name as an identifier. This is a quick way to see if we have linked data on a name. However we only have the full name in that service right now.
So... we have doco happening (the service has just changed rather a lot) and we'll keep people informed via twitter @AuBiodiversity. There are lots of other services.
edit: add weed and naturalised info:
OK APNI is just a name store, it stores facts about names wrt references. APC is a classification and it stores some opinion (CHAHs) about the names.
Currently the data about something being a weed or naturalised is in the form of notes by APC, so there is not clear cut yes/no.
Some examples:
APC Dist gives the distribution e.g. https://t.co/mo2OD9TbIV
if naturalised it says so e.g.https://t.co/0i4sAqS7Lg
weed status is a little sketchy and can be in status or comments e.g.
https://t.co/K4nzt6OOFD and https://t.co/h9WDU61XUQ
"
data request - Where to find number of companies by year revenue?,"
Government procurement systems
See this answer for details on how to get the revenue numbers for firms supplying goods to governments:
https://opendata.stackexchange.com/a/5149/1052
Though this sample may be biased (government consumption is somewhat unconventional), this data is just two minutes away.
Request data from a government agency
If you need only the bins by size without disclosing the names of the companies, you may send a request to IRS, BLS, or another agency that may have the data, from their surveys.
"
tool request - Mapping CSV Header to RDF + SPARQL console,"
Before building a completely new system from scratch, you should first check if an existing system satisfies your requirements. There are a few possible candidates out there:

As outlined in this answer, D2RQ and RDF HDT might be close to what you are looking for.
In addition there is Tarql which provides SPARQL access to CSV files — which is, as far as I can tell, exactly what you want.

One more comment: If you want performant search for your CSV files, you won't get around a search index. And instead of building one yourself, I recommend having a close look at D2RQ again.
"
Journalism examples using officials public data portals?,"
Well, here's one: http://www.theage.com.au/victoria/the-most-ignored-parking-sign-in-melbourne-20140512-zraek.html
Here's the background: http://www.theage.com.au/data-point/blogs/the-crunch--data-point/melbourne-parking--get-the-data-20140513-386wv.html
Summary: data journalists from The Age found data on the City of Melbourne's data portal, wrote a story from it.
"
Open Badges/Tin Can to open up credential & learning data,"
The differences between Tin Can and Open Badges seem to be that Tin Can is for one particular sector of micro-credentials (experiences) whereas Open Badges covers all sectors of micro-credentials.
"
machine learning - Data set of software fault prediction studies,"
you can look at the software-artifacts infrastructure repository. this repository contains open source software with manually injected faults,test cases and scripts. This repository is available for everyone to use, we can conduct controlled experiments on these fault injected software.
http://sir.unl.edu/portal/index.php
"
data request - Where I can find a repository of software usage/execution logs (traces)?,"
You might consider using the EVENT table from the Mozilla Labs ""Day in the Life of a Browser"" dataset.

Unfortunately the download links are inactive. But this dataset has come up on another question and hopefully Mozilla will fix it soon. 
License is Creative Commons Attribution 3.0 United States.
UPDATE: data is now hosted here
"
usa - Hyperloop; Which Open-Source License did Musk choose?,"
Surprisingly, I can find no mention of license/patent/rights in the Hyperloop Alpha PDF published by Musk in 2013. In a tweet, Musk stated:

I really hate patents unless critical to company survival. Will publish Hyperloop as open source. 

I sent an email to hyperloop@teslamotors.com inquiring. I'll update this answer if I hear back.
"
Where can I find a downloadable grocery store food ingredient database / data set?,"
have you tried Open Food Facts? It's a ""free, open and collaborative database of food products from the entire world.""  The data is offered under the Open Database License
"
research - Spectral reflectance data for iron rust,"
As mentioned in the comments, I found a resource that has a myriad of spectral samples of just about all minerals (including rust) - The RRUFF Project website, where it contains

an integrated database of Raman spectra, X-ray diffraction and chemistry data for minerals.

This site has a searchable database, the only (albeit minor) downside is that it does not allow for generic terms such as 'rust', but rather, the actual chemical names such as goethite - a main component of 'red' rust.
Raw spectral data can be downloaded from the pages.
If there is difficulty in determining the real name, then WebMineral.com can assist, through a search facility that takes the chemical name and provides mineral names.
"
"machine learning - For a recommender system, is there a real data matrix that is about 500 by 500 that is complete and has no missing entries?","
Here is a listing of Recommendation and Ratings Public Data Sets For Machine Learning.
500x500 may be ambitious, so instead of movies you may have to use music (shorter content means more reviews)
In particular, take a look at (copy/pasted from site): 

Last.fm - Music Recommendation Data Sets 
Yahoo! - Movie, Music, and Images Ratings Data Sets 
Audioscrobbler - Music Recommendation Data Sets 
Amazon - Audio CD recommendations 

Note that Last.fm also has an API
"
weather - Hourly temperature data for specific locations (Arkansas) for the year 2014,"
I would start here: http://www.climate.gov/hourlysub-hourly-observational-data-hourly-global-%E2%80%93-gis-data-locator.  It looks like you can go to ""View Data"", then select the stations of interest from the map, then specify the dates and data elements that you want.  It does not guarantee that hourly data will be available from every station.
"
openstreetmap - Importing XML data into ArcGIS Desktop? - Geographic Information Systems Stack Exchange,"
I would do as below:
Since this map uses openstreet map as a basemap so follow as below-
Run the query> Export result> Download geojson>add ineteroperability connection in arc map and add exported geojson> add basemap(openstreet map)

N.B In this case (arcmap 10.1) you need interoperability extesion installed in arc map
for 10.2 onward there is a tool called JSON To Features (Conversion) use this to convert this geojson to feature (or use interoperability)
Several format export is possible e.g as geoJSON,as GPX,as KML,raw data- all these format can be imported into arc map using interoperability connection.
"
data.gov - Latitude and Longitude of US Commuting Zones,"
You can find out information about Americans' commuting habits in relation to specific geographic areas through the U.S. Census American Community Survey.  Access to the data is available.  For example, if you are looking for how people in Los Angeles County get to work, you can find the answer through the Easy Stats online.

The longitude and latitude of all U.S. boundaries (from school districts to counties to roads) can be found via TIGER files (topographically integrated geographic encoding and referencing).
"
usa - Are there open data sets about commuting patterns in large US cities?,"
There's a related question.  This answer may be helpful here too Nick.
You can find out information about Americans' commuting habits in relation to specific geographic areas through the U.S. Census American Community Survey.  Access to the data is available.  For example, if you are looking for how people in Los Angeles County get to work, you can find the answer through the Easy Stats online.

The longitude and latitude of all U.S. boundaries (from school districts to counties to roads) can be found via TIGER files (topographically integrated geographic encoding and referencing).
"
business - Need sample E-commerce order data,"
A popular answer comes from a 2010 question on Stackoverflow - Source Link

Northwind database - (documentation & data model)
NopCommerce sample dataset
E-commerce dataset from Amazon / Google Products / Abt Buy 

Other ideas:

Tableau Superstore (Excel file) (although I think this is aggregated)

"
licensing - License of NOAA hourly temperature data,"
I have contacted NOAA directly with this question, and the answer was (emphasis mine):

As you have described it below, you are not in violation of Resolution
  40. This would apply if you are redistributing the data as-is for profit.

If you are unsure about your particular use case, I suggest contacting NOAA directly.
"
"data request - Find a missing dataset: ""A Week in the life of a browser - Version 2"" from Mozilla Labs","
I just snagged it off the wayback machine here:  
http://web.archive.org/web/20110711102216/https://testpilot.mozillalabs.com/testcases/a-week-life-2/witl_small.tar.gz  

but more files can be found here by searching for '.tar.gz' or '.db.gz'
http://web.archive.org/web/*/https://testpilot.mozillalabs.com/testcases/*


Download the data

Download links from Wayback Machine:

witl.db.gz (1.1 GB, SQLite 3.x database)
witl_large.tar.gz (469 MB, CSV files)
witl_small.tar.gz (7.4 MB, CSV files)


Details:

"
data request - Where can I find shapefiles for the rivers of Puerto Rico?,"
Here are the open data sets that match your requirements:

Data.gov's Puerto Rico hydrography data (shapefile format)
Data.gov's Puerto Rico hydrography data (all formats)

Seems to be the same data sets, but available via geoplatform.gov site:

Puerto Rico hydrography data (shapefile format)
Puerto Rico hydrography data (all formats)

Finally, these Puerto Rico hydrography data sets are in ESRI ArcInfo interchange file (E00).
"
business - Is there a data set listing which pharmacies have self-checkout lanes?,"
I think no such dataset exists. However, there might be some roundabout ways of approximating that information.
Here's a Yelp query for reviews that feature the phrase ""self checkout"" in the Boston area. You can see both mapping and also the metadata that the results are ""1-10 of 97"" and you can select specific establishments. You can mirror this query using the Yelp API without going through a browser. 
This isn't particularly comprehensive because a business needs a review that actually mentions ""self-checkout"" and the ""self-checkout"" phrase doesn't necessarily indicate that self-checkout is actually present (a review could include ""gee I wish this store had self-checkout"" and will come up). However it should still paint an approximate picture and would allow you to find specific locations that use self-checkout.
You can repeat this request for different geographies and if you automate it you could even cobble together your own database, or create your own tool for dynamically searching this data. Yelp has a fairly robust developer API and if you're able to accept the caveats of its information, it could be a good way to assemble a data set.
"
data request - where can I find shapefiles for the highways of Puerto Rico?,"
Since I have promised, I will answer this question without waiting for its migration, if it will ever happen. Basically, I think that the best and latest data set that you can find now is this one - from the US official open data repository's TIGER/Line database. This page is generated, based on a relevant search (Puerto Rico), and might also contain some data sets of your interest.
Other potentially useful data sets include ones within U.S. Atlas TopoJSON repository (on how to use the data via R, see this nice tutorial) as well as this repository of U.S. major roads ESRI shapefile and geoJSON data sets (you have to check whether this repository contains PR data).
"
government - Freshness of Unemployment Data,"
There are many sources of surveys/administrative in the United States data you might find useful.

Unemployment Insurance Claims - the number of people in a given week who: newly file for unemployment insurance, are continuing their unemployment insurance. This would be the most timely data.
Current Population Survey (CPS) - (reference period, data source) This is the monthly survey conducted since 1940 that is used to take an assessment of the country's labor force status. Whenever you hear about the ""official"" unemployment rate, it is usually coming from this survey which are published ""Employment Situation"" reports such as the ones here. This is a fairly well-organized source of information since the response rate is usually 93% and above and has less statistical noise since in any given month ~72% of the sample is carried over from the previous month and ~45% is carried over from the previous year.
Current Employment Statistics (CES) survey - (reference period, data) This is the monthly survey that polls random companies about their employment. Be warned, however, its timeliness is questionable since it takes up to three months for the average response rate to go above 93% ( usually in the first month, its response rate is ~60% on average).
Quarterly Census of Employment and Wages (QCEW) - (Data: source 1, source 2) - This is a far less timely source of data. However, it is a publicly released report of all payroll employment within a given quarter. Historically, it takes on average nine months for the data to release so by the time it is ready you are typically looking at data from a year ago.
Local Area Unemployment Statistics (LAUS) - This source of information is different from all the rest since it uses small area estimation to predict state and local unemployment rates. It uses the CPS as its primary source, along with other data such as UI claims or the CES survey act as secondary sources. This source is also less timely for your purposes.

"
metadata - CKAN exposing tables from a relational database,"
Have you already tried the resources here?
https://lists.okfn.org/mailman/listinfo/ckan-dev
"
usa - Trying to extrapolate patient costs by physician from public Medicare pricing data,"
For the benefit of the community here is a late answer. The average payment by the beneficiary (and third party payers) is: 
average_Medicare_allowed_amt - avg_Medicare_payment_amt
From the PDF:
average_Medicare_allowed_amt is the sum of the amount Medicare pays, the deductible and coinsurance amounts that the beneficiary is responsible for paying, and any amounts that a third party is responsible for paying.
It is customary in US health insurance billing to distinguish between submitted charges, that are largely detached from the actual allowed charges. The latter are split between the beneficiary and Medicare. So by subtracting what Medicare actually paid from the price it allowed you get what is left for the beneficiary (or third party payers, meaning other insurers that cover the beneficiary, like Medigap).
"
usa - Crime rate data for American cities,"
This site goes back several decades. Each file is pretty large but also has the data broken down by month. I don't think you'll be able to get around varying degrees of reporting, that's a common problem statisticians have to deal with. 
"
"finance - Business performance data of web company like number of employees, revenue etc","
Here's a link to datasets (Excel spreadsheets) from a NYU professor whom has been keeping corporate finance data on major corporations for 20 years:
http://pages.stern.nyu.edu/~adamodar/New_Home_Page/data.html
Filter the companies with classification ""Software (Internet)"", there are 759 of them. I generated the list at https://gist.github.com/nicolas-raoul/1145776cc37d5654c310 but here is an excerpt:
Fund.com Inc. (OTCPK:FNDM)
SpectrumDNA, Inc. (OTCPK:SPXA)
Net Savings Link, Inc. (OTCPK:NSAV)
Global MobileTech, Inc. (OTCPK:GLMB)
Santeon Group, Inc. (OTCPK:SANT)
Anchorage International Holdings Corp. (OTCPK:AHCP)
myContactCard, Inc. (OTCPK:MYCT)
Metatron, Inc. (OTCPK:MRNJ)
Optimum Interactive USA Ltd. (OTCPK:OPTL)
Verecloud, Inc. (OTCPK:VCLD)
Bigsupersearch Com Inc. (OTCPK:BSPR)
Idle Media, Inc. (OTCPK:IDLM)
Thwapr, Inc. (OTCPK:THWI)
Bizzingo, Inc. (OTCPK:BIZZ)
Digagogo Ventures Corp. (OTCPK:DOGO)
Rarus Technologies Inc. (OTCPK:RARS)
SinglePoint, Inc. (OTCPK:SING)
WrapMail, Inc. (OTCPK:WRAP)
Eventure Interactive, Inc. (OTCBB:EVTI)
Guard Dog, Inc. (OTCPK:GRDO)
BullsnBears.com, Inc. (OTCPK:BNBI)
Truli Media Group, Inc. (OTCPK:TRLI)
Wally World Media, Inc. (OTCPK:WLYW)
IL2M International Corp (OTCPK:ILIM)
EFH Group, Inc. (OTCBB:TWYF)

"
data request - Dataset of major newspapers content,"
Wikipedia: List of online newspaper archives
Many lack ""official"" interfaces and may restrict the use of materials (see their copyrights), but those that fit the requirements can be scraped with web services like import.io or Python's Scrapy:

http://en.wikipedia.org/wiki/Wikipedia:List_of_online_newspaper_archives

(It also includes Google's scans of old newspapers.)
Library of Congress: Newspaper Archives, Indexes & Morgues
The links to a few dozens of newspaper archives with full-text articles. International newspaper archives included. Mostly historical data.

http://www.loc.gov/rr/news/oltitles.html

All Digitized Newspapers 1836-1922 by state, ethnicity, language:

http://chroniclingamerica.loc.gov/#tab=tab_newspapers

Their API:

http://chroniclingamerica.loc.gov/about/api/

"
How to anonymously share data?,"
The OP says he is fine with Bittorrent now.  After all, Bittorrent was made for large files (outlawing Bittorrent would be like outlawing bombs.) Now, how to do it. First of all, Bittorrent isn't anonymous (again the creators had only one task in mind), so you will need a proxy or something. Now, create the torrent file. Now the torrent file itself will be small (a couple megabytes I think) so it would fit in a paste.
"
data request - Dataset of Facebook Users Connectivity,"
Facebook has the Graph API so you can construct your own queries and collect as much data as you want.
Otherwise, there are some collected resources described in this thread (warning: from 2010 and maybe involving hacked data).

Online Social Neworks from UC Irvine
100 million Facebook pages leaked as torrent


If you don't require Facebook, there are much better resources for Twitter (datasets and public search/stream API access)
"
economics - microcredit or microfinance data,"
Have you checked USAID's Microenterprise Results Reporting (MRR) Portal?  You may also be able to find other resources at USAID's Development Data Library.
usaidopen
"
finance - Why is my scraper returning inconsistent results and timing out?,"
Some tips for scrapers:

log: don't stop at ""sometimes I get data and sometimes I don't"" -- when you get unexpected results in the response, log it so you can learn what happened. Specifically when you are being throttled for making too many requests, you will often get an explicit message saying so.
cache: write your code to save files you retrieve and use saved versions before making a URL call. 

Python users can take advantage of some toolkits which make these two things easier. I've had some good experience with scrapelib and scrapekit. I do not know how to insert seamless caching into Mechanize—last time I wrote a scraper with Mechanize I just did it manually, which is basically what @philshem suggested.
"
industry - Data on metals and alloys properties (physical and chemical),"
MatWeb.com's page Metal & Alloy Composition Search has a searchable database, where you can search for metals and alloys by either (or both) of:

Choose a Material Category

or

Choose up to 3 Material Compositions

or search the entire UNS (Unified Numbering System for Metals and Alloys), which 

is a systematic scheme in which each metal is designated by a letter followed by five numbers. It is a composition-based system of commercial materials and does not guarantee any performance specifications or exact composition with impurity limits. Other nomenclature systems have been incorporated into the UNS numbering system to minimize confusion.

The data sheets provide details about the physical, chemical, mechanical, electrical, thermal and optical properties, and the properties of the components of alloys are provided.  Further references are provided in each fact sheet.
"
data request - USAID Dataset Downloads Published List,"
Thank you for the inquiry. The DDL is USAID's Development Data Library located at www.usaid.gov/data.  We do collect basic web traffic statistics and will start looking into this with our CIO's office to kick off the process of posting download  statistics on a regular basis. 
As of November 30, 2015, these stats are now posted here.
"
usa - How do I find data that helps US businesses export their products?,"
The International Trade Administration (ITA) has just released Version 2 of all of its APIs on its Data Services Platform.  This means developers can access even more export data, more easily.  
Here are the updates

New Data:  You asked and we answered!  We are now providing access
to:

Tariff Rates from all US Free Trade Agreements
FAQs About Exporting.

More Data: We are adding more data to our existing data sets:

Additional screening lists
Trade Events from more trade agencies
Overseas opportunities from FedBizOpps

Friendlier:  We’ve made it easier and more secure for developers to get the data they are looking for.

You can learn more about the details on ITA's wiki.
"
data request - Number of employees of large companies?,"
http://finance.yahoo.com
GE for example Full Time Employees:    305,000:
http://finance.yahoo.com/q/pr?s=GE+Profile
And https://en.wikipedia.org/wiki/Yahoo!_Query_Language for auto extraction.
"
open Big Data to solve cancer epidemiology challenge,"
If you're interested in brain imaging data, you could check out the Connectome Coordination Facility
I downloaded the original HCP 1200 for our institution a couple of years free of charge. That's 7T & 3T imaging data for 1200 individuals. It's a large dataset but it is freely available for research. 
"
data request - Website visitors statistics,"
To all my available knowledge having done significant research on digital business models for a number of years, no, no there is not much of a comprehensive, free source for this type of enterprise data, as it requires expensive resources to keep updated and maintained, and even then isn't scientific or, as you put it, is merely an estimate.
I know that's not a helpful answer necessarily, but it's reasserting the need for an open alternative and -- to my knowledge -- letting you know that I personally have not come upon one.
"
"data request - Database of all mailing addresses in France, and their coordinates","
The French government has just launched a national open database that aims to contain all currently valid addresses: http://adresse.data.gouv.fr
It is a collaboration between the government, the French National Geographic Institute, the postal service, and OpenStreetMap.
800 megabytes, one CSV file per French departement, WIN1252 encoding.
The data can be downloaded for free under the ODbL license without registration at http://openstreetmap.fr/ban
Excerpt:
id,nom_voie,id_fantoir,numero,rep,code_insee,code_post,alias,nom_ld,x,y,commune,fant_voie,fant_ld,lat,lon
ADRNIVX_0000000001948049,Chemin des Acacias,0001,15,"""",97401,97425,"""","""",329906.7,7653235.5,Les Avirons,0001,,-21.214471,55.361181
ADRNIVX_0000000002425821,Chemin des Acacias,0001,11,"""",97401,97425,"""","""",329895.1,7653266.5,Les Avirons,0001,,-21.214190,55.361072
ADRNIVX_0000000002255091,Chemin des Acacias,0001,15,BIS,97401,97425,"""","""",329875.4,7653241.4,Les Avirons,0001,,-21.214415,55.360880
ADRNIVX_0000000002425822,Chemin des Acacias,0001,60,"""",97401,97425,"""","""",329224.6,7652935,Les Avirons,0001,,-21.217122,55.354581
ADRNIVX_0000000002425943,Chemin des Acacias,0001,19,C,97401,97425,"""","""",329845.1,7653213.1,Les Avirons,0001,,-21.214668,55.360585

Description of the CSV fields (in French)
Shapefiles are also available.
"
"food - Data about spoon usage when eating spaghetti, by area","
No dataset was available, so I got 300+ people to answer a questionnaire about the topic.
General view of the data (green=spoon, red=no spoon):


Detailed analysis: http://aegif-labo.blogspot.jp/2015/04/eating-spaghetti-spoon-or-not.html
Raw data (as a Google Spreadsheet)
License: Public domain

"
api - OpenFDA Data: Labels with Boxed Warnings,"
You are pretty close, you just have a syntax error on the query that is getting interpreted as a search string: it should be +AND+set_id: not +AND+set_id=
Also, that particular set_id does not have a boxed_warning so it will not return anything. 
The following query confirm the syntax and that the example is missing a boxed_warning. 
https://api.fda.gov/drug/label.json?search=missing:boxed_warning+AND+set_id:3af2e694-6fea-46cf-b680-9ee0b0c83c88
Hope that helps.
"
"Open data of 1 million or more names, for fuzzy matching experiments","
Wikipedia
Wikipedia let you download the data conveniently, without API limits, so you can get the titles of their 4M+ English articles. Depending on your needs, you can try other languages as well. See
http://meta.wikimedia.org/wiki/Data_dumps
In particular, file *-all-titles-in-ns0.gz.
Alexa One Top Million Websites
These are domain names, though not sure it meets your requirements.
http://s3.amazonaws.com/alexa-static/top-1m.csv.zip
"
data.gov - Forbes.com writer looking for up-and-running examples of data driven applications that improve human services in government,"
Some colleagues and I started cataloging some different interesting ways government data is being used:
https://github.com/18F/ideas/issues/2
I'm no expert on this, and this doesn't quite answer your question, but hope it's helpful!
"
"data request - Free heightmap (elevation?) dataset for Kraków, Poland?","
Wow, wow, wow, based on @scruss's hint, I maybe have found something; not exactly free, but really, really cheap (i.e. not ""bussiness prices"" but ""average guy prices"" apparently); only I must yet verify if the data is really good enough for me and what I think it is, but there are some samples. Without further ado, the links are:
http://codgik.gov.pl/index.php/zasob/numeryczne-dane-wysokosciowe.html - an overview of the available subtypes of data, and details on how to request them;
http://codgik.gov.pl/index.php/obsluga-klienta/oplaty-za-udostepnianie-materialow-fotogrametrycznych.html - pricing (seems 2-4zł per km2 IIUC, which seems totally approachable).
(Note: found based on information in a GUGiK video.)
"
data request - source of character-level ngrams that include spaces,"
Based on @NeilSlater's comment, you can easily calculate character N-grams with a few lines of code. 
In this snippet, I use Python's Collections library, which is quite fast for these types of applications:
from collections import defaultdict

def make_char_ngram(text,N):
    data = defaultdict(int) # for speed

    for i in xrange(len(text)-N+1):
        x = text[i:i+N] # actual N-gram
        data[x] += 1    # add 1 to this N-gram key
    return data

print make_char_ngram('ABC the quick brown fox the quick brown fox the quick brown fox XYZ',8)

gives you character 8-grams as a dictionary, where the key is the character N-gram, and the value is the count of that case:
defaultdict(<type 'int'>, {'brown fo': 3, 'ck brown': 3, 'n fox th': 2, 'own fox ': 3, ' the qui': 3, ' quick b': 3, 'rown fox': 3, 'uick bro': 3, 'k brown ': 3, ' fox the': 2, 'e quick ': 3, 'fox the ': 2, ' fox XYZ': 1, 'ox the q': 2, 'wn fox X': 1, ' brown f': 3, 'ick brow': 3, 'BC the q': 1, 'the quic': 3, 'quick br': 3, 'ABC the ': 1, 'wn fox t': 2, 'C the qu': 1, 'n fox XY': 1, 'he quick': 3, 'x the qu': 2})

(similar question)

In terms of performance, this code went through a 6.2MB text file with 128k lines in 4.5 seconds on my laptop (but not writing the results).

If you want to parse massive amounts of text, consider writing these dictionaries to a NoSQL database like MongoDB. In this case, you can parse text in pieces and don't need to run the thing from the start every time. Writing a Python dictionary to MongoDB should be easy (perhaps transform the dict to JSON and then the import is direct).
"
data request - Has there been a spike in the birth rate in Germany?,"
It's much too early to carry out this analysis today. As you say, the 40 weeks period has just passed by. You should not expect the data to be ready in real-time. I am even surprised that you have already found data for 2014. I had a quick look at the two authoritative sources for this kind of data.
At the time of writing this answer, Destatis, the Federal Statistical Office of Germany, has only data up to 2013.
Eurostat, the statistical office of the European Union, has monthly data on live births. For Germany it has data for January to April 2014. But these figures are flagged as “forecast”.
As a conclusion I think that you will have to wait some more months before you can do this analysis.
"
usaidopen - How much data/information should be open?,"
Per Project Open Data, data should be ""complete"" and published ""with the finest possible level of granularity that is practicable and permitted by law and other requirements. Derived or aggregate open data should also be published but must reference the primary data.""  Please see our FAQ#8 for a discussion of aggregate data.
"
data request - Cumulative income by decile,"
Eurostat has data for the member states of the European Union as well as for some other countries. 
ilc_iw01 - Distribution of income by quantiles
If you are eligible you can also apply for an acceess to the data from the Luxembourg Income Study. It has a wider geographical coverage:
"
data.gov - Water level historical data for California,"
If you're okay with talking to people, you might want to contact the chief of the Media & Public Information branch of the California Dept. of Water Resources at (916) 653-9712
"
tool request - Time-Series data viewer,"
It's hard to suggest a good tool without knowing how deep into programming you want to go, or if the tool is for exploration or presentation.
But here is a sample of many good tools out there:
Javascript:

Envision.js
Rickshaw
Cubism

R:

Dygraphs
Google Charts

Python:

Matplotlib
Pandas (using Matplotlib)
Plotly blog - Time Series Graphs & Eleven Stunning Ways You Can Use Them

Tableau

If you don't mind that the data is shared, you can use Tableau Public for free
Alternatives to Tableau Public

"
us census - Is there a time series data set that includes employment information at the neighborhood level?,"
You will not find such observed data at such a small geographic level in the public sphere. Apart from small area estimation models that predict the proportion in an area based on indirect information, no such data surveys the population with enough of a sample that can sufficiently monitor the unemployed in a small area each and every month.
"
data request - Recent Macroeconomics dataset of all countries in the World,"
International organisations provide this kind of data. As you want to cover the whole World, have a look at organisations with the same geographical scope:
The World Bank has a database that covers a wide range of topics.  
If you are more focussed on economic indicators, you can also have a look at the World Economic Outlook Database of the International Monetary Fund (IMF).
Both sites allow you to view the data online as well as to download it further uses.
The United Nations Statistics Division also has a couple of databases on various topics.
"
"products - Downloadable smartphones data (name, specs)","
2022 UPDATE: The best answer is now Wikidata, see this answer.

The most pragmatic solution is to use DBpedia.

Go to the Smartphone page of DBpedia
Scroll to the is dbp:type of section
On the right is a list of hundreds of smartphones
Each of these pages has a lot of information like CPU speed, weight, battery, storage, etc
This information is available as RDF/JSON/CSV. Here is an excerpt of the CSV:


The list of smartphones is also available as RDF/JSON/CSV, so it is very easy to download all of this information programmatically.
License: Creative Commons Attribution-ShareAlike 3.0 Unported License
"
data request - Phonemic & Syllabic N-Gram Distributions of the World's Languages,"
Based on Stanislav's comment:
https://www.sttmedia.com/syllablefrequencies

With the Syllable Counter integrated in the WordCreator, it is possible to create frequency tables for syllables used in different texts.
The Syllable Counter can be used for arbitrary symbol systems, alphabets and Unicode-texts.
For some languages, we have created frequency profiles for two-party syllables (digrams) and three-party syllables (trigrams) on the basis of texts in the corresponding languages with at least 1.5 million characters.
The texts consists of different literature genres to provide the best results.
You can find the lists in the menu under ""Syllable Lists"".

"
where i should get USA water historical data for data science in python?,"
usgs water has some historic, and real-time water data
http://www.usgs.gov/water/
"
images - Where can I get in-car camera data?,"
You may do it a hard way. YouTube and other video hostings have a bunch of DVR videos that capture lots of real-world events.
Find videos on YouTube
Say, by the Russians:
http://www.youtube.com/results?search_query=russian+dvr
Take random screenshots from these videos
https://stackoverflow.com/questions/26850636/youtube-api-grab-screenshot-of-a-video-at-a-specific-time
Label it with Amazon MTurk
https://www.mturk.com/mturk/welcome
Then do the analysis.
You can also ask data from authors who published on your issue and may have well-behaving datasets.
"
"data request - Interesting, open datasets for scoring of location attractiveness?","
Often those data are released at a local level and the country / city you are interested in will defined what data are available. 
For example the city of Toronto (Canada) have build a Well Being application to map and weight hundreds of indicators: http://map.toronto.ca/wellbeing/ Once you selected the indicators, you can export the row data.
This is just an example. Indication on what municipality you are interested in will help to narrow down the answer.
"
data request - California water district boundaries,"
Update:
this data is now available on open data se's datahub.io account, here:
http://datahub.io/dataset/california-water-district-boundaries 
For those times when data was posted, but has since disappeared, you can try The Internet Archive's Wayback Machine.  It does have some limitations, as it won't violate a robots.txt file and it may not archive large files, but in your particular case, it seems to have copies of the two files from 2009-2010:

https://web.archive.org/web/%2a/http://projects.atlas.ca.gov/frs/download.php/245/usbr_wat_dist_state_2003_03_25.zip
https://web.archive.org/web/%2a/http://projects.atlas.ca.gov/frs/download.php/26/usbr_wat_dist_priv.zip

I've verified that the most recent copies both unzip without errors, but I haven't done any other testing to determine if they're intact / complete.
"
usa - Where can I find machine-readable data on which US states and DC border each other?,"
Another way to look at it is if the states are within a small distance of each other. Using PostGIS, you can do this rather easily:
SELECT 
  s1.name state1, 
  s2.name state2,
  ST_DWithin(s1.the_geom::geography,s2.the_geom::geography,500) share_border
FROM state_polygons s1, state_polygons s2
WHERE s1.name < s2.name
ORDER BY share_border DESC, state1 ASC, state2 ASC

I have a data table of states in my CartoDB account and ran that query. Depending on the data, you can choose a more accurate number than 500 meters as I did here. I pulled the state polygons from CartoDB's data library. The state polygons were originally from Natural Earth Data.
This produces a data table like this:

"
data request - Road surface roughness,"
In Italy there is a project named SmartRoadSense, started on February 21th, 2015. The goal of this project is monitoring of Italy's road surface (487,700 km [1]) and all results are open data.
Now, after nine months, has been collected more than 24,000 km of open data. In the site smartroadsense.it you can see two uses of this data, and donwload it.
The project use a simple Android application that senses the roughness via LPC coding (for more detail read the paper SmartRoadSense: Collaborative Road Surface Condition Monitoring). In the roadmap there is also the development of the application for the others platform.
You can see a short movie which showing the four phases of the SmartRoadSense process: sampling, map matching, aggregation, and presentation here.
The data are available at smartroadsense.it. You find an only CSV file with six fields:

LATITUDE
LONGITUDE
PPE, roughness value in this path
OSM_ID, ID of the road in OpenStreetMap
HIGHWAY, category of the road in OpenStreetMap
UPDATED_AT, date of the last update of this roughness


[1] ""Italy."" The World Factbook. Central Intelligence Agency, 2013.
"
ckan - How to exclude datasets with no data on datahub.io search?,"
It's kind of hacky, but you can use a search engine like Google to search within a domain, and then exclude certain strings that indicate the dataset has no data.

LINK
In this case, the search includes pages with:

ESIS (but can be left blank)
site:datahub.io/dataset/ (include the dataset folder because otherwise, the ""no data"" message comes in various languages)

Excludes pages with:

""This dataset has no data, why not add some?""
related (non-data folder)
activity (non-data folder)

In the results list, you'll see the ESIS page you mentioned is not included. You can removed ESIS and replace it with the term you want to search, or leave it blank to search all datasets.
"
data request - Soccer leagues/teams/players API,"
Football data is a page with free statistics in .csv with a large data about matches, beats, results, players and other interesting facts.
Another very common used by journalists (a friend who is in the business pointed) is the Wikipedia of Soccer, which you download in .xml only. This page is also free. 
"
Download East African NDVI data for 5*5km grids,"
Searching for 'NDVI' led me to a page from the NASA Earth Observatory website.  So I then searched data.nasa.gov, and got back a few links ... I have no idea which one is what you want, though:
https://data.nasa.gov/data?search=NDVI&category=
"
data request - Names of popular products found in retail stores,"
Open Product Data (an Open Knowledge project) has a lot of this kind of data: http://www.product-open-data.com/download/
In my experience, the data quality varies but is improving.
"
Linked Data vs Linked Open Data,"
In 2006, Tim Berners-Lee defined the four rules of Linked Data:


Use URIs as names for things
Use HTTP URIs so that people can look up those names.
When someone looks up a URI, provide useful information, using the standards (RDF*, SPARQL) 
Include links to other URIs. so that they can discover more things. 


In 2010, he introduced the 5 star rating system for Linked Open Data:


Available on the web (whatever format) but with an open licence, to be Open Data
Available as machine-readable structured data (e.g. excel instead of image scan of a table)
as (2) plus non-proprietary format (e.g. CSV instead of excel)
All the above plus, Use open standards from W3C (RDF and SPARQL) to identify things, so that people can point at your stuff
All the above, plus: Link your data to other people’s data to provide context


Both is published on his personal note, Linked Data - Design Issues, where he also explains:

Linked Open Data (LOD) is Linked Data which is released under an open licence

"
geospatial - ISO 3166-1 and ISO 3166-2 data plus borders (e.g. as kml),"
I've started to collect a list of world (country, cities, etc.) datasets, guides, etc. in the Awesome World List @ Planet Open Data. For example, the GeoNames, Natural Earth, etc. datasets might be some candidates. Cheers.  
Natural Earth Data
http://www.naturalearthdata.com/
you can download shapefiles here, not kml
"
finance - Art Market data,"
Maybe start an open database of auction results? It will never be as complete as commercial ones, but I'm sure there are many people who would use it (and  possibly add content).
"
medical - Global HIV Incidence Raw Data,"
There seems to be a tremendous variety of data on HIV available from World Health Organization: http://www.who.int/hiv/data/en/ 
This site also seems to have fantastic data plus it lists sources: https://ourworldindata.org/hiv-aids#data-sources
CDC: https://www.cdc.gov/hiv/statistics/overview/ 
UN data: http://data.un.org/Data.aspx?q=hiv&d=UNAIDS&f=inID%3a35
I hope some of these help. 
"
