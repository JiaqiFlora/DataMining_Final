question,answer
How will non-rich citizens make a living if jobs keep getting replaced by robots and are outsourced?,"This is an interesting question a lot of good labour economists have been thinking about for a while. There are a few conflicting theories as to what will happen. You could base a whole career on this question. The prevailing opinion seems to be that increased automation is not going to come at a cost to employment. There are countless examples of advancement lowing the returns to labour occurring throughout history (the plough, the steam train, industrial revolution). None of them has shown a long-run reduction in employment. The Solow Swan model for example,  includes inputs to labour, capital and technology. They show technology and labour being complementary. I know of no empirical evidence suggesting that this has changed. This HBR article suggests that we aren't really seeing a cost in jobs, more a benefit in productivity. It also mentions Robert Solow's famous remark (which was correct at the time): you can see the computer age everywhere but in the productivity statistics This MIT article presents a bleaker perspective with the concluding sentence: In other words, in the race against the machine, some are likely to win while many others lose. Another article suggests that ""this time it's different"".  The analogy used is that humans are horses and we are reaching peak human demand for the labour of horses today is vastly less than it was a century ago, even though horses are extremely strong, fast, capable and intelligent animals. “Peak horse” in the U.S. came in the 1910s In my opinion, this analogy is intuitively satisfying but is not particularly useful. We will see fewer humans working the supermarket checkouts and more in entertainment in the same way horses are no longer our ploughs and our taxis but are more likely to be racing and performing. Past that I think the comparison is a pretty big logical leap. [Keyne's] worry about technological unemployment was mainly a worry about a “temporary phase of maladjustment” as society and the economy adjusted to ever greater levels of productivity. So it could well prove. However, society may find itself sorely tested if, as seems possible, growth and innovation deliver handsome gains to the skilled, while the rest cling to dwindling employment opportunities at stagnant wages. More likely, we will see a (potentially painful) transition in the uses of labour. A factory that used to employ thousands will now employ hundreds, possibly eventually only a dozen. These people will seek employment elsewhere and probably find it, either in existing industries or ones that do not yet exist."
"If women are paid less for the same work, why don't employers hire just women?","There could be several reasons here are just few: Principal-agent problems. Firms are typically not managed by their owners but by managers (agents) who act on the behalf of owners/shareholders (principals). While, owners might desire to maximize profits agents can to some degree act to pursue their own goals (see discussion in Hendrikse Economics and Management of Organizations). For example, CEO might spend more lavishly on things like their private jet, limo, having HQ built by famous architect than profit maximizing CEO would. In the same way CEO/manager with taste for discrimination might pursue that taste even at the expense of profit (to a degree of course). Consumers could have taste for discrimination. This is in fact classic example coming from one of the earliest work on discrimination. If consumers simply have preference for seeing men instead of women at some work and are willing to pay for satisfying this preference, there will be wage gap between men and women (see Mankiw Principles of Economics pp 395). Mankiw has a good example of this for the race wage gap, but the logic of the example directly extends to gender wage gap: Studies of sports teams suggest that racial discrimination has, in fact, been common and that much of the blame lies with customers. One study, published in the
Journal of Labor Economics in 1988, examined the salaries of basketball players and
found that black players earned 20 percent less than white players of comparable
ability. The study also found that attendance at basketball games was larger for
teams with a greater proportion of white players. One interpretation of these facts
is that, at least at the time of the study, customer discrimination made black players
less profitable than white players for team owners. In the presence of such customer discrimination, a discriminatory wage gap can persist, even if team owners
care only about profit. It could be result of government intervention (See discussion in Mankiw Principles of Economics pp 395). For example, many governments around the world mandate that firms have to provide women with job guarantee during late pregnancy/mothers leave. Perks like these are costly and in competitive markets firms will just pass costs of these perks directly on their female employees (in form of lower wage - although this would not affect total compensation it is part of explanation for wage differentials). Most labor markets are not perfect. In imperfect labor markets there are often quasi-rents to have that can be split between employee and employer. If for example women tend to be less assertive during negotiation (e.g. see Amanatullah & Morris, 2010), they might end up with less quasi-rents than their male counterparts (this argument is covered in greater detail in the Papayapap's answer). The list above is not exclusive, but those are one of the major reasons you will commonly find in economics textbooks. To sum it up, the argument you make in the question is valid argument but it  is only guaranteed to work under host of additional supporting assumptions. For example, assumption that customers do not care about gender might be true in some profession but not in others (e.g. perhaps male anchor appears more trustworthy to audience?). Although careful studies that control for all relevant characteristics show that gender wage gap is much smaller than laymen commonly perceive it to be, based on senseless comparison of aggregates between male and female pay, many studies show that there still is some gender pay gap that cannot be accounted for by objective factors. See discussion of these results in this nice explainer from The Economist. For example, in 2016 the gender pay gap in the UK was estimated to be only $0.8\%$, in France$2.7\%$ and Germany $3\%$, once objective factors were accounted for. That is admittedly small, but should not be trivialized nonetheless."
What are some results in Economics that are both a consensus and far from common sense?,"The principle of comparative advantage As Paul Samuelson (1969) put it: thousands of important and intelligent men ... have never been able to grasp the doctrine [of comparative advantage] for themselves or to believe it after it was explained to them. Example Imagine that an American worker who devotes all his time to soybean production can produce up to 100 tons of soybeans per year. And if he devotes all his time to steel production, he can produce up to 4 tons of steel per year. In contrast, the corresponding figures for a Chinese worker are 30 tons of soybeans or 3 ton of steel. A layperson could reason: An American worker is literally more productive than a Chinese worker at everything. So why aren't we simply producing all of our own soybeans and steel? Instead, we're doing the foolish thing of importing steel from China! This reasoning is ""common sense"". It is also wrong. Although the American worker is ""better at everything"" (we say he has the absolute advantage in producing both soybeans and steel), the Chinese worker has the comparative advantage (CA) in producing steel. This is because by producing 1 ton of steel, the American forgoes 25 tons of soybeans, while the Chinese forgoes only 10 tons. And so, by the principle of CA, the American should focus on producing soybeans and the Chinese on producing steel. The two can then trade to mutual benefit. Numerical example: Say that without trade, the American spends a quarter of his time producing steel and the rest producing soybeans. The Chinese spends half his time on each. Hence: But they can do better by specializing and trading. The American, whose CA is in soybean production, should specialize in soybeans. And the Chinese, whose CA is in steel production, should specialize in steel. The American can then trade, say, 20 tons of soybeans for 1.2 tons of steel. End result: Comparing Scenarios #1 and #3, we see that with specialization and trade, both the American and Chinese workers are strictly better off.  Remarkably, each gets to consume more of both soybeans and steel than they did without trade. Thus, even though the American is ""better at everything"", the principle of CA offers a powerful rationale for why he should still import steel from China and be ""dependent"" on the Chinese worker."
Why is the oil future price negative?,"I'm answering from my memory of Reddit comments like blitzkrieg9
 and AdmiralAkbar1. Don't hesitate to edit this post to link to them, but I can't remember where exactly I saw them. And I haven't just copied and pasted – I've rewritten out typos.  Unprocessed crude oil is full of toxic and nasty stuff, and if not properly contained, many dangerous gasses like benzene or hydrogen sulfide will just evaporate out of it. You definitely don't want to be anywhere near an open swimming pool full of crude oil. You can't really do anything with it at home, and if you dump it, you go to prison. You get nasty byproducts that you'll have trouble selling in the tiny quantities you produce, but which are illegal to dump. But hey, you might get a gallon or two of usable gas out of it! Crucially, this is the price for crude oil which still has to be refined into usable products, and cannot easily be disposed of. The negative price reflects the costs associated with storing the stuff indefinitely. And you can't turn off producing wells like a tap. It takes time to shut them down without ruining them, so that they can be turned on again. Look how long it took to get Kuwait up and running again after the first Gulf War, when the retreating Iraqis destroyed wells. It's simple to shut in natural gas by turning the valve, as natural gas has a natural pressure. Oil does not have natural pressure. Once oil's shut in, it's not easily ""turned back on"". It's part of what causes wild price swings as supply can't move as fast as demand. The leases for oil and gas in America have ""shut-in clauses"" where after being shut in for a certain period of time (usually one year or so, but they vary) the lease holder has to go back to the mineral owner, to re-lease. Again with gas, operations can go and turn on a gas well for a month, then turn it back off just to get enough production time to avoid the shut-in clauses. You can't do this with oil.  Even old stripper wells need about $18-20 to pay to keeping them operating. Even if you can stop extracting oil, the oil producing countries are often in a sort of Mexican stand-off. If you produce too much, oil drops and you lose money. If you produce less, the other countries may produce more and you lose money. A futures contract is simply a contract to buy something in the future, like next month or next year. Some futures contracts ""settle to cash"". This means we determine the price when I bought the contract and the actual real-world price on expiration date, then one of us pays the other person the price difference.  But oil and most commodity contracts settle for the actual product! If you don't close out your soybean contract, your exchange calls you asking what you'd like to do with your soybeans that are waiting for you in Kansas City! Same with oil. Tankers are showing up in Houston with millions of gallons of oil and somebody needs to take delivery. Nobody can, because all the storage facilities are already full. So people are literally paying you over $30 a barrel for you to possess oil! You're referring to a specific contract for specific delivery to an oil pipeline in Oklahoma on Apr 21 2020. That is the crux of the issue. The supply vs demand kerfuffle is very localized at a major crude oil pipeline junction in middle America. This is unprecedented. But it is a LOCALIZED phenomenon.  What's happening now is what happened in Trading Places when the real report comes out: people realized that they overpaid for their futures and are selling to try and recoup some of their losses. There's less demand for oil right now, there's less hope of more demand for oil in May, and there's little storage space left. Low demand + high supply = low prices. A few days ago, the settlement price for a barrel of oil for futures contracts with May settlement dates was locked at ~\$18 a barrel. People expect it to be way lower than \$18 next month. So they're trying to sell off their futures contracts. It's going negative because they're literally paying you to take it off their hands if you have room for it. Although they know it'll be a loss, they figure it's less of a loss than holding onto worthless oil that they can't store."
Fundamental equations in economics,"Instead of proposing specific equations, I will point to two concepts that lead to specific equations for specific theoretical set ups: A) Equilibrium
The most fundamental and the most misunderstood concept in Economics. People look around and see constant movement -how more irrelevant can a concept be, than ""equilibrium""? So the job here is to convey that Economics models the observation that things most of the time tend to ""settle down"" -so by characterizing this ""fixed point"", it gives us an anchor to understand the movements outside and around this equilibrium (which may be changing of course). It is not the case that ""quantity supplied equals quantity demanded"" (here is a foundational equation)   $$Q_d = Q_s$$ but it is the case that supply tends to equal demand (of anything) for reasons that any economist should be able to convincingly present to anyone interested in listening (and deep down they all have to do with finite resources). Also, by determining the conditions for equilibrium, we can understand, when we observe divergence, which conditions were violated.  B) Marginal optimization under constraints
In a static environment, it leads to the equation of marginal quantities/first derivatives of functions.
Goods market: marginal revenue equals marginal cost.
Inputs market: marginal revenue product equals marginal reward (rent, wage).
Etc. (I left ""utility maximization"" out of the picture on purpose, because, here first one would have to present what this ""utility index"" is all about, and how crazy we are (not), by trying to model human ""enjoyment"" through the concept of utility).   Perhaps you could cover it all under the umbrella ""marginal benefit equal marginal cost"" as other questions suggested: $$MB = MC$$ Economists live in marginal optimization and most consider it self-evident. But if you try to explain it to an outsider, there is a respectable probability that he will object or remain unconvinced, instead usually proposing ""average optimization"" as ""more realistic"", since ""people do not calculate derivatives"" (we don't argue that they do, only that their thought processes can be modeled as if they were). So one has to get his story straight about marginal optimization, with convincing examples, and a discussion about ""why not average optimization"". In an intertemporal setting, it leads to the discounted trade-off between ""the present and the future"", again ""at the margin"" -starting with the ""Euler equation in consumption"", which in its discrete deterministic version reads $$u'(c_{t})=\beta(1+r_{t+1})u'(c_{t+1})$$ ...and one cannot avoid the theme of utility, after all: $u'()$ is marginal utility from consumption, $0<\beta<1$ is a discount rate and $r_{t+1}$ is the interest rate (don't consult wikipedia article on Euler's equation in consumption, the concept behind it is much more generally applicable and foundational than the specific application that the wikipedia article discusses).   Interestingly, although dynamic economics are more technically demanding, I find this more intuitively appealing since people seem to understand way better ""what you save today will determine what you will consume tomorrow"", than ""your wage rate will be the marginal revenue product of all labor employed"".  "
Why do economists disagree so much?,"Some areas of economics have more consensus and predictive power than others. Most economists would agree on the effects of trade barriers, could fairly accurately forecast the effects of a price change given a good demand estimate, would come to the same conclusion about the effects of allowing a merger between two large competing firms, know how asset prices will react to a change in interest rates, etc. The effect of a fiscal stimulus is one of the most complicated questions in economics because you are asking about the effect of stimulating a system with millions of moving parts (people and firms) and many dimensions (consumption/saving, employment/work, trade, investment, innovation, ...). This is a long way from the simple, closed systems for which natural science is able to give sharp predictions. In fact, when you look at parts of the natural sciences that deal with similar levels of systemic complexity, the overall (lack of) predictive power looks similar to that in economics: The other thing to note is that fiscal stimulus is a highly politicised topic and economists often do a (frustratingly) poor job of keeping politics out of the debate. If you read the economic literature, you will find a story similar to that for your car crash: any outcome is possible but some look more likely, depending on the exact circumstances. Given this ambiguity, politicians of different persuasions have no difficulty finding an economist willing to ignore this nuance and take a politically expedient position (just as conservative politicians can always find scientists willing to play down anthropogenic global warming). But that is less a failing with economics than with economists."
Criticism of Math in Economics,"I find that the essay ""The New Astrology"" by Alan Jay Levinovitz (an assistant professor of philosophy and religion, not an economist) makes some good points. ...the ubiquity of mathematical theory in economics also has serious downsides: it creates a high barrier to entry for those who want to participate in the professional dialogue, and makes checking someone’s work excessively laborious. Worst of all, it imbues economic theory with unearned empirical authority. ‘I’ve come to the position that there should be a stronger bias
  against the use of math,’ Romer explained to me. ‘If somebody came and
  said: “Look, I have this Earth-changing insight about economics, but
  the only way I can express it is by making use of the quirks of the
  Latin language”, we’d say go to hell, unless they could convince us it
  was really essential. The burden of proof is on them.’ The essay also makes a (more or less adequate—which, I leave up to you) comparison with astrology in ancient China to show that excellent math can be used to prop up ridiculous science and grant status for its practitioners."
Why do Australian milk farmers need to protest supermarkets' milk price?,"I would like to contradict Kenny LJ explicitly on his claim that ""the free market works"". It only works if the market is actually free, i.e. competition exists. Which, you can argue, is not the case in the milk market. What you are looking at is a cartel. (Excuse me for citing numbers for the German market, not Australia, as I am more familiar with that market and have numbers ready that I don't have for Australia. I am quite confident, however, that while the ratios might differ a bit, the overall structure will be the same.) There are about 100.000 milk farmers in Germany, producing about 28 million tons of milk per year. As Mick pointed out, however, they are in the farming / production business, not in the transportation / processing business, so they have to sell their milk to dairies. There are just over 100 dairies in Germany. And most of them have a monopoly on their region, meaning they're the only dairy a farmer could sell to. Those dairies sell the processed milk to supermarket chains and other retailers / business consumers. About 60% of all the milk produced is sold by less than a half-dozen major supermarket chains. Milk is a highly regulated food product, meaning that one dairy's milk is effectively the same as any other. There is very little competition in quality. The dairies don't even have ""a product"", the milk ends up packaged in the supermarket brand: Woolworths has bumped the cost of its own-brand milk by 10 cents a litre, but Coles has refused to follow suit because it might burden customers. So the only competition is in the price. Whoever sells cheapest. Customers buy the milk where it's cheapest (supermarket), because they simply do not have any realistic option to go for a ""premium"" milk product that's provided by middle- or small-sized, or regional, producers at a sustainable price with a transparent premium for the producer as well. It's just not available. There are ""bio"" brands, but that label is only saying something about how the cattle is kept and fed -- the price dynamics still apply: The supermarket buys where it's cheapest (dairy), and the dairy tells the milk farmers how much they are willing to pay, take it or leave it. A farmer that is not willing to sell his milk at the price the dairy dictates (even if it's at a loss), will not get his milk to market at all, but still have running costs to pay. Which means he'll be out of business pretty quick, forced to sell his business, land, and lifestock to some bigger farmer that is able to use scaling effects to produce at a lower price. You could claim that this is the result of an oversaturated market, that less milk on the market would drive the prices up. That is correct, in a way, but the market is oversaturated because producing large quantities of milk as cheap as possible is the only bargaining chip the producers have in a market where literally no-one is going to a farmer to buy produce. Rising prices would not equate to rising margins for the farmers, as they simply have no control over the supply chain. Concentration of power at the retailer end, a lack of competition in quality, and the inability of the producer to cut out the middle man (dairies) or create a meaningful ""premium"" market means the retailer is dictating the price, to the point where producer's margins are minimized and the only way to be a profitable producer is by economics of scale -- large combines swallow medium and small-sized producers, and the retailers are able to push the price even lower, repeating the cycle. This is beneficial to the customer insofar as that the product (milk in this case) is available at every supermarket and extremely cheap. But it comes at a high cost, destroying middle- and small-sized milk farms (and dairies!) in a race to the bottom. Which also includes a race for the most milk-efficient cattle, to the exclusion of all other considerations. Refuting Kenny LJ's wholesale dismissal of regulation: Suppose instead that these farmers instead benefited from low grain prices and enjoyed profits of 20 cents per litre of milk sold. Should consumers then have stood outside supermarkets demanding that milk prices be lowered by 10 cents per litre? The consumers wouldn't have had to, as the supermarket chain would already, preemptively, have realized the opportunity to lower prices (to stay competitive to the other supermarket chains who would have done the same). Addendum: In answer to a comment by @horns, this is a matter of official record and investigation. Rewe was sentenced to 20.8 million Euro antitrust fine in May 2013. Spar was sentenced to 3 million Euro antitrust fine in November 2014. (Just two I was able to duckduckgo ad-hoc.) Quoting the German Antitrust Office: Our investigations have shown that the contracts between milk producers and dairies in Germany have long periods of notice and duration. In addition, farmers in Germany are generally obliged to supply the milk they produce exclusively to their respective dairy. There is virtually no possibility for them to switch to another dairy. This is a problem for farmers and hinders possible newcomers on the dairy side or dairies wishing to extend their activities. Another widespread practice is that the price of raw milk is set only after delivery and is based on reference prices and market information systems. Also from that article: The Bundeskartellamt questioned 89 private and co-operative dairies, which in 2015 procured approx. 30.9 million tonnes of raw milk. This is equivalent to around 98 % of the total milk supply volume. The authority's investigations have shown that in 2015 97.8% of the volume of raw milk covered by the investigations was sold subject to exclusive supply obligations. In addition, contracts for more than half of the raw milk supply volume can only be terminated with at least two years' notice. There is no effective competition."
Seminal papers that later were proven to contain errors,"My favorite example is the initial formulation of Arrow's impossibility theorem in the first edition of Arrows' ""Social Choice and Individual Values"" (1951). In the first edition, Arrow claimed that, together with 4 other conditions, the following domain condition ``The domain $\mathcal{D}$ is sufficiently extensive so that there exists at least one free triple of
alternatives. (A triple is called free if all conceivable
combinations of individual orderings of this triple actually occur in
$\mathcal{D}$"" (rewording from Blau(1957)) implied that there exists no social welfare function $S : \mathcal{D} \rightarrow \mathcal{R}$, where $\mathcal{R}$ is the set of all possible orderings (i.e. complete and transitive binary relations) over the set of alternatives $A$. This was later showed to be false by Blau (1957) The Existence of Social Welfare Functions"" Econometrica Vol. 25, No. 2 (Apr., 1957), pp. 302-313 who provided a counter-example. Blau also showed (among other things) that the theorem could be corrected by replacing the above domain condition by the following condition Universal domain : the domain $\mathcal{D}$ of the social welfare function contains every possible profile of preferences over the set
of alternatives $A$ (with $|A| \geq 3$). Arrow later corrected this mistake in the second edition of Social Choice and Individual Values (1963), and the formulation of Arrow's theorem using the Universal domain condition has now become standard. This being said, the initial error in the first edition of Arrow's book was rather minor, and the solution proposed by Blau does not reduce in any strong sense the importance of Arrow's result and approach. Intuitively, the conclusion remains that on a vast domain of relevant economic problems, no social welfare function satisfies a set of rather basic and reasonable conditions. So this might not be exactly the kind of errors you were looking for (definitely a seminal paper though!), but I like the example so much I could not resist posting it . If such brilliant people like Arrow make these kind of mistakes, I guess it takes a little bit of the pressure off for everyone else?"
How do economists explain why people contribute to Wikipedia?,"Voluntarily contributing to a public good (such as Wikipedia) is a strong social norm. The tendency to follow such norms even if this is costly in the short run has developed over humans' evolutionary history, as in small to medium-sized hunter-gatherer communities this behavior was adaptive, e.g. due to reputation effects (""community enforcement""). This evolved behavioral tendency has become engrained in ""social"" preferences exhibiting a taste for strong reciprocity or for following ""moral"" imperatives. In modern societies it therefore also shows up in contexts where it is actually maladaptive, as under anonymity or in one-shot interactions without reputational benefits. The same arguments explain why experimental subjects cooperate in the prisoner's dilemma and reciprocate in the trust game, why many humans engage in costly activities to decrease their carbon footprint (even though this has a negligible effect on climate change), and several other non-selfish actions."
What is structural estimation compared to reduced form estimation?,"Structural estimation is a term coined by the Cowles commission which at the time seems to have been dominated by Haavelmo, Koopmans and a few others. The motto of the Cowles commission (after 1965) was: ""Theory and Measurement"". The phrase represents the underlying rationale of structural modelling, that measurement cannot be done without some kind of theory. To my knowledge, the phrase was first used by Koopmans in ""Identification Problems in Economic Model Construction"": Systems of structural equations may be composed entirely on the basis
  of economic ""theory."" By this term we shall understand the combination of (a) principles of economic of behavior derived from general observation--partly introspective, partly through interview or experience--of the motives of economic decisions,(b) knowledge of legal and institutional rules restricting individual behavior (tax schedules, price controls, reserve requirements, etc.), (c) technological knowledge, and (d) carefully constructed definitions of variables. Structural equations are then equations that come from an underlying economic (or physical, or legal) model. Structural estimation is precisely estimation which uses these equations to identify parameters of interest, and inform counter-factuals. Importantly, these parameters are usually taken to be invariant, and therefore counter-factuals taken from their estimates will be completely ""correct"". Counter-factuals were the main unit of interest to the Cowles commission. Koopmans also discusses reduced form estimation: By the reduced form of a complete set of linear structural equations... we mean the form obtained by solving for each of the dependent (i.e., nonlagged endogenous) variables, and in terms of transformed disturbances (which are linear functions of the disturbances in the original structural equations). The linearity is an artifact of the times (this was published in 1949!) but the point is that reduced-form equations are equations written in terms of economic variables which do not have a structural interpretation as defined above. So, a linear regression will be a reduced-form of some true structural model, because linear regression usually does not have a true economic interpretation. This does not mean that reduced form equations cannot be used to identify parameters in structural equations - in fact this is precisely how indirect inference works - just that they do not represent a deeper model of the data generating process. Reduced forms can (in principle) be used to identify structural parameters, in which cased you are still performing structural estimation, just through using the reduced form. Another way to look at this is that structural models are generally deductive, whereas reduced forms tend to be used as part of some greater inductive reasoning. For a comparison of this kind of Cowles commission structural modelling with Rubin causal modelling, check out this awesome set of slides by Heckman. For other resources I'd check out more of what Koopmans wrote, the book Structural Macroeconomics by DeJong and Dave, these lecture notes by Whited, this paper by Wolpin (written for the Cowles Foundation, in honour of Koopmans) and a response by Rust. Addendum: A simple example of reduced form and structural models. Suppose we were looking at data on the prices, $p_t$ and quantities, $q_t$ produced by a monopolist. The monopolist faces a series of unknown costs in the future, and a linear demand curve (this would really have to be justified). Let's say the $\hat q_t$ and $\hat p_t$ we observe are measured with some kinds of mean-zero error, $e_t$, and $v_t$ Noting that both price and quantity seem to be associated with changes in cost, a reduced form equation for this model might be:
\begin{align}
\hat q_t &= \gamma - \lambda c_t + \epsilon_t\\
\hat p_t &= \alpha + \beta c_t + \nu_t
\end{align}
Because this is a reduced form model, it needs no justification other than that it might work empirically. On the other hand, a structural model would start by specifying the demand curve (again to be strict this should start at the level of individual utility), and the monopolist's problem: \begin{align}
\text{Demand curve: }&p_t=a-bq_t\\
\text{Producer's problem: }&\max E\left[\sum_{t=0}^\infty\delta^t (p_t-c_t)q_t(p_t)\right]\\
\text{Measurement equations: }&\hat q_t = q_t + e_t\\
&\hat p_t = p_t + v_t
\end{align} From this further structural equations could be derived (structural because they are still representative of principles of economic behavior): \begin{align}
\hat q_t&=\frac{a-c_t}{2b} +e_t\\
\hat p_t&=\frac{a+c_t}{2} + v_t\\
\end{align} This is a case where a reduced form equation will have a meaningful structural interpretation, as consistent estimates $\hat a$ and $\hat b$ can be formed: \begin{align}
\hat a&= 2\hat\alpha \\
\hat b&= \frac{1}{2\hat \lambda}
\end{align} Another case of identification of structural parameters from reduced forms is the logit model in the case of valuations with extreme value errors (see McFadden (1974)). In general it is unlikely a given reduced form model will have a structural interpretation."
Is zero inflation desirable?,"The optimal level of inflation is very debated with unclear answers. There are many reasons, and a great answer would be very long. It should also distinguish between expected inflation and surprises. I'm not going to do any of this, but giving you three reasons for a desirable positive level of inflation. This list is of course incomplete, also there are many reasons against too high inflation. It is really unclear to economists why it is happening, but nominal wages seem to be downward rigid. It appears to be a behavioral thing (and might not be true at all, see Barro (1977)), but it appears that in crises, once presented with the choice of more firings or reduction of wages, most firms/workers decide to not cut wages but rather respond to the slump with increased separations. To the extent that we believe this lack of reduction in wages is suboptimal, the central bank can enforce a reduction in real wages through increased inflation rates, and thus preventing separations to some extent. This argument is based on Keynesian theory. Keynes claimed that the marginal propensity to consume out of income is smaller for rich households (and indeed, we do find this in the data  to some extent). Unexpected inflation is similar to redistribution from creditors to borrowers, as long as debt contracts are not indexed to inflation. To the extent that poor people consume more out of this unexpected wealth shock and than rich people decrease their consumption, this redistribution will lead to an increase in aggregate consumption. This argument was most prominently brought forward by Krugman around 2008. In recessions, you want to be able to decrease nominal interest rates in order to punish households and firms for ""holding cash"" and incentivize them to spend it instead. If you start with low (say, 2-3%) nominal interest rates during normal times, you don't have a lot of space to cut back nominal interest rates during the crises. If, instead, you would have higher nominal interest rates (Krugman argued for around 8%) and relatedly higher inflation during normal times, you could easier cut back nominal interest rates during busts and stimulate the economy."
What is the economic purpose of increasing the minimum wage?,"What makes you think that economists are so aligned against the minimum wage? Take a look at the IGM Forum that polls top academic economists. There is substantial disagreement about the effects and welfare implications of a minimum wage hike. Also, it's worth pointing out that besides empirical studies (like Card and Krueger), there are in fact theoretical reasons why a minimum wage might be welfare improving---see Lee and Saez (2012). (I'm just saying they exists, not commenting on the strength of the results.) In the Handbook of Public Economics, chapter 7, ""Optimal Labor Income Taxation"", they say this about that paper: Lee and Saez use the occupational model ... with endogeneous wages and prove two results. First, they show that a binding minimum wage is desirable
  under the strong assumption that unemployment induced by the minimum wage hits the
  lowest surplus workers first. ...  Second, when labor supply responses are along the extensive margin only, which is
  the empirically relevant case, the co-existence of a minimum wage with a positive tax rate on low-skilled work is always (second-best) Pareto inefficient.A Pareto improving policy
  consists of reducing the pre-tax minimum wage while keeping constant the post-tax
  minimum wage by increasing transfers to low-skilled workers, and financing this reform
  by increasing taxes on higher paid workers. Importantly, this result is true whether or not
  rationing induced by the minimum wage is efficient or not.This result can also rationalize
  policies adopted in many OECD countries in recent decades that have decreased the
  minimum wage while reducing the implicit tax on low skill work through a combination
  of reduced payroll taxes for low skill workers and in-work benefits of the EITC type for
  low skill workers."
Why is healthcare so expensive in the USA?,"There are several reasons for that, following Papanicolas et al (2018): High regulatory and administrative burden. US has one of the highest regulatory and administrative burdens. US healthcare market might be unregulated in terms of prices and range of services and procedures you can get but it is actually quite heavily regulated when it comes to licensing, building codes etc. in addition, US does not have unified administration system for insurance which also creates a lot of admin costs. High costs of pharmaceuticals (drugs) and medical equipment. High labor costs. This is actually one of the more important reasons since US medical labor costs are extremely high. US doctors and nurses are payed much more than in many other advanced countries. These are one of the primary reasons listed in literature, there are also some other issues that contribute to this, such as excessive testing due to US being very litigious country and hospitals wanting to avoid liability (see Brateanu et al 2014). A serious concern is also high market concentration in healthcare industry making it not very competitive which is also a reason why prices are very high (see Reinhardt et al 2004)."
Did real incomes drop significantly since the 1950s?,"Most of the US real earnings data which  go only as far as mid 60s. According to the statista data presented in this article by world economic forum the evolution of real hourly earnings in the US for production and non-supervisory workers looked like this:  If we extrapolate to 50s then the real earnings are now higher overall. However, this being said the real wages first peaked in the 70s at $\\\$23.24$ then fallen to about $\\\$20$ and recently in 2019 they finally again reached the previous peak from the 70s. Hence, we cannot really say they dropped significantly but rather they tended to stagnate. There is no single agreed upon explanation for this. Some authors argue this is due to technological change and decline in demand for low skill workers (see Fernandez 2001), other arguments include declining union membership (see David,  Katz, and Kearney. 2006),  or international competition (see here). Others argue that CPI overstates inflation and that this can be also due to increase in non-monetary benefits such as health insurance (see here).  A good explainer is also provided by this brookings article."
Is a universal basic income possible in the United States?,"This is a good question. To be concrete, I think it's easier to pick a single number - this is arbitrary, but I'll go with the figure of $10,000 offered in the proposal by Charles Murray (one of the most prominent conservative supporters of a universal basic income). I'll assume that this is offered to every adult in the US age 18 and over, expanding slightly on Murray's 21 and over proposal. This would be about 85% of the average poverty level for a single individual, just above 50% of the poverty level for a single parent with two children, and (at $20,000) about 85% of the poverty level for a family of four with two parents and two children. The direct budgetary cost of this program, of course, is easy to calculate: it is the number of Americans age 18 and over, times \$10,000. There are about 245 million American adults currently, making this cost \$2.45 trillion. This compares to total federal government expenditures that are currently about \$4 trillion. (See this NIPA table for some figures.) Remarkably, the sum of ""government social benefits"" and ""grants-in-aid to state and local governments"" (the latter of which is almost entirely grants for social programs like Medicaid) is currently about \$2.4 trillion. Hence, to a first approximation, we could say that the federal government could afford the $10,000 UBI given its current budget if it eliminated all other existing transfers. How plausible is this? Not very. More than 75% of the federal government's transfer spending is on three programs: Social Security, Medicare, and Medicaid. This spending is overwhelmingly concentrated on the elderly, with some also on the disabled (SSDI and Medicaid) and children (Medicaid). Taking away these programs and replacing them with the basic income, which is distributed evenly throughout the adult population, would surely leave these groups receiving far less in total than they currently do. This would be politically near-impossible to push through, and any sudden change would be dubious on policy and moral grounds too. (Even if one doesn't like the distribution of transfer spending embedded in these programs, millions of people have planned their lives around it.) This is just one case of a basic point - which is that unless one plans to achieve massive savings through improved administrative efficiency or improved work incentives, any reallocation of the existing transfer pie will involve offsetting winners and losers. Since existing transfer recipients (aged, disabled, poor children, etc.) are targeted for a reason, such a shift may be quite painful. Moreover, any improvements in administrative efficiency are unlikely to be large enough to save much money; the more likely source of savings would be improved incentives, particularly relative to the dysfunctional disability system and perhaps current overspending on health care. But I haven't seen a case made that these would be anywhere near large enough to make a UBI work within the current budget without large losses to some existing party. On a more positive note, I should mention that the headline cost of the UBI may not be quite as bad as it looks. Since the current tax-and-transfer system embeds very large implicit marginal taxes on some beneficiaries, it would be possible to replace these with higher explicit marginal taxes to claw some of grant back, without incurring additional distortions at the margin. That said, these high marginal taxes are heavily concentrated in certain segments of the population (e.g. single parents with children). The US tax-and-transfer system currently makes heavy use of tagging, which saves money at the expense of potentially perverse social incentives (conservatives have complained about incentives for low-income parents to avoid marriage for years). Doing away with tagging could limit these particular bad incentives and concentrated high marginal rates, but necessitate moderately higher marginal rates throughout the population. Finally, if it is not realistic to eliminate existing transfers, we can think about how difficult it would be to establish a UBI through increased taxation. If (given the substantial existing transfers to the elderly) we limited the \$10,000 to adults below age 65, we are down to a population of about 200 million and cost of \$2 trillion. Since personal consumption expenditures are currently about \$12 trillion, (unrealistically) assuming a constant tax base these \$2 trillion could be raised through a 24% VAT with 70% coverage. This would be quite high, but not totally out of line with international norms. Furthermore, since current GDP is about \$17.5 trillion, and additional \$2 trillion in transfer spending (again unrealistically assuming constant GDP) would increase the US's overall tax intake as a share of GDP by 11.5 percentage points. This would make the US similar to the typical European country, still below states like Denmark, Sweden, and France with the highest taxation."
Why does Taiwan dominate the semiconductors market?,"This is because semiconductors have economies of scale over extremely large number of units produced. Economies of scale mean that the more you produce the cheaper production gets. Many firms will have economies of scale over some range of production, but it is very rare to have economies of scale over very large quantities. In such cases you will typically observe very heavily concentrated industry or natural monopolies. This is usually due to high fixed cost. Semiconductors are one of the industries with economies of scale over very large numbers of units. This is because a modern semiconductor fab/foundry costs around 20 billion dollars (as reported by the Economist). A typical chip manufacturer will have several foundries not just one. Hence, chip making is not something you can simply do in your garage like, let's say, building computers. You need to produce an extremely large volume of chips just to break even and the more you produce the more profitable you get. Of course, at some point there would be diminishing returns but when it comes to semiconductors that level of production is hard to reach. As to why it is Taiwan and not some other place there are several reasons. Some important ones:"
"Why do we need to ""get the economy moving again""?","Previously, Kate preferred spending \$50 on food at Alice's restaurant (rather than cook her own food). And Alice preferred spending \$50 getting her hair cut at Kate's (rather than cut her own hair). That Kate must now cook her own food and Alice cut her own hair means that value has fallen (where value is broadly defined as the degree to which individuals' desires are satisfied). (Note though that the fall in value in the above example is probably not very large. In most other examples and cases, the fall in value will be much larger—consider for example the children who no longer go to school and are now supposed to be learning at home.)  You write: It seems to me that all the money that hasn't been spent in restaurants and hairdressers during the COVID-19 pandemic hasn't gone anywhere, it's still sitting in the bank accounts of the people who would otherwise have spent it.  This expresses a common misconception among non-economists. Money is not the source of value.† Value is the satisfaction of people's desires—and such value arises through production and consumption. If everyone is sitting at home and not producing anything, then no desires are satisfied, no value is produced, and all the money sitting in banks is worthless. †The mercantilists of the 17th and 18th centuries likewise believed that gold and silver were the sources of value and that one's country should try to accumulate as much gold and silver as possible. Adam Smith pointed out that this was mistaken--value comes from production and consumption, not precious metals, pieces of paper, or numbers in a bank account."
"If someone goes for a haircut, does it increase GDP?","Existing answers are correct, yes, you are increasing GDP. GDP is a crude measure of how much is spent during a year. This is used as a proxy for living standards (the thinking goes: if an economy is spending more, there's more economic activity and it's probably doing better). But the logic doesn't always hold, for example: Consider a scenario where you break my window and I break yours, and we both pay the glazier \$100 each to fix our windows. Our living standards are unchanged, despite the fact that our actions increased GDP by \$200!"
Are there any non-paywalled reputable economics journals?,"Theoretical Economics (TE) and Quantitative Economics are two open access, peer reviewed journals. The former I know is of very good quality, arguably the top ""field journal"" in microeconomic theory. "
Why do celebrities get high wages?,"If you ask yourself how much a potential employer would have to pay you to convince you to work for him, the answer is probably something like ""at least as much as I could earn by doing the same job for another employer"". So, provided there are several employers competing to hire workers, you can think of employers as bidding against each other for the best employees. But how much will firms bid? Each employer would be willing to pay up to the difference between its profit if it hired this particular worker and its profit if it took the next best alternative.  This is why the supply of alternative employees matters. If there are many people capable of doing the same job then a firm's profit if it hires you will be almost the same as the profit if it hire someone else. So why should it 'bid' a lot to hire you? An unskilled factory worker who asks his employer for a significant pay rise will likely be refused because virtually any worker doing the same job would have equivalent productivity. Celebrities are a different story. Just as factory workers are hired to produce goods for the factory owner, actors are hired to produce movie ticket sales for the studio. But viewed in this light, Tom Hanks or Angelina Jolie are not close substitutes for as yet unknown actors at all. Indeed well-known actors are considerably better at producing ticket sales than unknown talent. Studios are willing to bid a lot to attract top actors because having a star in your cast is worth millions of dollars in extra ticket sales."
What benefits do governments receive from not eliminating debt?,"Most of the same considerations apply to countries as apply to businesses and people, plus a couple of extra cons Pros of Being Debt Free Cons of Being Debt Free There is big difference between having no debt and having no net debt.  In the former case, you do not borrow any money and that is rare.  In the latter case you have the money but choose to borrow instead.  Many individuals do this by spending on credit card even though they have money in the bank, or not paying off all of their mortgage because the interest rate is good (so they can earn more with their money elsewhere). For governments there are extra benefits from having debt (even if you can pay it off). Perhaps the best way to understand this though is to look at a few notable examples, past and present. US 1836 If you don't set interest rates, someone else will The Federal Reserve was only established in 1913, prior to that the US had an uneasy relationship with the concept of central banking. It seems the founding fathers were against central banking, and it wasn't until Alexander Hamilton that ""First Bank of the United States"" was created in 1791, mandated to last for 20 years, after which it's mandate was not renewed. Second Bank was established in 1816.  Andrew Jackson was strongly against central banking (and banking generally!) and so when he came to power in 1832 he pulled the state money out of the bank.  The bank countered by tightening money supply to push the economy into recession.  Andrew Jackson held out and paid off the entire national debt by 1836.  The Second Bank did not have it's chartered renewed, and liquidated in 1838. With no debt, and no central bank the US money supply was effectively free.  Jackson also introduces the Specie Circular which required all government land be purchased in gold and silver.  The rest of the 1830 saw significant inflation and recession, generally attributed to Jackson defeating the central bank. The US government had lost control of their own economy, and in 1837 the Bank of England raise rates, forcing up domestic US interest rates, precipitating the 1837 panic.  The following years were marked by major recession. For more on this see: WDJ article on US debt and Wiki: Second Bank Norway Today Keep some debt for liquidity purposes and financial control Norway currently has around \$170bn of public debt, with a GDP of about \$500bn.  However in 1990 the government established what is now called the ""Government Pension Fund of Norway"" into which the excess income from Norwegian oil is poured.  It is an equity and bond portfolio currently estimated to be worth in excess of \$700bn. The government of Norway could choose to pay off all its debts easily, but chooses not to.  Instead they maintain issuance in sovereign debt markets in order to hold a liquid reserve to cover their daily payments.  They also mention using the money to ""develop well-functioning and efficient financial markets"".  A final consideration in their case is that their assets are abroad, and repatriating them would weaken the Krone, so there is some FX consideration here. Because of this, it is not surprising that Norway has a AAA credit rating, and that also makes it cheaper for Norway to borrow than even the US (based on 5Y CDS). For more on this see: Norway Ministry of Finance and Norges Bank Singapore Today Issue debt to give people something to invest in Singapore has had no foreign debt (ie. non-SGD) since 1995, and consistently runs with a fiscal surplus.  Despite this they issue T-Bills and Notes of various maturities consistently and simply invests the proceeds. Why?  According to the Monetary Authority of Singapore (MAS) the main objectives are: For more on this see: MAS MoneySense North Korea If you don't pay, its not really debt... This is only a semi serious one, but one country that basically has no debt is North Korea, for the simple reason that no-one will lend to them.  They do technically have debts though, including a debt to Sweden for some Volvos, but in 1984 they defaulted on them all and refused to pay anything.  It seems they have no-intention of ever paying.  I don't think I need to explain the downsides to the North Korean approach to economics. See: North Korea's Stolen Volvos"
Are there historical cases of country A printing the currency of country B for the main purpose of economic warfare?,"Very obscure historical example: From 1287 to 1295, the Danish nobleman Stig Andersen Hvide was leading a band of outlaws from the island of Hjelm supported by the king of Norway against the king of Denmark. Stig managed to kidnap expert coin makers and bring them to Hjelm, where they produced counterfeit Danish coins. This allowed Stig and his supporters to buy up all the supplies they wanted in Danish merchant cities. The heavy inflation and the problems it caused for the Danish economy made it difficult for the Danish king to afford assembling an army against Stig."
"If no one knew about inflation, would inflation take place?","In this case, I think it is best explained without any economics jargon. If your thought experiment is taken to the extreme and no one knows about the money, it would literally imply that the money that was printed never went into circulation. In this scenario, nothing happens. However, usually someone doesn't print money just to store it in a vault or use it as wallpaper. Let's assume the money is handed out to people (even if everyone thinks no one else got money). People have a tendency to spend their money to buy goods and services. Now, a large scale increase in money supply is equivalent to everyone (at least whoever receives the money) ""winning"" the lottery. However, there is a problem. What you can buy in an economy is not determined by how much cash there is, but by the productive capacity (quality of factories and workers plus availability of raw materials). Let's assume people dream of a Tesla and finally (think they) can afford it with their newfound wealth. Unfortunately, Elon's company already finds it hard to build enough cars. As soon as people try to spend the money, too many people are chasing too little goods and services. Even if companies wouldn't raise prices, they would run out of inventory.  If companies would miss the opportunity to raise prices to a price where demand meets supply, you may think prices may not change after all. However, if people really want something and have money, they will try to get it somehow (e.g. offer the lucky ones who got a Tesla more money until they are willing to sell). Bottom line is that some will be willing to pay more, and others will be willing to sell for a higher price. Eventually, if money supply grows very excessively indeed, people will realize money is so worthless that they will end up using it as wallpaper after all."
What has caused the recent 25% unemployment rate in Spain?,"Here's an explanation from Paul Krugman. You can read more about this in Krugman's book End This Depression Now!.  Since joining the Euro, Spain has experience large capital inflows—money flowing into Spain, mostly from Northern Europe. These inflows caused a boom in investment, coupled with an increase in prices of virtually everything (including labor) relative to other Eurozone countries. One consequence of this is that the recession in Spain has been exacerbated by the fact that high costs of production (especially high labor costs) made the Spanish economy less competitive so that the country wasn't able to rely on exports to substitute for the reduced domestic demand. In order to remedy this situation, Spain needs to become more competitive (i.e. for labor costs to fall relative to those in other countries). Normally, this would happen automatically: as a country exports less, demand for its currency from importers (and hence the currency's value) falls so that its products become cheaper for foreigners. However, the fact that Spain is in the Eurozone means that it can't devalue its currency—it doesn't have one of its own! Instead, Spain must rely on 'internal devaluation', i.e. reducing the wages of its workers relative to those elsewhere in the Eurozone. This is problematic because workers are typically reluctant to accept a pay cut (so-called downward nominal rigidities). Thus, the way that the economy adjusts is to have a sufficiently large share of the workforce unemployed that people are prepared to accept jobs on significantly lower wage than they might have expected in the pre-recession years. It should be noted that this line of reasoning is not without controversy. For one, a significant share of macroeconomists do not believe the nominal rigidities story."
Why aren't house prices included in CPI?,"The CPI stands for a Consumer Price Index. As in the price of things that are consumed (at a particular moment in time). Real estate prices are not the price of something consumed because they contain the value of current housing consumption but also the capitalized value of future housing consumption. As such, including house prices would make the CPI a mixture of consumption at different times, and therefore unsuitable for comparing the price of consumption bundles at distinct times. Instead, they use a purer measure of the price of housing consumption: rents.  Rents reflect the price of consuming a flow of real estate services at a moment in time. Of course, many homes are owned by the occupants and not rented. Therefore, the calculators of national accounts generate something called ""owner occupied rents"", which is an attempt to calculate what the rents would be on homes that are occupied by their owners. This measure has problems, but for many purposes is quite adequate (Crone, Nakamura, Voith (2004))."
"A law against selling any house cheaper than it was bought for, what consequences would that have?","That would be really, really bad. Any house that loses value will be unsellable, and thus virtually worthless. Most people living in such a house would be prevented from moving. They cannot sell it, since no one wants to buy an overpriced house, and they cannot afford buying another house with their capital already tied up in the current house.  A black market would appear, with people giving money back under the table. ""Buy my house for 4M SEK and I'll refund 1M SEK under the table."" I can tell you one thing it won't do. It won't stop prices from ever falling. "
How can I obtain Leontief and Cobb-Douglas production function from CES function?,"The proofs I will present are based on techniques relevant to the fact that the CES production function has the form of a generalized weighted mean.
This was used in the original paper where the CES function was introduced, Arrow, K. J., Chenery, H. B., Minhas, B. S., & Solow, R. M. (1961). Capital-labor substitution and economic efficiency. The Review of Economics and Statistics, 225-250.
The authors there referred their readers to the book Hardy, G. H., Littlewood, J. E., & Pólya, G. (1952). Inequalities , chapter $2 $. We consider the general case
$$Q_k=\gamma[a K^{-\rho} +(1-a) L^{-\rho}  ]^{-\frac{k}{\rho}},\;\; k>0$$ $$\Rightarrow \gamma^{-1}Q_k = \frac 1{[a (1/K^{\rho}) +(1-a) (1/L^{\rho})  ]^{\frac{k}{\rho}}}$$ 1) Limit when $\rho \rightarrow \infty$
Since we are interested in the limit when $\rho\rightarrow \infty$ we can ignore the interval for which $\rho \leq0$, and treat $\rho$ as strictly positive. Without loss of generality, assume $K\geq L \Rightarrow (1/K^{\rho})\leq (1/L^{\rho})$. We also have $K, L >0$. Then we verify that the following inequality holds: $$(1-a)^{k/\rho}(1/L^{k})\leq  \gamma Q_k^{-1} \leq (1/L^{k}) $$ $$\implies (1-a)^{k/\rho}(1/L^{k})\leq  [a (1/K^{\rho}) +(1-a) (1/L^{\rho})  ]^{\frac{k}{\rho}} \leq (1/L^{k}) \tag{1}$$ by raising throughout to the $\rho/k$ power to get $$(1-a)(1/L^{\rho}) \leq a (1/K^{\rho}) +(1-a) (1/L^{\rho})  \leq (1/L^{\rho}) \tag {2}$$
which indeed holds, obviously, given the assumptions. Then go back to the first element of $(1)$ and $$\lim_{\rho\rightarrow \infty} (1-a)^{k/\rho}(1/L^{k}) =(1/L^{k})$$ which sandwiches the middle term in $(1)$ to $(1/L^{k})$ , so  $$\lim_{\rho\rightarrow \infty}Q_k = \frac {\gamma }{1/L^k} = \gamma L^k = {\gamma }\big[\min\{K,L\}\big]^{k} \tag{3}$$ So for $k=1$ we obtain the basic Leontief production function.  2) Limit when $\rho \rightarrow 0$
Write the function using exponential as $$\gamma^{-1}Q_k=\exp\left\{-\frac k{\rho}\cdot \ln\big[a (K^{\rho})^{-1} +(1-a) (L^{\rho})^{-1}\big]\right\} \tag {4}$$ Consider the first-order Maclaurin expansion (Taylor expansion centered at zero) of the term inside the logarithm, with respect to $\rho$: $$a (K^{\rho})^{-1} +(1-a) (L^{\rho})^{-1} \\= a (K^{0})^{-1} +(1-a) (L^{0})^{-1} -a (K^{0})^{-2}K^{0}\rho\ln K- (1-a) (L^{0})^{-2}L^{0}\rho\ln L + O(\rho^2) \\$$ $$=1 - \rho a\ln K - \rho(1-a)\ln L+ O(\rho^2) = 1 +\rho \big[\ln K^{-a}L^{-(1-a)}\big]+ O(\rho^2)$$  Insert this back into $(4)$ and get rid of the outer exponential, $$\gamma^{-1}Q_k = \left(1 +\rho \big[\ln K^{-a}L^{-(1-a)}\big]+ O(\rho^{2})\right)^{-k/\rho}$$ In case it is opaque, define $r\equiv 1/\rho$ and re-write $$\gamma^{-1}Q_k = \left(1 +\frac{\big[\ln K^{-a}L^{-(1-a)}\big]}{r}+ O(r^{-2})\right)^{-kr}$$ Now it does look like an expression whose limit at infinity will give us something exponential: $$\lim_{\rho\rightarrow 0}\gamma^{-1}Q_k = \lim_{r\rightarrow \infty}\gamma^{-1}Q_k = \left(\exp\left\{ \ln K^{-a}L^{-(1-a)}\right\} \right)^{-k}$$ $$\Rightarrow \lim_{\rho\rightarrow 0}Q_k =\gamma\left(K^{a}L^{1-a}\right)^k$$ The degree of homogeneity $k$ of the function is preserved, and if $k=1$ we obtain the Cobb-Douglas function.  It was this last result that made Arrow and Co to call $a$ the ""distribution"" parameter of the CES function."
"From an economics perspective, what are the ramifications of a currency with fixed money supply?","FooBar is quite right that unless you expect GDP growth to stop, fixed nominal supply currencies will lead to deflation. A moderate degree of currency inflation serves a number of useful functions in the economy. The most obvious are: Without inflation you miss out on these benefits. The first benefit might not seem like a big deal if you think you can simply set the rate of inflation at zero percent. But it is very hard to hold inflation constant at some target level so attempting to hit zero inflation will almost certainly result in occasional lapses into deflation, with the attendant negative economic consequences."
How to intuitively understand the 'Intuitive criterion'?,"A concise, completely informal way of putting it is this: The intuitive criterion rules-out any out-of-equilibrium beliefs that can only be correct if some player did something stupid. Below is a slightly more long-winded explanation with an informal example. In many signalling games (that is, games in which one player—the sender—can communicate information to another—the receiver), there are often a lot of implausible equilibria. This happens because the Perfect Bayesian solution concept does not specify what the receiver's beliefs must be when the sender deviates; we can therefore support a lot of equilibria simply by saying that if the sender deviates from those equilibria then he will be ""punished"" with very bad beliefs. Such punishment will usually be enough to make the sender play a strategy that would otherwise not be a best response. For example, in Spence's classic job market signalling paper there is an equilibrium in which high-ability individuals invest in education (learning is easy for them) whilst low-ability individuals do not (because they find it too costly to do so). Education is then a signal of ability. We might ask: is there also an equilibrium of this game in which nobody chooses to get an education and no information is transmitted to the receiver? The answer is 'yes'. We can support such an equilibrium by saying that a deviation in which a sender is educated causes the receiver to adopt the belief that the sender is certainly low-ability. If education has the effect of signalling low-ability then, of course, everyone is happy to play along with the putative equilibrium and not get educated. It is also clear that this equilibrium is not very plausible: the receiver knows that it is less costly for a high-ability agent to get an education than a low-ability one, so it doesn't make much sense for him to think of an education as signalling low-ability. The intuitive criterion rules out this kind of equilibrium by requiring beliefs to be ""reasonable"" in the following sense: Suppose the receiver observes a deviation from the equilibrium. The receiver should not believe that the sender is of type $t_{\text{bad}}$ if both of the following are true: Returning to the education signalling model: Suppose that the equilibrium is that nobody gets an education and that the receiver believes that a deviation to getting education signals low ability. Anticipating these beliefs, a low ability worker is made worse-off by deviating because he not only incurs the cost of the education but is then thought of as a bad type as a result. Thus, condition 1. is satisfied. Can we find some alternative belief such that the high-ability worker would like to deviate to getting education? The answer is yes: if the receiver believes that education signals high ability then this deviation is indeed profitable for the high-type. Thus, condition 2 is also satisfied. Since both conditions are satisfied, the intuitive criterion rules-out the implausible pooling equilibrium."
Why is a stock market crash so bad?,"Why FED/ECB/whatever raising interest rates is bad for stock markets? (I am aware that this is an assumption - my information could be wrong) This is because interest rates critically determine price of stocks. For example, using simplistic (but for your question sufficient) Gordon stock price model, the price of stock is: $$P = \frac{D_0}{i-g}$$ Where $D$ is dividend the stock pays, $i$ interest rate and $g$ growth rate of a dividend.
As you can see increase in $i$ will decrease the stock price. The intuition for this is that stock is nothing else, just infinite sum of discounted future dividends (which grow at some rate $g$). The higher interest rate is, the lower the present value of money that you recieve in future. For example, if I offer you \$100 in 1 year with interest rate 5%, present value of my offer is $\frac{100}{1.05} \approx 95.24$. If interest rates suddenly change to 10% suddenly the present value of the same offer (getting 100 in one year) is only $\frac{100}{1.1}\approx90.91$. As you can see interest rate critically determines present value of future cash flows. The Gordon stock pricing model is simplistic but even in more complex models you will see the same relationship, higher interest rates mean lower stock prices - ceteris paribus. Why is it so bad if the stock market crash? I mean if no body wants shares of companies A,B,C or everyone is in rush to sell shares of companies A,B,C, does it mean that the company should stop operation? A) Not every stock crash has severe consequences for wider economy. For example, Black Monday (1987) was one of the largest stock crashes in history but the recession it caused was very mild (see discussion of this period in Clarson 2007 or  see this Fed history blog). B) Stock market crash can spill over to the real economy because it impedes investment. Again using simplistic, but for your question sufficient, macroeconomic model of a closed economy (See Blanchard et al Macroeconomics an European Perspective Ch 3-5): $$Y = C +I + G$$ Where $Y$ is the output/income of the economy, $C$ consumption which we can assume to follow $C=c_0 +c_1(Y-T)$ where $c_0$ is autonomous consumption (consumption that does not depend on income), $c_1$ is marginal propensity to save (must be $0<c_1<1$ as you cannot save more that 100% of your income),  $T$ are taxes and $I$ is investment and $G$ government spending (for simplicity assume balanced budget T+G$. We can show that the good market equilibrium will be given by: $$ Y = \frac{1}{1-c_1} \left( c_0 + I + c_1 T \right)$$ As you can see if $I$ falls because investment spending falls (which could occur in the aftermath of stock market crash - when people are not willing to invest into stock which gives company money for further investments), the output/income of an economy $Y$ will fall as well. What even more because someones spending is someone else income, there will be multiplier effect, so output might drop by more than the fall in investment itself (since $\frac{1}{1-c_1}>1$). Does it mean no one would buy the goods/services produced by these companies? No necessarily, as shown above, fall in investment means some people have less income, so they will spend less. The effect here is mainly indirect. It is not that fall stock market crash would make people desire to buy products less in itself, most people probably do not even know if there is stock market crash unless they hear it in the TV, but when companies invest less then someone's income is reduced and thus naturally their spending declines as well. This being said, negative news might make people panic and decide to buy less goods and services today because they are worried about future, so there can be direct effect as well. PS: There is of course also an extra bit of an nuance to everything above, I restricted myself to simple 101 models since you stated you are not an economist. But generally more complex models would tell very similar story."
Why do people buy negative interest rate bonds?,"Some pictures and text are from Schroders, Marketwatch and other websites, but I don't remember all them. Of note, Germany first sold negative-yielding bonds in Aug 2019, €2bn worth of 30-year bonds that offer no interest payments at all. Cash is obviously most liquid, but some central banks like Japan impose negative interest rates on cash deposits, which trickles to the rate banks charge institutional investors. Then highly liquid but only slightly negative yielding government bond looks better! Because the UK government unlikely will default, bond buyers are just paying a small sum to the government to guard your money, like in a vault! Investors who buy negative-yielding bonds are betting on the value of the securities to keep rising. Although investors buying bonds with subzero interest rates are paying for the privilege to hold a bond, they can profit way above their cost if the security’s price rises. For example, if you expect a central bank to buy more assets,  bond buyers can rely on the central bank to hoover up their negative-yielding securities. In July 2020, an auction for €4 billion euros of 10-year German government bonds TMUBMUSD10Y, 0.644% sold at $-0.26%$, but at a premium price of 102.6 cents to the euro. The benchmark bund is now trading at a price of a 106.9 cents to the euro,. So  investors who bought this at auction would've gained around 4% from the price increase alone. When prices fall, most asset classes don't perform well in deflation. An exception are fixed interest government bonds! Because they fixed  their coupon and principal, they retain their value and will positive real (inflation-adjusted) return, if inflation $<$ their yield. In other words, a negative yielding bond can  positive real return if there is deflation. So you can hedge against deflation with negative-yielding bonds. Negative yields don’t mean negative income for some bond buyers. Unlike European and Japanese investors, US investors are often paid to hedge against fluctuations of foreign currencies because U.S. interest rates are much higher than in other developed markets like Europe and Japan.  Currency hedging can annualized return 3% for U.S. investors, like  American fund managers, buying negative-yielding  euro-denominated debt like European government bonds, according to Jens Vanbrabant, senior portfolio manager at Wells Fargo Asset Management. In Sep 2009, the 10-year German Bund yield at -0.6% looked unattractive to US investors.But currency hedging is priced at the difference between US and German short term interest rates, which can be positive even if a central bank cuts interest rates. Since US short term interest rates are much higher than Germany's, US investors are paid money to hedge euro exposure! With currency hedging, US investors can earn 2.2% when investing in German 10 year bonds (see Figure 2 by Schroders), higher than the 1.6% available on US 10 year Treasuries.  What other bolt holes can weather volatile global equity markets, Brexit, US-China trade war? Traders like volatility to a degree, but they try to diversify the risk so their portfolios aren’t fully exposed to the market's unpredictability. So buying notoriously safe bonds is a risk-averse strategy. You'll lose some money for that safety, but that loss can constitute ‘over-performance’ if other asset classes tumble. When risky assets sell off, negative yields on government bonds may be overshadowed by their proven ability to rally during recessions. Even at negative yields, bonds still are important. Figure 3 by Schroders  proves the 3-year rolling correlation between German Bunds and the German DAX stock market index. The two strong negative correlated, if you overlook the period between 2014 and 2017 when the European Central Bank’s (ECB) quantitative easing raised higher bond prices (lower bond yields) and higher stock prices.  Bonds  perform well when equities don't, vice-versa. In times of distress, you must reduce risk like this! If you are running for safety,  capital leave stocks to government bonds. Even if negative-yielding bonds lower your return compared to normal conditions, they can still reduce portfolio risk. You're assuming the bondholder is holding it long term, but many don't. In periods of uncertainty, demand for bonds increases, which makes their price increase. So you can profit from buying and selling government debt. If you think this  volatility will last, buy bonds now in the hopes that more investors flock to them later. You don't even need fellow investors to buy them! The central banks may be the ones to buy bonds to try to stimulate the economy. Bond buyers can take advantage of the yield curve’s slope, which still can be steep even for negative-yielding bond markets in Germany and Japan. For example, a trader can buy a negative-yielding 3-year bond and sell it after a year. Since debt prices inversely correlate with yields, all else being equal, the value of the 3-year bond should be higher than, for example, a 2-year bond. So long as shorter-term bonds' interest rates are more negative than their longer-dated counterparts,  the long-term bond's price should rise closer to maturity. But they can profit from this “rolling down the yield curve” as a short-term strategy, and must sell the bond well before maturity, because the bond will trade only at par when it expires. For example, the Swiss 10-year bond was yielding -0.1% in Jan 2009. But at 22 August 2009, it had returned 5.6%! Negative yields can still positive return! Of course, this works both ways. Any rise in yields can make you lose money. The other reasons are about speculation or managing risk. But institutions can be legally mandated to buy bonds with negative yields. Certain institutions must follow rules dictated by the banks, pensions funds or insurance companies who provide the bulk of the capital. Those rules mandate the managers to invest just in certain things like investment-grade bonds. Regulators force some clients to buy certain assets, like banks can only buy liquid assets. Liability-relative investors, such as insurance companies and pension funds, don't care the absolute return or yields from bonds. They often buy bonds to “match”  their liabilities. The present value of these liabilities is calculated in many ways, but factors in government bond yields. For example, in Sep 2009 in the euro area, insurance industry used negative rates up to the 11 year maturity point (Source: EOPIA. See data from 31 July 2019). These liability-driven market participants can buy negative-yielding German Bunds to match a future liability . Their values will move in tandem with each other.  If they don't buy negative matching assets, they'll be exposed to significant risks if interest rates fell further."
People pathologically hoard so much cash that they impoverish the entire nation,"But when people pathologically hoard so much cash that they impoverish
  the entire nation This sentence seems to imply that we should fault the rich not because they are rich, but because they do not spend their riches.   Ok, let's scrutinize this assertion, and not go into philosophical and sociopolitical arguments about inequality, justice, etc, which is a totally different discussion and off-topic on this site (although I suspect that your aunt may in the end has something like this in mind...) Who said rich people don't spend their riches? 1) Last time I heard, we are all too ready to criticize them for ""lavish life-styles"". But lavish life-styles cost a lot of money, so it appears they do spend a lot...so part of their riches becomes income for the rest. But they are still rich! 2) Yes, because most of their wealth is invested in productive economic activities (directly or indirectly through mutual funds etc). It follows that wealth is NOT ""hoarded"" - because hoarding means ""stash away wealth, don't spend it, don't invest it, just stash it away from any interaction with other humans and economic activities"". A ""Scrooge McDuck"" kind of situation. This is not what is happening in today's world, and so the specific sentence from this excerpt is simply factually wrong. Now, the fact that even without hoarding, rich people may stay rich or become richer, the fact that there are people dying of hunger while others live lavish life-styles, in other words the issue of wealth and income inequality could be certainly an on-topic subject here if, indicatively, a) It was examined as regards its effects on economic efficiency (there is growing evidence that efficiency and distribution are linked and that inequality affects negatively efficiency and economic growth, see for example this recent IMF report) or b) It was placed and studied in the context of normative economics (and ""normative"" is not a synonym for ""value judgements"" or ""opinions""). I would suggest to check this meta-thread on the matter."
"What's wrong with the ""airline marginal cost pricing"" argument?","One possible answer is that Mankiw's argument takes consumer demand for airline tickets as fixed and given. I would speculate that cheap last-minute tickets are a substitute good for regularly priced tickets, and that if enough people came to prefer them, demand for regularly priced tickets would fall and the airlines would lose revenue. In other words, instead of paying full price, too many customers would deliberately hold out for the heavily discounted tickets. This is the same reason why so many stores destroy their obsolete/stale inventory instead of selling it cheaply or donating it, and is an example of a perverse market incentive, as it leads to unnecessary waste."
Why do banks take deposits if they do not need them to make loans?,"I think there are a few separate issues here. First, semantics: if an institute doesn't let you deposit money into your account, I think we'd be hard-pressed to call it a ""bank"". This really doesn't matter to the fundamental aspect of your question, but I think it's of some use. Because a ""bank"" has to allow deposits (at least by a naive definition, if not a legal one), you're artificially limiting yourself to institutes that do so. If we broaden the scope of the question to include more than just ""banks"", we start to get to the meat of the problem. Second, the real question: why would an institute let you deposit money into their possession to begin with? I'm sure a hundred super-smart, very educated experts on the subject will give you a hundred and five different answers that are all the absolute, only truth in all the universe. But the gist of it is that people are willing to pay the bank to safeguard their money. Money in the bank is insured against theft and heavily guarded, so many people would rather have large sums of money in the bank than in their house. In return, they might pay for the service directly or through microtransactions related to the account. Many banks charge fees for various things, such as cashier's checks, notary services, or financial advice, services you might not bother with if you aren't already in possession of a checking account. Further, banks can take the money in your checking account and invest it elsewhere (an obvious form of investment being loans to other people). A single, smart person investing a thousand people's money can make enough profit to be worth their while without being directly paid a cent. Scale that up and you get large banking corporations. Banks often make money off credit card transactions, even if you make payments before any direct interest is due. They also make money off overdraft fees (a form of lending with very high ""interest"" rates), interest on the credit card (more lending), and so forth. And, of course, they hope that if you need to borrow money, you'll do it through them. Third, the side question: do institutes need deposits to make money off loans? No, they absolutely do not. Payday loan establishments don't take deposits, instead relying on high-interest, low payback period lending to make their money. Pawning an item is a form of payday loan with physical collateral: you leave the collateral with the pawn shop and get paid for it, then if you return on time you get most of your money back, with the difference being the shop's fee for the loan. Pawn shops make a lot of money through simply buying low and selling high, but they still make money off pawned items and don't take deposits. That said, you can only lend money if you have it, so having a large amount of cash sitting in your vault from deposits allows you to lend more money than if you only lend from your own finances. There's more risk involved, but when you start getting thousands and millions of account holders, the aggregate risk goes way down and you can start treating random statistical values like static rates. Because bank deposits allow the bank to loan more money, it raises the interest they earn on the loans, so it's a good incentive to allow people to deposit into bank accounts. Fourth, the question not asked: what does the quoted article mean? I'm not an expert here, but from my reading it's basically just a way of twisting around the semantics to describe how things can be treated in an opposite fashion to common convention, but doesn't actually prove the convention wrong. The article claims that lending money ""creates"" that money. This is true to a degree, but is meaningless in the big picture. When a modern bank gives you a loan, they don't actually hand you any physical thing that's worth money. They just write a number on a (probably digital) ledger that says you ""have money"". But they can only get away with that because they own assets that are worth real-world value equivalent to the amount they've ""created"", and because, in the long run, you will give them the amount of money they've lent you, balancing the sheet yourself. A bank can only get away with ""creating"" money for a borrower if they can afford to transfer that money to whomever the borrower is trying to buy something from. It does no good to have fictional money if you can't actually spend it, and you can't spend it if the bank doesn't have a commodity the seller's bank wants. In practice, the commodity transferred is often virtual money established by digital ledgers, but that commodity is very real, and very controlled, and therefore has actual, real-world value. As such, when a bank ""creates"" money out of thin air, it's doing so by spending a real-world commodity it only regains if you pay back your loan (or they sell your collateral after taking it back). There's a particularly egregious bit of word-twisting the article makes. It claims: Bank deposits are simply a record of how much the bank itself owes its customers. So they are a liability of the bank, not an asset that could be lent out. A related misconception is that banks can lend out their reserves. Reserves can only be lent between banks, since consumers do
not have access to reserves accounts at the Bank of England. It's true that reserves themselves can't be lent. That's the definition of the required reserve. But it's also irrelevant. Only 10-12½ % of deposits are required reserves (Bank of England, relevant to the article; other banking systems will have other reserve ratios). The other 87½-90 % of deposits are free to be lent out as desired to make money for the bank. Deposits are absolutely an asset for the bank to lend out in the normal sense of the word. They're also liabilities in a strict sense, but they're liabilities that earn the bank a lot of money. Imagine you loan me £1000 of tools. I owe you either the tools, in good shape, or £1000, or some equivalent combination. Additionally, I'm probably paying you a fee, say £100 a month, to borrow them. Financially, that's a ""liability"". But I'm able to use those tools to earn £5000 a month doing something like construction or automotive repair. In normal speak, the tools are an enormous asset, not a liability. I just need to make sure I can return the tools and borrow other tools if you want yours back. So while the text is ""true"", it's also completely misguided in rebuking common ""misconceptions"". In short, it's using smoke and mirrors to make you think you're wrong about something even though you really aren't. Summary Institutes don't require deposits to make money off lending, nor do they require lending to make money off deposits, but the two functions synergize very nicely with each other to create what we would typically think of as a bank. The article you're quoting is mostly just using some hand-waving semantics to explain how modern banking uses a different mindset to approach the same thing we've been doing for millennia, with virtual currencies replacing physical bases for value. But there's no epiphany behind the hand-waving; the fundamental nature of what's going on behind the layers of bureaucracy and pedantry hasn't changed since a caveman borrowed one sheep from his neighbor and gave back two sheep the next year. Note Most everything I've said is actually written in the quoted article. It talks about how a home loan that ""creates"" money is then balanced out by the bank paying the home seller's bank. It talks about gaining deposits so the bank can afford to lend more money. It talks about needing to pay interest to the central bank to maintain the assets upon which the commercial bank's finances are based. The article isn't really bad or inaccurate. It just makes a few over-reaching, semantic claims that don't hold up under scrutiny in an attempt to make you think about banking a bit differently. And those are the claims that threw you off. It also throws in the concept of growing the aggregate amount of money in the country (the ""broad money""), a process typically called ""inflation"", to maintain a stable, thriving economy. So in that sense, banks do ""create"" money (though ultimately the central bank does the creation by effectively forgiving tiny amounts of debt here and there through carefully-controlled interest rates as well as simply handing out newly-printed notes), but that's really an entirely different topic that's muddying the issue."
How do Gini coefficients correlate with the cost of higher education?,"I think this gives an answer reasonably close to what you're looking for (from the NYTimes article, ""In climbing the income ladder, location matters,"" featuring research by Raj Chetty, Emmanuel Saez, and others). The team of researchers initially analyzed an enormous database of earnings records to study tax policy, hypothesizing that different local and state tax breaks might affect intergenerational mobility. What they found surprised them, said Raj Chetty, one of the authors and the most recent winner of the John Bates Clark Medal, which the American Economic Association awards to the country’s best academic economist under the age of 40. The researchers concluded that larger tax credits for the poor and higher taxes on the affluent seemed to improve income mobility only slightly. The economists also found only modest or no correlation between mobility and the number of local colleges and their tuition rates or between mobility and the amount of extreme wealth in a region. (emphasis added) Note that the study is primarily about income and social mobility, but they also mention that tey find modest or no correlation between tuition rates and amounts of extreme wealth in a region."
Why is the spot price of electricity determined by the highest price that gets offered? Can't they scale it according to the actually offered prices?,"There are two major issues to consider here, first is the difference between a typical uniform price auction used in many electricity markets and the pay as bid mechanism you're recommending. The auction design impacts bidder behavior. I'll spare the mathematical proof, but if a bidder is uncertain about what others in an auction will bid: Second, an electricity generator will want to produce when prices are at or above their marginal costs. This means they might earn zero profits some of the time. Most methods of creating electricity, however, require expensive capital equipment and they sink these investments prior to making production decisions. Generators (can, depending on the market design) recover these fixed capital costs by selling electricity when prices are above their marginal costs. Absent periods when price are above marginal costs, a generator would never be able to cover those capital costs and would go out of business. This also creates an incentive for new firms to enter electricity markets when prices are high. If generators are able to freely enter the market (and again I will spare the mathematical proof), they should earn just enough short-run profit to cover the depreciation of their capital. Large profits in electricity markets will lead to entry, until entry is no longer profitable. What we're seeing in EU (and to a lesser degree other countries including the US) is the spike in natural gas prices is causing a windfall for generators who rely on other energy sources to produce electricity. If these windfalls are something people expect, there should be sufficient entry that long-run profits of generators are still close to zero. If the windfalls are unanticipated, they result in extranormal profits. Whether firms should be able to retain those windfalls is an equity or political issue, not one of economics. Remember, however, that investors see these high current fuel prices and will respond to them by investing in new (and likely not gas-powered) generation capacity. In the end, large profits now will lead to the entry of new generation assets which will bring prices down, even if gas prices were to remain high. If you change how generators are remunerated you can destroy that incentive."
Could eliminating all taxes and only creating new money theoretically work?,"It would not work. Historically speaking, governments that try to fund their operations primarily via the printing press experience not just inflation but hyper-inflation. Why? Well, one explanation is that without taxes nobody has any reason to want the government's currency. You can try to force people to use it, but that just encourages black marketeering. Another way to look at it is that a policy of funding the government by printing money is equivalent to funding the government entirely with taxes on holding currency. But since the tax only applies to currency, and not gold, or land, or massive stockpiles of cigarettes, people have an incentive to avoid holding onto money at all costs. No matter the current price of cigarettes, using your spare cash to buy cigarettes for barter later on will always seem like a good deal because the cigarettes are more likely to maintain their value than the cash. But if no one wants cash, by law of supply and demand the value of cash should plummet. Edit for the benefit of pedants: I am not saying that any ""inflation tax"" will automatically destroy the value of a currency. I am saying that people need reasons to hold your currency which are strong enough that they'll tolerate the inflation tax—and the larger the de facto tax on holding currency, the better people's reasons for doing so need to be. In the normal world, taxes are a key source of demand for fiat currency, take that away and the system stops working."
What are examples for the phenomenon that more (or better) information makes everybody worse off?,"Traffic reports are another type of information that potentially makes everyone worse off. A recent working paper (Wiseman and Wiseman 2019, https://drive.google.com/open?id=10B6VudYJOQB5w2UVBrYEjYc5wwzGZc8Z) shows this in a simple model of traffic.  I quote from the paper, ""...public information about road conditions tends to increase average travel time when...3) the scale of the traffic network is large."" This corresponds to their Theorem 3, where ""...in a scaled-up version of any environment with excess capacity, traffic reports increase expected travel time."" Yet another broad way of thinking about your question is in terms of monitoring structures of repeated games. A recent paper (Sugaya and Wolitzky 2018 JPE, http://economics.mit.edu/files/11455) look at imperfect monitoring in cartels and show that less transparency can facilitate collusion, making all participants better off."
How can power/electricity prices be negative?,"Yes, customers who were exposed to day-ahead wholesale market prices were paid to use electricity. It is a combination of several different factors that make the day-ahead wholesale electricity market special. Firstly, very few electricity consumers participate in it directly. So the demand side is very illiquid, with very low short-run elasticity - and it quickly becomes perfectly inelastic. Secondly, there's very little time-arbitrage. The market for electricity at 0500 and the market at 1500 are two separate markets: there are very few arbitrageurs who can buy at 0500 and sell at 1500. That's because until recently, there's been very low rewards for doing so (more on this, below). Thirdly, there are several suppliers for whom reducing supply would incur expense: so, for market-clearing purposes, they have negative short-run marginal costs. There are two ways that can happen. very inflexible conventional generators (usually nuclear) just can't turn their output up and down every hour or two, without incurring maintenance costs. generators with contracts that enable them to sell all their output at a pre-specified positive price: they would need wholesale prices to go negative by the more than that amount, to make it worthwhile for them to reduce their output. Germany now has such a high combination of those two kinds of generators, that there are times when their combined output is greater than domestic consumption, and that's when prices go negative. Fourthly, although Germany shares a grid with many other continental European countries, the opportunities for spatial arbitrage are very limited, because total German generation is much much larger than the interconnection power capacity to the rest of the grid: so that interconnection capacity strictly limits the amount of energy that can be arbitraged with other countries within a given trading hour. Finally, though it seems strange, these negative prices are a good thing. They're a temporary condition that will only last for a transition period, as electricity markets decarbonise. At the moment, we've got transmission networks and market structures that were designed for, and built around, excessively-polluting suppliers such as coal and gas. We have to stop that pollution - the cost of it way exceeds the benefits. So, the industry has to change structurally, and markets and networks will have to change too. Coal- and gas-fuelled generators have been very helpful for in-day markets, because they can rapidly change the amount they supply. So the near-absence of participation from the consumption side hasn't been a problem at all, until recently - and why the rewards for arbitrageurs were low. But now, there is an increasing number of hours when those very flexible generators no longer dominate the market. As we move to a grid that no longer has that supply-side responsiveness, we need responsiveness from new arbitrageurs, and from the demand-side. Period of negative pricing provide strong incentives for those new market participants to come forwards. And, as more and more of those new participants enter the market, the negative prices will become rarer and rarer. So this isn't just Germany. It has happened, and will happen, elsewhere too."
Partial vs. general equilibrium,"Let's put the succinct answer by @TheAlmightyBob into an abstract model: We want to model the labor market. Markets' structure assumptions: goods market and labor markets are perfectly competitive. All participants are ""too small"" economically, and they cannot affect equilibrium price through their quantities demanded/supplied - they are ""price takers"". Markets ""clear"" - i.e. prices adjust so that quantity actually supplied equals quantity actually bought. Agents assumption: There are $n$ identical workers, and $m$ identical firms, that participate in the market. Both populations are fixed. Other assumptions: a) deterministic environment, b) one perishable good produced, c) model in ""real terms"" (real wage etc, scaled by the price of the good produced). The typical firm produces according to the technology
$$Y_j = F_j(K_j,L_j;\mathbf q)  \tag{1}$$ where $\mathbf q$ is a vector of parameters. Perfect competition in the goods market, and a perishable good imply that all output produced is sold.The goal of the firm is maximization of capital returns over the choice of labor.   $$\max_{L_j} \pi_j = F_j(K_j,L_j;\mathbf q) - wL_j$$ We are modelling the labor market, so we are interested in the first-order condition $$\frac {\partial \pi_j}{\partial L_j} = 0 \tag{2}$$
and the corresponding input demand schedule $$L_j^* = L_j^*\left(K_j, \mathbf q, w\right) \tag{3}$$ Total Labor demand is $L_d = m\cdot L_j^*$. 
The labor market equilibrium assumption implies $$ L_d = L_s \Rightarrow m\cdot L_j^*\left(K_j, \mathbf q, w\right) = L_s \tag{4}$$ which implicitly expresses the equilibrium wage as a function of technology constants, of per-firm capital, and of labor supplied. In order to fully characterize the labor market, we need to derive also the optimal labor supply.   Each identical worker derives utility from consumption and leisure, subject to a biological limit of available time, $T$, and the budget constraint that consumption equals wage income: $$\max_{L_i} U(C_i, T-L_i;\mathbf \gamma),\;\; \text{s.t.} \;C_i= wL_i$$ where $\gamma$ is a vector of preference parameters, indicating the relative weight between utility from consumption, and from leisure.
This will give us individual labor supply as $$L_i^* = L_i^*(T,w, \mathbf \gamma)  \tag{5}$$ and total labor supply is $L_s = n\cdot L_i^*$. Plugging this into $(4)$  we obtain $$mL_j^*\left(K_j, \mathbf q, w\right) =n L_i^*(T,w, \mathbf \gamma) \tag{6}$$ If we stop here, we have a partial equilibrium model that examines the labor market. We have fully described the market, and the goals and the constraints of the participants in it (firms and workers), related to the specific market. We can perform comparative statics in order to see how the various components of $(6)$ affect the equilibrium wage. Among them, there is the capital-per-firm term, whose effects on wage we can also consider based on $(6)$, by treating it as varying arbitrarily.  In order to turn this model into a general equilibrium model:
a) We need to specify things about capital: who owns it/controls it/makes decisions on it. What is the objective functions of these decision makers. This will lead us to an optimal $K_j^*$ as a function of the structure we will impose here. Then, comparative statics with respect to $K_j$ will turn into comparative statics with respect to the factors that affect the determination of $K_j^*$, which may very well prove to involve also $\mathbf q, w$ and even the other parameters in $(6)$, changing in this way the comparative statics results obtained in a partial equilibrium setting.   b) We also need to take into account any macroeconomic identities that characterize this economy, something along the lines of $mY_j \equiv ...$ where the right hand side will be determined by the assumptions we make related to capital, but also, for example, by whether we will assume that the economy is closed or open, or partially open to the outside economic system. So, apart from being more complicated as a model, it may also lead us to different conclusions than partial equilibrium analysis. "
What are FOCs and SOCs?,"Suppose you have a differentiable function $f(x)$, which you want to optimize by choosing $x$. If $f(x)$ is utility or profit, then you want to choose $x$ (i.e. consumption bundle or quantity produced) to make the value of $f$ as large as possible. If $f(x)$ is a cost function, then you want to choose $x$ to make $f$ as small as possible. FOC and SOC are conditions that determine whether a solution maximizes or minimizes a given function.  At the undergrad level, what is usually the case is that you need to choose $x^*$ such that the derivative of $f$ is equal to zero:
$$f'(x^*)=0.$$
This is the FOC. The intuition for this condition is that a function attains its extremum (either maximum or minimum) when its derivative is equal to zero (see picture below). [You should be aware that there are more subtleties involved: look up terms like ""interior vs corner solutions"", ""global vs local maximum/minimum"", and ""saddle point"" to learn more].  However, as the picture illustrates, simply finding $x^*$ where $f'(x^*)=0$ is not enough to conclude that $x^*$ is the solution that maximizes or minimizes the objective function. In both graphs, the function attains a zero slope at $x^*$, but $x^*$ is a maximizer in the left graph, but a minimizer in the right graph.  To check whether $x^*$ is a maximizer or a minimizer, you need the SOC. The SOC for maximizer is 
$$f''(x^*)<0$$ 
and the SOC for minimizer is
$$f''(x^*)> 0.$$
Intuitively, if $x^*$ maximizes $f$, the slope of $f$ around $x^*$ is decreasing. Take the left graph, where $x^*$ is a maximizer. We see that the slope of $f$ is positive on the left of $x^*$ and negative on the right. Thus, around the neighborhood of $x^*$, as $x$ increases, $f'(x)$ decreases. The intuition for the case of minimizer is similar. "
Why is fuel more expensive on the high way in Europe?,"Because there is no indication in your question that you are a student or practitioner of economics, I am writing an answer for a lay audience. Let me know if you would like more technical detail. A fairly general prediction from economic models of competition between firms is that the price that maximises their profit is higher the less sensitive is demand to that price (this sensitivity is measured by the ""price-elasticity of demand""). Intuitively: There are lots of things that affect the sensitivity of demand. One example, as DornerA mentioned in a comment, is geographic location. If the nearest competing seller is very far away or inconveniently located then buyers will be reluctant to shop elsewhere, meaning their demand is likely to be less sensitive to price increases at locations that are geographically isolated in some way. This seems like a reasonable way to think of highway fuel stations. A little thought should convince you that the same principle (that prices are higher when demand is less sensitive) holds for other cases where demand is price-insensitive too. For example: Edit: A comment by AndrejaKo reminds me to add that the other very common reason why firms might increase their price is that they have higher costs. However, economics theory predicts that only unit costs (i.e. those that increase when you sell more units) matter for optimal pricing. Thus, for example, the fact that land near a highway is expensive would not seem like a very convincing explanation for high fuel prices (because the land costs the same regardless of how many units of fuel are supplied). But if, for some reason, supplying each litre of fuel was more expensive at highway fuel stations (because, for example, transporting the fuel there is more expensive) then we would indeed have an explanation for higher highway fuel prices."
How does Black Friday work?,"Though the Black Friday savings do exist, they are less significant than in some of the early years when the ""Black Friday"" sales were just starting to get really popular.  Raising prices is one way that merchants could try to respond, but then they might exclude the deal-seekers whom they are targeting. Instead, merchants have been doing other things.  This includes selling lower quality merchandise, including a lot of Refurbished equipment.  (I'm mainly thinking of websites selling computer equipment.  A lot of deals this year was refurbished stuff.)  This year, I read (somewhere) that manufacturers have been making specific models of some stuff, which is designed to be cheaper, in anticipation that these models will become Black Friday sales and sell out. Stores can benefit from increased traffic, including selling add-ons (like overpriced USB cables needed to make wired printers work), and having lots of staff members but still having a high customer-to-staff ratio just due to the sheer large number of customers.  They may be able to gain some efficiency because they can successfully predict a higher amount of foot traffic compared to normal days. In a nutshell: Raising prices is one way to respond to increased demand, but that would seem to go against the spirit of things, and places don't want the resulting bad rep.  So, instead, they are using every other trick in the book that they can come up with.  The major merchants are definitely aware of Black Friday, and they do manage to make it a very profitable day."
When Optimal Control fails (?),"I believe the problem is that the steady state may not exist, and the system instead exhibits steady growth (depending on parameters). The reason is because the model is equivalent to the standard consumption-saving problem with exogenous and constant interest rate. To see that, first consider the first order condition for labor choice $f_2(k,\ell) = w$ (here, $f_i$ is partial derivative of $f$ wrt. $i$th argument). Using the definition of constant returns, marginal product of labor is
$$
\frac{\partial }{\partial \ell} f(k,\ell) = \frac{\partial }{\partial \ell} \left[ f \left( \frac{k}{\ell},1 \right) \ell \right] = f_1 \left( \frac{k}{\ell},1 \right) \frac{-k}{\ell} + f \left( \frac{k}{\ell},1 \right)
$$
which is a function of capital-labor ratio only. If wage is constant, labor FOC determines uniquely the optimal $k/\ell$ ratio as a function of wage $w$ and other parameters. Since marginal product of capital
$$
\frac{\partial }{\partial k} f(k,\ell) = \frac{\partial }{\partial k} \left[ f \left( \frac{k}{\ell},1 \right) \ell \right] = f_1 \left( \frac{k}{\ell},1 \right)
$$
also depends on $k/\ell$, it will be constant along optimal path. Denote this value of marginal product $r^*$, and denote the return net of depreciation $r = r^* - \delta$. Equations (1) - (2) for dynamics of capital and consumption are then
$$
\begin{split}
\dot c_t &= (r - \rho) c_t \\
\dot k_t &= r k_t - c_t 
\end{split}
$$
and the specific solution which satisfies transversality condition should be $c_t = \rho k_t$ with $k_0$ given, i.e. a constant part of wealth is consumed at each moment. Both capital and consumption grow at rate $(r-\rho)$, so there is no steady state unless the return on capital (which here depends on exogenous wage rate $w$) equals rate of time preference."
Should we expect more structural technological unemployment if growth becomes more limited by natural resources and less by labour?,"I understand that you are asking the following Up to now, we have avoided structural unemployment because we increased
  production by more than what the technological efficiency gains
  implied (and so eventually re-employed the labor that had become
  initially obsolete, usually in other industries). But if finite resources put constraints on how
  much we can increase production, won't a time come that structural
  unemployment will emerge? As @Foobar notes in a comment, technically speaking this appears to have to do with the rate of increase of efficiency in resource use compared to the rate of decrease of resources available. But one could argue that, efficiency in resource use has its limits, and the constraint will eventually set in. In such a technological unemployment scenario (which I don't believe anyone can really put a reliable time-line on), there is one resource that won't decrease but rather increase in availability: labor. So, and if nothing truly life-changing happens (like devastation of human population, massive colonizations of other planets, or the equivalent of Amalthea's Horn of Plenty), production will tend to utilize this available resource (which we should expect to command a relatively lower reward being in high supply). Meaning, a tendency for production to grow in sectors where services delivered by humans remain central, by their nature (or by the prevailing consumer preferences). Servants, personal assistants, waiters, nurses, sex workers, trainers, teachers, baby sitters, security guards, body guards, arts and sports entertainers... even if only as a relative ""luxury"", rather than strictly needed in the amounts employed. And we may even see innovative ways of using labor (i.e. once more creating new sectors) -but I leave that to anyone's imagination...
Of course this labor should get paid something for its services, so those employing it should have the income/wealth needed to pay this something... which indirectly brings in the issue of wealth and income inequality, and whether it plays (or not) a part in all these."
Why is bartering uncommon in modern countries?,"The main likely reasons why barter is not more common are: A further point is that most countries with VAT have a significant ""VAT threshold"", that is, a minimum value of annual sales at which businesses are required to register for VAT and charge VAT (where applicable) on their sales.  In the UK, for example, the threshold is currently £85,000. So an individual could provide services not exceeding that amount in any year and would not have to charge VAT.  If two such individuals were to enter into a barter arrangement, there would be no VAT saving. The Netherlands is atypical in that its VAT threshold is unusually low: 1,345 Euros (see here, table on p 2)."
What are Giffen Goods?,"Consider the Slutsky equation,
$$
\frac{\partial x}{\partial p} = \frac{\partial x^c}{\partial p} -
\frac{\partial x}{\partial I} x.
$$ A giffen good is the case where the income effect $\frac{\partial x}{\partial I} x$ is negative and large (in magnitude) enough so
that $\frac{\partial x}{\partial p} > 0$. From Wikipedia: There are three necessary preconditions for this situation to arise:
  (1) the good in question must be an inferior good,
  (2) there must be a lack of close substitute goods, and
  (3) the good must constitute a substantial percentage of the buyer's income, but not such a substantial percentage of the buyer's income that none of the associated normal goods are consumed. (Sufficient when (1) also adds that the good is so inferior that the income effect is greater than the substitution effect.) A Giffen good does not generate utility directly through its price. Contrast this to a Veblen good where there the user actually derives utility directly from the price. See this link for some info about empirical evidence."
Why do low-income earners tend to spend more?,"Marginal propensity to consume  is the proportion of an aggregate raise in pay that a consumer spends on the consumption of goods and services, as opposed to saving it. If someone gets extra income $\\\$1000$ and consumes  $\\\$750$ of this additional income their marginal propensity to consume is 0.75. The marginal propensity to consume is higher for poor people since they have to devote higher share of an income to necessities. Suppose that person needs to spend at least $\\\$500$ a month for their living expenses. Now in such situation if a poor person would get a $\\\$500$ of income per month (relative to situation when the person would have nothing) they will have marginal propensity to consume 1 whether such person likes it or not as all additional income they get will be spend on these necessities. Person whose income would increase $\\\$1000$ (again relative to 0) will have to have at least marginal propensity to consume 0.5 and person with income increase of $\\\$10000$ (again relative to 0) at least marginal propensity 0.05. Of course, the richer people wont consume only necessities but the point is that as peoples incomes decrease the necessities become higher share of their budget. At the same time as people get richer they spend a smaller proportion of their income as they already enjoy a high standard of living and thus are able to devote more of their income to savings.  You can read more about this in Blanchard et al Macroeconomics: a European perspective or any other macroeconomic textbook."
Has the Nash Equilibrium led to any significant economic discoveries?,"Two areas that have been profoundly affected by game theoretic research stemming from Nash's contribution are There are actually a few examples of what would come to be known as Nash equilibrium in the industrial organization literature that predate Nash's work (for example, Cournot's 1838 analysis of oligopoly competition). However, until Nash (and Selten, Harsanyi, and others) made game theory a general purpose tool, industrial economics was primarily focused on relatively naive models of competition. In the last 30-40 years there has been a revolution in industrial organisation as economists have used game theory to essentially reinvent the study of market competition around oligopoly theory and the study of strategic interaction. Our modern understanding of consumer search, limit pricing, strategic entry and entry deterrance, predatory pricing, strategic advertising, switching costs, product differentiation, platform competition, horizontal and vertical integration, etc. are all predicated on models that rely mostly on Nash equilibrium (or a refinement thereof) as the solution concept. Jean Tirole was recently awarded the Nobel prize largely for work in this area. This work has also found great practical application in areas such as antitrust policy. Prior to the 1960s, antitrust enforcement in the US (and, to a large extent, elsewhere) was inconsistent and based on unsound economic principles. A combination of the insistence by scholars (especially those based in Chicago) on more careful analysis, and the new tools of oligopoly theory have lead to a much more robust and well-grounded approach to regulating competition. The study of auctions is game theoretic by its very nature: most auctions involve very direct strategic interaction between a relatively small number of bidders. It should come as little surprise, then, that auction theory essentially did not exist prior to the work of Nash (the formal study of auctions can be traced to W. Vickrey (1961) ""Counterspeculation, Auctions, and Competitive Sealed Tenders,"" Journal of Finance 16(1); also the recipient of a Nobel prize). None of the cornerstones of auction theory (revenue equivalence, the linkage principle, optimal auctions—source of yet another Nobel prize, etc.) would exist without the solution apparatus that can be traced to Nash. This work, too, has been of great practical importance. From radio spectrum licenses to carbon emissions permits, and from public procurement to Google ad auctions, auction theory has had a significant effect on informing good auction design. See Klemperer (2004) Auctions: Theory and Practice, Princeton University Press for an accessible summary of the theory and its applications."
Who exactly foots the bill if Greece defaults,"Who would pay depends on the terms of the default. Sometimes holders of similar debts are not treated equally, and this can play out in different ways. Greeks could default on external debts but continue to pay internal creditors. Or because the ESM and other entities are providing ongoing financing, perhaps they will continue to be repaid when others are not to keep those spigots open. Or, less extremely, they could vary the degree of forbearance, term changes, and extent of default.  I found the following two charts from 2013 and 2014 helpful for understanding who are the current holders of Greek public debt and how that compares with other rich countries.  
Source: STARLING CITY  Update:
I couldn't find a time series of the holders of Greek debt but here are some additional charts to see the evolution. The first one is more comparable to the pie chart above than the second one which shows only bank holdings. If this is of interest, the BIS's consolidated banking statistics could be used to create a time series of the bank holdings of Greek debt 
"
Why is collective bargaining by a group of employees not the same thing as price-fixing?,"This is more of an elaboration of The Almighty Bob's answer: It is true that if we start from a competitive market (i.e. large numbers of buyers and sellers), then granting market power to sellers (e.g. workers) by allowing the formation of a monopolistic cartel is bad for efficiency. Those sellers will use their market power to increase the price (and reduce the quantity traded), resulting in a deadweight loss. Thus, we tend to look suspiciously upon practices that create market power. Note that here, the policy intervention we have in mind is to break up the cartel and return us to a competitive world. Why should the labour market be viewed differently? Part of the answer is that the relevant counterfactual has changed. Begin with a world without labour unions. The market will then typically not be competitive because there are often a small number of employers who themselves enjoy market power. Just as a monopolist seller can drive up the price, these monopsonistic (or oligopsonistic) buyers of labour can use their power to drive down the price. Now we are faced with the following policy problem: How can we correct for employers' market power and restore wages toward the (higher) efficient level? Two simple solutions come immediately to mind: Reduce employer's market power by stimulating competition between employers. This is achieved, to some extent, by antitrust policy. But it's hard to do much more here short of forcing more businesses to hire more workers. Allow workers to form unions so that both workers and employers have market power. If the firms try to use their power to drive wages down and the workers use it to drive them up then there is a sense in which the two will 'cancel out' and the result can be closer to the efficient wage than a market in which only employers have market power.  Whether the second solution indeed works or not depends upon a whole range of factors. Here are a few:"
How do Marxist economists solve the Diamond-Water Paradox?,"The Labor Theory of Value has been replaced by the theory of Marginal Utility, which was already accepted by Marx time. In fact he acknowledged: ""nothing can have value, without being an object of utility"" -Wikipedia: Marginal Utility - The Marginal Revolution and Marxism Marginal Utility addresses the diamond - water paradox by explaining that the more of a resource or commodity one has ready access to, the less one needs access to even more. The declining marginal utility means that because water is nearly ubiquitous and easily accessed it has very little ""value"" per unit to an individual because they know they have access to many many units. ""Modern"" analytical Marxists, including John Roemer have built a strong foundation (if somewhat heterodox) based on the theory of marginal utility."
Does diversity lead to more productivity?,"Does diversity lead to more productivity? Some diversity enhances productivity some doesn't. For example, age diversity Ilmakunnas & Ilmakunnas 2011), cultural diversity or knowledge/experience (Navon 2010) diversity  among employees is related to increase in productivity. On the other hand educational (Ilmakunnas & Ilmakunnas 2011) or linguistic diversity (Dale-Olsenab Finseraas 2021) is shown to negatively impact productivity. These are some examples. A workforce diversity can be measured along plethora of multiple dimensions so no answer can look at every single possible dimension diversity. I don't immediately see what racial diversity per ce may have to do with performance. Neither the poster, nor the report to which you link, talks about racial diversity. It talks about homogeneous vs heterogenous teams. A team composed of same racial group can be heterogenous (i.e. diverse) along some other dimension such as education or age. Underrepresented group is any group that is not proportionally represented in some area relative to their proportion in total population, so for example in some industries that can be women, or Hindu etc. Team heterogeneity can be measured among myriads of different dimensions (some examples: age, sex, gender, sexual orientation, experience, skill set, education, culture, language, geographic, nationality, religion, ideology etc...)."
Why do different countries have different currency?,"I will focus on some economics reasons not mentioned explicitly so far. There are economic benefits to having your own currency. Your question essentially raises the question of so-called ""Optimum Currency Areas"" (OCAs). There was a lot of interest in the question of what areas should have the same currency. It is in general not immediately obvious that different countries should have the same currency, or even that the same country should have the same currency everywhere. Your question boils down to asking whether the whole world is an OCA - and the short answer is no. The pros of having different currencies are that you can use monetary policy to offset shocks. Especially for trade shocks, because monetary policy can change the exchange rate of a currency to affect trade. For example if Germany and France are hit by different shocks, then they will want to conduct different monetary policies, which they can't if they share the same currency. The main cost of different currencies are transaction costs of exchange, which can hamper things like trade and tourism. Therefore an area should have the same currency (is an OCA) if it is subject to the same shocks or other factors are present, such that those shocks are absorbed without monetary policy. This thinking leads us to the four most important criteria for an area to be an OCA. You can use these criteria to evaluate whether countries should have the same currency and to answer your question. It is worth noting, that the pioneer in this literature was Robert Mundell with his 1961 paper. The other two most important works are Kenen (1969) and Mckinnon (1963). The criteria are: The regions should have similar business cycles. As mentioned, if the countries tend to experiene similar shocks, then they will require the same monetary policy. In that case there is no reason for them to have different currencies! Openness of the economy (Mckinnon). This can be divided in two parts: 2.1. High labor mobility across regions. If there is a recession in one region and people can move to another one as a response, then monetary policy is less important in adjusting to the shock, as labor itself adjusts. This is a reason why countries can usually have the same currency within their whole territory. For example, people who lose their job in one U.S. state often move to another. If that wouldn't be possible, then adjusting to the recession with monetary policy would be more important. That is why the freedom of movement agreement is so important for the Euro to be sustainable. Note that wage flexibility would be required here as well. 2.2. Capital mobility. The reasons are similar as for labor mobility. If a region becomes less developed, then returns to capital there will increase. If markets are free, then capital can travel from the more prosperous region to the one hit by the shock, thereby mitigating the negative effects of the shock. Note that price flexibility must be given here for these effects to occur. A risk sharing system, such as fiscal transfers. Since monetary policy can't be used, we would need fiscal policy. Fiscal transfers from unaffected areas can help with negative shocks in another region. The Eurozone has a no-bailout clause, so this condition wasn't given during the Greek crisis. However, this was de facto abandoned. This is unsurprising to those familiar with the theory of OCAs. Product Diversity (Kenen). Trade shocks, which monetary policy can help with, usually occur to certain industries and not the whole economy. If countries produce a variety of products, they will be less likely to suffer from large demand shocks. Hence, more diverse economies will face fewer trade fluctuations and see smaller increases of unemployment if shocks occur to one industry. This also reduces the need for monetary policy stabilization, since any given shock has a small impact on the overall economy. Sometimes further criteria are also given, but they are less important. These include ""solidarity"" as well as homogenous preferences across regions, for similar reasons as the condition of similar business cycles.  So you can use these criteria to evaluate whether certain countries or areas should have the same currrency or not. All these criteria are definitely not fulfilled for the whole world, so there is a role for different currencies. An issue with the theory of OCAs is that it is unclear how much weight we should give to each criterium. So it could be unclear how to evaluate whether two countries who partially fulfill some of the criteria should have the same currency or not."
Can inflation occur in a positive-sum game currency system such as the Stack Exchange reputation system?,"In the Stack Exchange reputation ""economy"", we can think of the reputation points as ""money"". And the ""goods"" are: To my knowledge, these prices (1-point cost of each downvote and privilege thresholds) have never changed. And so, there has never been inflation. What has merely happened is that ""wealth"" (the total stock of reputation points) has kept increasing. Response to Pere, who writes: Newer sites have lower thresholds for several privileges - for practical reasons - and those thresholds rise as the site expands. For example, being able to create tags ""costs"" 150 reputation in Economics SE, 300 reputation in Cross Validated SE and 1500 reputation in StackOverflow. Therefore, privilege prices are subject to inflation as sites grow. Each individual SE site is its own individual ""economy"" that is sealed off from all other SE sites.¹ And so, whether ""prices"" differ across SE sites does not matter. What matters is whether ""prices"" have changed over time, within each SE site.  To my knowledge, the privilege threshold ""prices"" on Economics SE have never changed. And it also seems that for the original, flagship site (Stack Overflow), ""prices"" have also not changed for many years. We can, for example, compare today's ""prices"" with those from 2013:  As you can see, all prices have remain unchanged since 2013. There has therefore been zero inflation since (at least) 2013. The only change is the introduction of a ""new good"", namely ""Access to site analytics"", which is priced at 25,000 points. Response to @kbelder, who writes: What about bounties? I bet those have historically trended upwards in value The ""prices"" of bounties have likewise been fixed. Bounties may be offered at ""prices"" of 50, 100, 150, ..., and up to 500 points. I believe these prices have been fixed since the inception of Economics SE and at least since 2013 for all SE sites. Moreover, bounties are pure transfers from one user to another.² It may be true that thanks to increased overall ""wealth"", the average value of a bounty has risen (i.e. in the past, most bounties were offered at close to the minimum price of 50; whereas today higher prices are offered). However, we do not consider this ""inflation"". Instead, we consider this to be real wealth effects. Analogy: Say that in 1919, the New Yorker on the street might on average give \$0.20 (in 2019 dollars) to a homeless person. In 2019, this might instead be \$1 (in 2019 dollars). The average real transfer from a New Yorker to a homeless person has risen five-fold in real terms. This is not inflation. Instead, it is a consequence of increased overall wealth. Footnotes. To my knowledge, there is one thing that links reputation points across the various SE sites. This is the automatic +100 points for established user accounts that are ""trusted"" on the SE network. I'm not sure how this works. But I would say that these +100 points are relatively inconsequential and may be safely ignored. They would perhaps be the real-world equivalent of an immigrations officer giving a ""trusted"" visitor \$1 the first time that visitor arrived in that country. There are actually three things that can happen to a bounty. First, the full bounty is awarded (I believe this is the most common case). Second, half the bounty is awarded to the highest score (provided that answer has a score of at least +2 — see Economics.SE bounty rules here). Third, no bounty is awarded. In the latter two cases, we do not quite have a pure transfer; instead, we have a leaky bucket where either half or all of the points are ""lost to society""."
Why would a company sabotage its product's ability to be used for a particular purpose?,"This topic is explained by market segmentation with price discrimination in monopolies. This is widely used in, for example, price discounts for senior/retired people. By targetting different market segments with different prices you can
maximize profits because their price sensitivity is different (i.e. miners are likely to buy at higher prices, while regular consumers will sharply reduce their willing to buy at those same prices). This is true as long as the following conditions are met: This is the same reason why consoles used to be region locked. Otherwise it would be convenient to travel to the cheapest region, and then bring those products back to your home country for resell. In order to enforce these requirements, GPUs used for gaming can't be used for mining. Additionally there could be other reasons. For example NVIDIA wants to heavily push Raytracing because it gives them an edge compared to the competition. However games will not adopt Raytracing if a large portion of their users don't have capable cards. Right now the number of titles supporting RT is low. And those who do, it is just an optional gimmick that doesn't add much to the experience; and the first gen RT cards (i.e. RTX 2080) aren't fast enough for a raytracing-rich experience."
"In what sense are ""new-Keynesian"" models ""new"" and in what sense are they ""Keynesian""?","In essence New-Keynesians adapt micro to macro theory. This is in contrast to new classicals which adapt macro theory to orthodox neoclassical market-clearing microfoundations. New-Keynesians adapt the rational expectations hypothesis but accept that market may fails due to wage and price stickiness and Friedman's natural rate hypothesis. The New-Keynesians, the RBC school and New Classicals focus on issues relating to aggregate supply and have been the dominating schools since the 1970's, especially the new-Keynesians have dominated the last one to two decades. On the other hand ""old""-Keynesians and the orthodox monetarists mainly focused on issues relating to aggregate demand and these dominated economic thinking pre-1970's.  As an example of the differences between New-Keynesian New Classicals and the RBC school consider an increase in money: This increase will have real effect in a New-Keynesian model due to these market imperfections. In a new classical model on the other hand money will only have real effects if they are unanticipated (Lucas' Island model) while in a RBC model the increase will only feed into higher prices due to perfect clearing markets and rational expectations.  Note that it has been argued by N. Gregory Mankiw that the New-Keynesian school could just as well have been called New-Monetarist. For an excellent reference on the different schools, what they represent and their differences see Snowdon and Vane: ""Modern Macroeconomics: It's Origins, Development and Current State"""
What do supply-demand curves really look like?,"You sometimes find textbooks drawing the supply and demand curves as concave upwards, as such:  The straight-line supply and demand curves can be thought of as a magnification of this graph, where the two intersect. Thus, the units on the axes would give you a clue as to how high up the graph is being drawn, or how far to the right (if the units start at a number other than 0 or skip an interval). This shows you why they are indeed called 'curves', even when they are sometimes straight lines. Supply and demand curves are drawn using straight lines for simplicity. For example, two straight-line equations may be given, from which it is relatively simple to calculate the point of intersection. Supply and demand curves are an approximation of what happens in real life. The curves are a simplified model which show the general trend in the two functions. In reality, supply and demand curves are approximated using data that is collected over many years, with many short-term variables affecting the results, if the curves are even drawn up at all (the curves are more a theoretical model than a calculative method).  However, due to the firm's variable costs, output is always greater than that at the turning point of the marginal cost curve. "
Most common programs used by Economists,"There's three important dimensions for programs/languages: In terms of frequency of usage among academic economists, here's my ranking: This list is of course my personal opinion, and for academic economists only. I believe that no one will dispute the top tier, but the second tier/specialists can be somewhat debated. And then there's some more who are even more specialist (for example, Octave as a open source Matlab alternative)"
Unemployment and the Minimum Wage---what are the main counter-arguments to Card and Krueger?,"Isaac Sorkin, a grad student at Michigan, has addressed this. Here is Miles Kimball blogging it, Isaac Sorkin: Don't Be Too Reassured by Small Short-Run Effects of the Minimum Wage, Jonathan Meer and Jeremy West: Effects of the Minimum Wage on Employment Dynamics, and The Economist—Destination Unknown: Large Increases in the Minimum Wage Could Have Severe Long-Term Effects. The main argument there is that previous work measures short run elasticities, which are less responsive than in the long run. You can surely find more by checking Sorkin's citations."
Experiments contradicting the expected utility model,"this paper http://else.econ.ucl.ac.uk/papers/uploaded/243.pdf (Choi 2007) has a nice state of the art experiment that deals with rationality and expected utility is a special case of it. In general only 17% of consumers are compatible with rationality ergo the remaining part cannot be expected utility maximizers.  Quah has a nice paper on the revealed preference theory of expected utility (among other models), he uses Choi dataset to test expected utility hypothesis that is going to be rejected more times than rationality https://ideas.repec.org/p/lec/leecon/13-24.html"
Where to start learning economics as a mathematician?,"McCloskey's Applied Theory of Price is somewhat legendary as a model of clarity. It is, sadly, no longer in print. However, a PDF is available here: http://www.deirdremccloskey.com/docs/price.pdf. The level of mathematical sophistication in this book (like in most good economics!) is not that high, but the book gives a deeper look at the important concepts than is to be found in most extant introductory undergraduate texts. If you really want a more mathematically formalised treatment then you are going to have to turn to graduate-level econ books. For microeconomics, the classic text of choice is Microeconomic Theory by Mas-Colell et al. If you do go down the more formal route, I would urge you not to give up on simpler treatments altogether; they're great for building intuition. More than a decade of experience in economics has taught me that good intuition built through simple models is often far more valuable than a less intuitive understanding of a more sophisticated model."
Why do many occupations show a gender bias?,"I should say pre-emptively that I'm not satisfied with ideas that merely may explain the effect, as yesterday's question produced a proliferation of rationalisations and claims that lacked rigour. I'd prefer to see an explanation that attempts to be comprehensive, or supports contentious claims (such as those about women's productivity) with research. Your question and the yesterday's one are contentious even in the literature, which means there is no consensus among the economists and no one can give an answer that satisfies everybody. Given the complexity of these questions, they can involve and interact with many different issues, all of which matter to some extent. Even in the literature, the common way is to attack them from one specific perspective rather than to offer a ""comprehensive"" answer. Also a convincing explanation usually requires careful data analysis, qualified modelling and many robustness checks, which suits more to an academic paper rather than to an answer in this forum. However, I agree with you that we should avoid rationalisations and claims that lack rigour and the support of academic references. In the comments of this question someone has already mentioned that some recent papers argue that commuting requirement can be one potential reason. I thus list some recent papers that suggest other deep-in-root causes below. Adda, Dustmann, Stevens 2017 finds that different occupations can diverge in the amenity ""child raising value"" and in the loss of accumulated skills when interrupting work careers. Given the fertility consideration, women would thus more likely to choose the occupations that have lower wage growth. Occupation choice is also related with education choice. Women's underrepresentation in the math-intensive fields (i.e. STEM fileds) translates into their underrepresentation not only in high-paying occupations but also in occupations where the gender wage gap within the occupation is particularly small. A survey paper by McNally 2020 reviews many arguments behind this gender differences in tertiary education. Hsieh, Hurst, Jones, Klenow 2019 studies the roles that the discrimination in the labor market, the barriers to forming human capital, and the differences in preferences or social norms play in individuals' occupation choice. They find that the improvement in the discrimination in forming human capital is the most important driver behind the increased participation of women in high-wage occupations in recent decades. In additional to typical economics reasoning, Bertrand 2020 suggests that endogenous preferences and self-fulfilling prophecies caused by gender stereotypes can be important for the low share of women in STEM careers and occupations. Also, Grosjean & Khattar 2019 shows that cultural persistency can affect womens' occupational choices."
"If Bitcoin becomes a globally accepted store of value, would it be liable to the same problems that mired the gold standard?","It would not just be plagued by the same problems it would create some new ones. Following the Weber (2016) who actually written research exploring exactly the question that you are asking here: The scope of monetary policy would be more limited under the Bitcoin standard than
under the gold standard. The ability to issue fiduciary currency would give central banks
limited ability to act as lenders of last resort. However, virtually costless arbitrage of Bitcoin across countries would prevent central banks from implementing interest rate policies to affect their domestic economies. An empirical examination of countries’ experience with the gold standard leads to the
following conjectures about how the Bitcoin standard might perform: The paper concludes by speculating that even if the Bitcoin standard were to come into
existence, it would not last long, for two reasons: (1) The payments world is changing so
rapidly that there will be a technological innovation that provides a potential medium of
exchange with the same or greater benefits of Bitcoin or with lower costs. Such an innovation
could come either from the private sector or from the government. (2) There would be
pressure to return to a fiat money system so that a more activist monetary policy could be
pursued. In a nutshell, it would be pretty similar to gold standard but with some extra strings attached that would just additionally constrain monetary policy making it even slightly worse. As a consequence, if Bitcoin standard would ever become adopted it would likely perform even worse than Gold standard did (which is saying something), and would likely go the same way.  You can find more detail on the workings of such standard in the paper itself. Response to Edit: The above still applies but let me also respond to the new infographic that was added to the question."
What is the likely result of rent control in Berlin?,"Rent control will over time decrease the supply of housing while increasing demand causing a housing shortage. Moreover, there are also studies that show that under rent control there are not just shortages of houses but also misallocation, that is the houses are no longer allocated to people who have highest utility/consumer surplus from having them (Glaeser and Luttmer, 2003). A recent literature review on rent control concluded (Jerkins, 2009): I find that the preponderance of the literature points toward the conclusion that rent control introduces inefficiencies in housing markets. Moreover, the literature on the whole does not sustain any plausible redemption in terms of redistribution. Furthermore, the IGM forum, a platform that surveys the top policy economists, shows that virtually all of them think that rent control (at least as applied in US) had a negative effect on availability and quality of housing (see here). However, note an important caveat is that as any policy also rent control is not always homogeneously applied. I saw some studies showing net positive redistributional effects of rent control but it’s only small minority of all studies and usually the positive results come from rent controls that were applied only to special subsection of housing market for example government owned social housing. Those studies to my best knowledge do not really apply to the Berlin case, which is going more or less the similar route as San Francisco or New York where the effects are shown to be significantly negative."
Why are real median household incomes stagnant?,"The metric of median household income is also used by others to argue the presence of income inequality:
https://en.wikipedia.org/wiki/Income_inequality_in_the_United_States#Causes However, it seems that it is not only the median but also the mean that stagnates:  (I used family instead of household income because I could not find a time series for mean household income.) If the problem was only one of inequality then the mean income should still increase. I think there are several reasons why median household income seems to stagnate. First of all I think there is a visible upward trend that is only somewhat masked by the 2008 financial crisis. Granted this trend is still smaller than the increase of productivity. Why median household income grows at a slower pace than before is still a question. I think the problem arises because neither the number of households nor their distribution is constant. One thing that I believe has an effect is that average household size has decreased over the period you are looking at.  (I had some trouble opening the Census Bureau's xls files.) So while total household income has in fact increased it is now divided between more numerous, smaller households making both the mean and the median smaller. Another reason might be immigration. According to this site (I don't know if the statistics are reliable, there are a lot of links on the site I consider to be politically biased) the mean immigrant income is lower than the mean 'native' income. This is not surprising, immigrants need time to adjust and build the social networks necessary to get good jobs. Depending on the measure of productivity you use this can mean that while productivity in any given industry increases the weight of low-productivity industries increases as well. Thereby immigration may decrease mean and median income.
Note that this does not imply anything about the effect of immigration on the welfare of 'natives'. I am not claiming that the median household income of 'natives' is increased or decreased, I am pointing out that the weights used in the calculation are shifting."
"If I don't pay a debt, then the creditor takes my goods. Why, then, do Greek creditors not take Greece?","In general, there are three kinds of debt:"
Is net neutrality not important in a competitive market of internet providers?,"Yes, if there were more competition then net neutrality would not be such a big issue. Any throttling would cause an ISP to lose customers to competitors. Competition is a main assumption behind why markets work. However, in this case an ISP has market power which breaks the assumption. The market power allows them to easily put smaller ISPs out of business.  Here is a link to a rather long article on how ISPs get this market power. Essentially, the reason is that starting an ISP is entering an economy of scale, which requires a large amount of capital to start: http://arstechnica.com/business/2014/04/one-big-reason-we-lack-internet-competition-starting-an-isp-is-really-hard/"
How do economies grow?,"GDP is a measure of economic activity. One typical way to look at the question is the way shown in another answer. Here I want to put some assumptions in place in order to give a bit more structure. It is widely accepted that economic output is created through capital and labor, and facilitated by technology. For the sake of answering this question, we can assume that somehow we can aggregate all the decentralized small economic activities into one aggregate economic activity. For our economy then, GDP $Y$ is produced using labor $L$, capital $K$, and technology $A$, and some sort of production function $F$. $$ Y = F(A, K,L)$$ This entails all ways that growth can happen, and I shall walk quickly through them. When there is more work being done, we produce more. This includes both the intensive margin (each worker spending more time per week, month, year on on his job(s)), and the extensive margin (there being more workers, due to larger population).  Note that it is unlikely to get persistent growth from the intensive margin - eventually people are working as much as they can. The extensive margin, however, can give you persistent growth.  One can also think about $L$ as effective labor, in the sense that human capital also influences output. A stronger man can harvest more fields than a weak one, and a smart man can design more bridges. If workers get more skills that are useful in their economic activity, that will also stimulate growth. When there is more machines, we can produce more, for the same amount of labor. $K$ in that equation entails both quantity and quality of machinery, similar to the human capital point.  Capital is part of economic output. Hence, in order to create GDP, we need GDP. We also need people to actually decide that saving previously earned GDP and ""transforming it into capital"" is better than consuming it.  Capital naturally depreciates. This is because technology may change, and hence old tools might be of less use, but also naturally due to fatigue etc. Hence, even if we don't want to grow, we still need to consistently devote a share of GDP towards investment. If we want positive growth, we need to invest further. Technically, technology incorporates both $A$ and $F$ in the previous equation. If we have a large population, but only little capital, creating technologies that make more efficient use of labor, and need less capital, will increase growth. Regarding $A$, using technologies that allow us to produce at a higher level of efficiency we can produce a higher level of output for the same amount of inputs.  Higher investment means more capital, and hence more to produce. An example policies to increase investment are decreasing capital taxation, and anything that increases the return to capital. We cannot get persistent growth from increasing the working hour per person, although this may be a valid policy in the short run. Increasing the labor force (more immigration, higher fertility) is a  second. Be cautioned, that that also means that the population gets a smaller share of the GDP, as we have more people to split the cake.  Increasing human capital hence seems like the natural way. Policies that allow people to become better at their jobs will increase growth. That could be  This is a no-brainer, and most likely the most important driver of economic growth over the past century. In effect, many people are worried that the time of big growth is over, just because the time of big inventions and technological booms appears to be over. Policies that incentivize research and development will increase growth. Such as"
How does everyone not become poor over time?,"In short because economy is not a zero-sum game and because economic production does not need to decline but  can grow over time. First of all, forget about money. Money in economics is just extra tool that solves issue of double coincidence of wants, allows you to store value over time and to do accounting. In fact, traditionally money consisted of commodities such as stone, or precious metals or even products, coffee, cigarettes or even alcohol etc at some point in time and place served as money. As a non-economist this might come as a surprise to you but most microeconomic models do not even have money in them. Money can play an important role in macroeconomic perspective because in the short-run it can affect the real economy, but from micro perspective and even long-term macro perspective money is neutral and not important. Next, as you pointed out a world's economy, taken as whole, is a closed system but that does not mean people should get poorer over time. This is not an equivalent of thermodynamically closed system where entropy has to dissipate energy overt time. A closed economy can be described as: $$Y  = C + I + G$$ which says that output/income (they are economically equivalent) $Y$ has to be equal to consumption $C$, investment $I$ and government spending $G$. In turn, $Y$ is a result of production. A common specification for production process would be to use Cobb-Douglas production function, so I will do that here as well, although in principle there could be multiple functional forms that production process could have. Moreover, for simplicity I will restrict myself to two factors of production (labor and capital) even though in principle you could put into Cobb-Douglas also human capital (education), land and other factors, but I omit them for sake of brevity. Given these assumptions the production process would be given by: $$Y = A K^{\alpha} L^{\beta}$$ where $A$ is the available technology (broadly defined - in economics better production strategies count as a technology as opposed to just 'gadgets' such as PC), $K$ is the stock of capital and $L$ is the stock of labor. Alpha and beta are parameters of the production function that determine other characteristics of production such as what sort of economies of scale production exhibits (e.g. increasing, decreasing or constant). Thus, we can say that the consumption, investment and government spending is equal to this production process: $$ A K^{\alpha} L^{\beta} = Y  = C + I + G$$ So as long as technology $A$ or capital $K$ or labor grows, output $Y$ will grow as well and even completely closed of economy can consume more, invest more or have more government spending. Furthermore, what we really care about is usually output/income (in economics output and income are equivalent and interchangeable) per head. So if we for simplicity assume constant returns to scale ($\beta = 1-\alpha; 0 <  \alpha < 1$)  we can divide the last expression by $L$ to get whole expression per capita as: $$A \left(\frac{K}{L}\right)^{\alpha} = \frac{Y}{L} = \frac{C}{L} + \frac{I}{L} + \frac{G}{L} $$ Here, if we keep the population growth constant then we can see that per capita incomes/output and consequently per capita consumption, investment and government spending will grow when our capital stock and technology stock grows. You get growth in capital stock from investment (investment is part of production that is not consumed but saved for future, possibly for further production), and growth in technology from human ingenuity or also depending on which economic growth theory you buy to also investment. In addition, all the above is up till now just manipulation of definitional expressions. We could further continue by adding on some growth theory which could yield further insights. For example, the most popular growth model presently is the Solow-Swan growth model where it can be shown that what really matters is growth in $A$ because increase in investment and capital stock can only increase level of output not growth. If you buy into endogenous growth theories investment can even increase the growth rate further beyond that (you can read more on this in Barro & Sala-i-Martin: Economic Growth 2nd ed). In order for people to get poorer over time (in per capita terms) an economy would have to experience either destruction of technology (for example ancient Romans, Greeks, Carthaginians etc had plumbing but this knowledge was lost in many parts of World - but not all - during dark ages, or another example would be concrete) or by destruction of capital (e.g. war, violence, natural disasters etc. but also just wear and tear i.e. depreciation). Empirically, and let me add thankfully, people historically turned out to be better at producing new technologies and adding new capital through investment, than at destroying either of them. Consequently, to sum up people do not get poorer over time and in fact get richer because of growth in economic production (which in turn determines people's income), and economic growth occurs primarily thanks to growth in technology and investment. In fact note because the economic growth is to large extent driven by technology, it is possible for economic production to grow indefinitely (assuming technology has no limit - e.g. in real life it is possible you can't get at an end of technology tree like in some strategy game as it might have no end). In addition, more rich models would also include other important factors such as human capital (education) but the explanation above would not conceptually change (education macroeconomically functions in same way as capital - hence why economists call it human capital)."
The efficacy of death penalty in reducing violent crimes,"The literature on this issue flip flops. The first seminal study by Sellin (1959)¹ did not find any significant effect, but early literature was riddled with methodological errors. Ehrlich’s (1975)² study at the time was far more sophisticated, and it showed a strong deterrent effect of capital punishment. However, ironically, this study later too became criticized for lack of empirical rigor (as the statistical method advanced). Cochran and
Chamlin (2000)³ and Lamperti (2008)⁴ showed that not only it did not deter violent crimes, but it could also have even increased them via the 'brutalization' effect. However, later Dezhbakhsh,
Rubin, and Shepherd (2003)⁵ employing panel data again showed it has a deterrence effect, but that study was later itself challenged by Kovandzic, Vieraitis, and Boots (2009)⁶. New meta analyses show that evidence is somewhat mixed too. For example, meta analysis of Yang & Lester (2008)⁷ argues there is a significant deterrence on some crimes, but meta analysis of  Dölling et al. (2009)⁸ finds that the effect of capital punishment on violent offenses is not significant. It is very hard to fish out the true effect because of endogeneity issues. Places in US with high violent rates often have capital punishment, is that because those places need the deterrent most or because it is ineffective deterrent and other places have better policies? Although it is matter of ongoing discussion most researchers now tend to lean more to the side that capital punishment is not effective deterrent but the research is not fully settled. References"
Supermarket selling seasonal items below cost?,"Is it simply for saving warehouse costs? Probably yes, holding onto inventory is very expensive. You have to pay for warehousing of the good, it takes the spot of some other inventory that might be in high demand. Food is also perishable so it cannot be stored indefinitely. Stores have to always guess what demand for their products will be, sometimes they get it wrong and order excess inventory. It can often be more profitable to sell excess inventory below the production cost or many times even just to throw it away to make space for new products. Holding onto it would be a sunk-cost fallacy. The costs are already incurred in the past, even if they were a result of a mistake it does not make sense to try to double down on the mistake and hold onto them. It is more profitable to ignore the sunk-costs of getting them and make new decisions of what to do with the inventory based on new information."
Why do wages not equalize across space?,There are several explanations for this in the literature (the order does not necessarily reflect importance of each explanation). I think the above summarizes all the major factors but there might be some further ones as well for that you can see the sources above and ones cited therein.
Why did banks give out subprime mortgages leading up to the 2007 financial crisis to begin with?,"If you want all details read this excellent Brookings report on the Origins
of the Financial Crisis by Baily et al. Its an excellent source on this topic and it is not too technical for laymen to understand. Here is a summary of the report: First, there was quite large market of people in the US who traditionally were not able to get mortgages due to poor lending scores and other issues. This in itself is not reason to extent mortgages to such people because banks of course don't want to get bankrupt, however it meant that there was always large market that businesses would want to tap into if they could. This became possible in late 70s thanks to several factors. First was financial innovation, namely adjustable rate mortgages with no down payment and teaser rates. Second, was securitization  which was pioneered by government-sponsored enterprises devoted to mortgage lend­ing, Fannie Mae and Freddie Mac. These organizations were originally set up by government to buy mortgages from banks that met certain criteria to promote borrowing to poorer households at low interest rate and securitization was seen as an innovation that can help with this. This is because mortgages to low income households were always riskier and hence paradoxically the poorer you are the more interest you would have to pay or you might left without ability to tap into credit market at all. Securitization was supposed to solve this issue by combining such mortgages with higher quality mortgages where the whole bundle would be considered safe. This securitization got more complex over years as financial firms developed collat­eralized debt obligations, and later these were even insured by insurance companies in cause of default. All this securitization made people believe that these financial assets are extremely safe (hindsight is 20/20). This emboldened banks to start lending to households they would not lend before. This was further fueled by very easy monetary policy that kept interest rates low (which in turn means that the adjustable rate mortgage were actually very affordable), and in addition by lack of financial regulation that would impose more conservative lending standards. Moreover, moral hazard likely make this even worse as some banks considered themselves too big to fail. This system worked while the adjustable mortgages were affordable for people in low interest environment and with house prices rising (which increased value of collateral for the household). Fed was pursuing loose monetary policy for long time already, and house prices had long-run upward trend for decades. As a result very few people considered these lending practices highly risky at the time. People simply believed that securitization solved the issue. In addition some of the financial instruments were given better rating from rating agencies than they should. When both interest rates increased and house prices dropped it resulted in meltdown. The problem with adjustable rate mortgages is that although they can be much cheaper  they make household exposed to interest rate risk. The fall in house prices also meant that people were not able to refinance so default was only option for many. So to summarize it, banks did not perceived these loans as risky. Mortgage backed securities had good ratings. Moreover, the whole idea of securitization is actually a sensible idea, it was developed by government sponsored enterprises with good intentions in mind. Moreover, when central banks pursue loose monetary policy for some reason many financiers expect it to last forever (even recent hikes in interest rate resulted in bank failures despite of all the new regulations that force banks to be more conservative in lending). In addition at the time lack of financial regulation combined with implicit government guarantees (because of too big to fail issue) created incentives for people to worry less about risk then they would do if there are no guarantees."
Is the Occupy Wall Street famous 1% stable?,"I've not see a strict analysis of the top one percent but I have seen an analysis of the top 400 tax payers, the very highest income taxpayers: Who are the rich? It depends who you ask and when. Since 1992, the IRS
  has tracked the top 400 earners in terms of adjusted gross income—the
  so called fortunate 400. ... Most importantly, this IRS report
  demonstrates there is a lot of income mobility at the top. Of all the
  filers who have made the list since 1992, 73 percent were on the list
  just once. Virtually no one remains on the list for all 18 years, but
  for privacy concerns the IRS did not report exactly how many did, if
  any. In last year’s report, just 4 people remained on the list for all
  17 years. This suggests that most top earners do not have a portfolio
  of big investments that can be cashed in year after year, but rather
  one big asset, such as a family farm or business or stock, the sale of
  which triggers a capital gain. The Fortunate 400 By William McBride A slightly more targeted answer: Considering those whose information was found in both years,
  approximately half  of taxpayers in the lowest and highest income
  quintiles remained in the same quintiles 20 years later. Nearly
  one-fourth of those in the bottom quintile moved up one quintile,
  while 4.7 percent moved to the top quintile. About one-fourth of those
  in the top 1 percent were also in the top 1 percent 20 years later,
  but nearly 70 percent remained in the top income decile. The overall
  results suggest that, while there is considerable persistence among
  observed taxpayers, there is also meaningful movement even within this
  narrow age cohort. Some taxpayers start from the bottom and move to
  the top and vice versa. NEW PERSPECTIVES ON INCOME MOBILITY AND INEQUALITY
Gerald Auten, Geoffrey Gee, and Nicholas Turner (2013)"
Destroying the dollar,"I think you misunderstand what ""electronic money"" is - moving electronic money around isn't simply a matter of sending the right ""codes"" - it is ultimately about asking the central bank of that currency to move money around. Sure you can open up excel and write in it ""I have \$100"" but that isn't USD, much as writing \$100 on a piece of paper doesn't make it a \$100 bill. In order to lend you dollars I actually have to have some dollars to lend you.  That means I need a reserve account with one of the 12 Federal Reserve banks.  There are no electronic USD that are not ultimately in a Federal Reserve Account. The ""ultimately"" in that sentence is there because banks can (and do) form hierarchies.  Only the largest (""Tier 1"" banks, sometimes called ""clearing banks"") actually have a reserve account with the central bank in a given currency.  Other, Tier 2, banks will simple hold accounts with Tier 1 banks.  Smaller local banks may even be Tier 3.  When it comes to foreign currency dealing, you could be even further down the chain (i.e. a small bank in Australia may be Tier 4 for USD) Because of this hierarchy a certain amount of electronic banking can be done without moving reserves.  A transfer only needs to progress up the hierarchy until it gets to a common bank between the two customers. So for example: A payment between two accounts at the same bank, can be done on that bank's systems. A payment from an account at a Tier 2 bank, to a different Tier 2 bank might be done using a shared Tier 1 bank.  The Tier 2 bank's are ""customers"" of the Tier 1 bank, and so the Tier 1 bank can do the transaction on their system. A payment between customers that ultimately fall under two different Tier 1 banks must be done by at a Federal Reserve bank. The US adds an extra complication, because there are 12 Fed Reserve banks - if two Tier 1 banks don't bank with the same Fed Branch, then the Fed Reserve System also has to go ""one step higher"" and the New York Fed acts as the very top bank: ""How do reserves move between the 12 federal reserve banks?"" explores exactly how that is done. So the problem in your scheme is you couldn't get your central bank ""into"" this pyramid of banks.   Your central bank's computer can show $1tr on screen but the only way to transfer to another bank, would be to instruct YOUR Fed bank to take money out of YOUR fed reserve account and put it into the other bank's account.  Your central bank can't make reserve dollars. Aside:  An alternative way of looking at this, for those familiar with how BitCoins work, is to note that the important feature of electronic money is to prevent double spending.  BitCoin does this with the BlockChain; the USD equivalent of the block-chain is held on the Fed Reserve computers.  You can't just declare yourself to have billions of bitcoins - to spend them you have to get your transactions onto the BlockChain.  With electronic USD you have to get your transactions onto the Fed ledger.  Only the Fed has a copy of this and their say is final.  There would be no way for your central bank to change the Fed ledgers."
Lexicographic preference relation cannot be represented by a utility function,"We can say more generally that lexical preferences are not representable using a continuous utility function. Lexical preferences are not continuous. Note the definition of a continuous preference relation. The preference relation $\succeq$ is continuous if for any sequences of consumption bundles $(x_{i})_{i \in \mathbb{N}}$ and $(y_{i})_{i \in \mathbb{N}}$ with $x_{i} \to x$, $y_{i} \to y$, and $x_{i} \succeq y_{i}$ for each $i \in \mathbb{N}$, then $x \succeq y$. That is, continuity preserves the relation at the limit point. Consider $(x_{i})_{i \in \mathbb{N}}$ defined by $x_{i} = (\frac{1}{2^{i}}, 0)$ and $(y_{i})_{i \in \mathbb{N}}$ defined by $y_{i} = (0, 1)$. Clearly, $x_{i} \succeq y_{i}$ for each $i \in \mathbb{N}$. However, $x_{i} \to (0, 0)$ while $y_{i} \to (0, 1)$. So this preference relation is not preserved at the limit point. Even more generally, no utility function represents the lexical preference relation. I prove for $\mathbb{R}_{+}^{2}$, but this argument extends to $\mathbb{R}_{+}^{n}$ by projecting into $\mathbb{R}_{+}^{2}$.  Proof: Suppose to the contrary that some utility function $u : \mathbb{R}_{+}^{2} \to \mathbb{R}$ represents $\succ_{lex}$. We thus have $u(x, 1) > u(x, 0)$, as $(x, 1) \succ (x, 0)$. We construct the interval $I(x) = [u(x, 0), u(x, 1)]$. Now for any two distinct $x, y \in \mathbb{R}_{+}$, $I(x) \cap I(y) = \emptyset$ as we have either $x > y$ or $y > x$ (so WLOG, we have $(x, 0) \succ (y, 1)$).  Define $\mathbb{I} = \{ I(x) : x \in \mathbb{R}_{+} \}$, and let $\phi : \mathbb{R}_{+} \to \mathbb{I}$ be given by $\phi(x) = I(x)$. Observe that $\phi$ is an injection, as each $I(x), I(y)$ are disjoint for distinct $x, y$. Note that $\mathbb{Q}$ is dense in $\mathbb{R}$. So there exists a rational number in each interval. Define $\tau : \mathbb{I} \to \mathbb{Q}$ such that $\tau(I(x))$ returns a rational number contained in $I(x)$. So $\tau$ is an injection. We have $\tau$ composed with $\phi$ an injection, which implies $|\mathbb{R}_{+}| \leq |\mathbb{Q}_{+}|$, a contradiction. QED."
Why is capital income taxed differently than wage income?,"One reason is the inflationary gain problem.  Let me give an example with simple numbers.  I make \$100 in income and pay 20% tax of \$20.  I have \$80 left, which I invest in a stock.  The stock goes up in value at the same rate as inflation, about 3.5% a year.  After 20 years, it's worth about \$160, but \$160 has the same value now as \$80 did when I earned it.  So in effect, I have made no gain.  If I have to pay a 20% tax on the \$80 nominal gain, then I actually have \$8 less spending power than when I made the investment.  Just in order to keep pace, I would have needed returns of 125% inflation.   Another reason is to encourage investment.  A lower tax on capital should encourage savings, and the US is generally low on savings.   There is also a historical argument that if the income tax rate is 90% (which was the top rate starting in World War 2 and ending in the Kennedy administration), then it is impractical to tax investments at the same rate.  From 1921 to 1986, there was a lower rate on long term capital gains than on other income.  In 1991, capital gains tax was capped at 28%.  In 1997, long term rates were reduced again.   The current system is designed to encourage long term capital investments by decreasing the rate the longer something is held.  This does very little to help with nominal gains but does provide an incentive for long term savings.  It's unclear if it actually increases saving.  It certainly doesn't increase net savings, as the US tends to be fully leveraged, borrowing as much as is saved.   There have been proposals that focused more on the nominal gains issue.  For example, in the late 80s, early 90s there was a proposal to switch investments to working like traditional IRAs:  deductible at the time of investment but taxed the same as any other income at the time of sale or withdrawal.  Since this delayed the original tax to be paid at the same time as the capital gains, there is no separate tax on the nominal gain.  "
Productivity vs real earnings in the US -- what happened ca 1974?,"Here is one explanation, albeit focusing on the wider economy, and comparing median wages. Still the method applies:  This is taken from Bivens and Mishel (2015). In essence, they decompose labour productivity into its different components (see technical appendix, page 25). These components are three: Labour share: how much of total product (they use Net Domestic Product, NDP) is paid to workers Terms of trade: discrepancies between NDP deflator and CPI. Recall real wages are deflated with CPI, whereas GDP price index include other stuff like price of investment, terms of trade, etc.  Inequality of pay: a rough measure of the difference between mean and median pay. If wages are equally distributed, these two are equivalent. If you are interested on mean wages, then the third component does not apply. The other two do. Thus, a lot of the problem is due to prices issues. As the authors note: That is, workers have suffered worsening terms of trade, in which the prices of things they buy (i.e., consumer goods and services) have risen faster than the prices of items they produce (consumer goods but also capital goods). Thus, if workers consumed investment goods such as machine tools as well as groceries, their real wage growth would have been better and more in line with productivity growth. We sometimes refer to this terms-of-trade wedge as the difference between “consumer” and “producer” price trends. (p.6)"
"What does Yanis Varoufakis mean by ""surplus recycling mechanism""?","""Surplus recycling"" is a term coined (to my knowledge) by Varoufakis to describe the fact that a country that enjoys a trade surplus should reinvest the surplus in the domestic economies of its trading partners. Such a policy was conduced with success by the United States in the years following WW2, where the Marshall plan and similar policies in Asia took place, mainly for political reasons. As Varoufakis states it, there is no reason for the market to proceed to such a transfer, yet it does sound reasonable to keep trade partners in good shape. In Europe, Germany's trade surplus is notable for being the only important European surplus. Surplus recycling is one way the Greek government is trying to get Germans to feel right about handing out money to European countries that are struggling with their economy, another infamous attempt being WW2 reparations. I believe you're somewhat wrong in your interpretation because the question here is not whether to reinvest or not (this is a problem which is well treated by ""classic"" macroeconomics), but where to reinvest the money. The alternative is hence between domestic and foreign investments, and there is no known economic mechanism that supports foreign investments unconditionally. Varoufakis work aims at fostering foreign investment from Germany to southern Europe. I don't see anything controversial in his definition, but he is defining a policy, not a model. The problem with solely-domestic investments is that, on the long term, it weakens trade partners, which weakens domestic economy in turn. I don't think there is much debate among the fact that trade balance is not a self-correcting problem. The real debate (and it's political for the moment, an open field for economic research) is on what end the policies to solve the problem has to be carried: the one for which it is easier to solve (Germany), or the one for which the problem is worse (Greece). The amount of money we're talking about is sizable to say the least, and can be viewed here as suggested in a comment."
How accurate is duality?,"Let me focus on the use of duality theory in demand analysis (as this is what I'm most familiar with). Usually, in order to obtain a demand system (which you can then estimate) you need to take the following steps. Step 3 is the trickiest part as usually no closed form solutions are possible. To see this, notice that the first order conditions give the following system:
$$
\frac{\partial u(x_1, \ldots, x_n)}{\partial x_n} = p_i\,\, \lambda,\\
\sum_i p_i x_i = m.
$$
Solving for $\lambda$ we have:
$$
\dfrac{\dfrac{\partial u}{\partial x_i}}{\sum_j \dfrac{\partial u}{\partial p_j}} = \frac{p_i}{m}
$$
This gives the (normalized) prices on the right hand side as a function of the quantities (so the inverse demand functions). To obtain a closed form solution of the quantities in terms of the functions, this system needs to be inverted. Except in some particular cases (e.g. Cobb-Douglas or CES) we do not know how to do this. The dual approach starts from a specification of the expenditure function and uses known duality results to obtain a closed form expression for the demand functions. It takes the following steps: As an exemple, consider the widely used Almost Ideal Demand System specification of Deaton & Muellbauer Start from the following (flexible) specification for the expenditure function.
$$
\ln e(p,u) = \alpha_0 + \sum_i \alpha_i \ln(p_i) + \sum_{i,j}\frac{1}{2}\gamma_{i,j} \ln(p_i) \ln(p_j) + u \prod_i p_i^{\beta_i}
$$
We impose $\gamma_{i,j} = \gamma_{j,i}$, and by homogeneity of degree 1 we have that $\sum_i \alpha_i = 1$ and $\sum_{i = 1}^n \gamma_{i,j} = 1$ and $\sum_{i} \beta_i = 0$. First we can take the derivative $\frac{\partial \ln e(p,u)}{\partial \ln p_i}$ to get the demand share equations. Shephard's lemma gives:
$$
\frac{\partial \ln e(p,u)}{\partial \ln p_i} = \frac{p_i h_i(p,u)}{e(p,u)} = \alpha_i + \sum_j \frac{1}{2}\gamma_{i,j} \ln(p_j) + u \beta_0 \beta_i \prod_i p_i^{\beta_i}
$$
Here $h_i(p,u)$ is the Hicksian demand for good $i$. Writing by $v(p,m)$ the indirect utility function, we know by duality between utility maximisation and expenditure minimisation that:
$$
m = e(p,v(p,m)) \text{ or } u = v(p,e(p,u)).
$$
Using this, we can invert the expenditure function to obtain the indirect utility function.
$$
v(p,m) = \frac{\ln m - \alpha_0 - \sum_j \alpha_j \ln(p_j) - \sum_{i,j}\frac{1}{2} \gamma_{i,j} \ln(p_i) \ln(p_j)}{\beta_0 \prod_i p_i^{\beta_i}}
$$ Also by duality, if $x(p,m)$ is the Marshallian demand function then:
$$
x_i(p,m) = h_i(p,v(p,m)).
$$
So we can substitute $v(p,m)$ into $h_i(p,u)$ to get the shares as a function of prices and income. $$
\begin{align*}
s_i(p,m) &= \frac{p_i x(p,m)}{m} = \alpha_i + \sum_j \frac{1}{2}\gamma_{i,j} \ln(p_j) + \beta_i \left(\ln m - \alpha_0 - \sum_j \alpha_j \ln(p_j) - \frac{1}{2}\sum_{i,j} \gamma_{i,j} \ln(p_i) \ln(p_j)\right),\\
&= \alpha_i + \sum_j \frac{1}{2}\gamma_{i,j} \ln(p_j) + \beta_i \ln(m/P).
\end{align*}
$$
where:
$$
P = \alpha_0 + \sum_j \alpha_j \ln(p_j) + \sum_{i,j} \frac{1}{2}\gamma_{i,j} \ln(p_i)\ln(p_j).
$$
This is the Almost Ideal Demand System. Conditional on $P$, shares are linear in the log of income and log of prices. The symmetry and homogeneity restrictions immediately translate to conditions on the coefficients of this system."
Why can't a country print its own money to spend it only abroad?,"In order for one country to spend in another, it needs to exchange its currency for the currency of the other nation. This is done through something called the foreign exchange market. Like most markets however, the laws of supply and demand apply here too. If a country suddenly starts printing a lot of money, the actual value of its money in the foreign exchange market will go down (due to its increased supply). This will ultimately mean the currency will depreciate with respect to others, and will thus have lower buying power."
"Car prices haven't changed in 20 years, no inflation?","Inflation is measured against a basket of goods. It's a symptom of what's going on in markets. Some products go up in price over time. Some go down in time. Some stay the same price, but change their specification. So it's looking down the wrong end of the microscope, to ask why inflation hasn't affected car prices. Car prices are part of inflation. Changes in car prices affect inflation. The causal link the other way is very very weak. Inflation puts pressure on wages. If this causes wages to rise, then the cost curve shifts, and equilibrium prices change. But for cars, wage costs are a very small part of total car manufacturing cost. And the market for labourers in the industry has shrinking demand and over-supply, so upward pressures are very weak. The inflation experienced by car manufacturers is very different to the inflation experienced by the public. We've been through a global financial crisis and a super-cycle in commodities. A general-public inflation measure is a very poor measure of input-cost inflation for car manufacturers."
"I lost $100 in the laundry, who won?","The central bank that issues the currency won; they can now issue an additional $100 without increasing the price level. This is similar to seignorage rents due to overseas circulation— in fact, to a central bank, the two are indistinguishable, as the central bank would be unaware that your note had been destroyed."
Visualization tools for game theory: Decision trees,"The forest package of LaTeX allows you to draw game trees with pretty simple syntax. After copying a pre-set template into the LaTeX preamble, one can build up the game tree using a nested [] syntax, then the program takes care of node placement/spacing/etc.  Here's an example from the forest manual (""Decision tree"" in Section 5.1):   The game theory explorer is developed by a few people at LSE. It allows users to input matrix-form games or build extensive form games through a GUI. It also seems capable of converting between normal and extensive form games. Additionally, the software comes with a solver that looks for Nash equilibria of the inputted game.  Screenshots: 
 "
What is the importance of Epstein-Zin preferences?,"I think CompEcon covered most of the points that I was going to mention.  Just a few last thoughts: 1) Why are Epstein-Zin preferences important? The preferences are important because they allow you to separate two of the dimensions along which people care about their allocations; namely, risk aversion and intertemporal substitution. Additionally, one short coming of standard (i.e. CRRA) is their inability to achieve the Hansen-Jagannathan lower bound for the ratio of the standard devation of the stochastic discount factor to its expected value, $\frac{\sigma(m)}{E(m)}$.  In a 2000 paper (I think his Job Market paper), Tallarini showed that recursive preferences are able to come into the Hansen-Jagannathan bounds at less ridiculous levels of risk aversion (although still not totally feasible risk aversion). 2) How does recursive utility differ from other preference models in general? What do they capture that can't be captured otherwise? It allows you to capture the risk aversion - intertemporal substitution differences. They are a more general set of preferences than CRRA.  I'm pretty sure you can actually write CRRA utility using Epstein-Zin preferences with the right parameters. Would be interested in hearing if other people know more about this.  I know thy can be have model misspecification interpretations.  Would love to hear more about that. 3) What are some good resources to learn more about them? Like I said earlier in a comment, I have found that Ljungqvist and Sargent provide a pretty good explanation of the main things happening in recursive utility. Additionally, the paper mentioned by CompEcon in another question earlier is a pretty good resource.  I'm actually working through this right now when I have some spare time."
Why is fractional reserve banking allowed?,"I recommend a careful read of the Wikipedia page for Fractional-reserve banking because it seems you are confused about how fractional reserve banking works.
Banks only lend out money they have under fractional reserve banking and, in fact, strictly less than they have. Those funds are a mix of deposits, funds from other debt issuance, and bank equity. They don't lend out funds they don't have and they don't simply credit accounts at will in the way you suggest. When they credit a borrower's account with funds, they have to debit those funds from elsewhere on their balance sheet. So, lend-able funds go down as borrower's funds go up, and then the resulting loan is a new asset of the firm. While they do create money this way, they do so at the expense of these lone-able funds so the loaning out doesn't create new assets for the bank."
Complete Markets in Continuous Time,"I am the last person that should be answering continuous time questions like these, but if there's no one else I guess I'll give it a shot. (Any correction of my dimly remembered continuous-time finance is very welcome.) My impression has always been that this is best interpreted as a consequence of the martingale representation theorem. First, though, I'll loosely establish some notation. Let the probability space be generated by the $n$ independent Wiener processes $(Z_t^1,\ldots,Z_t^n)$. Let there be $n+1$ assets, where the value of the $i$th asset at $t$ is given by $S_t^i$. Assume that asset $i=0$ is a riskfree bond $dS_t^0=r_tS_t^0dt$,  while assets $i=1,\ldots,n$ are each risky and are driven by the corresponding $Z_t^i$:
$$dS_t^i=\mu_t^idt+\sigma_t^idZ_t^i$$
Assume there is a strictly positive SDF process $m_t$ normalized to $m_0=1$, such that $m_tS_t^i$ is a martingale for each $i$ (basically the definition of SDF) and where
$$dm_t=\nu_t dt+\psi_t\cdot dZ_t$$
(I use $\cdot$ as the dot product, which will be convenient.) Finally, let the $n+1$-dimensional vector $\theta_t$ be our portfolio at time $t$, such that net worth $A_t$ is given by $A_t=\theta_t\cdot S_t$. Assume that $A_0$ is fixed and that further we have
$$dA_t=\theta_t\cdot dS_t$$
Now I'll state the objective, which captures the essence of complete markets. Suppose that the world ends at time $T$, and that we want net worth $A_T$ to equal a certain stochastic $Y$, which can depend on the full history up until time $T$. Suppose that $A_0=E_0[m_TY]$, so that in a world with complete markets we could (at $t=0$) use our initial wealth $A_0$ to purchase the time $t=T$ payout $Y$. In the absence of these direct complete markets, the question is whether there is nevertheless some strategy for the portfolio $\theta_t$ that will allow us to obtain $A_T=Y$ in all states of the world. And the answer, in this setting, is yes. First, one can calculate $d(m_tA_t)=\theta_t\cdot d(m_tS_t)$. Thus $m_tS_t$ being a martingale implies that $m_tA_t$ is a martingale. Thus we have $A_T=Y\Longleftrightarrow m_TA_T=m_TY$ iff
$$m_tA_t=E_t[m_TY]$$
for all $t\in[0,T]$. Note that this is true for $t=0$ by assumption; hence to get equality it is only necessary to prove that the increments are always equal on both sides. Now the martingale representation theorem comes in. Since $E_t[m_TY]$ is a martingale, we can write
$$E_t[m_TY]=E_0[m_TY]+\int_0^t \phi_s\cdot dZ_s$$
for some predictable process $\phi_s$. So we need to be able to show $d(m_tA_t)=\phi_t\cdot dZ_t$. Writing
$$d(m_tA_t)=\sum_i (m_t\theta_t^i \sigma_t^i +A_t\psi_t^i)dZ_t^i$$
we see that we need $m_t\theta_t^i\sigma_t^i +A_t\psi_t^i=\phi_t^i$ for each risky asset $i=1,\ldots,n$, which we can invert to give the needed portfolio choice $\theta_t^i$:
$$\theta_t^i=\frac{\phi_t^i-A_t\psi_t^i}{m_t\sigma_t^i}$$
The riskless asset portfolio choice $\theta_t^0$ can then be backed out from $A_t=\theta_t\cdot S_t$. The intuition here is simple: we need to always have $A_t$ adjust to maintain the equality $m_tA_t=E_t[m_TY]$, but both the expectation on the right and the SDF $m_t$ on the left are moving in response to the driving processes $dZ_t^i$. Hence we need to pick a portfolio $\theta_t$ such that $dA_t$ precisely offsets these movements and the equation continues to hold. And we can always do this as long as locally, our assets span all the risks $dZ_t^i$ -- which can happen more generally, even for $n$ correlated assets as long as their increments are locally linearly independent. (The case here of $n$ risky assets each drien by an independent Brownian motion is a special one.)"
Implications of abolishing Fractional Reserve Banking on mortgages and interest rates,"There is a long standing problem in most discussions of Fractional Reserve Banking (FRB), around the precise definition of money. Cash money (that is M0) is an asset on the banking system balance sheet, while deposit money (the liability) is the money that is created by lending.  Ever since the introduction of cheques specifically, and in general the ability to make direct transfers between two deposit accounts, deposit money has been the de facto form of money that is used in the economy. Cash money is maintained at a fractional reserve of deposits (it used to be a regulator of total deposits and lending, but in practice this isn´t the case anymore), and used primarily for inter-bank liquidity settlements, as transfers occur between bank accounts at different banks. Reserve requirements are now down to 2% or less in most countries. So taken literally, the question is what happens to an economy when its money supply is reduced by 98%, and at the same time total lending shrinks by whatever proportion bank lending is of total lending in the economy (typically 1/3 or more). In the short run, congratulations, you no longer have an economy. Nobody can pay their non-bank debts any more, because there isn´t enough money to pay them with. Bear in mind debt payments do not automatically adjust to changes in the money supply. That triggers a debt cascade causing wide-spread company failure. The accompanying impact on all prices causes massive disruption in the supply chain, because everything in the economy is now completely mis-priced relative to everything else. A 20% reduction in the liability money supply in the US in 1930-33 for example caused the price of food bought from farmers in some parts of the USA to be less than the cost of the fuel required to get the food to the market for sale. This event would clearly be far worse than that. Market based systems can and will adjust prices over time to compensate for money supply changes, but that´s the long term. It has been said that 'every civilisation is only 2 meals away from barbarism', and I´m afraid you just pushed yours over the edge.  In the long term, after a period of market exchanges being performed with cigarettes, and other money-substitutes, and those responsible for this mess being hunted down and dealt with appropriately, people will probably feel that fractional reserve banking wasn´t such a bad idea after all.  Some form of Germany Year 0 event will take place and the banking system will be restarted. Since all debt was effectively discharged during the ""unfortunate event"" the next few decades will probably be referred to as a golden age. By the survivors. If instead we assume that rather than reducing the money supply (which we know is a bad idea), the banking system is instead converted so that there is no fractional reserve lending, asset cash is injected to replace debt, and maintain the current quantity of deposits. All deposits in the system are simply held constant from this point on, and you now have a constant money supply economy. As a side note, everybody now has to pay for their bank accounts, because since banks are no longer lending institutions, they need a revenue source to fund their operations. Quite a large revenue source - ATM networks aren´t cheap. The problems caused by a sudden drop in total lending are still present. In practice this will disrupt the economy in the short term, but hopefully not quite as badly as in the previous scenario. The short answer though, is we simply don´t know how a constant money economy behaves in practice. There hasn´t been a constant money economy since fractional reserve banking was invented, and that´s at least 3-400 years now (depending where you put goldsmith banking in the banking continuum, and what country you´re in).  A lot of financial statistics would probably start looking very strange, since even correcting for CPI doesn´t adequately control for the impact monetary growth has on price measurements. It´s possible that the economy would settle down after a few years as everything adjusts around the constant money supply - that´s essentially the general equilibrium theory argument. It´s also entirely possible that the growth in the money supply that FRB provides over time makes it easier for adjustments to occur - a lot of arguments in favour of small positive inflation amounts essentially revolve around this - and that a bunch of issues we didn´t previously know about emerge and cause new problems.  While this is very much an open research question, it´s interesting to note that a side effect of some of the recent Basel 3 regulatory changes is that at least for the last couple of years, several countries including the UK have experienced stable money supplies. If this continues (it´s perfectly possible to regulate fractional reserve banking so that it doesn´t expand the money supply if you know what you´re doing - whether the central banks really know what they´re doing is also an open research question), then we may start to have some empirical data that can help shed light on this very interesting question."
Has an increase in housing supply in popular cities including Amsterdam led to an increase in house prices?,"Let's just say that economists don't really agree with this thesis, and most city-level empirical evidence points in the opposite direction. E.g. We ultimately conclude, from both theory and empirical evidence, that adding new homes moderates price increases and therefore makes housing more affordable to low- and moderate-income families. [...] A few studies use panel data and find that the imposition of more stringent land-use controls
  leads to lower supply and higher prices. Jackson (2016) uses longitudinal data from California’s
  cities to assess the effect a city’s adoption of additional land-use regulations has on the number of
  new construction permits issued, and finds that each additional land-use regulation adopted
  reduced multifamily and single-family permits by an average of more than 6% and 3%, respectively,
  and that regulations reducing allowable density had even larger effects. Zabel and Dalton
  (2011) use longitudinal data from localities in Massachusetts and find that increases in minimum lot
  sizes are followed by significant increases in prices. Looking at longitudinal data on municipalities
  in the Boston (Massachusetts) metropolitan area, Glaeser and Ward (2009) find that the adoption of
  stricter local regulations leads to higher house prices, but the coefficient falls in magnitude and
  loses significance once they control for population demographics. They point out that this is
  expected, if homes in other jurisdictions are seen as perfect substitutes. Thus, whereas supply
  restrictions may increase prices in a market as a whole, they may not increase them disproportionately
  in the particular locality where they are imposed due to spillover effects across jurisdictions. Several other researchers use instrumental variables to try to more clearly assess the causal
  effects regulatory restrictions have on housing supply and prices. Ihlanfeldt (2007) uses such an
  approach to study regulation in localities in Florida and finds that predicted regulations significantly
  increase the price of single-family homes. Saks (2008) uses instrumental variables and shows
  that increases in labor demand lead to less residential construction and larger increases in housing
  prices in metropolitan areas with more restrictive housing supply. Hilber and Vermeulen (2016)
  show that changes in demand lead to increases in local house prices rather than increases in
  supply in municipalities in England with greater regulatory restrictions, measured by the refusal
  rate of proposed residential projects and the number of project approvals delayed more than
  13 weeks. In sum, the preponderance of the evidence shows that restricting supply increases housing
  prices and that adding supply would help to make housing more affordable. [...] although it is surely true that land is constrained, especially in certain
  markets (Saiz, 2010), land can be used more intensively to allow for more housing. The limits on the
  land with which housing is bundled make housing different from many goods, but the difference is
  one of degree: the supply of housing can and does increase even in constrained markets, and
  prices should generally fall in response (see the review by Dipasquale, 1999; Mayer & Somerville,
  2000). [...] most housing filters down, or loses
  value as it ages, representing new supply in submarkets at lower price points. [...] Indeed, recent research shows that filtering
  was the primary source for additions to the affordable rental stock between 2003 and 2013,
  whereas new construction was the largest contributor for the higher priced rentals, and tenure
  conversion was the largest source for moderately priced rentals (Joint Center for Housing, 2015, fig.
  14). Further, Weicher, Eggers, and Moumen (2016) report that 23.4% of the rental units that were
  affordable to very low-income renters in the United States in 2013 had filtered down from higher
  rent categories in 1985. Another 21.8% were conversions from formerly owner-occupied homes or
  seasonal rentals.9 Most of the higher priced rental units that filtered down to become affordable in
  2013 were moderate-rent units in 1985, but 15% of those that filtered down were high-rent units in
  1985. Note that filtering occurs over a shorter time frame too; among affordable units in 2013,
  19% had been higher rent units as recently as 2005. Recent research analyzing the incomes of successive occupants of homes also suggests substantial
  downward filtering, particularly of the rental stock due to tenure conversion; as the owner-occupied
  stock ages, a portion converts to rental (Rosenthal, 2014). Rosenthal also finds, however,
  that filtering rates are considerably lower in areas with high house price inflation, although
  downward filtering still occurs. In short, new construction is crucial for keeping housing affordable, even in markets where
  much of the new construction is itself high-end housing that most people can’t afford. A lack of
  supply to meet demand at the high end affects prices across submarkets and makes housing less
  affordable to residents in lower-cost submarkets. Finally getting to the discussion of the precise thesis you ask about: Some skeptics argue that even if additional supply could help make housing more affordable in the
  short run, it won’t in the long run because the additional supply will induce more demand, especially
  among buyers or renters wealthier than the existing residents in the neighborhood (Redmond, 2015).
  The claim is analogous to the argument that building more highways will not reduce congestion
  because the lower cost of travel will simply cause more people to drive or to take that particular route
  (Gorham, 2009). In this case, the argument is that by making the jurisdiction more affordable, adding
  housing supply will attract new demand—both from current residents who would otherwise leave,
  and from people living elsewhere who will now choose to move to the jurisdiction. Further, the argument goes, lower rents and prices may also induce latent demand—people who are living with
  roommates or family members may choose to form their own households (Ellen & O’Flaherty, 2007) or
  people may choose to invest in pied-à-terres in a city. That additional demand will drive prices back up
  until supply can again respond, causing housing to be more affordable, at best, only cyclically,
  according to the argument, and increasing the density of the jurisdiction, with the attendant costs
  of congestion. Although building additional highways does appear to induce more demand (Duranton &
  Turner, 2011), in the case of housing, additional demand is unlikely to completely offset the new
  supply. Such an offset requires demand curves to be perfectly elastic—or, in other words, it
  assumes that neighborhoods and jurisdictions are perfect substitutes and that there are no
  constraints on the ability and willingness of households to move. That is unrealistic. Moving
  homes is not like driving a few extra miles (Lewyn, 2016), and costs associated with moving may be
  high. Any additional demand induced by new housing is limited by personal and economic
  constraints on the ability and willingness of households to move, restrictions on immigration, and
  uncertainty and other factors that might inhibit renters and buyers from renting or buying in the
  market in which housing supply increases. Indeed, mobility rates have fallen sharply over the past
  several decades, and although the reasons for the decline are being debated, the decline reveals
  significant constraints on the ability and willingness to move. Thus, in the long run, whereas some additional households may be drawn from outside (or from
  within the city) to buy or rent homes as supply increases, it is highly unlikely that prices will end up
  at the same level that they would have reached absent any new supply. Finally, as noted above, the
  empirical evidence shows that allowing more supply leads to lower housing prices; if adding
  supply induced sufficient additional demand to offset the increased supply, the studies would
  not find an association between supply and prices. It can happen indeed in some very limited circumstances, and with localized [e.g. neighborhood] effect, which is probably on what this faulty generalization is based on: Many renters in neighborhoods where market-rate housing is proposed express concern that the
  construction of new housing will actually make their affordability problems worse by raising rents or
  house prices, fueling gentrification, and potentially displacing existing residents (Atta-Mensah, 2017;
  Savitch-Lew, 2017). Hankinson (2017) theorizes that renters’ opposition to local additions to supply is
  driven by such worries; he argues that it is plausible that the construction of an attractive new building
  will increase prices locally (by improving the physical landscape, bringing new amenities to the
  neighborhood, and signaling that the neighborhood is improving), even as it reduces them citywide. Testing this proposition empirically is quite challenging, given that developers will naturally be
  attracted to areas where prices and rents are rising. There is evidence that improvements to
  blighted housing can, in some circumstances, increase surrounding property values, even when
  the new or improved housing is subsidized, low-income housing (Diamond & McQuade, 2016;
  Schwartz, Ellen, Voicu, & Schill, 2006). The new housing studied, however, typically replaced
  vacant, abandoned buildings and littered vacant lots, in essence removing a disamenity. [...] There is little empirical evidence about the net effect new market-rate housing has on the prices or
  rents of nearby homes, and what exists may not be causal. One recent study examines the effect of
  market-rate single-family homes newly constructed on infill sites, and finds that newly constructed
  single-family homes can have positive impacts on the sales price of other single-family homes nearby,
  but the effect varies with context (Zahirovich-Herbert & Gibler, 2014). A study of multifamily high-rise
  infill developments in Singapore found positive price effects on nearby houses (Ooi & Le, 2013), as did
  a study of single multistory apartment buildings constructed in Helsinki (Kurvinen & Vihola, 2016).
  These studies all consider property values and not rents, and none is able to prove a causal relationship
  given that market-rate developments aim to target neighborhoods where they expect property
  values to improve. Unfortunately, we found no study examining impacts on rents, although one study
  by the California Legislative Analyst Office concluded that additional market-rate construction is linked
  to lower displacement rates (Taylor, 2016). Examining low-income neighborhoods in the Bay Area
  between 2000 and 2013, these researchers found that the production of market-rate housing was
  associated with a lower probability that low-income residents in the neighborhood would experience
  displacement. Although a singular study, the findings suggest that for neighborhoods in high-demand
  cities, blocking market-rate construction may place greater pressures on the existing stock. In short, although it is clear that the construction of new homes will moderate price and rent
  increases citywide, neither theory nor empirical evidence provides clear guidance about when
  localized spillover effects might occur and when they might actually cause an increase in the prices
  and rents of immediately surrounding homes. There's a more recent study on the core issue of the question, not covered in the above review: Preliminary results using a spatial difference-in-differences approach suggest that any induced demand effects are overwhelmed by the effect of increased supply. In neighborhoods where new apartment complexes were completed between 2014-2016, rents in existing units near the new apartments declined relative to neighborhoods that did not see new construction until 2018. Changes in in-migration appear to drive this result. Although the total number of migrants from high-income neighborhoods to the new construction neighborhoods increases after the new units are completed, the number of high-income arrivals to previously existing units actually decreases, as the new units absorb a substantial portion of these households. On the whole, our results suggest that—on average and in the short-run—new construction lowers rents in gentrifying neighborhoods. See also a blog discussing it (which also does a good job at debunking the roads analogy); in summary, the gist of the latter study is in this chart:  This chart shows that rental prices for apartments close to the new building fell relative to the prices for apartments located further away. The dashed “before” line has a negative slope, suggesting that prices declined the further you got from the site of the new building.  The solid “after” line has a positive slope (prices increase the further you get from the new building).  Overall, prices are higher (the solid line is above the dashed line), but prices actually went down next to the new building, and increased far less than in the area further away from the new building. These data are a strong challenge to the induced demand theory.  If a new building made an area more attractive, one would expect the largest effect in the area very near the building. But, consistent with the traditional “more supply reduces rents” view, the addition of more units in an area seems to have depressed rents (or at least rent increases) compared to buildings in the surrounding area. In other words, even the theory that prices might increase locally in the neighborhood of a new building doesn't hold up, on average, to a difference-in-differences test. Without a difference-in-differences approach one can of course draw the wrong conclusion since overall demand in a city may be increasing faster than the overall supply. Even more recently, another paper on NYC (not yet published in a peer-reviewed venue, but was mentioned in a recent NYT article): I find that for every 10% increase in the housing stock within a 500-foot buffer,
  residential rents decrease by 1%. The rent reduction is caused by the completion of new
  high-rises rather than their approval. Across neighborhoods, the impact is smaller in more
  central areas, presumably due to more elastic demand. Within neighborhoods, the impact
  is smaller for lower-rent buildings. Finally, the negative impact appears to be driven by
  supply effects rather than dis-amenity effects, like changes in neighborhood physical features,
  blocked views, or shadows. New housing units alleviate the growing demand for existing
  housing units and moderate the rapid growth of residential rents. Residential property sales prices also decrease when new high-rises within 500 feet
  are completed. [...] Because 99% of new high-rises are condos and rental
  buildings, they do not significantly affect sales prices of co-ops and 1-5 family homes. The negative impact is bigger for closer substitutes, further confirming the negative impact is due to supply effects. To address the hypothesis about amenity effects, I find new high-rises and their
  high-income tenants attract new full-service restaurants, cafes, and coffee shops. These consumption amenities likely make neighborhoods more attractive and potentially increase rents
  and sales prices (Couture and Handbury, 2017). However, the amenity effect is dominated by the supply effect, given that rents and sales prices still fall on net. As was mentioned in a OP's comment below, living in a big city vs a small town (and vs the prairie) is an amenity effect. However, there seems to be a limit to this effect after which more housing supply lowers the (rental) price. The amenity effect has been studied for decades but alas often in isolation, most prevalently perhaps with the the Roback model, which basically adapted Rosen's more general spatial equilibrium model to ""nontraded"" goods like housing. (""Nontraded"" because Roback only considered effects on rents.) Using this model, one essentially gets (rental) price-amenity gradients. Roback assumed that ""the cost of changing residences is zero"" (i.e. costless migration). Modelling ""partially open"" cities, i.e. with non-zero (and non-infinite) relocation costs across its boundaries is a relatively more recent endeavour. Likewise for considering the combined effect of housing supply and amenities on rents. As one such theoretical work notes as housing is supplied more elastically [...] the rent effect of amenities approaches zero."
Is money we make completely taken away by taxes?,"No, this will not happen. Government does not tax people just for 'shits and giggles'. Governments do not hoard money at some pile as a some sort of dragon from a fantasy novel.  Taxes are levied so government can spend that money on some public goods or to use them as transfers to alleviate poverty or inequality. As you correctly point in your question, your spending is someone's else income. Government spending works the same way. When government spends money on a new bridge it creates profits for the contractor, wages for the workers, rents for landowners and interest to capital owners. When it comes to transfers, transfers are not income per se but then spending of the people who receive transfers will create incomes for whatever business and its employees and investors the people getting transfers choose to patronize. Sure if the government would keep all the money they tax, and if money supply would be kept constant by central bank, then at some point all the money would be eventually siphoned out of the economy and government would get 'all the money', while the economy would be in ruin because people would have to revert to barter and there would be no provision of basic public goods such as police etc. However, in real life governments rarely if ever avoid spending money they tax. In fact most real life governments are running relatively large debts (e.g. see OECD statistics) showing that most governments spend usually more than they even tax."
Why aren't we seeing carbon taxes in practice?,"I think it’s mainly politics. For example, when France tried to implement increases in tax on  oil (indirect way of taxing carbon) it led to yellow jacket protest. As you pointed out it’s easy to track who produces the carbon in the economy. Especially on industrial level in developed countries. The reason 1 could be problem in some developing countries though. As you pointed out the revenue can offset any negative effect as with Pigovian taxes there is no deadweight loss - if set properly  Again you answered yourself plus I would add in a long run it’s better to undergo temporary shock to avoid higher cost down hill. I don’t think you can say it’s unfair. In fact I would say the fact that the industries create externality in the first place due to lack of property right is unfair (at least if we agree that creating externality is not fair to society which I think most moral thinkers would - although that’s question for philosophy SE). You again answered yourself. If you look at the attempts to introduce carbon taxes then you will see 6-8 are the main culprits. In France you get yellow jacket protests in Netherlands farmers protests, in Washington voters refused carbon taxes in referendum last year (see here)etc. - all responses to trying to introduce Pigovian taxes of one kind or another. People of any political affiliations generally don’t like taxes when they are taxes on them as well. I suppose many people also are skeptical the government will make it revenue neutral later by lets say sending check to all people so their income stays the same. In case of redistribution some might not like that and others also don’t believe social spending will increase."
"In today's money, what was the value of a 1492 Spanish maravedí?","Walsh (1931): The author of this work has translated maravedis into dollars of 1929 by reference to statistics on purchasing power in wheat, corn and other staples. Hence his opinion that the maravedi was worth about two American cents of 1929. According to the BLS inflation calculator, \$1 in 1929 is about \$15 today (2020). So, if we accept Walsh's estimate, then one maravedi in 1492 converts to about \$3.00 today. (And if we also accept Satava's estimate, then Columbus's voyage cost 1,765,734 $\times$ \$3.00 $\approx$ \$5,297,200 in today's USD.)"
Is Malthusian theory of population growth being realized?,"The annual growth rate of the global population has been in decline since about 1967 (five decades ago). The absolute annual growth peaked in 1987 (three decades ago). Malthusians claim that: If either of those do not hold, then Malthusian theory does not hold. And it turns out that neither of them hold: Some relevant factors: cheap, reliable, ubiquitous contraceptives; education and emancipation, particularly for women; cheap and plentiful crop fertilisers; mechanisation of farming; and selective crop breeding, by taking the old practice of eating the best of each crop and sowing the worst, and doing the exact opposite. Here are two charts, using the global historic data from the US Census organisation for 1800-1950, and the UN data for 1950-2014. "
Use of mathematics and imprecise definition of terms,"Edesess is attacking what is really just a straw man of economics. I'm not sure he really understands the field. To start, economics is not math. We're not claiming that it is. It's more of an ""applied"" science. Economists have never claimed that these definitions are precise in the way that mathematics is. These definitions are modeling constructs---they're for applied work. They're use is temporary in a way. The point is to try to convey an idea in a more precise way than just in words---but everyone knows that they're not a precise as we would like and not as precise as they ultimately should be. They're meant to be debated and later refined. But, as all applied scientists know, you've got to start somewhere and sometimes ideas are best conveyed through simpler---if less detailed mean.   Coming up with better definitions is a huge part of the economic science. Consider these examples. When the Cowles Foundation was founded in 1932 its motto was ""Theory and Measurement"" (the motto was first adopted in 1952). Measurement is not an easy thing to do. As another example, a lot of Larry Kotlikoff's work has dealt with how a lot of fiscal measures are not economically well-defined concepts. Einstein taught us that neither time, nor distance are well-defined physical concepts. Instead, their measurement is relative to our frame of reference – how fast we were traveling in the universe and in what direction. Our physical frame of reference can be viewed as our language or labeling convention. ... Kotlikoff, along with Harvard's Jerry Green, offered a general proof of the proposition that deficits and a number of other conventional fiscal measures are, economically speaking, content-free, concluding that the deficit is simply an arbitrary figment of language in all economic models involving rational agents. Also, take another example of current interest. Lars Hansen's recent work (winner of the 2013 Economics ""Nobel"" prize) has focused on the difficulty and ongoing failure to define certain economic concepts, including ""bubbles"" and systemic risk. See his essay ""Challenges in Identifying and Measuring Systemic Risk"". I'm a fan of the dictum he relays, attributed to Lord Kelvin, I often say that when you can measure something that you are speaking about,
  express it in numbers, you know something about it; but when you cannot
  measure it, when you cannot express it in numbers, your knowledge is of the
  meagre and unsatisfactory kind: it may be the beginning of knowledge, but you
  have scarcely, in your thoughts advanced to the stage of science, whatever the
  matter might be.  He notes that ""an abbreviated version appears on the Social
Science Research building at the University of Chicago."" So, yeah, economists (as social scientists) definitely take this seriously. So, the point is that economists are well aware of the problems in these ""definitions."" They are a part of ongoing research in the field; sometimes they're ignored if people don't think they're first-order to the problem; etc... "
Topological concepts in economic theory,"I strongly suspect that an emerging important area for applications of measure theory will be in approximate dynamic programming techniques. Approximate dynamic programming (aka ""reinforcement learning"" in the computer science literature) has been the direction of research work in the last ~10-20 years of the dynamic programming literature. Economics is only just now starting to adopt some of these advances. For example of the direction of the DP literature, see Bertsekas' most recent 4th edition expansion of his dynamic programming series, or Powell's Approximate DP: Solving the Curse of Dimensionality. Economists are just starting to pick up some of these tools, both directly and indirectly, and I suspect that they will have a growing impact on the literature over the next few years. Some of the analytical background for convergence of these methods is topology and dynamical systems. A good example of theoretical contribution to this type of literature from economists is Pál and Stachurski (2013), Fitted Value Function Iteration With Probability One Contractions (ungated version here). Peruse that paper and you can see the importance of a good grasp of measure theory. Stachurski's book Economic Dynamics is actually a very nice exposition of dynamic programming from this perspective, building at a pace which works for multiple levels of graduate student/professional (measure theory comes in formally at the end I believe -- I'm still working towards those insights).  Hopefully this answers your question to some degree. I'm afraid that the phrase ""post-1960s mathematics"" is somewhat ambiguous to me (due to my own lack of knowledge of history of maths literature), so if I've completely missed the mark, my apologies!"
Most notable papers in Economics in 2021,"This is an opinion question, but I'll give my opinion. In terms of methods, I like Arkhangelsky et al.'s synthetic diff-in-diff. In terms of applied economics, I liked Goncalves and Mello's study of racial bias in the U.S. police force."
Are democracies more economically productive than autocracies?,"In economics, it is accepted that countries with good 'inclusive' institutions, such as strong property rights, are more productive and able to develop faster (or even develop at all) than countries with bad 'extractive' institutions, such as forced labor (see Acemoglu 2008, Acemoglu & Robinson 2000a, 2000b, 2001, 2006, 2008; Olson 1984, Bates 1981, 1983, 1989 and sources cited therein*) Democracies do  adopt inclusive institutions more often than dictatorships but not always (e.g. see Lindert 2004, 2009 + sources from previous paragraph). Furthermore, the ability of country to develop and become more productive and prosperous, or fall of a country in terms of prosperity and productivity may not only depend on a single institution but rather whether country predominantly has inclusive or extractive institutions (although some inclusive institutions like property rights are more important than others). Consequently, it would be fair to say that it is accepted that democracies tend to be  more economically prosperous, but it is all conditional on the institutions they adopt. For example, a Venezuela is de jure a democracy but in indexes that measure strength of property rights it gets very low scores (e.g. see evolution of their property rights index (0-100) between 1995-2020). On the other hand consider China that is clearly a dictatorship both de jure and de facto, but their property rights index score between 1995-2020 got better. To sum it up, given the above it would be fair to say that the view that democratization leads to better economic outcomes**  has some merit but with caveats. It is not that democracies are more productive by virtue of being democracies, it is thanks to their better (inclusive) and more robust institutions and in principle dictatorships could adopt the same institutions, but they tend not to. This is because while the extractive institutions might be bad for a country as a whole, as their name suggest, they help political leaders to extract economic resources for themselves. * Note this research is also summarised in Acemoglu & Robinson: Why Nations Fail? which is book that is written in a way that is more accessible to laymen readers than the papers mentioned above).   ** Meaning, among other things, government can often in the end collect more revenue even with lower tax rates because tax bases critically depend on size of output being taxed (e.g. $10\%$ tax on tax base 1,000,000€ gives government more revenue than $50\%$ tax on tax base 1,000€). "
How does one derive the elasticity of substitution?,"How to derive elasticity of substitution The first step is to recall the definition of a differential. If you have a function $f: \Bbb R^n \to \Bbb R$, say, $f(x_1,\cdots,x_n)$, then: $${\rm d}f = \frac{\partial f}{\partial x_1}{\rm d}x_1 + \cdots + \frac{\partial f}{\partial x_n}\,{\rm d}x_n. $$ For example,  $$d\log v = \frac{1}{v}dv$$ Now suppose $v = \tfrac{y}{x}$, then we have $$ d\log(y/x)=\frac{d(y/x)}{(y/x)}$$ and for $v = \tfrac{U_x}{U_y}$ $$ d\log(U_x/U_y)=\frac{d(U_x/U_y)}{(U_x/U_y)}$$ In other words, if you reduce the problem to (1) understanding the definition of a differential and (2) use a simple change of variable, the problem becomes very straightforward. You then get $$\sigma \equiv \frac{d\log\left(\frac{y}{x}\right)}{ d\log\left(\frac{U_x}{U_y}\right) }= \frac{ \frac{d(y/x)}{(y/x)} }{ \frac{d(U_x/U_y)}{(U_x/U_y)} } $$ ASIDE: Note, it is important to recognize that $ d(y/x)$ is a meaningful concept. You simply apply quotient rule and you find $$ d(y/x)= \frac{xdy-ydx}{x^2}$$ This makes sense because $$ d\log(y/x) = d\log(y) - d\log(x) = \frac{dy}{y}-\frac{dx}{x}$$ And if you compute $$ d\log(y/x)=\frac{d(y/x)}{(y/x)}=\frac{ \frac{xdy-ydx}{x^2}}{y/x} =  \frac{xdy-ydx}{xy} = \frac{dy}{y}-\frac{dx}{x}$$ Same logic applies to $d(U_x/U_y)$. Thus, all of $\sigma$ is well-defined in the sense we are using the calculus tools correctly / legally. What is elasticity of substitution? Elasticity is by how much % one thing changes relative to a % change in another. Therefore, in this case, it is % change in ratio of two goods  relative to a single % change in the $MRS$ for those two goods."
What is the difference between intensive margin and extensive margin in labor economics?,"In Labor Economics, ""Extensive margin"" refers to ""how many people work"". ""Intensive margin"" refers to ""how much a given number of people work, on average"". To copy from a freely available recent study by Blundell, Bozio and Laroque 2011, ""...we split the overall level of work activity into the number of individuals in work and the intensity of work supplied by those in
  work. This reflects the distinction between whether to work and how
  much to work at the individual level and is referred to, respectively,
  as the extensive and intensive margin of labour supply. At the
  aggregate level the former is typically measured by the number of
  individuals in paid employment and the later by the average number of
  working hours."" Evidently, it is an important distinction, especially when one wants to analyze changes in employment, measured say in total number of hours worked: Say, did they increase? Why? Because more people work, or because the same number of people work more? And if the answer is ""both"", then what portion is attributable to a change in the intensive margin, and what portion to a change in the extensive margin? (this is why the word ""margin"" is used). Of course ""intensity"" as a word has a more general meaning, as for example the one mentioned in another answer here -it may be the case that people work ""faster"" and so in the same time interval, they produce more. Such changes are usually put under the concept of ""changes in efficiency"" (for example in Growth theory this is how they are called). In Labor Economics the terms have the meaning given above."
Why is money in circulation a liability of the central bank?,"Balance sheets always balance, so assets equal liabilities. Imagine a commercial bank goes to the central bank and wants cash. The central bank provides the cash, but asks for some of the commercial bank's loans (or government bonds) in return. The central bank now has the loans (or government bonds) as assets and the cash as liabilities. The cash is a liability, because if the commercial bank goes back to the central bank and gives back the cash, the central bank will have to give back the loans (or government bonds). So while it's true that cash is not backed by gold, it is still backed by something. You can take your cash to the central bank, exchange it for government bonds, earn cash interest on your government bonds and then use this cash to pay your taxes. More generally cash is backed by the goods that you can purchase for it."
"When a stock market crashes, how does money just disappear?","Stocks are not money. The valuation of a company - the market captialisation - is the number of shares multiplied by the share price. The share price is the price people are willing to trade at right now. It does NOT mean all the shares have been traded at that price. If a company issues 1 million shares, at a starting price of £10, the market cap is £10 million. They may only sell a few of these shares, so much less than £10 million actually changes hands. If the price rises to £20, only a few of the shares changed hands at that price - but the market cap is reflected in all the shares, and is now £20 million. People have commented on the flaws in this methodology. The same happens in a crash. Only a fraction of the shares actually change hands, but the media report on the change in the market cap. If you want an analogy, consider a house burning down. You buy the house for £200,000. Then it sadly burns down (and without insurance). It is now worth £0. However, no money has been created or destroyed; it's a loss of value of an asset."
Should Finland leave the eurozone?,"The closest we can get to an answer would be by looking at previous exits from currency unions. Rose published a paper studying extensively all exits after WWII. The abstract resumes well the conclusions of the paper: This paper studies the characteristics of departures from monetary unions. 
  During the post-war period, almost seventy distinct countries or territories 
  have left a currency union, while over sixty have remained continuously in 
  currency unions. I compare countries leaving currency unions to those 
  remaining within them, and find that leavers tend to be larger, richer, and 
  more democratic; they also tend to have higher inflation. However, there are 
  typically no sharp macroeconomic movements before, during, or after exits The effect denoted are very small, which leads me to conclude the choice should be made on political, not economical grounds, but everybody is free to have their own answer about this."
How could the economic cost of the world not speaking the same language be estimated?,"To measure the costs of different people speaking different languages, researchers use a ""linguistic distance"" metric, see for example this paper. However, measuring the cost of linguistic diversity appears to be challenging. Some effects are shown along the following lines. Impact of linguistic distance on international trade In this paper, they construct new series for common native language and common spoken language for 195 countries, which they use together with series for common official language and linguistic proximity in order to draw inferences about  Results show that the impact of linguistic factors, all together, is at least twice as great as the usual dummy variable for common language, resting on official language, would say. Impact of linguistic distance on international migration In this paper they examine the importance of language in international migration from multiple angles by studying the role of linguistic proximity, widely spoken languages, linguistic enclaves and language-based immigration policy requirements. To this aim they collect a unique data set on immigration flows and stocks in 30 OECD destinations from all world countries over the period 1980–2010 and construct a set of linguistic proximity measures.  Results: Migration rates increase with linguistic proximity and with English at destination. Softer linguistic requirements for naturalisation and larger linguistic communities at destination encourage more migrants to move. Linguistic proximity matters less when local linguistic networks are larger. This paper on the Costs of Babylon—Linguistic Distance in Applied Economics
may also be relevant. Impact of linguistic distance on literary translations Interestingly, one of the latest Freakonomics Radio episode is called ""Why Don't We All Speak the Same Language? (Earth 2.0 Series)"". There are 7,000 languages spoken on Earth. What are the costs — and benefits? They quote (with references) There's also the roughly \$40 billion a year we spend on global
  language services — primarily translation and interpretation. And
  another \$50-plus billion a year spent learning other languages. Useful references"
"When a country adopts the Euro, what happens to its debt?","Yes, upon the introduction of the euro on January 1, 1999, all debt (indeed, all nominal contracts) in participating countries was converted from national currency to euros at a legally defined conversion rate. See this press release from December 31, 1998, which states: In accordance with Article 109l (4) of the Treaty establishing the
  European Community, the irrevocable conversion rates for the euro were
  today adopted by the EU Council, upon a proposal from the Commission
  of the European Communities and after consultation of the European
  Central Bank (ECB) for effect at 0.00 on 1 January 1999 (local time).
  In compliance with the legal framework for the use of the euro, the
  irrevocable conversion rate for the euro for each participating
  currency is the only rate to be used for conversion either way between
  the euro and the national currency unit and also for conversions
  between national currency units. The euro conversion rates are the following... For the particular case of Greece, which joined the euro on January 1, 2001 instead, the conversion rate with the drachma was apparently set at 1 euro = 340.750 drachma by this regulation in June 2000."
Why does China buy so much U.S. Treasury debt?,"As was said above, buying debt to affect the exchange rate and make Chinese exports more attractive may be one reason to buy these Treasuries. Surely all of these are not reinvested into buying more debt and some of it is put into other civic projects. The other reason to buy debt is to have a string of payments over time that may become more valuable later on. Since the US is a large nation and powerful nation, China probably feels it is very unlikely that the USA will default, and if they did, that would give China a legitimate complaint against the US, that might help them in foreign relations. It may even be that they want the US to be stable so as not to crash the global economy and so that they can still sell lots of products to the US, so they buy up the debt to keep the USA stable. The more important part of that whole paragraph though is to have a string of payments. For the same reason that governments like to hold debt even when they have the ability to pay it off, China might find that investing into debt will provide a better rate of return than infrastructure at the moment, and later on, once infrastructure has a better rate of return (capital steady state changes) then they can take a stream of income from interest payments from the US and taxes and then invest into that. Finally, I couldn't help but notice that you see China as needing to lift itself out of poverty. That may be true, but why would investments into infrastructure be the solution to helping the rural poor--places that don't need as much infrstructure? Most of the poor in China are in rural areas, so there's something to think about."
How can banks pay interest to the central bank?,"You anticipate the answer when you ask: This question can be easily answered if there were any way in which
  new money can leave the central bank without being paid back. Are
  there such transactions I don't know about? Indeed, there is always a way that money leaves the central bank without being paid back: the central bank does something with its net interest earnings, usually sending them to the central government, which treats them as part of its revenue, and is therefore able to get by on a little less tax revenue than it would otherwise need. For instance, in 2013 the Federal Reserve remitted $78 billion in profits to the US Treasury. Let's incorporate this observation into an augmented version of your example. Suppose that the central bank keeps the supply of money at \$10, with a corresponding loan of \$10 to banks, and the annual nominal interest rate is 10%. Suppose that the central government spends \$5 per year and runs a balanced budget.  Each year, banks pay \$1 in interest to the central bank. This is profit and is sent to the central government, which then only needs to raise \$4 in taxes to pay for its \$5 in spending. The net effect of this is to put \$1 in the hands of the public, which ultimately finds its way to paying for the \$1 in interest that banks owed to the central bank. (After all, these banks are presumably lending out the money and collecting interest from the public themselves.) One can make the example much more intricate, but the key point is that we don't get an exponentially growing debt owed to the central bank by the rest of the economy - because the central bank sends its profits to the government, and then they're recycled into the rest of the economy. (As user4385 points out in a comment, the situation is similar for any kind of debt. Suppose, for instance, that Connecticut is a net creditor to the rest of the country - where does the rest of the country get the money to pay interest to Connecticut? The answer is that Connecticut eventually spends its interest earnings on goods and services from the rest of the country - funds flow in both directions.)"
Does merger control really affect market structure?,"Clifford Winston´s excellent book Government Failure versus Market Failure: 
Microeconomics Policy Research and Government Performance surveys the literature on the effects of government regulation generally and antitrust merger control specifically. He doesn't exactly answer your question on economy-wide structural effects, but largely concludes that merger control is rather ineffective. A small quote: Evidence from the finance literature indicates that mergers that were
  challenged by the DOJ and the FTC were in general not anticompetitive
  and would have been efficient had they been allowed to go through
  (Eckbo 1992). Mergers that were challenged or opposed by antitrust
  regulators but were consummated anyway have often resulted in gains
  for consumers (Schuman, Reitzes, and Rogers 1997; Morrison 1996).
  Finally, Crandall and Winston (2003) analyzed the effects of merger
  policy on price-cost margins and found that the antitrust authorities
  have primarily attacked mergers that would enhance efficiency, either
  by blocking them in court or allowing them to proceed only if the
  merger partners agreed to conditions that turned out to raise their
  costs."
Solving the Hamilton-Jacobi-Bellman equation; necessary and sufficient for optimality?,"(This perhaps should be considered a comment.) If you have solved the HJB equation, it is sufficient to get the optimal solution. So you  do not ""have to be concerned with any other optimality conditions,"" which I believe appears to answer your question. It appears that you are concerned about the ""necessary"" component of the theorem. The necessity side of the statement is as follows: if there is an optimal solution, there must exist a solution to the HJB equation. I have not worked with this particular problem, but the answer in general is that we do not expect to have a differentiable function V. Therefore we do not have a solution to the equation as it is stated. Instead, we need to look at generalised derivatives, and convert the HJB equation into an inequality. In which case, you may get a ""viscosity solution."" If we extend to use generalised derivatives, it may be possible to prove that a such a solution always exists. Glancing at your proofs, they will not help on the necessity conditions, as you are assuming differentiability."
Why are banks providing mortgages?,"A bank (or anyone else) considering possible investments needs to consider both return and risk. Stock market investment is risky in two respects: a) individual stocks may achieve more or less return than the market average; b) even if an investor has a well-diversified portfolio of stocks so that their combined return in any one year is close to the market average for that year, the market will have good and bad years reflecting general economic conditions. Investing in mortgages, by contrast, is less risky because: a) the interest rate is defined in advance (or perhaps variable at the lender's discretion); b) if the borrower cannot keep up their payments, the lender has the security of being able to  obtain possession of the property (which they could then sell to recover their capital). This does not provide complete security because the market price of the property may have fallen and become less than the amount lent, but it greatly reduces the risk to the lender."
"If I gain, then someone else loses. Correct?","I completely agree with denesp's answer, however I think you can make it even simpler. On a very small scale, it's certainly true that if I gain, somebody
  else might lose. If I take away my brother's chocolate, then he will
  lose it, and will most probably not get anything comparable. OK, let's say I prefer chocolate to wine gums and my brother likes wine gums better than chocolate. Then taking his chocolate away and giving him my wine gums is good for both of us, so we both win and no one loses. So the answer is no. You could even consider the extreme case in which your brother hates chocolate and you are doing him a favor by taking it. (Works not as well with chocolate, but you might think of recycling.) In general these ""trades"" are called Pareto improvements. But this is only one example, if you are interested in the subject, you might be interested in one of the following basic economic ideas: Trade between two countries in which one of them is more efficient than the other: Ricardo's comparative advantage example Your brother likes to give: altruism / warm glow (I really don't like the wiki page here, but was not able to find a decent non-scientific explanation of it.) Or maybe other other-regarding preferences, for example fairness (your brother has a lot of chocolate and feels better if he gives you some): Theories of Fairness and Reciprocity (On page 3 is a brief ""Non-technical summary"" which might be interesting.) As you can see there are many examples for a ""win-win"" situation and there are many many others, depending on the situation."
Why is the Free Rider Problem a problem?,"According to the standard criterion used to evaluate welfare (Pareto efficiency), the problem with 'free riding' is that goods are not produced even though they cost less to produce than they are worth to consumers. That means that we are forfeiting an opportunity to help people without harming anyone else. To illustrate using a standard example, suppose that 10 people live on a street, and that each would value an additional street light at £100 (i.e. each would pay up to £100 to bring a new street light into existence). The street light would cost £101 to produce, but if produced can be used by all for free: this is known as 'non-excludability'. Will it be produced? If people are self-interested, then the answer is no. Although any resident would benefit from the street light, building it would cost them £101, which exceeds the benefit (£100). That's a problem since the total benefit (10 x £100 = £1000) greatly exceeds the cost of the light. If the inhabitants could be compelled to pay for the street light, sharing the cost equally, then they would all be better off. I should stress that, from the perspective of Pareto efficiency, the problem with 'free riding' has nothing to do with 'equity'. To use your example of parks, the problem is not that people get to use the park without paying. Rather, the problem is that, since people are expected to free ride, the park won't be created in the first place."
Applications of Trig functions in Economics?,"The main property of trig functions is their cyclicality. Then one would think that they could be ideal in time series analysis, to model ""fluctuations around a trend"". I believe that the reasons they are not actually used in such a setting are  1) They are deterministic functions, so they do not allow for fluctuations to be stochastic 2) If the researcher wants to create a model that produces up and down fluctuations (oscillations) around a trend, he would want to obtain that property from the behavioral and other assumptions of the model. If he were to use a trig function, he would a priori impose on the model the sought theoretical outcome. Instead, one opts for difference-differential equations. There we obtain oscillations (damped or not) if some characteristic roots are complex -and then the trig functions appear, but as an alternative representation, not as buidling blocks."
"What was economics like as a field before Adam Smith, the father of *modern* economics?","Direct predecessors to Adam Smith within the classical tradition (maybe a more useful distinction than modern) include Hume, Locke and Dudley North. Before the classical economists, there were the physiocrats (18th century), such as Francois Quesnay and Turgot. The physiocrats emphasised agricultural productivity as a driver of the wealth of nations. They were contemporaneous with, but also preceded by mercantilists (16th century - 18th century). Mercantilists mostly concentrated on creating a favorable balance of trade, which would allow reserves to accumulate in the possession of an absolute ruler. They emphasized government control of the economy as an extension of state power. The mercantilists had a lot of influence in the policies of nations, for instance in the form of Colbert. Famous writers might include de Malynes and Mun. Mercantilists were one of the main targets of Adam Smith's critique. The distinction being that mercantilists believed the quantity of reserves to be the ultimate source of a nation's wealth and so emphasized protectionism, while Smith focused more on trade allowing inputs to become more productive. In doing so he was very influenced by Quesnay to whom he had considered dedicating The Theory of Moral Sentiments. The inclusion of gains from trade, self-interest and competition leading to increased productivity, and division of labor distinguishes Smith from these earlier writings and forms the link to ""modern"" economics. There were also ""economists"" long before the 16th century, the connections between them and economics is we understand it tends to become more tenuous the farther back you go. Iba Khaldun is a good example, Aristotle (or more likely one of his students) literally wrote a book called ""Economics"" (this deals with economics in the literal sense of management of a household), you could also talk about Hesiod's ""Works and Days"" (a very old text indeed) being an example of early economics. "
What benefits does Bitcoin (i.e. cryptocurrency) offer?,"I'd say that some benefits of Bitcoin and other cryptocurrencies (i.e. ""altcoins""), include:"
Outputting Regressions as Table in Python (similar to outreg in stata)?,"You can use code like the following (making use of the as_latex function) to output a regression result to a tex file but it doesn't stack them neatly in tabular form the way that outreg2 does: The as_latex function makes a valid latex table but not a valid latex document, so I added some additional code above so that it would compile. The result is something like this for the print function: and like this for the latex:
 Update:
Not as full featured as outreg but the summary_col function does what you ask.  Which has the following output: As before, you can use the dfoutput.as_latex() to export this to latex. "
"Osborne, Nash equilibria and the correctness of beliefs","Introducing the language of beliefs here is slightly strange, given that beliefs do have a very specific meaning in other parts of game theory. Indeed, Osborne's description is reminiscent of a Bayes Nash Equilibrium. We could introduce the notion of beliefs into the normal form of a complete information game as follows: suppose that with probability $a_i$ each player, $i$, is a ""strategic"" type who will play according to (Nash) equilibrium, and with probability $1-a_i$ he will select some strategy uniformly at random (because, say, he is indifferent across all actions). We thus have a Bayesian game where thinking about beliefs is more natural. The Bayes Nash solution concept then says that $i$'s strategy must be optimal given the expected play induced by the other players' strategies and the beliefs over their types implied by $\{a_j\}_{j\neq i}$. If we look at the limit as $a_i\rightarrow 1$ for all $i$ then the Bayes Nash equilibrium of this game will coincide with the solution concept described by Osborne. I guess the reason Osborne wrote it like this is a pedagogical one, given that this is an introductory text. When we introduce students to static games, we tell them that player $i$ best responds to the actions of the other players. Students naturally want to know ""how can they respond to a strategy chosen simultaneously without knowing what that strategy will be?"" This is, in many senses, a philosophical question. Common answers are It seems that the predictions in the second point correspond to the ""beliefs"" invoked by Osborne. However, it is important to stress that these predictions/""beliefs"", are merely an informal/intuitive tool for helping us to conceptualise what is going on in an equilibrium and are not part of the definition of such an equilibrium. The concept of Nash equilibrium itself is completely agnostic on the notion of beliefs (as you note in a comment, it is defined only over actions), which is why, when Osborne goes on to formally define Nash equilibrium, he does so without invoking the idea of beliefs at all."
Real Exchange Rate vs PPP rate,"Both answers here are giving really good definitions of the concepts, but I think it's important to additionally talk about the relation between the two to clarify the justified confusion and to compare them. tl;dr RER is combining PPP theory with the current exchange rate to produce a rate that is taking purchase power into account. Read the two last paragraph to understand the confusion. From Wikipedia: Purchasing Power Parity (PPP) is a theory that measures prices at different locations using a common basket of goods. PPP is giving us a ratio (rate) that is fair in purchase power between different locations (according to the basket of goods). The market exchange rates are not following PPP. This is why the RER exists. We apply PPP theory to the current exchange rates to get a ""real"" rate that takes purchase power into account. In other words, RER is using PPP. From Wikipedia: The real exchange rate (RER) is the purchasing power of a currency relative to another at current exchange rates and prices This is confusing because, at least in online litterature, we don't seem to be explicitly mentioning their relation. For example, when we talk and teach about PPP, we often associate it with simple price indices to give usage examples (e.g. the Big Mac Index). These indices are using the current exchange rate to compare between countries, so they are simply RERs normalized to one currency and using a specific good basket for the comparison (in this case one single item, the Big Mac). Since this is not mentionned and one leads to the other, the distinction often becomes unclear. Another confusion factor is that you can easily find PPP rates, exchange rates and price indices online, but not RERs."
Consumer optimum in an economy with a continuum of commodities,"The completely rigorous thing would be to write the Euler lagrange equation of this calculus of variations problem, this will give you a strong solution that is what you have or a weak solution that is written with respect to a distribution. "
How do reserves move between the 12 federal reserve banks?,"This is surprisingly subtle.  When, for instance, when bank A in the Richmond Federal Reserve district sends $1000 in reserves to bank B in the Minneapolis Federal Reserve district, reserves are taken out of bank A's account at the Richmond Fed and placed into bank B's account at the Minneapolis Fed.  Now, bank A's reserves are a liability on the books of the Richmond Fed, while bank B's reserves are a liability on the books of the Minneapolis Fed. Without any offsetting change, therefore, the process would result in the Richmond Fed discharging a liability and the Minneapolis Fed gaining a liability - and if this continued, regional Fed assets and liabilities could become highly mismatched.  The principle, then, is that there should be an offsetting swap of assets. It would be too complicated to swap actual assets every time there is a flow of reserves between banks in different districts. (There's over $3 trillion in transactions every day on Fedwire, the Fed's RTGS system - and if even a fraction of those are between different districts, the amounts are really enormous.) Instead, in the short run the regional Feds swap accounting entries in an ""Interdistrict Settlement Account"" (ISA). In the example above, the Minneapolis Fed's ISA position would increase by \$1000, while the Richmond Fed's ISA position would decrease by \$1000, to offset the transfer of liabilities. So far, this is all very similar to the controversial TARGET2 system in the Euro area, in which large balances between national banks have recently been accumulating. The American system is different, however, because ISA entries are eventually settled via transfers of assets. Every April, the average ISA balance for each regional Fed over the past year is calculated, and this portion of the balance is settled via a transfer of assets in the System Open Market Account (the main pile of Fed assets, run by the New York Fed). Hence, if in April the Minneapolis Fed has an ISA balance of +\$500, but over the past year it had an average balance of +\$2000, its balance is decreased (by \$2000) to -\$1500, and it has an offsetting gain of \$2000 in SOMA assets.  As this example shows, since it is average balances over the past year that are settled, not the current balances, ISA balances do not necessarily go to zero every April. Historically, they were fairly tiny anyway, but since QE brought dramatic increases in reserves, these balances have sometimes been large and irregular. In the long run, though, the system prevents any persistent imbalances from accumulating. (Note: the process in April is a little bit more complicated than I describe, since some minor transfers of gold certificate holdings are also involved. Basically, gold certificates are transferred between regional Feds to maintain a constant ratio of gold certificates to federal reserve notes; the transfers of SOMA assets are adjusted to account for this. Wolman's recent piece for the Richmond Fed is one of the few sources that describes the system in detail.)"
"Are there theories that invalidate the ""tragedy of the commons""?","As you implied, the tragedy of the commons was the standard theory in Economics. This is no longer the case. However--and this is an important point--rejection of this one-size-fits-all approach to public resource management did not come from the emergence of new theories; rather, it resulted from actually studying real-world outcomes. In fact, it's for her work doing exactly this that Elinor Ostrom won the Nobel Prize in Economics. As described by the Nobel committee: [B]ased on numerous empirical studies of natural-resource management, Elinor Ostrom has concluded that common property is often surprisingly well managed. Thus, the standard theoretical argument against common property is overly simplistic. It neglects the fact that users themselves can both create and enforce rules that mitigate overexploitation. The standard argument also neglects the practical difficulties associated with privatization and government regulation. Since these case studies there has been theoretical work--some of it by Ostrom herself--in order to reconcile the discrepancies between the previous theory and the observed outcomes (developed using the theory of repeated games in Non-Cooperative Game Theory).  However, what's most interesting to me is that this theory was rooted in actual observations and not the other way around (i.e. first the theorized behavior followed by real-life observations). Did this answer your question?"
How is price elasticity determined in practice?,"In many practical instances, price elasticity of demand (PED) is calculated in a back of the envelope fashion, just as taught in the textbooks! Firms can adjust their price by some small amount and observe the demand response. For relatively small changes in price and quantity, little accuracy is lost by assuming that the demand function is locally linear, so that the change in price and demand jointly give an estimate for $$\frac{dQ}{dp}.$$ Since $p$ and $Q$ are already known, this is enough to calculate the PED:
$$\eta=\frac{dQ}{dp}\frac{p}{Q}.$$ This method yields only a point estimate of elasticity at the current price. However, one can get an incredibly long way with just this estimate thanks to the so-called Lerner condition: that a firm with marginal cost $c$ facing a price elasticity of $\eta$ maximises profit when $$\frac{p-c}{p}=-\frac{1}{\eta}.$$ Once the price elasticity of demand is estimated in the above fashion, this formula can be used to infer if the firm's price is above or below its profit-maximising level (allowing a firm to correct towards that level). Alternatively, this kind of analysis is often used as a heuristic in competition policy (antitrust) because, by estimating the right hand side of the Lerner formula, competition authorities can get an estimate for the left hand side (i.e. for how much power the firm has to price above marginal cost). One drawback of this approach is that, at least in its simplest implementation, it does not control for factors such as how a change in the price of a product affects the demand of other products sold by the same firm (and thus overall profit). You can see a nice informal discussion of Amazon's book pricing based on this kind of back of the envelope work here. For more formal purposes, and when data is readily available, the process is often similar but slightly more careful in the estimation of demand. An excellent example of this kind of work can be found in Ellison & Ellison's 2009 Econometrica Paper, Search, Obfuscation, and Price Elasticities on the Internet. They proceed by estimating the firm's demand function econometrically (rather than via the heuristic method described above), and then calculate the implied PED from this estimated demand. Using an equation analogous to the Lerner condition, they are able to infer how far from the competitive case the market is, and attribute this discrepancy to search obfuscation. In practice, for economists working outside of a firm, the main difficulty is often obtaining the data necessary to estimate the PED (Ellison & Ellison had excellent data thanks to collaboration with a firm in the market)."
Have there been instances where economists have advocated a trade embargo?,"Free trade is, on the whole, one of the few otherwise controversial policy topics on which economists have near-perfect consensus. Historically, this consensus has long been strong in the English tradition (Hume, Smith, Ricardo, Mill), albeit less strong elsewhere. Famously, 1028 American economists signed an unsuccessful petition in 1930 begging Herbert Hoover not to approve the Smoot-Hawley tariff. If the IGM Economic Experts panel is any guide, consensus remains firm today. That said, off the top of my head, various cases in which some modern economists have departed from advising free trade include: Distributional consequences. This is a common refrain in popular critiques of free trade: many pundits argue that even if trade is beneficial in some aggregate sense, its adverse distributional impact (e.g. hurting the already-suffering manufacturing workforce) negates the overall benefit. Indeed, in models where gains from trade arise from differing factor endowments, the whole point is that some factor (the domestically scarce but internationally not-so-scarce one) will lose; this is the idea behind the Stolper-Samuelson theorem.  Traditionally, most economists have argued that it is better to have free trade and address any distributional or insurance objectives through the overall tax-and-transfer system. Whether not this conclusion holds in a formal model, however, depends on exactly what instruments are available to the government; it's conceivable that trade barriers would be an optimal second or third-best policy in some cases. The left-wing heterodox economist Dean Baker has strenuously argued along these lines (though he has certainly not offered a formal model). More in the mainstream, an early version of Autor, Dorn, Hanson (AER 2013) made a suggestive stab in this direction with a back-of-the-envelope calculation showing that the deadweight loss from transfers induced by Chinese trade was a substantial fraction of the theoretical gains from trade - though this calculation was rough and evidently removed from the published version. Notably, Autor was one of the few IGM panelists with an ""uncertain"" reply about the benefits from trade. "
How much do second-hand good purchases affect first-hand demand?,"Coase (1972) has a classic treatments of this issue for monopolists selling durable goods. The general idea is that if Alice sells a textbook to Bob, Bob can resell it to Charlie when he is done with it. The ability of Bob to resell to Charlie means that Alice can charge a higher price to Bob, because he can recover some of the price by doing so. But because Alice has to sell new goods in a market where the old goods are a substitute, that competition effect lowers prices. Which effect dominates depends on the situation. Do these effects actually exist? Chevalier and Goolsbee (2009) say yes. Popular wisdom holds that publishers repeatedly revise college
textbooks in order to kill off the secondary market for used books.
However, many neo-classical authors argue that, if consumers are
forward-looking, such behavior should not be profitable; consumers’
willingness to pay for new books will fall if they know that they
can't resell their used books. Using a large new dataset on all
textbooks sold in psychology, biology and economics in the 10
semesters from 1997 to 2001, we estimate a demand system for textbooks
by college students and test whether textbook consumers are forward
looking. Our estimates strongly support the view that students are
forward looking and that, when they buy their textbooks, they take
into account the probability that they will not be able to resell
their books at the end of the semester due to a new edition release.
The demand estimates do suggest, however, that students are overly
optimistic in their forecasts and that there are also some important
frictions in the market for used books that can affect publisher
revision decisions. Simulation results suggest that students are
sufficiently forward-looking that publishers cannot raise revenues by
accelerating current revision cycles, even if revising were costless
to the authors."
Are monotonic and continuous preferences necessarily rational?,"Consider a preference relation in $\mathbb{R}^2$ such that $x=(x_1,x_2)\succsim (y_1,y_2)=y$ $\iff$ $x_1\geq y_1$ and $x_2\geq y_2$.  1) You might like to argue whether this preference relation is strictly monotonic and continuous.  2) Is the relation defined above complete? Then, as a side dish, you might also reconsider your claim that continuity is the cause of transitivity. Note: I just wrote this particular one with the purpose of providing a thought experiment. More in a way to challenge your understanding. I am not sure whether this example provides an answer to your question or not."
The Economics of Coronavirus,"With the right keywords (and author searches) from earlier papers, I actually found some academic output already; this might not be peer-reviewed yet, i.e. preprints: McKibbin and Fernando (2 March 2020) The Global Macroeconomic Impacts of COVID-19: Seven Scenarios examines the impacts of different scenarios on macroeconomic outcomes and
  financial markets in a global hybrid DSGE/CGE general equilibrium model.
  The scenarios in this paper demonstrate that even a contained outbreak could significantly
  impact the global economy in the short run. [...]
  These results are very sensitive to the assumptions in the model, to the shocks we feed in and
  to the assumed macroeconomic policy responses in each country. And that's already cited by Wang et al. COVID-19's Impact on China's Economy Based on Data of Spring Festival Travel Rush this paper starts with the analysis of the daily railway passenger volume data during the Spring Festival travel rush, and establishes three time series to model the railway passenger volume, GDP in the first quarter model, and GDP in the last three quarters respectively. The forecast results indicate that: (a) Affected by the epidemic, China’s economy will lose 4.8 trillion yuan in the first quarter of 2020, which is an expected decrease of 20.69% and a year-on-year drop of 15.60%; (b) China's expected GDP growth rate for the full year of 2020 will reduce from 6.50% to 1.72%. However, there are some positive factors not taken into account in these models, which means that the forecast results may be underestimated. With the global spread of the epidemic, the instability of the world economy will in turn impact China’s economy. A more recent (March 30) paper, Greenstone and Nigam that emphasizes/quantifies the benefits of the lockdowns, in the US: Using the Ferguson et al. (2020) simulation model of COVID-19’s spread and mortality impacts in the United States, we project that 3-4 months of moderate distancing beginning in late March 2020 would save 1.7 million lives by October 1. Of the lives saved, 630,000 are due to avoided overwhelming of hospital intensive care units. Using the projected age-specific reductions in death and age-varying estimates of the United States Government’s value of a statistical life, we find that the mortality benefits of social distancing are about \$8 trillion or $60,000 per US household. Roughly 90% of the monetized benefits are projected to accrue to people age 50 or older. Overall, the analysis suggests that social distancing initiatives and policies in response to the COVID-19 epidemic have substantial economic benefits. JP Morgan has put out some estimates already, although they don't detail their methodology as much as a peer-reviewed publication would (have to). The U.S. economy is projected to contract by 14% in the second quarter, after experiencing a 4% contraction in the first quarter, before recovering to 8% and 4% growth in the third and fourth quarters. Euro area GDP will suffer an even deeper contraction, with double-digit declines of 15% and 22% in the first and second quarters, before rebounding by 45% and 3.5% in the third and fourth quarters. “There is no longer doubt that the longest global expansion on record will end this quarter. We now think that the COVID-19 shock will produce a global recession, as nearly all of the world contracts over the three months between February and April,” said Bruce Kasman, Chief Economist at J.P. Morgan. Initially, the expectation was the novel-recession may generate limited labor market damage, but J.P. Morgan Research is now forecasting the unemployment rate for developed markets as a whole will rise 1.6 percentage points in the next two quarters. “The rise in unemployment will be sharper in the U.S. than in the Euro area. Most immediately, U.S. initial jobless claims should spike above 400,000 in the coming weeks,” said Michael Feroli, Chief U.S. economist at J.P. Morgan. McKinsey has a somewhat spread-out (March 25) presentation, but a couple of slides with their scenarios/estimates I could fish from there: 
 Likewise, PWC has a March [white]paper mostly focused on Australia... but which does come with a global map attached of their estimates of GDP losses, alas in absolute figures only (and I think over one year based on what they say on the previous slide):  Before the COVID-19 outbreak, there has been a niche academic domain of estimating the economic impact of pandemics already, typically using a CGE model. Results depend of course on the assumptions used. E.g. Smith, Keogh-Brown et al. had a series of papers approximately a decade ago. Quoting from a later one that covered more than one European country: Results suggest GDP losses from the disease of approximately 0.5–2% but school closure and prophylactic absenteeism more than triples these effects. So they went as high ~10% GDP losses. While they did not envisage government mandated lockdown(s), they modelled ""prophylactic absenteeism"" in response to ~3% death rate media reports as the most severe case. An earlier paper of theirs, on UK only is open-access in the BMJ. Quoting their assumption from there almost everybody in the population will know someone who has died once the mortality rate reaches one death per 300 people, triggering prophylactic absenteeism. The same authors (Keogh-Brown and Smith) also published in 2008 a fairly detailed post-hoc account of how economic predictions for SARS fared in hindsight... which generally wasn't all that good, meaning that most models overestimated the economic impact of SARS. It cites positively an 2003 ADB paper of Fan that hedged its bets on two different scenarios (early containment being one of them). Another paper Verikios et al. (2015) in a similar vein but attempts a global estimate (GTAP CGE), and also discussed prior/related work: The two pandemic scenarios modelled give quantitatively and qualitatively different
  results. [...] Our results show that the peak GDP effects on the world economy of an influenza pandemic are in the range 0.06 to 1.01 per cent in the peak year depending on the nature of the pandemic. Our results are smaller than the only previous estimates of the global economic effects of influenza pandemics by McKibbin and Sidorenko (2006). They estimate reductions in global GDP of between 0.8 and 12.6 per cent depending on the severity of the pandemic. Our results are not directly comparable to McKibbin and Sidorenko as they did not model a continuum of pandemic scenarios across the virulence-infectiousness continuum. Rather, they assume a given reproducibility of the virus (infectiousness) and vary virulence across their four scenarios. Further, they implicitly assume multiple waves for all their scenarios as they presume that
  new infections continue to occur for greater than one year. This is likely to characterise only more extreme pandemics (such as the 1918–19 flu) rather than milder pandemics. Differences in the range of influenza pandemics modelled by us and McKibbin and Sidorenko are justifiable given the uncertain nature of future pandemics. Our results thus build upon the findings of McKibbin and Sidorenko by extending the estimation of potential GDP estimates over a further range of potential pandemic threat scenarios. [...] Our results show that the global macroeconomic effects of an influenza pandemic may be significant but are also likely to be short-lived. The largest economic impacts of an influenza pandemic are driven by reduced international tourism, due to risk-modifying measures by households and travel restrictions imposed by health authorities, and lost workdays, due to illness or formal social distancing measures designed to contain the virus. This is consistent with the work of Keogh-Brown et al. (2009, 2010). [...]  Verikios seems to work for KPMG Australia nowadays and they have released a COVID-19 model that is fairly detailed in explaining its assumptions, for example it has separate pre- and post-stimulus arms/scenarios, but it mostly covers the Australian case, in particular only models the government response of that country. KPMG uses a NiGEM (New-Keynesian) macro model, and they've basically tweaked some parameters of this using estimates of shocks in four areas: reduction in productivity due to additional sick leave by workers, reduction in productivity due to additional carer’s leave by workers, incremental investment premium, capacity utilisation adjustment (i.e. supply chain disruptions). The first two were informed by fairly detailed labour & health statistics, the latter two seem a bit more arbitrary to me, e.g. they added a flat 1% investment premium. I was hoping the IMF would have something more concrete than this, but I guess
they are weary of advancing anything more detailed yet: The COVID-19 pandemic would cause a global recession in 2020 that could be worse than the one triggered by the 2008-2009 global financial crisis, but world economic output should recover in 2021, the IMF said on Monday. Georgieva issued the new outlook after a conference call of G20 finance ministers and central bankers, who she said agreed on the need for solidarity across the globe. [...] The outlook for global growth is negative and the IMF expects “a recession at least as bad as during the global financial crisis, or worse,” Georgieva said. Earlier this month, Georgieva had warned that 2020 world growth would be below last year’s 2.9 percent rate, but stopped short of predicting a recession. Georgieva on Monday said that a recovery is expected in 2021, but to reach it, countries would need to prioritize containment and strengthen health systems. “The economic impact is and will be severe, but the faster the virus stops, the quicker and stronger the recovery will be,” she said. The CBO has now posted its own assessment for the US economy. It disclaims that it is ""highly uncertain, especially for later periods"", but basically, for the second quarter of 2020 it predicts 10 percent unemployment and the economy to contract by ""more than"" 7 percent. The WTO Secretariat has posted this self-explanatory chart on April 8: "
Submodularity property in congestion games?,"This proposition is in general not true. One can show that it is true in the case $n=2$ and $m=2$. Here, I exhibit a counter example when $n=3$ and $m=2$. A brief comment. We can rephrase the question in words: does a Nash equilibrium that is ""more random"" ($e'$ versus $e$) is less efficient? Intuitively, as more mixed strategies are played, the realized outcome is more random and it can be very inefficient due to a lack of coordination between agents. When agents play pure strategies, we can think that we reduce the coordination problem given that we consider Nash equilibria. This intuition does not hold if the proposition is false, as I will show when $n=3$ and $m=2$. Denote $A$ and $B$ the two possible actions. The delay functions are defined as follows:
$d_A(1)=5$, $d_A(2)=7$, $d_A(3)=10$ and $d_B(1)=1$, $d_B(2)=6$, $d_B(3)=7$.
It means that when $x$ agents play $A$ (resp. $B$), they receive the payoff $-d_A(x)$ (resp. $-d_B(x)$). This is a (symmetric) congestion game as long as the delay functions are increasing.  Define $e$ as the equilibrium when 1 agent plays $A$ and 2 agents play $B$. Define $e'$ as the equilibrium when 1 agent always plays $B$, and the 2 others plays $A$ with probability $\mu=2/3$ and $B$ with probability $1-\mu=1/3$. It satisfies the property  $sup(e) \subseteq sup(e')$. First, we show that $e$ is a Nash equilibrium. The agent who plays $A$ is maximizing her payoff given the two other players' strategy when choosing $A$ is better than choosing $B$, $d_A(1)<d_B(3)$ (i.e. $5<7$). Both agents who play $B$ are playing optimally if $d_B(2)<d_A(2)$ (i.e. $6<7$). $e$ is thus a Nash equilibrium and its social cost is $d_A(1)+2d_B(2)=17=\frac{153}{9}$. Second, we show that $e'$ is a Nash equilibrium. On one hand, the agent who plays $B$ is maximizing her payoff when the two others play mixed strategy if she is better off playing $B$ than $A$,
$$(1-\mu)^2d_B(3)+2\mu(1-\mu)d_B(2)+\mu^2d_B(1)<(1-\mu)^2d_A(1)+2\mu(1-\mu)d_A(2)+\mu^2d_A(3)$$
i.e. $\frac{1}{9}5+\frac{4}{9}7+\frac{4}{9}10<\frac{1}{9}7+\frac{4}{9}6+\frac{4}{9}1$, which is true.
On the other hand, each one of the agents playing the mixed strategy is indifferent between choosing $A$ or $B$ if
$$\mu d_A(2)+(1-\mu)d_A(1)=\mu d_B(2)+(1-\mu)d_B(3)$$
i.e. $\frac{19}{3}=\frac{19}{3}$.
$e'$ is then a Nash equilibrium and its social cost is
$$(1-\mu)^2 [3d_B(3)]+2\mu(1-\mu)[d_A(1)+2d_B(2)]+\mu^2[2d_A(2)+d_B(1)]$$
 which is equal to $\frac{1}{9}21+\frac{4}{9}17+\frac{4}{9}15=\frac{149}{9}$. Finally, we have shown that $sup(e) \subseteq sup(e')$ but $SC(e) > SC(e')$. The mixed-strategy Nash equilibrium results in a lower social cost than the pure-strategy one."
Stochastic growth in continuous time,"More of a comment: There should be an expectation operator in the statement of the problem, otherwise problem doesn't make sense. That ""...the deterministic and stochastic value function must be the same..."" is not quite right. The value of $\sigma^2$ is crucial in the restriction \begin{align}
\rho = \left(-n + \sigma^2\left(1 - \frac{\alpha\gamma}{2}\right)\right)(1-\alpha\gamma).
\end{align} If $\sigma^2 = 0$, then presumably $\rho < 0$ for economically reasonable $\alpha$ and $\gamma$, in which case the deterministic problem may be ill-posed. What is true is that the stochastic value function takes the given form only if the parameter restriction holds. Factoring out the Ito term $\frac{1}{2} \sigma^2$ from the right hand side $$
\sigma^2\left(1 - \frac{\alpha\gamma}{2}\right) (1-\alpha\gamma),
$$ the restriction can be written as $$
\rho + n (1-\alpha\gamma) = \frac{1}{2} \sigma^2 [ (1-\alpha\gamma) - (-\left(1 - \alpha\gamma\right)^2)].
$$ On the right hand side, we have a elasticity of intertemporal substitution term $(1-\alpha\gamma)$ and a risk aversion term $-\left(1 - \alpha\gamma\right)^2$. What the restriction says is that, with a particular choice of $\sigma$, they offset each other, up to time preference $\rho$ and the drift $n(1 - \alpha\gamma)$. Therefore the value function is independent of $\sigma$. That the value function is independent of $\sigma$ is an artifact of the restriction, and choice of CRRA $u$. Not true in general."
Should I stay or should I quit?,"A relevant literature seems to be that on optimal stopping problems. There is a fairly technical Wikipedia article here and here's a book chapter.   In economics, such models have been used to think about how sellers learn about selling opportunities, investment in R&D, and optimal labor search strategies."
Contest: earliest written instance of economic thought,"I was recently amazed to discover instances of computational social choice in The Nine Chapters on the Mathematical Art, the Chinese counterpart of Euclid's element, written by several generations of scholars from the 10th up to the 2nd century BCE. The core issues in social choice theory is the question of fair allocation, or fair collective decision. Once an allocation rule is devised, it remains to make it applicable. One of the topics of computational social choice is precisely to build efficient algorithm to compute the solution of allocations rules given some inputs. This is precisely the topic of chapter 3 and chapter 6 of The Nine Chapters on the Mathematical Art, which are entitled (resp.)  ""Proportional Distribution"" and ""Fair Levies"". The two fair division rules considered are the proportional division and the weighted proportional division rule. These rules are taken as given. They are not justified in any axiomatic way in the book, which would be the standard practice in modern social choice theory. Yet algorithm are proposed to apply them which in my views makes it a legitimate early instance of computational social choice ( because it answer the question  ""given this allocation rule, how can we compute its solution for any possible inputs?"" at a time when the answer to this question was far from obvious for the allocation rules considered). Some example problems you will find in chapter 3 : (All the examples below are taken form the excellent commented edition of the Nine chapters by Kangshen Shen, John N. Crossley, Hui Liu Oxford University Press, 1999. This edition contains great references to instances of these problem in written sources from other civilization, such as in Euclid's Elements) [Problem 1] "" Now Given five officials of different ranks : Dafu, Bugeng, Zanniao,
  Shangzao, and Gongshi jointly hunting 5 deers. Tell : how many does
  each get if [the deer are] distributed according to their ranks?"" [Problem 2] ""Now given a cow, a horse and a sheep have eaten up the seedlings of
  someone's field. The landlord demands 5 duo of millet as compensation.
  The shepherd says : ""My sheep eats hals as much as the horse."" The
  horse owner says : ""My horse eats half as much as a cow."" The
  compensation is to be distributed according to the rates. Tell : how
  much should each repay?"" An example problems you will find in chapter 9 : [Problem 1] ""Now given the task of transporting tax millet is distributed among four counties. County A, 8 days from the tax bureau, has 10 000 households; County B, 10 days from the bureau, has 9500 households; County C, 13 days form the bureau, has 12350 households; County D, 20 days from the bureau, has 12 200 households. The total tax millet is 250000 hu needing 10000 carts. Assume the task is to be distributed in accordance with the distance from the bureau and the number of household. Tell : how much millet should each county transport? How many cart does each county employ?"""
Economic policies to decrease obesity (would they be effective?),"Yes, sugar tax! This is probably as controversial as tobacco tax was back in the days. If you walk through a supermarket, you will find that half of the food section is food full of sugar. Sugar is what makes you fat, not fat itself. It has been known for a while, at least since the professional sports were invented. Yet the lobby of the enormous sugar industry keeps regulators from labeling it as hazardous and taxing its use. If you are doing a research into obesity I highly recommend devoting two hours to watch That Sugar Film (2014). I would also recommend taking a multidisciplinary approach and bringing some arguments from biochemical and nutrition fields."
The supply and demand of Virtual Products,"The book Information Rules by Google Chief Economist Hal Varian (with Carl Shapiro) deals with many issues raised by the particular features of the digital economy. In general, he finds you don't need new models, and that things are well approximated by high fixed and low or zero marginal cost of production."
"What happens if the ""control variables"" are also endogenous?","I don't want to emphasize this too much, but it's worth mentioning that this is not true in general. The following derivation will hopefully provide some understanding of the ""contamination"" you mention. As a simple counterexample, suppose that the data generating process is given by
$$
Y = X_1 \beta_1 + X_2 \beta_2 + Z \gamma + \varepsilon,
$$
where $Z$ is unobserved. Let $Cov(X_1,Z) = 0$, $Cov(X_2, Z) \neq 0$, and 
$Cov(X_1,X_2) = 0$. Then, it
is clear that $X_2$ is ""endogenous."" But notice that because $Cov(X_1,Z) = 0$, our
estimate of $\beta_1$ will still be ok:
$$
\text{plim}\, \hat \beta_{1} = \beta_1 + \gamma \frac{Cov(X_1^*, Z)}{Var(X_1^*)} = \beta_1,
$$
where $X_1^* = M_2 X_1$ and $M_2 = [I - X_2(X_2'X_2)^{-1}X_2']$. Because $Cov(X_1,X_2) = 0$, $X_1^* = X_1$. So $Cov(X_1^*,Z)=0$.  One of the mains challenges of doing good econometrics is thinking of potential identification strategies. In the type of situation you describe, there is probably nothing you can do but to try to approach the problem a different way."
The relationship between the expenditure function and many others!,"Following up on the excellent MWG diagram in Amstell's answer, the fundamental observation needed is that holding $p$ fixed, $e$ and $v$ are inverses of each other. $e$ tells us the amount we need to spend to get a certain amount of utility $u$, while $v$ tells us the maximum amount of utility we can get from a certain expenditure $w$. Whenever we want to convert from utility to wealth, we use $e$; and whenever we want to convert from wealth to utility, we use $v$. All the key identities can be derived from this observation. For instance, suppose we want to derive an identity for $\partial v(p,w)/\partial p_i$. We already know the corresponding identity for the expenditure function, $\partial e(p,u)/\partial p_i=h_i(p,u)$. To turn this into an identity for $v$, we substitute $w=e(p,u)$, obtaining $v(p,e(p,u))=u$, and differentiate with respect to $p_i$. The chain rule implies
$$\frac{\partial v(p,e(p,u))}{\partial p_i} + \frac{\partial v(p,e(p,u))}{\partial w}\cdot\frac{\partial e(p,u)}{\partial p_i} =0\\
\Longleftrightarrow \frac{\partial v(p,w)}{\partial p_i} = -\frac{\partial v(p,w)}{\partial w}\cdot x_i(p,w)$$
which, if we divide by $-\partial v/\partial w$ on both sides, becomes Roy's identity. Or, suppose that we want to derive the Slutsky equation, which gives the relationship between the derivatives of Marshallian and Hicksian demand (decomposing a Marshallian demand change into substitution and income effects). Analogously to above, we can substitute $w=e(p,u)$ into Marshallian demand $x(p,w)$ to obtain $x(p,e(p,u))=h(p,u)$. Then, differentiating with respect to $p_i$ on both sides and applying the chain rule gives
$$\frac{\partial x(p,e(p,u))}{\partial p_i} + \frac{\partial x(p,e(p,u))}{\partial w}\cdot \frac{\partial e(p,u)}{\partial p_i} = \frac{\partial h(p,u)}{\partial p_i}\\
\Longleftrightarrow \frac{\partial x(p,w)}{\partial p_i}=\frac{\partial h(p,u)}{\partial p_i} - \frac{\partial x(p,w)}{\partial w}\cdot x_i(p,w)
$$
In general, I think the heuristic ""switch between $w$ and $u$ as needed using $v$ and $e$"" allows you to get pretty much everything here. (A similar heuristic is also useful if you ever deal with Frisch demand systems, where marginal utility $\lambda$ plays the same role that $w$ and $u$ do in Marshallian and Hicksian demand systems.) Of course, there is one other key fact used above, which is $\partial e(p,u)/\partial p_i = h_i(p,u)$, which for $w=e(p,u)$ becomes $\partial e(p,u)/\partial p_i = x_i(p,w)$. This is best viewed, instead, as a direct consequence of the venerable envelope theorem.  ($\partial v/\partial p_i$ can also be derived from the slightly more advanced version of the envelope theorem, where constraints as well as the objective are allowed to depend on a parameter. Since varying $p_i$ in the utility maximization problem changes the budget  constraint rather than the objective, the envelope theorem says that its effect will depend on the Lagrange multiplier on that constraint, which is the marginal utility $\partial v/\partial w$ of wealth. This is a good intuition for why the expression for $\partial v/\partial p_i$ is more complicated than the expression for $\partial e/\partial p_i$, picking up an extra factor.)"
Does the Federal Reserve buy and sell stocks?,"No, the Fed is not allowed to buy stocks, they are allowed to buy government securities in open market operations in order to achieve the target rate for the federal funds rate. The guidelines for this are explained in the Section 14 of the Federal Reserve Act. You can find the Fed holdings in the Federal Reserve Statistics. However other central banks, like the Bank of Japan, started buying stocks as a measure to support their financial institutions (their banks were subject to too much market risk because of their stock holdings). The have detailed their stock purchasing plan in their website."
Why do lenders dislike early loan repayments?,"The reason that lenders dislike early repayments (known as ""prepayments"" or ""voluntary prepayments"") is that most lenders match their assets— the loans they've made to others— with liabilities of their own. This can lead to lenders facing significant interest rate risk. This is important to understand— while default risk is certainly significant, interest rate risk can be very large as well, sometimes greater than default risk. Consider: a lender loans you money at 6.5% for 30 years to take out a mortgage, and funds this by issuing debt in the secondary market for 10 years at 4%. A recession comes along, the Fed drastically cuts interest rates, and the prevailing interest rate for mortgages drops to 3.5%. You repay your mortgage early by taking out a loan from another lender at 3.5%, and now the original lender has the principal back, but cannot reinvest it at a rate high enough to repay its own interest rate expense. If this happens on a broad enough scale, the lender is now bankrupt, with all that entails. That is why they don't like it. With regard to prepayment penalties— the option to prepay debt is what is known as a call option, and it's often quite valuable. Prepayment penalties are simply a way for lenders to be compensated for the exercise of that option. The specific case that you're talking about sounds an awful lot like the way that some credit card lenders feel about consumers who take out loans at low (often zero) ""teaser"" rates and refinance them before they ever have to pay anything to the lender, which bears its own interest expense in the meantime. That's probably best regarded as a strategy by consumers that takes advantage of lenders who are hoping that the consumers will not repay the debt in time. This hope on the part of lenders can sometimes cross the line into predatory behavior (see, for example, the discussion of teaser rates on mortgages in the Financial Crisis Inquiry Commission report), so the fact that the borrowers and lenders in such a situation aren't particularly fond of one another comes as little surprise."
Is there any way retirement systems can cope with declining population?,"There is no magic trick that the government can use to solve this issue. Government pensions can be kept funded by: The government will have to implement some combination of the above measures, but government can only really directly affect the first 3-4. Private pensions are also not necessarily a solution to the problem. Low population growth will likely lead to low economic growth, affecting the saving and investment model as well. People will either have to save more, receive lower benefits, or retire later. There aren't any other options. Also smaller population will reduce value of assets which people use to save (since there will be less demand for them). You should read Barr's textbook The Economics of Welfare State. He explains this in chapter 9."
Are there Utility Monsters in Economics?,"There is a type of protection called a liability rule, where I, $A$, can take something from $B$, if I pay the damages $c$ which are court-ordered preemptively. Copyright law is all about liability rules. If the damages are correlated with $B$'s valuation appropriately, then efficiency holds. You are interested in the opposite case. If IP law doesn't get it right, this would be an example matching your hypothetical as I understand it. And some might say this is justified. A company might want to keep some research under wraps to prevent competitors from also using it and making it even more valuable to society through investment. So the expiration of a patent might fit this scenario in a roundabout way too if we start the model where the company already owns the patent and a court is deciding when to let it expire. Reference: Exchange Efficiency With Weak Property Rights."
"What is the definition of ""First Best"", ""Second Best"", etc. in contract theory?","In contract theory The first-best refers to the best you could do if you knew agents' preferences over labor and income (i.e., if you did not have to impose the incentive compatibility constraint), and the second-best is the best you can do if agents have to reveal their preferences themselves. In mechanism design A useful reference is Galichon, Alfred, Ex-Ante vs. Ex-Post Efficiency in Matching (May 21, 2011). Available at SSRN: http://ssrn.com/abstract=1837321 or http://dx.doi.org/10.2139/ssrn.1837321  : ""an assignment is called ex-post efficient if no other deterministic assignment is improving on it in a Pareto sense; and ex-ante efficient if no lottery over deterministic assignments is."" (my emphasis) Difference between the two There isn't many connexions between the two notions as defined above. Every combination of the two notions is a priori possible. Both a mechanism and a contract can be First-best ex-post efficient (i.e., efficient when incentive compatibility constraint is not imposed and the outcome of the mechanism/contract must be deterministic) First-best ex-ante efficient (i.e., efficient when incentive compatibility constraint is not imposed and the outcome of the mechanism/contract can be random) Second-best ex-post efficient (i.e., efficient when incentive compatibility constraint is imposed and the outcome of the mechanism/contract must be deterministic) Second-best ex-ante efficient (i.e., efficient when incentive compatibility constraint is imposed and the outcome of the mechanism/contract can be random) In the literature Yet, in general, you are more likely to find the first-best/second-best terminology in contract theory (random contracts are not that common in contract theory), and the ex-post/ex-ante efficiency in mechanism design (knowledge of the preferences is rarely assumed in a mechanism: the fact that we do not know agents preferences is the raison d'être of mechanism design). Thus you can expect to see 3. and 4. being discussed in the mechanism design literature, and 1. and 3. being discussed in contract theory. Beware This being said, the notions of second-best and first-best are often used outside of contract theory in a rather permissive way, which may be confusing. For some people, first-best simply means ""if we do not impose some constraint $X$"" and second-best means ""if we do impose constraint $X$"". Thus you may hear people talk about (this is where things can gets confusing): Hope that clarifies it (or at least does not confuse you more)."
What is the difference between contract theory and mechanism design?,"Contracts are a subset of all mechanisms where agreements are enforcable. An example of a mechanism that is not a contract: 
A second price auction (or Vickrey auction) is a truth-telling mechanism where the enforcability of contracts is not required. In the truth-telling equilibrium no one has any incentive to change their bid, no matter the outcome. This is not because changing the bid would be a violation of contract and result in penalties."
Explaining mixed strategies for one-shot games,"Ariel Rubinstein tends to be insightful regarding these kinds of questions.  He addresses the interpretation of mixed strategies in section 3 of this paper.  A few possible interpretations aside from deliberate randomization: An interesting quote regarding player $i$'s mixed strategy reflecting uncertainty among $-i$'s regarding what $i$ will do: Mixed strategy can alternatively be viewed as the belief held by all other 
  players concerning a player's actions. A mixed strategy equilibrium is then an 
  n-tuple of common knowledge expectations, which has the property that all the 
  actions to which a strictly positive probability is assigned are optimal, given the 
  beliefs. A player's behavior may be perceived by all the other players as 
  the outcome of a random device even though this is not the case. Adopting this 
  interpretation requires the reassessment of much of applied game theory. In 
  particular, it implies that an equilibrium does not lead to a prediction (statistical 
  or otherwise) of the players' behavior. Any player i's action which is a best 
  response given his expectation about the other players' behavior (the other 
  n - 1 strategies) is consistent as a prediction for i's action (this might include 
  actions which are outside the support of the mixed strategy). This renders 
  meaningless any comparative statics or welfare analysis of the mixed strategy 
  equilibrium and brings into question the enormous economic literature which 
  utilizes mixed strategy equilibrium. "
Why in most macro models technology is labor-augmenting?,"The mathematical reason, is that this happens in order for the model to have a steady-state in terms of growth rates: variables like Consumption, Capital, Income, grow at the steady-state, but grow at the same rate, so their ratios remain constant (and it is in this sense that this situation represents a ""steady""-state). If they were to grow at different rates, their ratios would tend to either zero or infinity which is not very realistic, since it would imply that the economy tends towards one or the other ""corner"" situation.   The mathematical proof can be found in Barro & Sala-i-Martin book (2nd ed) , section 1.5.3, pp 78-80. Relevant and useful is also the discussion in section 1.2.12, pp 51-53. For functional forms like (generalized, even) Cobb-Douglas, it is really indistinguishable (not separately identifiable), especially since we predominantly use the exponential function: $$Y_t = A\cdot \left (K_te^{zt}\right)^\alpha \left (L_te^{vt}\right)^\beta = A\cdot K_t^{\alpha}\left (L_te^{(v+\frac {\alpha}{\beta}z)t}\right)^\beta = A\cdot K_t^{\alpha}\left (L_te^{wt}\right)^\beta$$ So strictly speaking in such a functional setup we can say that technology is also capital augmenting. But since for other functional forms, the above does not hold, and so we must explicitly assume that technology is ""labor-augmenting"" for the reason stated previously, authors settled in labeling it as such in order to cover all cases, and when they want to keep the functional form unspecified.   Regarding the conceptual issue the OP poses, which is insightful, a conceptual way out is to think of ""Technology"" more like ""Knowledge"". So ""Knowledge"" that goes into the machines, is part of the Investment that augments capital, while the other knowledge turns raw labor $L$ into human capital: essentially a production function with ""exogenous labor-augmenting technology"", is equivalent to a formulation that includes Human Capital instead of labor but where the investment in Human Capital is not subject to optimizing behavior but ""automatic"" (which points to Arrow's ""Learning-by-Doing"" concept of human capital accumulation)."
Is there a “runaway” threshold for Debt-to-GDP Ratio in the U.S.?,"Is there a threshold we can pass in terms of debt-to-GDP that will cause a runaway sort of effect? No there is no threshold or magic number (at least not one we know of). Few years ago there was an influential research claiming that there is a threshold at about 90% debt-to-GDP ratio published by Reinhart and Rogoff  where debt was supposed to start having strong negative effects on growth, but that research was discredited later on when it was discovered it was based on several mistakes in their calculations (see Herndon et al 2014). As the literature currently stands, there are no arguments for some 'magic' number. Debt can start being a problem for a country at 30-50% debt-to-GDP, but it might be no problem at all even at 300% of debt-to-GDP. It all depends on the structure of debt (e.g. who holds the debt?), how the debt is denominated (i.e. does country borrow in its own currency or foreign one?), and also what is the long term debt trajectory (e.g. see Pescatori et al 2014). The debt trajectory is very important because even country with large debt can out-grow it since debt-to-GDP ratio is literally ratio of $\frac{\text{debt}}{GDP}$ and thus you can reduce it both by reducing debt, or by keeping debt constant and growing GDP or just making sure debt does not grow as fast as GDP. However, while there is no magic threshold for debt crisis/overhang high debt can have serious negative effects at some point (although for US it is likely quite high so you should not necessarily worry about scaremongering of some politicians. Japan that is somewhat similar (high income, industrial and aging country), is able to sustain debt-to-GDP in excess of 250% - see Statista data here). What are these supposed effects? This will depend on the nature of debt crisis once it is triggered. Although the Reinhart and Rogoff work on debt threshold was discredited, rest of their work which describes negative effects of debt crises  is actually still quite solid. Reinhart and Rogoff show that debt crises are often having following negative effects: However, note not all countries will suffer from all of the above problems. Some of the above problems are policy choices, others depend on exact structure of debt. Its difficult to say what would happen to the US so I won't speculate on that."
Log-linearization of Euler equation with an expectation term,"Let's ignore for the moment the existence of the expected value. If this was a deterministic set-up, linearization through taking logs would be straightforward, and without the tricks of the links the OP provided. Taking natural logs on both sides of the first equation we obtain: $$0 = \theta \ln \delta-\frac {\theta}{\psi}\ln \left (\frac{C_{t+1}}{C_t} \right )-(1-\theta) \ln(1 + R_{m,t+1}) + \ln (1 + R_{i, t+1}) \tag{1}$$ Set  $$\hat c_{t+1} = \frac{C_{t+1}-C_t}{C_t} \Rightarrow \frac{C_{t+1}}{C_t} = 1+\hat c_{t+1} \tag{2}$$ Also, note that it is standard approximation to write $\ln (1+a) \approx a$ at least for $|a|<0.1$. Usually this is the case with growth rates and financial rates so we obtain $$0 = \theta \ln \delta-\frac {\theta}{\psi}\hat c_{t+1} -(1-\theta) R_{m,t+1} + R_{i, t+1} \tag{3}$$ which is a clear dynamic relation that links the three variables present.
If in the model, the steady-state is characterized by constant consumption and constant returns, then at it we will have $\hat c_{t+1} =0$ and so the steady-state relation will be  $$R_{i} = - \theta \ln \delta + (1-\theta) R_{m} \tag{4}$$ But we did all these ignoring the expected value. Our expression is $E_t\left[f\left(C_t, C_{t+1},R_{m,t+1},R_{i,t+1} \right)\right]$, not just  $f\left(C_t, C_{t+1},R_{m,t+1},R_{i,t+1} \right)$. Enter first-order Taylor expansion of $f()$. We need a center of expansion. Represent the four variables simply by $\mathbf z_{t+1}$ (it doesn't hurt that a variable with $t$-index is present in $\mathbf z_{t+1}$). We choose to expand the function around $E_t(\mathbf z_{t+1})$. So $$f\left(\mathbf z_{t+1}\right) \approx  f\left(E_t[\mathbf z_{t+1}]\right) + \nabla f\left(E_t[\mathbf z_{t+1}]\right)\cdot \big(\mathbf z_{t+1}-E_t[\mathbf z_{t+1}]\big) \tag{5}$$ Then $$E_t\left[f\left(\mathbf z_{t+1}\right)\right] \approx f\left(E_t[\mathbf z_{t+1}]\right) \tag{6}$$ Obviously this is an approximation, i.e. it has error, even if only because of Jensen's inequality. But it is standard practice. Then we see that all the previous work we did on the deterministic version, can be applied in the stochastic version inserting conditional expected values in place of the variables. So eq. $(3)$ is written  $$0 = \theta \ln \delta-\frac {\theta}{\psi}E_t[\hat c_{t+1}] -(1-\theta) E_t[R_{m,t+1}] + E_t[R_{i, t+1}] \tag{7}$$ But where are the steady-state values? Well, steady state values in a stochastic context are a bit tricky -are we arguing that our variables (which are now treated as random variables) become constants? Or is there another way to define a steady-state in a stochastic context?   There are more than one ways. One of them, is the ""perfect foresight steady state"", where we forecast perfectly a not-necessarily constant value (this is the concept of ""equilibrium as fulfilled expectations""). This is for example used in Jordi Gali's book mentioned in a comment. ""Perfect-foresight steady state"" is defined by
$$E_t(x_{t+1}) = x_{t+1} \tag{8}$$ Under this concept, eq. $(7)$ becomes eq. $(3)$ which is now the ""perfect-foresight stochastic steady state"" equation of the economy. If we want a stronger condition, saying that variables become constant in the steady-state, then it is also reasonable to argue that, again, their forecast will eventually be perfect. In that case, the steady-state of the stochastic economy is the same as that of the deterministic economy, i.e. eq. $(4)$. "
Rosen's Diagonal Strict Concavity condition,"The diagonally strict concavity property is better known as the strict monotonicity property of the pseudo-gradient. An operator $\Psi:\mathbb{R}^n \to \mathbb{R}^n$ is strictly monotone if the following holds true:
$$\forall x_0, x_1 \in \mathbb{R}^n: \left< x_0 - x_1, \Psi(x_0) - \Psi(x_1)  \right> >0. $$ In your case $\Psi$ would be $g$.
The intuition behind it is that for every line of $R^n$, the projection of $\Psi$ on that line is strictly monotone. I join here an example: the payoff function are given by $\varphi_i$ for both players $i \in {1,2}$.  "
Current knowledge about the empirics of consumer theory,"The primary literature concerned with this type of question (at least where classical results break down) is behavioral economics. There's a great general compilation of papers put together by the Russell Sage Foundation called the ""Behavioral Economics Reading List"" that includes, among other things, a General Introduction section with overview papers by some of the big movers and shakers (Camerer, Kahneman, Laibson, etc.). Many of the papers you will find through the Russell Sage paper list will be on alternative methods to classical consumer theory. If you want just the tests of assumptions/predictions, I would recommend looking through the abstracts of John Lists's papers on Testing Economic Theory. List is one of the most prolific experimentalist authors on the subject and has weighed in on most questions that other people work on in this field as well. Just reading his abstracts on that page should give you a pretty good idea of the state of the literature up through 2011 (the website isn't updated). His CV is updated, but doesn't give you the abstracts without looking up the papers individually."
Available code for computing solutions to matching algorithms?,"While answering a comment, I realized I had a post-worth response. R has become the ""default language"" for a lot of computational research statistics (for a number of reasons; nice NYT article here). It's high level, free and open-source, and has a closely-related journal for publishing statistical algorithms. Citations and peer review are key for academia, so you get a lot of well-described code posted to the R archives (CRAN) with descriptions posted to JStat. This spills over into a lot of blogs and quick demonstration code posts. That is to say, there's an enormous user-create code base for R. When I need to find an algorithm online, I'll often first look to the massive R codebase. A quick search for R code turned up the following: From an R blogger, with code (see the gist link): The Deferred Acceptance Algorithm (DAA) goes back to Gale and Shapley (1962). They introduce a rather simple algorithm that finds a stable matching for example for college admissions or in a marriage market. ... Variations of this algorithm are used in Hospital assignments in the USA, whereby recently graduated doctors submit preferences over hospitals, and hospitals submit preferences over graduates. ... Here I'm going to use R to make a little simulation of this From an install-able github repository for matching markets: R package matchingMarkets comes with two estimators: stabit: Implements a Bayes estimator that estimates agents' preferences and corrects for sample selection in matching markets when the selection process is a one-sided matching game (i.e. group formation). stabit2: Implements the Bayes estimator for a two-sided matching game (i.e. the college admissions and stable marriage problems). and three algorithms that can be used to simulate matching data: hri: Constraint model for the hospital/residents problem. Finds all stable matchings in two-sided matching markets. Implemented for both the stable marriage problem (one-to-one matching) and the hospital/residents problem, a.k.a. college admissions problem (many-to-one matching). sri: Constraint model for the stable roommates problem. Finds all stable matchings in the roommates problem (one-sided matching market). ttc: Top-Trading-Cycles Algorithm. Finds stable matchings in the housing market problem. Functions hri and sri allow for incomplete preference lists (some agents find certain agents unacceptable) and unbalanced instances (unequal number of agents on both sides). Hopefully one of these can help. The second one in particular looks extremely useful, particularly if it provides an empirical estimator."
First Order Condition for Profit Maximization in Gambling Industry,"The expression in question is in footnote $11$ of the referenced article. Reading the paper, we see that the decision variable here is ""the payout rate"", which is the reciprocal of $P$. So equivalently, we can solve the maximization problem with respect to $P$ (and not w.r.t. $Q$). More over, ""price elasticity of demand"" involves the derivative of $Q$ with respect to $P$, and not the other way around:  $$E_{PQ} = \frac {dQ/dP}{Q}P $$ and we expect it to be negative (higher price means lower payout rate leading to less demand for the quantity measure here, i.e. less ""demand for prizes""). We can write the maximization problem as 
$$\max_{P}N = \max_{P}\left[P\cdot Q(P) - Q(P) - C(Q(P))\right]$$ The  first-order condition is $$\frac{\partial N}{\partial P} = Q + P\cdot Q'  - Q' - C'\cdot Q' = 0 \tag{1}$$ Multiply throughout by $P/Q$: $$Q\frac PQ + P\cdot Q'\frac PQ   - Q'\frac PQ  - C'\cdot Q'\frac PQ  = 0$$ $$\Rightarrow P +P\cdot E_{PQ} - E_{PQ} - C'\cdot E_{PQ}=0$$ $$\Rightarrow -E_{PQ} = \frac {P}{P-1-C'} \tag{2}$$ This makes sense. Plugging in the values presented in the reference, we have $$-E_{PQ} = \frac {2}{2-1-.12} = \frac {2}{0.88} \approx  2.27 $$ which is very close to the value resulting from the equation presented by the authors. I haven't been able, by whatever algebraic manipulations I tried, to replicate their formula, but eq $(2)$ is correct in any case. If a reconciliation comes up, I will update."
Economics of Homelessness: Are housing prices to blame?,"This paper is close to what you want.  It is generally believed that the increased incidence of homelessness
  in the United States has arisen from broad societal factors, such as
  changes in the institutionalization of the mentally ill, increases in
  drug addiction and alcohol usage, and so forth. This paper presents a
  comprehensive test of the alternate hypothesis that variations in
  homelessness arise from changed circumstances in the housing market
  and in the income distribution. We assemble essentially all the
  systematic information available on homelessness in U.S. urban areas:
  census counts, shelter bed counts, records of transfer payments, and
  administrative agency estimates. We estimate similar statistical
  models using four different samples of data on the incidence of
  homelessness, defined according to very different criteria. Our
  results suggest that simple economic principles governing the
  availability and pricing of housing and the growth in demand for the
  lowest-quality housing explain a large portion of the variation in
  homelessness among U.S. metropolitan housing markets. Furthermore,
  rather modest improvements in the affordability of rental housing or
  its availability can substantially reduce the incidence of
  homelessness in the United States. Quigley, John M., Steven Raphael, and Eugene Smolensky. ""Homeless in America, homeless in California."" Review of Economics and Statistics 83.1 (2001): 37-51. The results in table 5 provide the strongest evidence that housing
  market tightness is an  important determinant of homeless. For
  permanent caseloads, vacancy rates have a strong negative and
  statistically significant  effect (at the 1% level) on the incidence
  of homelessness. Moreover, these elasticity estimates are quite
  similar across specifications. In the two specifications including
  fair market rents (specifications (1) and (3)), rents exhibit a
  significant  positive effect on the incidence of homelessness as
  predicted by theory. In addition, the specification including the
  ratio of rents to income indicates a strong positive effect. Hence,
  for homelessness, as measured by the incidence of  homeless
  house-holds seeking permanent assistance, measures of housing market
  tightness consistently exhibit strong and statistically significant
  effects, which is consistent with the predictions of theory. We also
  find a negative and significant effect of per capita income. There are
  no important effects of county unemployment rate  s and no consistent
  and significant effect  of the SSI populations across specifications.
  Similar to the results from the S-night and continuum-of-care data
  sets, the results in table 5 also indicate that warmer weather is
  positively associated with homelessness The key results table:  Open access version?"
Larry Summers on the causes of secular stagnation,"French economist Jean-Baptiste Michau has published in 2018 a relatively simple model of secular stagnation: To investigate secular stagnation, I add two features to a standard Ramsey model with money: (i) Households have a preference for wealth; (ii) Wages are downward rigid. In this framework, there exists a frictionless neoclassical steady state equilibrium characterized by a low natural real interest rate. In addition, if wages are sufficiently rigid and the natural real interest rate sufficiently low, then there also exists a Keynesian secular stagnation steady state characterized by under-employment, low inflation, and a binding zero lower bound on the nominal interest rate. As wages become more flexible, the Keynesian steady state diverges away from the neoclassical steady state, until wages are so flexible that it ceases to exist. If monetary policy is excessively restrictive, then the secular stagnation steady state is the unique steady state equilibrium of the economy. The optimal policy response to secular stagnation is to move the economy to the neoclassical steady state. This can either be achieved by raising the central bank's inflation ceiling or by taxing wealth and subsidizing investment in physical capital. This optimal tax policy is revenue-neutral. A free prior version of that paper is available on HAL. Insofar this paper of Michau doesn't have a lot of citations. As an aside, as the US economy got better, so have the critics of secular stagnation (as applying to the US now or as relevant even during the post-2008 recovery) sharpened their criticism, e.g. Stiglitz has a 2018 piece against it, saying that secular stagnations was used as an excuse for an insufficient stimulus/reform (of the more redistributive kind Stiglitz favors). Summers has written a response to that piece. Dutch economists van den End and Hoeberichts have published (also in 2018) an empirical assessment for some OECD countries whether low real interest rates are a driver of secular stagnation: We empirically test whether there is a causal link between the real interest rate and the natural rate of interest, which could be a harbinger of secular stagnation if the real rate declines. Outcomes of VAR models for seven OECD countries show that a fall in the real rate indeed affects the natural rate. This causality is significant for Japan in all model specifications, for Canada, France, UK and Germany in some specifications and it is not significant for the US and Italy. The policy implication is that to avoid secular stagnation, expansionary monetary policy to reduce the real rate is less effective than policies aimed at raising the natural rate. Acemogulu and Restrepo (2017) studied empirically whether aging population is causing secular stagnation, and they find that it didn't happen, i.e. automation compensated for population aging: Several recent theories emphasize the negative effects of an aging population on economic
  growth, either because of the lower labor force participation and productivity of older workers or
  because aging will create an excess of savings over desired investment, leading to secular
  stagnation. We show that there is no such negative relationship in the data. If anything, countries
  experiencing more rapid aging have grown more in recent decades. We suggest that this
  counterintuitive finding might reflect the more rapid adoption of automation technologies in
  countries undergoing more pronounced demographic changes, and provide evidence and
  theoretical underpinnings for this argument. Acemogulu and Restrepo seem to be using GDP per capita for their counterpoint, which may or may not be what Hansen intended (he didn't give more technical formulation, apparently). In contrast to these last two papers, Eggertsson, Mehrotra and Robbins have an updated (2017) paper claiming that US is/was experiencing a secular stagnation (up to 2015 at least): This paper formalizes and quantifies the secular stagnation hypothesis, defined as a persistently
  low or negative natural rate of interest leading to a chronically binding zero lower bound (ZLB).
  Output-inflation dynamics and policy prescriptions are fundamentally different from those in the
  standard New Keynesian framework. Using a 56-period quantitative life cycle model, a standard
  calibration to US data delivers a natural rate ranging from –1:5% to –2%, implying an elevated
  risk of ZLB episodes for the foreseeable future. We decompose the contribution of demographic
  and technological factors to the decline in interest rates since 1970 and quantify changes required
  to restore higher rates. I guess the standard comment about the dismal science applies at this point. Also, I was curious where they break down the contribution of technological and demographic factors, but I can't seem find it explicitly in their 90-page paper. I guess they just mean the (negative coefficient for the) labor share for the former (in tables A.7-A.13). There's also an empirical 2017 EU-focused paper: Is secular stagnation—a period of persistently lower growth such as that seen following the
  financial crisis of 2008/09—a valid concern for euro-area countries? We tackle this question
  using the well-established Laubach-Williams model to estimate the unobservable equilibrium
  real interest rate and compare it to the actual real rate. In light of the considerable increase in
  heterogeneity among EU member countries since the beginning of the financial crisis, we
  apply our approach to twelve euro-area countries to provide country-level answers to the
  question of secular stagnation. The presence of secular stagnation in a number of euro-area
  countries has important implications for ECB decision-making (i.e., voting power in the
  Governing Council) and EU governance. Our results indicate that secular stagnation is not a
  significant threat to most euro-area countries, with one possible exception: Greece. And using basically the same methodology (as the previous paper), but a different (longer) time frame, different conclusions are drawn in another 2017 paper: U.S. estimates of the natural rate of interest – the real short-term interest rate that would prevail absent transitory disturbances – have declined dramatically since the start of the global financial crisis. For example, estimates using the Laubach–Williams (2003) model indicate the natural rate in the United States fell to close to zero during the crisis and has remained there into 2016. Explanations for this decline include shifts in demographics, a slowdown in trend productivity growth, and global factors affecting real interest rates. This paper applies the Laubach–Williams methodology to the United States and three other advanced economies – Canada, the Euro Area, and the United Kingdom. We find that large declines in trend GDP growth and natural rates of interest have occurred over the past 25 years in all four economies. These country-by-country estimates are found to display a substantial amount of comovement over time, suggesting an important role for global factors in shaping trend growth and natural rates of interest. And an ECB economist studying the US data points out in a 2018 paper that there's additional (obvious too) reason recently, deleveraging: I extend the model of Laubach and Williams (2003) by introducing an explicit role for the financial cycle in the joint estimation of the natural rates of interest, unemployment and output,
  and the sustainable growth rate of the US economy. By incorporating the financial cycle --
  arguably an omitted variable from the system -- the model is able to deliver more plausible
  estimates of business cycle dynamics. The sustained decline in the natural rate of interest in
  recent decades is confirmed, but I estimate that strong and persistent headwinds due to financial
  deleveraging have lowered temporarily the natural rate on average by around 1 p.p. below its
  long-run trend over 2008-14. This may have impaired the effectiveness of interest rate cuts to
  stimulate the economy and lift inflation back to target in the immediate aftermath of the GFC. The only firm conclusion I can draw from all of these is that secular stagnation is hot topic for research still/nowadays. Another thing I've noticed is that these last two papers seem to deliberately eschew using the ""secular stagnation"" term explicitly; it only appears in their references. I suspect their authors might dislike it for its imprecision and/or multiple meanings (Hansen's vs Summers'); they are largely addressing/using the latter (Summers'), implicitly."
Barro's (2009) rare disaster model in the AER: How to derive equation (10)?,"I think Barro means in the footnote that Giovanni and Weil find the same equation, $U_t=\Phi C^{1-\gamma}$, but using the optimal path of $C_t$.
In Barro's paper, the approach is different given that the dynamics of $C_t$ is exogenous: $C_t=Y_t$ by assumption. Barro uses the limit case when the length of a period gets close to 0. Maybe what may bother the reader is that the model is defined as discrete. First, we can rewrite the model with a length of period $\delta$ and then use $\delta\to 0$.
The GDP dynamics write
$$\log(Y_{t+\delta})=\log(Y_t)+g\delta+u_{t+\delta}+v_ {t+\delta}$$ 
with $u_{t+\delta}\sim \mathcal{N}(0,\delta\sigma^2)$, and $v_{t+\delta}=0$ with probability $1-p\delta$ and $\log(1-b)$ with probability $p\delta$.
The utility satisfies
$$
U_t=\frac{1}{1-\gamma}\left\lbrace C_t^{1-\theta}+\frac{1}{1+\rho\delta}\left[(1-\gamma)E_tU_{t+\delta}\right]^\frac{1-\theta}{1-\gamma}\right\rbrace^\frac{1-\gamma}{1-\theta}.
$$ From now suppose there is a $\Phi$ such that $U_t=\Phi C^{1-\gamma}$ (note that $\Phi$ depends on $\delta$ a priori).
Define $H(U)=[(1-\gamma)U]^\frac{1-\theta}{1-\gamma}$, the utility satisfies
\begin{align}
H(U_t)= C_t^{1-\theta}+\frac{1}{1+\rho\delta}H(E_tU_{t+\delta}).
\end{align}
We substitute $U_t$:
\begin{align}
H(\Phi)C_t^{1-\theta}= C_t^{1-\theta}+\frac{1}{1+\rho\delta}H(\Phi)\left(E_t\left[C_{t+\delta}^{1-\gamma}\right]\right)^\frac{1-\theta}{1-\gamma}.
\end{align}
Hence, we obtain for $C_t\neq 0$,
\begin{align}
\frac{1}{H(\Phi)}= 1-\frac{1}{1+\rho\delta}\left(E_t\left[\left(\frac{C_{t+\delta}}{C_t}\right)^{1-\gamma}\right]\right)^\frac{1-\theta}{1-\gamma}.
\end{align} The trick is to find the expectation in the right-hand side from the GDP dynamics.
\begin{align}
\left(\frac{Y_{t+\delta}}{Y_t}\right)^{1-\gamma}= \exp\left((1-\gamma)g\delta\right).\exp\left((1-\gamma)u_{t+\delta}\right).\exp\left((1-\gamma)v_ {t+\delta}\right).
\end{align}
Taking the expectation and using the independence between $u_{t+1}$ and $v_{t+1}$, it follows
\begin{align}
E_t\left(\frac{Y_{t+\delta}}{Y_t}\right)^{1-\gamma}= \exp\left((1-\gamma)g\delta\right).E_t\exp\left((1-\gamma)u_{t+\delta}\right).E_t\exp\left((1-\gamma)v_ {t+\delta}\right).
\end{align}
The expectation of $\exp(X)$ where $X$ follows $\mathcal{N}(0,\sigma^2)$ is $\exp(\sigma^2/2)$. $\exp\left((1-\gamma)v_ {t+\delta}\right)$ is a random variable equal to $1$ with probability $1-p\delta$ and $(1-b)^{1-\gamma}$ with probability $p\delta$.
We substitute the expectation operator:
\begin{align}
E_t\left(\frac{Y_{t+\delta}}{Y_t}\right)^{1-\gamma}= \exp\left((1-\gamma)g\delta\right).\exp\left(\frac{(1-\gamma)^2\sigma^2\delta}{2}\right).\left(1-p\delta+pE[(1-b)^{1-\gamma}]\delta\right).
\end{align}
Finally, we use $C_t=Y_t$ to compute an equation for $\Phi$:
\begin{align}
\frac{1}{H(\Phi)}&= 1-\frac{1}{1+\rho\delta}\left\lbrace\exp\left((1-\theta)g\delta\right).\exp\left(\frac{(1-\gamma)(1-\theta)\sigma^2\delta}{2}\right)\right.\\
	&\left. .\left(1-p\delta+pE[(1-b)^{1-\gamma}]\delta\right)^\frac{1-\theta}{1-\gamma}\right\rbrace.
\end{align} The last step consists in taking a first-order approximation (I abusively keep the equal symbol):
\begin{align}
\frac{1}{H(\Phi)}&= 1-(1-\rho\delta). \left(1+(1-\theta)g\delta\right).\left(1+\frac{(1-\gamma)(1-\theta)\sigma^2\delta}{2}\right)\\
	& .\left(1-\frac{1-\theta}{1-\gamma}p\delta+\frac{1-\theta}{1-\gamma}pE[(1-b)^{1-\gamma}]\delta\right).
\end{align}
Pursuing the first-order apprixmation (all the $\delta^i$ with $i>1$ can be neglected), we have
\begin{align}
\frac{1}{H(\Phi)}&= \rho\delta -(1-\theta)g\delta-\frac{(1-\gamma)(1-\theta)\sigma^2\delta}{2}\\
	& +\frac{1-\theta}{1-\gamma}p\delta-\frac{1-\theta}{1-\gamma}pE[(1-b)^{1-\gamma}]\delta.
\end{align}
Substitute $g$ using $g^*=g+\frac{\sigma^2}{2}-pEb$, 
\begin{align}
\frac{1}{H(\Phi)}&= \rho\delta -(1-\theta)g^*\delta+(1-\theta)\frac{\sigma^2}{2}\delta -(1-\theta)pEb\delta -\frac{(1-\gamma)(1-\theta)\sigma^2\delta}{2}\\
	& +\frac{1-\theta}{1-\gamma}p\delta-\frac{1-\theta}{1-\gamma}pE[(1-b)^{1-\gamma}]\delta.
\end{align}
We take $\delta=1$ and invert function $H$ to find the solution in the footnote 7 of the paper. The right-hand side of this equation ""simplifies"" to the within braces in the formula."
Does a store suffering increased theft generally cause higher prices?,"prices should have already been set to maximize the trade off between profit-per-sale and volume sold But profit-per-sale depends on costs, which depends on the theft numbers, so if theft increases, the equation changes."
"Understanding the ""But they will lose jobs!"" argument","if they were to go out of business, there'd be more unemployment and people in economic troubles in total Not necessarily. Customers are paying money for the ill-gotten services of company X. This reduces the amount they have available to spend on other goods. If company X were banned (or perhaps more realistically its unethical practices were stopped and as a consequence it went out of business), its former customers would have more money to spend on other goods. If they chose to spend that money, they would provide additional business for other companies which would therefore need to employ more people. Whether the net effect on employment and aggregate wage income would be neutral would depend on various imponderables including customers' marginal propensity to consume and the output-to-labour ratios in the relevant industries.  However, the notion that the direct reduction in employment and income from banning company X would not be offset by secondary effects as described above can be seen to be fallacious. Given the comments below, I will add that it isn't relevant to the above argument whether any staff taken on by other companies include former employees of company X.  In a (typical) situation where there is some unemployment in an economy, the effect on overall economic welfare (as assessed by a suitable social welfare function) of some employees losing their jobs can be (partly or wholly) offset by others gaining employment."
Economics term for those who benefit even though they didn't contribute,I think the economic term is free-riders.
"Econometrics: Is elasticity meaningful in my, or any, regression?","The answer to the question is yes, it is indeed meaningful (at least mathematically speaking). If you estimate the linear equation $$ W = \beta_0 + \beta_1 PTR, $$ then $\beta_1=\frac{\partial W }{\partial PTR}$, meaning that $\beta_1$ represents the marginal change of $PTR$ over $W$. Now, if you estimate $$ log(W) = \beta_0 + \beta_1 log(PTR), $$ then $\beta_1=\frac{\partial W}{\partial PTR}\cdot\frac{PTR}{W}$, which is the very definition of elasticity. Generally speaking, linear transformations only affect the interpretation given to the coefficients, but the validity of the regression itself (in broad economic terms) is given by the model's assumptions and the economic phenomena being analyzed."
"If public roads are public goods, in what way are they non-rivalrous?","The property of rivalry is a continuous (rather than binary) variable.* A good is rivalrous if my consumption of it reduces the amount that can be consumed by others. So, a particular Big Mac is fully rivalrous, because each bite I take from it reduces (by that exact same amount I've bitten) the amount left for you. The degree to which roads are rivalrous is lower: My driving on a deserted road at night does not reduce at all the amount that can be driven on the same road by others. But your doubt is correct—it is not generally $0$: My driving at peak hour at a city center does reduce the amount that can be driven by others. Ideas are a good (and possibly sole) example of a good that's perfectly non-rivalrous. (My ""consumption"" of the Pythagorean Theorem leaves no less for anyone else.) Side point: But with the right laws and accompanying enforcement—e.g. patents, copyright—even ideas can be made excludable. Defense is often given as a classic example of non-rivalry. But I would argue that although defense exhibits a very high degree of non-rivalry, it is less than perfectly non-rivalrous. (The resources used to defend John in New York from a nuclear attack does reduce the US Department of Defense's ability to defend Jane in Hawaii. The resources used to defend a nation with over 1B people are greater than the resources used to defend a nation with less than 1M people.) *Likewise, the properties of excludability and being a public good are continuous (rather than binary) variables. (Many introductory textbooks tend not to emphasize this important point sufficiently.)"
R in econ departments?,"In my university the choice of
program is considered generally irrelevant. We focus on results, and it is up to each student to determine which program is best suited for the task and user preference. You will find that using one language translates very well to another. With resources like stackoverflow, I would not be too concerned about which. I would very carefully consider the statement ""two equal universities"". My experience has suggested to me such a thing does not exist, granting personal ambitions, career goals, and topic preference. I think programming choices are perhaps only a proxy for technical rigor and budget, little more.  Given you are only accepted to a finite number of schools, take time to evaluate this criteria closely."
Is complex analysis used in economics?,"It should be pointed out that just because one encounters complex numbers does not mean one is doing ""complex analysis"", e.g. complex eigenvalues, complex Borel measures, Fourier transforms, etc. where trivial properties of complex numbers come up.  Complex analysis is a very focused subject unlike, say, real analysis, which is eclectic by comparison. At its core are holomorphic functions of one or more complex variables.  This paper http://papers.ssrn.com/sol3/papers.cfm?abstract_id=932693 is a specific instance of an economic model where complex analysis is used. The model solution technique used there is the identification between holomorphic functions on the unit disk and their continuation on the boundary. (The resulting function space is called the Hardy space, which contains the players' strategy spaces in the game being played in the paper.)  "
Why must the rate of GDP growth be positive?,"You would expect the rate of growth to be generally positive because inventing things or inventing more efficient ways of doing things, is generally a one way process - things don't get un-invented. So we would expect things that are made by machines to be made ever more quickly as the machines evolve and improve. Periods of negative growth are likely to correspond to some financial cock-up (e.g. asset bubbles) or increasing scarcity of some natural resources for which a replacement can not be found."
What is a substitute/complement in terms of mixed partial derivatives?,"It is very important here to note that there are multiple, mutually inconsistent, possibilities for how to define a substitute/complement. One way is to say that $x$ and $y$ are complements if an increase in $y$ raises the marginal utility of $x$ (or, given symmetry of mixed partials, vice versa):
$$\frac{\partial^2 U}{\partial x\partial y}>0\tag{1}$$This is the suggestion in foobar's answer. Another way is to say that $x$ and $y$ are complements if a decrease in the price of $y$ raises the Hicksian (aka compensated) demand for $x$. Since Hicksian demand is the derivative of the cost (aka expenditure) function by Shephard's lemma, this can also be expressed as a condition on mixed partials:
$$\frac{\partial^2 C}{\partial p_x\partial p_y}<0\tag{2}$$
This is the suggestion in snoram's comment, and it is the notion more commonly taught in micro classes. These definitions are not equivalent! Indeed, in any case with only two goods, those two goods must be substitutes according to (2), regardless of whether the cross-partial of $U$ in (1) is positive or not. One can give fruitful labels to these concepts (though these labels are more common in the case of production rather than utility functions). Following Hicks, we can call complements by definition (1) q-complements: if $x$ and $y$ are q-complements, an increase in the quantity of $y$ leads to an increase in the marginal value of $x$. Meanwhile, we can call complements by definition (2) p-complements: if $x$ and $y$ are p-complements, a decrease in the price of $y$ leads to an increase in the demand for $x$. See, for instance, Seidman (1989) for a brief overview. Both concepts are useful in different situations - it depends on what you're interested in! More technical note: you might notice that (1) and (2) do not seem very similar to each other: (2) is a compensated concept, keeping us on the same indifference curve, while (1) is not. This is a valid criticism, and indeed there is an alternative notion of ""q-complements"" that is compensated, and a notion of ""p-complements"" that is not.  The compensated notion of q-complements, which is probably more relevant for most consumer theory applications than (1), asks whether the marginal return to $x$ increases as we increase $y$, while staying on the same indifference curve. (It's more relevant for consumer theory because it doesn't depend on the inherently ambiguous cardinality of $U$. Indeed, apparently Hicks introduced this as the consumer-theory definition of ""q-complements"" in his 1956 Revision of Demand Theory, though I don't have a copy of it myself.) This notion also has a mixed partial characterization, in terms of something called the distance function, which is a cool micro theory tool that no one learns anymore; the matrix of mixed partials of the distance function is called the Antonelli matrix, and it is a generalized inverse of the beloved Slutsky matrix.  If we wanted to think about other versions of p-complements, there are several options. One way is to hold income constant, and say that $x$ and $y$ are complementary if a decrease in the price of $y$ increases Marshallian demand for $x$. This is a valid notion (called ""gross"" complementarity rather than ""net""), but it's not very nice because it's not symmetric (due to income effects) and hence doesn't have a mixed partial characterization.  Another, nicer way is to hold marginal utility of wealth constant (this is called ""Frisch"" demand, and is the consumer theory analog of profit maximization, which holds price of output constant), and then ask whether a decrease in the price of $y$ leads to an increase in the demand for $x$. This depends on entries in the inverse of the Hessian matrix of mixed partials of $U$, revealing an inverse relationship with (1) (which depends on the Hessian matrix itself) that parallels the inverse relationship noted above between the Antonelli and Slutsky matrices."
Monopolies are just a mathematical misunderstanding,"$PQ(P)=TR$, Total Revenue.  $\frac{∂Q}{∂P}P+Q$ is the derivative of $PQ(P)$ with respect to $P$. $MR$, Marginal Revenue, is the derivative of $TR$ with respect to $Q$. So in general $\frac{∂Q}{∂P}P+Q \neq MR$"
What is the point of all the models in an economics degree?,"Models are more than just math. Model is a simplification of a reality that allows you to study the underlying mechanisms. Models do not need to be mathematical. Many people actually create models without even realizing it. For example, if someone says ""minimum wage will not lead to unemployment"" that person is actually having a model even if they never present any mathematics. Such person definitely does not take into consideration full reality - every choice of every single individual, movement of every atom and so on.  Hence, whether you use mathematics or not you are using some model anytime you are describing any mechanism, such as x causes y, or there is such and such relationship between x and y because of this or that. Thinking otherwise is just self-deception as no human mind can take into account full reality when examining any issue. You have to simplify whether you do it with math or without. However, I get that at the heart of your question is why economists use mathematical models, but I wanted to highlight that there is no way of escaping models.  Mathematical modelling has advantages over plain text.   Dani Rodrick put it best when he said: We need the math to make sure that we think straight--to ensure that our conclusions follow from our premises and that we haven't left loose ends hanging in our argument. In other words, we use math not because we are smart, but because we are not smart enough. We are just smart enough to recognize that we are not smart enough. And this recognition, I tell our students, will set them apart from a lot of people out there with very strong opinions about what to do about poverty and underdevelopment.  Hence the reason why economists use mathematical models is that they are tremendously helpful in disciplining your thinking and by forcing you to say exactly what you mean. Written text is imprecise, words can have double meaning, people can easily cover up unsound logic by good use of literary devices or appealing to your emotions all of which can cloud your judgement.   This makes text  harder to actually analyze analytically. For example, even in moral philosophy - the least mathematical field I can think of, to fully analyze argument made in text and determine whether it makes sense you have to often resort into translating it from plain English into symbolic logic - which is form of math. Mathematics itself is just a language based on logic, that's also why in mathematics we often talk of equations as sentences or statements.  When you reduce model to set of mathematical expression you are forcing yourself to make your own thinking bare. You cannot easily conceal faulty logic anymore. You can argue that math can be used to 'fool policy-makers' as you put it but you cant easily fool other scholars with math while with text you can easily fool both policy makers and scholars.  My argument here is not that the math is bullet proof, as argued by Romer some scholars can try to mislead with using ""mathiness"", but lying with math is much much more harder than lying with text.  Models actually do help predict what will happen. First, do not confuse prediction with forecasting. For example New Keynesian model predicts that if there will be sudden shock to demand wage rigidity will lead to higher than natural level of unemployment - that is a prediction but its different from forecasting as the prediction is based on the ceteris paribus conditions while the word is constantly changing.  Predictions that model make can be useful in itself for several reasons. Predictions actually allow us to test whether one theory is superior than another. In fact the only way of testing a theory is to test its prediction against empirical observations. If someone has elaborate theory written in text that says that minimum wage does not affect employment and other person will make equally compelling case in text against that how will you decide which person is right? One way how to do that is to turn the first person text into a mathematical model. The person says that minimum wage does not affect unemployment? Okay that means he postulates the following relationship: $$ U(w_{min}) = a + bw_{min}$$ where $b=0$ Now converting the thinking of that person into mathematical model actually allows us to test it - we can based on the model above construct regression of unemployment on minimum wage: $$U = \beta_0 + \beta_1 w_{min} +\epsilon $$  and test whether the first person was correct by testing the null hypothesis of $\beta_1=0$ against alternative hypothesis $\beta_1 \neq 0$. Predictions also allow us to do counterfactual analysis. We can ask ourselves what would happen if in the presence of wage rigidities government increases spending during the recession. New Keynesian models provide you with framework to think analytically about such questions and fully explore all their logical conclusions.  Does this mean that math is the only way of studying economics? No. Of course, every individual is different. Some people are visual learners, some people are auditory learners.  Some students respond better to narration than to math. Moreover, as you yourself admitted math helped you to understand some problems so it worked for you well at least a couple of times. While you might have learned better most concepts by reading about them  other student might have completely opposite experience.  Moreover, depending on your career choices you might end up not using most of the math in real life. However, a master degree especially if its master of science and not just master of arts is often a stepping stone (at least in the Europe) towards PhD or more academic career in policy institutes or government institutions where you will need the math if not directly then at least indirectly to understand new advances presented in academic journals. For example, even routine psychologists must have some statistical understanding if they want to keep up to date with new techniques to help their patients as in order to see if some technique is better than other some testing has to be done. Otherwise you will be just at mercy of trusting what authors say in conclusions, or some intermediary like academicians who like to blog for wide audience etc. Furthermore, you should not interpret anything I said above as saying math is the only way of doing economics or that narrative analysis is useless. Sometimes narrative analysis can be more nuanced, and with some problems we might not even yet discovered ways how to model them mathematically in satisfactory way. However, models and especially mathematical models are incredibly important tool in any economist's toolkit. "
Is there an economic analysis of the rationality of buying lottery tickets?,"There are definitely economic justifications for playing the lottery, even if all (I hope) players understand that it is unlikely to pay off. One such justification is that what you actually buy when purchasing a lottery ticket is the fantasy of winning. Here are a few sources. Lotterys are relatively well understood in economics. The Economics of Lotteries: A Survey of the Literature(pdf)  - is an excellent article covering basically your entire question. It discusses the microeconomic forces at play, particularly income elasticity and risk aversion. This source (note: this is now a dead link) has some excellent lecture notes on the market for risk. I recommend it highly. Some basic points are:"
Solow Model: Steady State v Balanced Growth Path,"This is when the attempt at accuracy creates confusion and misunderstanding.   Back in the day, growth models were not incorporating technological progress, and led to a long-run equilibrium characterized by constant per capita magnitudes. Verbally, the term ""steady-state"" seemed appropriate to describe such a situation.  Then Romer and endogenous growth models came along, which also pushed the older models to start including as a routine feature exogenous growth factors (apart from population). And ""suddenly"", per capita terms were not constant in the long-run equilibrium, but growing at a constant rate. Initially the literature described such a situation as ""steady state in growth rates"".   Then it appears the profession thought something like ""it is inaccurate to use the word ""steady"" here because per capita magnitudes are growing. What happens is that all magnitudes grow at a balanced rate (i.e at the same rate, and so their ratios remain constant). And since they grow, they follow a path..."" Eureka!: the term ""balanced growth path"" was born.   ...To the frustration of students (at least), which have now to remember that for example, the ""saddle path"" is indeed a path in the Phase diagram, but the ""balanced growth path"" is only a point! (because in order to actually draw a Phase diagram and obtain a good old long-run equilibrium, we express magnitudes per effective worker, and these magnitudes do have a traditional steady-state. But we continue to call it ""balanced growth path"", because per capita magnitudes, which is what we are interested in, in our individualistic approach), continue to grow). So ""balanced growth path"" = ""steady state of magnitudes per efficiency unit of labor"", and I guess you can figure out the rest for your phase diagram."
What is the definition of exogenous and endogenous preferences?,"Exogenous variables are believed to have some value given by nature. They are not caused by your theory's variables of interest. This is why they are said to be outside the model. Endogenous variables have values dependent on your theory's variables of interest. They both cause, and are caused by your topic.  Example: In the study of wages, some individuals are born superior, genetically, perhaps. They are superior exogenously. Because of their superior innate quality, they may choose to seek additional education knowing they will get higher returns for their investments. So the amount of education is endogenous to wages, because a person's expected earnings informs their education."
How is freelancing viewed under Marxism?,"Marx addresses this about two-thirds of the way through Section 1 of the Manifesto. In the standard English edition of 1888, it reads: The lower strata of the middle class - the small tradespeople,
  shopkeepers, and retired tradesmen generally, the handicraftsmen and
  peasants - all these sink gradually into the proletariat, partly
  because their diminutive capital does not suffice for the scale on
  which Modern Industry is carried on, and is swamped in the competition
  with the large capitalists, partly because their specialized skill is
  rendered worthless by new methods of production. Thus the proletariat
  is recruited from all classes of the population. So his basic argument is that, sure there might be small business owners (freelancers in this example), but that because of 1) inability to compete against large firms ostensibly taking advantage of economies of scale and 2) rapid development of technology, they become obsolete and are driven to the proletariat."
Quality of French wine in the US vs French wine in France,"This is called the Alchian–Allen effect.  The Alchian–Allen effect was described in 1964 by Armen Alchian and
  William R Allen in the book University Economics (now called Exchange
  and Production). It states that when the prices of two substitute
   goods, such as high and low grades of the same product, are both
   increased by a fixed per-unit amount such as a transportation cost or
   a lump-sum tax, consumption will shift toward the higher-grade
   product. This is true because the added per-unit amount decreases the
  relative price of the higher-grade product. Suppose, for example, that high-grade coffee beans are \$3/pound and
  low-grade beans \$1.50/pound; in this example, high-grade beans cost
  twice as much as low-grade beans. Now add a per-pound international
  shipping cost of \$1. The effective prices are now \$4 and \$2.50;
  high-grade beans now cost only 1.6 times as much as low-grade beans.
  This reduced ratio of difference will induce distant coffee-buyers to
  now choose a higher ratio of high-to-low grade beans than local
  coffee-buyers. (Prices are illustrative only). The effect has been studied as it applies to illegal drugs and it has
  been shown that the potency of marijuana increased in response to
  higher enforcement budgets, and there was a similar effect for alcohol
  in the U.S. during Prohibition. Another example is that Australians drink higher-quality Californian wine than Californians, and vice versa, because it is only worth the  transportation costs for the most expensive wine. From the Alchian–Allen Wikipedia page, emphasis mine.  I learned about this paradox in the context of Why the Best Washington Apples are Shipped out of State, discussed in a chapter of a fun popular economics book Puzzles and Paradoxes in Economics which also happens to be a great source of undergraduate economics exam questions."
Uses of convex analysis in Economics,"A partial answer: convex analysis is extensively used in axiomatic decision theory, at least in its recent developments. Most of these papers focus on individual behavior. You can have a look for instance at the following papers on ambiguity-averse preferences: Here is a paper that applies convex analysis to a model of trade under ambiguity aversion: ""Subjective Beliefs and Ex-Ante Trade"" (Rigotti, Shannon & Strzalecki). Beyond models of ambiguity aversion, virtually all recent work in axiomatic decision theory makes use of convex analysis, and applies its tools to study various phenomena: regret aversion (Sarver, Ergin), cost of thinking (Ortoleva), random choice (Gul, Pesendorfer)... Please tell me if you want more precise suggestions. For the mathematical part, a very good reference is Convex Analysis by Rockafellar (1970). It is cited by most of the papers above ;-)."
What is it about certain business types that seem to restrict how much large they can grow?,"That's a very interesting question and there are multiple potential reasons for this. One reason is that production is simply not efficient at a larger scale. There are diminishing returns to scale: i.e. doubling inputs will result in less than twice the output. This means that two smaller production facilities can be more efficient than one large one. One way to think about this are kids shoveling snow in winter. Beyond a certain number of kids with shovels they're just going to get in each other's way shoveling the same driveway. Now they could of course split up into two or more groups, these two groups could be one firm or these could be two firms. But what's the rationale for combining these two firms into one? They probably will not economize on any fixed expenses. They need to ring the doorbell, bring their shovels, and ask people whether they wish their driveway to be shoveled. Sure one kid could specialize in negotiating a price and ringing many doorbells and scheduling appointments, but 
the gains from specialization are probably not such that it's worthwhile. It's not how that ""industry"" works. You ring, you shovel, and you move on. So there is no good reason for combining. The kid ringing and making appointments is an unnecessary overhead. It's better that he shovels too. Another reason are the fixed costs of certain regulations. You're probably familiar with the fact that in some jurisdictions firms above a certain size need to provide certain services to their employees. If you're a small firm, then you will incur that fixed costs all at once if you expand over that threshold. You'll have to get your existing employees those services and the new people. These additional fixed costs might not make it worthwhile to hire extra people unless you can be certain that you can expand your business well beyond that.  A third reason is that many people do not want to expand their businesses. They enjoy the work that they do and the extra income is not worth it. They'd have to spend less time doing the work that they enjoy and more time on administrating the work done by others. Effectively this is a change of employment. So you might as well ask why do some people do work A and not work B? These are three reasons. The reasoning is the same each time though. The benefits of expanding do not justify the costs of expanding in that particular situation. Given that there are many different situations there will be many different reasons why people choose not to expand their business.  Does this answer your question?"
"When treating a relative, normalized utility function as a pmf, what is the interpretation of Shannon entropy or Shannon information?","After the exchange with the OP in my other answer, let's work a bit with his approach.   We have a discrete random variable $X$ with finite support, $X = \{x_1,...,x_k\}$, and probability mass function (PMF), $\Pr(X=x_i)=p_i, i=1,...,k$ The values in the support of $X$ are also inputs in a real-valued  cardinal utility function, $u(x_i) > 0\; \forall i$. We then consider the normalized utility function $$w(X): w(x_i) = \frac {u(x_i)}{\sum_{i=1}^ku(x_i)},\;\;i=1,...,k \tag{1}$$ and we are told that $$w(x_i) = p_i \tag{2}$$ Note that we do not just make the observation that a normalized non-negative discrete function of finite domain, satisfies the properties of a probability mass function in general -we specifically assume that $w(x_i)$ has the functional form of the PMF of the random variable whose values $w(x_i)$ takes as inputs. Since $w(x_i)$ is a measurable function of a random variable, it, too, is a random variable. So we can meaningfully consider things like its expected value. Using the Law of the Unconscious Statistician we have $$E[w(X)] = \sum_{i=1}^kp_iw(x_i) = \sum_{i=1}^kp_i^2 \tag{3}$$ This is a convex function, and if we try to extremize it over the $p_i$'s under the constraint $\sum_{i=1}^kp_i=1$ we easily obtain $$\text{argmin} E[w(X)] = \mathbf p^*: p_1=p_2=...=p_k=1/k \tag {4}$$ and we have obtained a general result: The normalized utility function as defined above has minimum expected
  value iff the distribution of $X$ is Uniform. Obviously in such a case $w(X)$ will be a constant function, a degenerate random variable with $E[w(X)]=1/k$ and zero variance. Let's turn to Shannon's Entropy which is the focus of the OP. To be calculated, Shannon's Entropy needs the probability mass function of the random variable... so we should find the PMF of the random variable $w(X)$...   But it is my impression that this is not what the OP has in mind. Rather, it views Shannon's Entropy as a metric that has some desirable algebraic properties and perhaps can measure compactly in a meaningful way something of interest.   This has been done before in Economics, specifically in Industrial Organization, were Indices of Market Concentration (""degree of competition/monopolistic structure of a market"") have been constructed. I note two that look particularly relevant here. A) The Herfindahl Index, has as its arguments the market shares of the $n$ companies operating in a market, $s_i$, so they sum to unity by construction. Its unscaled version is  $$H = \sum_{i=1}^n s_i^2$$ which is an expression that has the exact same structure with the expected value of $w(X)$ derived above. B) The Entropy Index
$$R_e = -\sum_{i=1}^n s_i\ln s_i$$
which has the exact mathematical form with Shannon's Entropy. Encaoua, D., & Jacquemin, A. (1980). Degree of monopoly, indices of concentration and threat of entry. International Economic Review, 87-105., provide an axiomatic derivation of ""allowable"" concentration indices, i.e. they define the properties that such an index must possess. Since their approach is abstract, I believe it may be useful to what the OP wishes to explore and attach meaning to."
What have been the main developments in macroeconomics and financial economics since the 2008 financial crisis?,"Well, in macroeconomics, namely in DSGE modelling, VOXEU has recently published a report on its uses by Central Banks (CBs), and future lines of improvement, which academics have been tackling but still hasn't found its way into CBs policy analysis. There's no space to explain all the new topics, nor do I think it's the intention of this question. So, I'll just enunciate the topics, with some references. More can be found in reading the report. Also, I'll focus on the theoretical considerations, i.e., it's known that, empirically, the the VAR obtained by solving the DSGE model can be considered as misspecified (see here). This is a WP, but you can easily find many articles on this subject, increasingly so since the crisis) Finally, one critique, with which I agree, and Romer has written very funny and interesting 'WP' on, is the degree of exogeneity, through shocks, that is needed to explain the economy. Behavioural economics (2017 Nobel prize winner was from this area) may supply us with an answer. It allows to endogenize some of the shocks. Here's an article on VOXEU with many references, and analysis of an example of how it's done.  Edit: To complement Kitsune answer, I find this image from the wikipedia page really good. In a very succint way, it explains the difference between macro and micro prudential perspectives. "
Understanding the construction of stochastic processes,"This construction you describe is not fully general. In fact it characterizes strictly stationary time series. You see that it's shift-invariant. This operator $S$ is essentially a shift operator. For comparison, here's the usual definition of, let's say discrete-time, processes: Definition A stochastic process is a sequence $\{ X_t \}$ of Borel measurable maps on a probability space $( \Omega, \mathcal{F}, \mu )$.   Now for what you're describing, you have a fixed Borel measurable map $X: \Omega \rightarrow \mathbb{R}^n$. It's the underlying measure that is evolving according to $S$. The map $S$ induces a new ""push-forward measure"" (in measure-theoretic parlance) on $\Omega$ by just taking preimages: define a measure $\mu_S$ by $$
A \in \mathcal{F} \stackrel{\mu_S}{\mapsto} Pr(S^{-1}(A)).
$$ So the random vector $X: ( \Omega, \mathcal{F}, \mu_S) \rightarrow \mathbb{R}^n$ is $X \circ S$ by construction. They induce the same push-forward measure on $\mathbb{R}^n$. Do this with $S^t$ for each $t$ and you have your time series. As for your question about $\omega$, inspecting the proof for the other direction should clarify this---i.e. any strictly stationary time series must necessarily take this form for some $( \Omega, \mathcal{F}, Pr)$, $X$, and $S$.  The basic point is that, from a general point of view, a stochastic process is a probability measure on the set of its possible realizations. This is seen in, for example, Wiener's construction of Brownian motion; he constructed a probability measure on $C[0, \infty)$. So in general, an $\omega$ is a sample path and $\Omega$ consists of all possible sample paths.   For example, take the two processes you named above. They are strictly stationary, if let's say the innovations are Gaussian. (Any covariance-stationary time series driven by Gaussian innovations is strictly stationary.) The construction would then start by taking $\Omega$ to be the set of all sequences, $\mathcal{F}$ the $\sigma$-algebra generated by coordinate maps, and $Pr$ the appropriate measure. For the white noise process (2), $Pr$ is just a product measure on an infinite product.  Reference This characterization/construction by shift of strictly stationary time series is mentioned in White's Asymptotic Theory for Econometricians."
When does the Divine Equilibrium refinement coincide with that of Perfect Sequential Equilibrium?,"Some input:   In Cho, I. K., & Kreps, D. M. (1987). Signaling games and stable equilibria. The Quarterly Journal of Economics, 179-221, Banks & Sobel's Divine (and Universally Divine) equilibrium concepts are presented in Section IV.4 as a stand-alone concept.
On the other hand Grossman & Perry's Perfect Sequential Equilibrium concept is just mentioned in Section IV.5 which has the title ""Never a weak best response"". In the Banks, J. S., & Sobel, J. (1987). Equilibrium selection in signaling games. Econometrica, 647-661. paper on Divine Equilibrium, page 654 (near the end of Section 3), we read ""(...) This condition is more restrictive than universal divinity because (...)"" , ""this condition"" being ""never a weak best response"". So it appears that Perfect Sequential Equilibrium (PSE) is a stronger equilibrium filtering criterion than Divine Equilibrium. This accords with Theorem 2 of Banks & Sobel : Every signaling game has a divine equilibrium
to be contrasted with Section 4. of the Grossman, S. J., & Perry, M. (1986). Perfect sequential equilibrium. Journal of economic theory, 39(1), 97-119. paper introducing PSE, where they show by means of an example that a Perfect Sequential Equilibrium may fail to exit. A paper that applies both concepts is Beggs, A. W. (1992). The licensing of patents under asymmetric information. International Journal of Industrial Organization, 10(2), 171-191.. In Section 3.2 a result is derived by an appeal to the PSE concept. Then the authors note that, given an additional condition, they could obtain the same result by an appeal to Divinity. This agains shows that PSE, when it exists, is stronger than Divine equilibrium. Here too an example is offered for a case when the PSE does not exist."
Nash equilibrium - mistake in proof of paper?,"The objective is to show that, as long as $f'(n\hat\theta)\ne \alpha$, a firm can always engineer a package $(p',\hat\theta')=(p+\Delta p,\hat\theta+\Delta \hat{\theta})$ such that (i) a caring consumer strictly prefers this package, and (ii) the firm makes positive profits.  1) Your maths are correct: the inequalities are always false when f is increasing and strictly concave. In that case, the proof in the paper is unclear.
The package $(p',\hat\theta')=(p+f'(n\hat\theta)\Delta \hat{\theta},\hat\theta+\Delta \hat{\theta})$ does not satisfy (i). 2) You can show the proposition with the following reasoning.
Assume $f'(n\hat\theta)>\alpha$. Take any $h$ within the interval $(\alpha,f'(n\hat\theta))$.
For $\Delta \hat{\theta}$ small enough and positive, a firm that offers the package $(p',\hat\theta')=(p+h\Delta \hat{\theta},\hat\theta+\Delta \hat{\theta})$ satisfies (i) and (ii). With a first-order appoximation, we show (i): $$b-p'+f(\hat\theta'+(n-1)\hat\theta)>b-p+f(n \hat\theta),$$
and (ii):
$$\Delta p - \alpha \Delta \theta>0.$$ Assume $f'(n\hat\theta)<\alpha$, and take $h$ within the interval $(f'(n\hat\theta),\alpha)$.
For $\Delta \hat{\theta}$ small enough and negative, a firm that offers the package $(p',\hat\theta')=(p+h\Delta \hat{\theta},\hat\theta+\Delta \hat{\theta})$ satisfies (i) and (ii)."
Where can I find data on income and social mobility over time? How far back is data available?,"Maybe check Clark's work. He allegedly uses long time series.
http://jasoncollins.org/2014/09/30/the-genetic-basis-of-social-mobility/"
Differentiate a positive externality and the absence of a negative externality. Tax or subsidize?,"A negative externality arises when the private net marginal benefit (i.e. the marginal benefit minus the marginal cost) of an activity exceeds the net social benefit. In such cases, the self-interested private decision maker will increase their participation in the activity even though it is socially inefficient for them to do so. A positive externality arises when the private net marginal benefit (i.e. the marginal benefit minus the marginal cost) of an activity are smaller than the net social benefit. In such cases, the self-interested private decision maker will not increase their participation in the activity even though it is socially efficient for them to do so. This is a semantic distinction to the extend that if one thinks activity $A$ has a negative externality then one can define a new activity $B$, which is simply the ""act of not doing $A$"" so that $B$ has a positive externality. On this basis, one can argue that every externality is positive or that every externality is negative. For example, many people think that education has a positive externality because educated people make better citizens (e.g they make more informed voting decisions that benefit others). As a matter of semantics, one could argue that this is, in fact, not a positive externality and that what is really going on is that people who do not educate themselves are exerting a negative externality on those who do by virtue of their ignorance. Whilst there is some merit to this reasoning, I do not find it helpful. Often, when we study the effects of behaviour, we are interested in comparing those effects to some baseline or benchmark in which the behaviour is absent. When communicating economics to others, it is usually the case that some benchmarks are more intuitive that others. We could, for example, rewrite all of consumer theory in terms of ""the dis-utility people experience from not having goods"" and look at the ""problem of non-consumption dis-utility minimisation"". Doing so would be formally equivalent to the more conventional approach of consumption utility maximisation (only the language changed), but would probably be less intuitive for people trying to understand the economics.(*) At least to me, it is more intuitive to think that people actively choose to undetake some level of education and exert a positive externality on others than to think that everyone receives infinite education by default and the choice of abstaining results in a negative externality. Besides education, another example that I think fits most intuitively into the positive externalities box is network effects. If I buy a telephone then all of my phone-owning friends are made better off because now they can use their telephone to call one more person that they couldn't reach before. It seems weird to think of the negative externality of not owning a phone. In terms of taxes versus subsidies: to get to the socially optimal intensity we need to ensure that the private net marginal benefit is zero precisely when the social net marginal benefit is zero. In the case of a negative externality this can be done either by increasing the private marginal cost (via a tax) for the activity or by increasing the private marginal benefit of not participating in the activity via a subsidy. For example, we could either subsidize low carbon firms or tax heavy polluters. As far as aligning incentives are concerned, the two are equivalent. In most practical cases, the more important consideration is likely to be that of budget constraints and politics: In most cases, thinking about these political and financial constraints makes it clear whether a subsidy should be used. Sometimes a combination of both is used. For example, in the UK the government both taxes petroleum consumption and subsidises electric car ownership. (*) Nevertheless, economists do often find it useful to convert the utility maximisation problems into their dual expenditure minimisation problems, which is somehow similar. This technique, though, is usually reserved for more advanced students who already have a well-developed intuition for the economics."
Does land value tax create an incentive to merge land ownerships?,"Some people strongly disagree that LVT is not distortionary:  George was right that other taxes may have stronger disincentives, but
  some economists now recognize that the single land tax is not
  innocent, either. Site values are created, not intrinsic. Why else
  would land in Tokyo be worth so much more than land in Mississippi? A
  tax on the value of a site is really a tax on productive potential,
  which is a result of improvements to land in the area. Henry George’s
  proposed tax on one piece of land is, in effect, based on the
  improvements made to the neighboring land. And what if you are your “neighbor”? What if you buy a large expanse
  of land and raise the value of one portion of it by improving the
  surrounding land. Then you are taxed based on your improvements. This
  is not far-fetched. It is precisely what the Disney Corporation did in
  Florida. Disney bought up large amounts of land around the area where
  it planned to build Disney World, and then made this surrounding land
  more valuable by building Disney World. Had George’s single tax on
  land been in existence, Disney might never have made the investment.
  So even a tax on unimproved land can reduce incentives.
  ... Zachary Gochenour and Bryan Caplan have pointed out that while the
  surface value of land is more apparent, especially for farming
  purposes, many lands have hidden natural resources, such as gold,
  water, and oil. These resources require investment on the part of
  owners to discover and produce. “Information about the land can be
  considered an improvement in its own right.” To tax the entire, or
  even a large, value of the mineral resources would create enormous
  disincentives for exploration and production The Concise Encyclopedia of Economics: Henry George (1839-1897 )  To answer this more definitively we'd need to take a stand on how we think the tax assessor would measure unimproved value. We could then ask if combined plots would distort this measure. I read the quote above as saying that the assessor looks at the sale prices of undeveloped lots in the area. But the value of these lots is necessarily higher due to the improvements of others (or even your own). Combining plots, to the extent that it makes big, valuable projects possible, distorts this measure but in the wrong direction. Some other method that tried to strip out the indirect value of your own improvements on the unimproved value of your own land but left in the increase in value of the improvements of others would bias towards combining properties. "
Is there a solution to the Joker's game in The Dark Knight?,"Suppose first that the groups are not altruistic and care only about their own survival. It is not exactly a prisoner's dilemma since the outcome obtained by mutual cooperation (if both groups wait) is not Pareto-improving: everyone dies in that case. The only equilibrium is that one of the groups destroys the other boat as soon as possible; and any action played initially by the other group is possible in equilibrium, since this team is indifferent between waiting and triggering other the bomb (they will die one second later anyway). As @Alecos_Papadopoulos wrote, the game becomes more interesting if the groups have pro-social preferences. For instance, they might be reluctant to sacrifice the other group and prefer that everyone dies (including themselves). If there is no uncertainty, the outcome is trivial: the only equilibrium is that both groups wait until the Joker triggers the bombs.  The most interesting scenario is the one in which the types of the groups are uncertain: each boat can be either selfish or altruistic. In that case, it seems reasonable (but other specifications are possible) to assume that cooperation is desirable only if the other group is also altruistic, but if the other group is selfish the individuals prefer to kill them first and survive. The equilibrium strategies are the following: "
Why do people buy high dividend stock?,"Startups and heavily growing companies usually invest their earnings themselves, rather than paying dividends. Established companies in stable markets don't have as much opportunity to invest the earnings without expanding to new business areas, so they more often pay them out as dividends. Thus by selecting stocks by dividend payment history, you are choosing established, stable businesses. Not so much room for surprise gains, but often a more predictable return on investment. Naturally there are other ways to go about analyzing stock choices, but looking at the dividend history is easy to understand and very concrete in the form of dividend yield."
What metrics would indicate a house bubble rather than genuine market values?,"The first metric to look at is house-price-to-rent ratios. Rental prices capture the value of the housing (and housing-linked) services provided by a property, including things like how safe a neighborhood is, how good the schools are, et cetera. In contrast, house prices are the capitalized value of the future stream of housing services PLUS the price of an asset (mostly land; structure value beyond the housing services it provides is negligible). As a result, a higher ratio of house prices to rents means that, controlling for the value of housing services provided, we observe people paying more for housing assets (i.e., land). This doesn't mean that the asset is necessarily overvalued, but it at least moves you in the right direction by separating out the asset price from the value of housing services. You can then start thinking about the supply/demand fundamentals that apply to housing assets, and what you'd have to assume to support a particular valuation of the asset (things like, as Jamzy noted, whether growth in price-to-rent ratios is linked with increased home purchase borrowing)."
How do economic sanctions work? How do they not create an arbitrage opportunity?,"As the other answer says the sanctions include prohibition on selling to third parties. However, this still creates incentives to cheat on these sanctions. It is not always so easy to determine final destination of goods. It gets also easier to cheat on sanctions if some countries actually tacitly support the cheating here is an example of Russia doing so to undermine North Korea sanctions. It is harder to cheat if it’s not at least tacitly sanctioned by government but there always were and always will be smugglers. However, one should not immediately jump to the conclusion that cheating means sanctions are absolutely ineffective. This is analogous to cheating on taxes. High taxes give people incentives to cheat on them, and some governments lose few % of revenue to GDP as a result, but that does not necessarily mean government is not raising a lot of revenue. Likewise, these workarounds can be thought of being a scalar that reduces the effectiveness of sanctions. It’s then an empirical question of how much."
Prove the sample variance is an unbiased estimator,"I know that during my university time I had similar problems to find a complete proof, which shows exactly step by step why the estimator of the sample variance is unbiased. The proof I used can be found under http://economictheoryblog.wordpress.com/2012/06/28/latexlatexs2/ The proof itself is not very complicated but rather long. That also the reason why I am not writing it down here and probably it is not fair towards the person who actually provided it in the first place."
Why are high/rising property values considered a good thing?,"Considered a good thing by whom? Financial institutions and speculators: yes, since houses for them are just another asset. Higher asset prices is a key mechanism of profit making for financial institutions (capital gains). In the recent financial crisis house prices collapsed and people defaulted on their mortgage payments. Thus,  the fall in the value of houses repossessed by banks meant the return on financial assets associated to housing translated into massive losses for financial institutions and government agencies associated with the housing market like Fannie Mae and Freddie Mac in the US. Home owners: partly yes. A price increase is good, particularly if you have a mortgage to pay (so if you sell the house you can pay the loan back). Rising house prices are also good for pensioners, which main source of wealth is their house. Rising prices also give people the feeling they are richer (and hence spend more). Some argue that the positive appreciation of the media of rising house prices is partly due to the fact that newspapers are written and read mainly by older people, not by youngsters i.e. renters. Renters: no, because rents are associated with house prices. If incomes are not growing at the pace of house prices, renters will be squeezed, to the benefit of landlords and homeowners. This is true in the UK (and in many other advanced economies, see here):  Central banks, and perhaps the overall economy and population: not necessarily, because rising house prices fueled by speculation and indebtedness (e.g. speculators and investors entering into the buy-to-let market) mean possibilities of a bubble bursting and a subsequent financial crisis. For example, in the UK, Mark Carney has expressed such concerns in the past. Estate Agents: yes, as their business as intermediaries depends partly on fees from sales. People is more willing to sell when the market is on the up, fostering the property chain. If prices are falling, people prefer to wait instead of selling their house, breaking the property chain."
Why are imports subtracted from GDP?,"When we import something, we consume it. So when calculating consumption we are bound to count import as a positive component of GDP. Since it is not (we did not produce imports domestically), we subtract it to make it neutral."
References to learn continuous-time dynamic programming,"For continuous-time stochastic dynamic programming, the small, nontechnical Art of Smooth Pasting by Dixit is a wonderful option. It does a very effective job of conveying the basic intuition. Stokey's more recent The Economics of Inaction is also decent, but for a practical-minded person it probably underperforms Dixit - its much greater length and somewhat heavier notation do not yield commensurate rewards. If the underlying stochastic processes are not Itō diffusions, then I'm not sure what the best reference is. The most common such case I've seen (and that I use myself) is the case of discretely many exogenous states, where if we are currently in state $s$ there is some constant hazard rate $\lambda_{s,s'}$ of a switch to state $s'$. Fortunately, this is a pretty simple case in practice: one can just modify the HJB equation to account for the flow probability of switching from $V(\cdot,s)$ to $V(\cdot,s')$. (You can see this, for instance, in equations (1)-(5) in this Acemoglu and Akcigit paper. Conceptually it's no different from setting up the HJB equation when we have an Itō diffusion as the driving process, except that it's simpler because we just get a system of linear equations and we don't need to think about Itō's lemma, etc.) Of course, maybe there is a good textbook reference for this too - but unlike the potentially much more complicated cases involving stochastic calculus, this is straightforward enough that a text has never seemed necessary to me."
What Level of Government Debt to GDP Ratio is Sustainable?,"As you have pointed out: where it comes from is very important. As to the Japanese situation it is quiet different from the US position from example. In fact most of the Japanese debt is owned by Japanese people (90% of the current debt). More specifically the BoJ plays a big role as a buyer, and puts pressure on Japanese yield, which makes it cheaper for the government to issue bonds !  Another interesting point that is usually left by anlysts: Japan is the biggest creditor in the world. The country holds a net amount of about 3 trillion USD (367 trillion yen) of financial assets through the world, which makes Japan the first creditor worldwide (before China !). An interesting further reading which gives other information on the Japanese debt under stress tests of the IMF (P.40): https://www.imf.org/external/pubs/ft/scr/2015/cr15197.pdf You also might want to read this paper of Rogoff and Reinhart called ""Growth in a time of debt"": http://www.nber.org/papers/w15639 . It was really criticized but is a good first glimpse... Afterwards you might want to go a little bit deeper into the debt sustainability analysis of the IMF..."
Why are cost functions often assumed to be convex in microeconomics?,"There are several reasons: Didactic Reasons: Other users seem to have missed it but in your question you specify you are talking about ""(introductory) microeconomics"" [emphasis mine]. Well the most prosaic answer is simply that it is much easier to solve
cost minimization, or various other models when costs are assumed to
be convex. This in itself is sufficient reason to construct problems with convex cost functions in introductory microeconomic courses. Demand and supply are not linear, yet in most textbooks and introductory problem they will be assumed to be linear. In addition, in real life demand can be sometimes even upward sloping if a good is a Giffen good, and supply can actually be downward sloping (e.g. some labor supply in some special cases depending on people's preference between consumption and leisure). Yet introductory textbooks typically show downward sloping demand and upward sloping supply (e.g. see Mankiw Principles of Economics that discusses these concepts but only briefly, or more narrowly micro introductory books such as Frank Microeconomics & Behavior). This is to a great degree for didactic reasons. It is much better for students to first master basics with simple models and when it comes to learning about costs having nicely behaved convex cost functions with single minimum makes learning easier than having to teach cost minimization with concave cost curves. Hence, even if empirically most cost curves would be concave not convex it would be very bad teaching practice to start with concave functions (or just go for full blown realism where cost functions might be piecewise, have different concavity/convexity at different points, be ill defined somewhere etc). Because of Decreasing Returns to Scale - This was covered in great detail by Bayesian, but let me add more arguments and also rebuff some of your arguments in the question. First, it is not unreasonable to assume that costs are convex in a long-run. In a world of scarcity firm cannot forever increase its demand for factors of production without affecting costs of these factors or inputs as well, their prices will rise eventually (ceteris paribus). We have crystal clear evidence that wages rise in tight labor markets, or that generally speaking shift in demand to the right (ceteris paribus) rises prices.
You argue that in perfect competition models firms are assumed to be small, but that is not a good argument in this case. This is because firms are assumed to be too small in terms of their output being able to affect market price of their output so price of output can be taken as given (See Frank Microeconomics and Behavior pp 337). Perfect competition does not require price of inputs to be taken as given. In fact, firm might operate on perfectly competitive market while facing just monopolistically competitive factor market (where the firm is consumer not producer). Next, you argue that thanks to fixed costs one firms could just continuously invest in a new factories, but this argument should be false. A fix cost by definition cannot vary with output. If firm increases output by building new factory, the cost of factory ceases to be fixed costs. In fact fixed costs primarily exist in short-run as in a long-run most costs are variable (see Mankiw Principles of economics pp 260). In a long-run as you try to build more and more factories you run into the same problems of scarcity of land, capital and labor and thus bid up their prices. In fact this is nicely visualized and explained in the Mankiw textbook with the picture below:  Empirically, we observe that many industries have decreasing returns to scale (although constant returns to scale are common as well), and increasing returns to scale are rare (although not completely uncommon). See for example: Basu & Fernald, 1997; Gao & Kehrig 2017. Introductory texts by their nature will not deal with specific cases but more general ones. Most introductory textbooks again do not spend too much time on Giffen goods not just because modeling them would be difficult for 101 students but also because they are not very often seen (although, I am not claiming non-convex cost functions are as rare as Giffen goods). On the Aesthetics: I think Giskard raises a valid point that there are probably many economists who assume convex costs just for mathematical elegance. However: I think Giskard slightly exaggerates the problem and is bit too cynical about it. For sure there are economists who value mathematical elegance uber ales, but there is increasing trend in share of empirical papers (see Angrist et al 2017), even in microeconomics, and I think that a reasonable non-cynical explanation for the small share of micro empirical papers is that until very recently there was always lack of good micro data (in addition this is also due to breakdown, you can see the share of industrial organization empirical papers (that also heavily use cost functions) is quite high). Empirically, most industries do not exhibit increasing returns to scale. While non-convex functions are definitely real (especially along some points of cost curve), empirical evidence does show that decreasing returns to scale (although constant returns to scale as well) are quite common (e.g. see Basu & Fernald, 1997; Gao & Kehrig 2017), but I think Giskard has definitely valid point that some modelers will ignore empirics for sake of mathematical elegance. Lastly, but not least, I think mathematical elegance can explain why such assumption features heavily in some published theoretical work, I don't think it can explain why it is featured in introductory micro texts. Is really quadratic cost function $c=q^2$ mathematically elegant? I don't think so but that is probably the most commonly used cost function you will ever see in intro texts. Regarding the Varian quote. Varian on page 67 states that he will first cover situation with fixed factor costs and later move to variable factor costs. Hence, unless I am misreading Varian I think the statement on the page 68 is made under assumption of constant factor prices. However, the explanation above by Mankiw does not assume that."
Does the US stock market tend to rise after natural disasters?,"This news article with some statements from financial workers makes a case that usually big storms don't impact the national economy that much, even despite the large localized damages. While insurance companies will suffer in the stock market because of all those payouts they'll have to give, oil prices as you mentioned will be impacted, but in this case, gas prices will rise because of the change in the supply curve, so they will actually benefit in the stock market. The various effects combined end up with the stock market not really going up or down. It's more or less a wash.  To think of it another way, a shock in the short term capital in the economy won't particularly change the steady state level of capital. If markets know this, there isn't a need for huge changes in prices. In the case of the insurance companies, their money isn't so much based on capital as it is based on states of the world, so they will end up needing to adjust prices. There are other reasons why the stock market may not have moved up or down in particular, but for the most part it is speculation, and even my answer is just intuitive exposition."
Has monopoly theory incorporated network effects as a source of monopoly?,"Here's a solid example of it in formal literature, with about 1k citations: 
Competition with Switching Costs and Network Effects
by Joseph Farrell and Paul Klemperer The general thought of the article is that customers who are ""locked in"" to a particular product can lead to competitors preferring to separate markets rather than competing with one another.  To be pedantic, they are not full monopolies (but they're certainly not classically competing either)."
Is belief in future dividends the only long term value of stock?,"Disclaimer: There are a lot of interesting aspects to this question. Shareholder voting rights, control over the company, etc. are all interesting things to consider. Here, I only focus on a few parts of the question. Claim: Are actual stock prices much too volatile to be explained by dividends? Is this evidence that the price of a stock depends on more than just future dividends? It's perhaps not much of an exaggeration to state that the paper that won Robert Shiller his Nobel prize in economics is the paper ""Do Stock Prices Move Too Much to be Justified by Subsequent Changes in Dividends?"" In this paper, Shiller looks at historical prices and dividend payments and asks the question, suppose market expectations of future dividends were on average correct? What should the prices of a stock been historically if those expectations were on average correct? He argues that we can do this by looking at the dividend payments and prices and constructing and ""ex-post rational price"" at each point in time, called $p^*$. He then compares that price to the actual historical prices, $p$. This is shown if Figure 1 below for the S&P Composite Stock Price index from 1871 to 1979, after detrending both series. The result is that $p^*$ appears to be far too smooth relative to $p$. He argues that this is a problem because it implies that actual stock prices are too volatile to be explained just by movements in future dividends.  References and other resources: Important Note: In the current asset pricing literature, there is a strong belief that this ""excess volatility"" puzzle is not a puzzle at all. Empirical and theoretical investigation seems to indicate that this ""excess volatility"" arises from variation in discount rates. Shiller constructed this ex post ration prices by assuming that the discount rate was constant over time. Empirically, this seems to be far from the truth. For a good discussion of this, see the following: Cochrane, John H. ""Presidential address: Discount rates."" The Journal of finance 66, no. 4 (2011): 1047-1108. For a video of a presentation of this paper, see here: https://www.youtube.com/watch?v=ZDsOiftolUI Class lectures of this can be seen here: Claim: Perhaps stock prices can be explained by a belief that prices will continue to grow in the future, even if dividends do not? That is, suppose the price of a stock is not derived from a belief in future dividends but in the belief that prices will continue to growth without bound. There is a literature that investigates the possible effect of ""rational bubbles"" on the price of a stock. This is summarized nicely in the ""Discount Rates"" paper cited about. One way to investigate this question, as described in the Discount Rates paper is to use the Campbell-Shiller (1988) present value identity, $$
dp_t \approx \sum_{j=1}^k \rho^{j-1} r_{t+j} - \sum_{j=1}^k \rho^{j-1} \Delta d_{t+j} + \rho^k dp_{t+k},
$$ where $\Delta d_{t+j} = d_{t+j} - d_{t+j-1}$, $dp_t := d_t - p_t = \log(D_t/P_t)$, $D_t$ is the dividend, $P_t$ is the price, $r_{t+1} = \log R_{t+1}$ is the log return of the asset, and $\rho \approx 0.96$ is a constant of approximation. This is an identity based on the definition of returns $R_{t+1} = (D_{t+1} + P_{t+1})/P_t$. This says that the normalized prices of an asset ($dp_t$) is derived from three possible terms: Investigation of the contribution of each of these terms to the volatility of normalized prices indicates the nearly all of the volatility comes from variation in the discount rates term and essentially none comes from the dividend growth and rational bubble terms. This also explains the above ""excess volatility"" puzzle. Note that this still means that dividend explain the value of a stock. It just rules out that changes in the price-dividend ratio of a stock are not explained by changes in future dividends nor by belief in rational bubble growth in prices. You can view more of the details of the empirical investigation in the paper or the same videos linked in the previous two section. The attack against the present discounted value model of stock prices embodied in the ""excess volatility"" puzzle seem to have an explanation in the fact that discount rates vary over time. An investigation of the contribution of future dividend growth, discount rates, and ""rational bubbles"" to dividend-price ratios indicates that ""rational bubbles"" do not seem to explain prices. Rather, dividend yields seem to be entirely explained by discount rate variation. Further investigation seems to indicate that prices and dividends seem to be cointegrated. All of this combined is consistent with the theory that prices reflect the present value of expected future dividends. However, as noted in my disclaimer above, this leaves open many interesting questions. There is still much to do and learn in this literature."
What is the reasoning behind the sharp fall of oil prices starting in mid 2014?,"My understanding of the recent drop in oil price is due to recent changes in US drilling. They are now the worlds largest oil producer. This article by the economist does a good job explaining. My answer (borrowed very heavily from the economist.) Demand is low because of weak economic activity, increased efficiency, and a growing switch away from oil to other fuels. America has become the world’s largest oil producer, creating a lot of spare supply.  The Saudis and their Gulf allies have decided not to sacrifice their own market share to restore the price. They could curb production sharply, but the main benefits would go to countries they detest such as Iran and Russia. Saudi Arabia can tolerate lower oil prices quite easily. It has \$900 billion in reserves. Its own oil costs very little (around $5-6 per barrel) to get out of the ground. For further reading, the NY times has a decent article explaining it a little more. edit:
I think it is safe to assume that OPEC has market power. They can get oil far cheaper than their competitors (as referenced above). With that in mind,
The below reference provides a model which explains what we are experiencing here. Cournot competition seems a perfect model to use. A key quote from the above article: ""After reducing or eliminating competition, the predatory firm
can raise its price and compensate the loss by earning monopoly profits in the long run"". This second provides a very brief overview of OPEC and their behaviour. I couldn't find an empirical study comparing a Stackleberg model to the OPEC cartel but I do not doubt that this has been done to death."
Self-selection bias during the course of experiments,"Apparently this is called attrition bias. It's very similar to survivorship bias. This paper suggest correcting for it using Heckman correction. Propensity score matching may also help somewhat. My experience with both has been mixed, but they are commonly used. You should figure out what exact approach is most appropriate for your setting. One last edit: These two papers, which talk about bounding the average treatment effect, may also be of use to you."
Marshallian Demand for Cobb-Douglas,Since $a + b=1$ the equations are exactly the same. Substituting in for $a+b$ with $1$ in the third and fourth equations gives the first and second equations. 
How interest rate affects currency,"Its important to keep in mind that the exchange rate is a ""price for currency"" and just like any other price it is determined by supply and demand. The main question now is what determines supply and demand for currency? There are two main models that tell us how exchange rates behave based on the two main forces driving demand (and supply) for currency. One factor is trade of goods (the goods market). If foreigners buy our goods they need our currency, so they demand our currency and higher demand ceteris paribus (all else staying constant) leads to a higher price and the value of the domestic currency increases. Based on such considerations in the goods markets we have the model of Purchasing Power Parity (PPP). However empirically it does not always hold and in reality it is mostly expected to hold in the long run. Answer to Q1: Beyond the goods market discussed above, the second main factor is the capital market and that is what Investopedia is referring to. This gives rise to the model of Uncovered Interest Rate Parity, called UIP (there is also a ""Covered Interest Rate Parity Model""). This says that if interest rates are higher in the domestic country compared to the foreign country, then foreign investors would like to invest in our country to get the higher returns. To do so they need our currency. So they buy it (demand it) and therefore as long as supply of currency doesn't increase (the central bank printing more money) the price and value of a currency must increase. Also note that there's no reason to expect the central bank to change the supply in a floating exchange rate regime, which most countries have (i.e. there is no attempt to intervene in the exchange rate to keep it fixed). Answer to Q2: It is not necessarily true that a higher interest rate should increase inflation in general. However increasing the interest rate can decrease inflation. Your understanding of inflation is correct. The second part of the second quote you gave, that higher inflation decreases the value of the currency, however is correct. This is due to the first model we discussed, the PPP. The idea is that inflation makes goods more expensive and therefore our goods have less foreign demand, which leads foreigners to have less demand for our currency (they buy less of our goods so they need less of our money to buy our goods) and less demand for the currency reduces the value of the currency. Answer to Q3: I believe this follows from the general discussion above."
What are the empirical techniques to show causation?,Natural experiments are usually a setting for causal inference rather than a causal inference tool per se. You often need to employ something like difference-in-difference or instrumental variables anyway even when you have a natural experiment.
What is an example of a utility function where one good is inferior?,"A good cannot be inferior over the entire income range. The paper A Convenient Utility Function with Giffen Behaviour shows that for a person with utility of the form: $$U(x,y) = \alpha_1 \ln(x-\gamma_x)- \alpha_2 \ln(\gamma_y - y) $$ X is inferior if $\gamma_x$ and $\gamma_y$ are positive, $0<\alpha_1<\alpha_2$, and in the domain $x>\gamma_x$ and $0\leq y<\gamma_y$.   Update:
$$U(x,v) = x + \ln(v)$$
If the budget is $w$, $v^* = \min(P_x / P_V, w)$ so for $w>P_x / P_V$ $v$ is inferior sticky good. Realized this is actually a zero income elasticity not a negative one so it is not inferior. I found another funky functional form for a utility function where one good is inferior but it too has increasing marginal utility of the other good: An Inferior Good and a Novel Indifference Map  $$U = A_1 \ln(x) + y^2 /2 $$ That function gives a crazy indifference map.  The classic example to me of inferior goods are things like cheap food, where delicious food which is much more expensive crowds it out because there is an additional constraint (stomach capacity) which eventually binds. It should be readily possible to make an example where inferiority is a consequence of this second constraint rather than the utility function.  Update with another example: The paper The Case of a “Giffen Good” (Spiegel (2014)) shows that for a person with utility of the form:
$$ U = \begin{Bmatrix} \alpha X - \beta X^2 / 2 + \lambda Y + \delta Y^2 / 2 & for & 0\leq X\leq \alpha/\beta \\ \alpha^2 /2 \beta + \lambda Y + \delta Y^2 /2 & for& X > \alpha/\beta\end{Bmatrix} $$
where $\alpha, \beta, \lambda,$, and $\delta$ are constant and positive values.  But as in the above functions, this utility function has increasing MU in one good (Y). This is apparently common in Giffen settings: In the case of an additive utility function where the marginal
  utilities of all goods are diminishing with the consumption of the
  goods, that is, the marginal utility of income is dimin­ishing, all
  goods are normal and net-substitutes for each other. However, if for
  some good (in our case, good Y) the marginal utility is positive and
  increasing and for the other good(s) the marginal utility(ies) is
  (are) diminishing (in our case, good X), then the marginal utility of
  income is increasing. The good that exhibits increasing marginal
  utility is a luxury good, whereas the good that exhibits dimin­ishing
  marginal utility is an inferior good. These characteristics were
  proved by Liebhafsky (1969) and Silberberg (1972) and wen: used to
  develop the utility function above that illustrates the case of a
  Giffen good."
How does a country devalue its currency?,"Typically, a devaluation is achieved by selling the domestic currency in the foreign exchange market and buying other currencies. Suppose China sells one trillion Renminbi and buys 157 billion US dollars. From the point of view of the market, it is as if the supply of Renminbi just increased. As in any competitive market, an increase in supply will cause the price (i.e. the exchange rate) to fall: one Yuan will be worth less than before. Devaluations are good for a country's balance of trade. Companies based in China ultimately care about how many Yuan they end up with. Suppose that the value of the Renminbi is 10 Yuan = \$1US. That means that a Chinese product priced at 10 Yuan would cost an American \$1 to buy. Now suppose that the value of the Renmimbi falls by half: 10 Yuan = \$0.50. Now the same product, still priced at 10 Yuan, will only cost an American 50 cents. It's as if everything China exports just got cheaper! This fall in the apparent price of Chinese exports will make people in other countries want to buy more Chinese products so that China will experience an increase in its exports. The argument also works in reverse: to a Chinese person, the devaluation makes it look as if American products got more expensive, so Chinese will demand fewer American (and British, and German, etc.) products and China will import less. Together, these two effects mean that China's balance of trade (i.e. the difference between exports and imports) will improve as a consequence of the devaluation."
What are some good repositories for economic data,"Be more specific on what you need.
Quandl would be a pretty general source which hasn't been mentioned yet.
For macro data the St. Louis Fed is pretty good and thorough. Eurostat for European data. historicalstatistics.org for historical data."
Conventions for reading mathematically rigorous academic articles in economics,"Welcome to Economics Stack Exchange elasticity6565. I was asking myself the same question as an undergrad and found that others struggle with this as well. I am now at the end of my masters degree and can read such articles well, so here's my approach: So those are my (maybe a little tounge-in-cheek) recommendations, I hope it helps. For instance, would you spend ample amounts of time dissecting each notation, assumption, e.t.c, trying to visualise in your mind how it all plays out and where it’s going, If that is the goal: yes or are you confident enough to read through it quickly, as though you are reading a conventional news article. No, and I don't think that anyone can."
What is the Gross Domestic Product (GDP)?,"Gross Domestic Product As per wikipedia, we learn that the OECD, an organisation of international recognition for economic policies and macroeconomy, defined the GDP as Gross domestic product is an aggregate measure of production equal to the sum of the gross values added of all resident institutional units engaged in production (plus any taxes, and minus any subsidies, on products not included in the value of their outputs). The sum of the final uses of goods and services (all uses except intermediate consumption) measured in purchasers' prices, less the value of imports of goods and services, or the sum of primary incomes distributed by resident producer units. -- OECD, Glossary In somewhat simpler terms, it is a quantitative measure of the amount of values produced by all industries and services of a given country. For example, if a company buy some tomatoes and produce a tomato sauce, the difference between the money received for selling the sauce and the money spent for buying the tomatoes (and possibly other products) will be added to the GDP of the country where the tomatoes are being transformed1. The GDP is based on price, and not all prices are known (for example, prostitution, black market, etc.) or need to be estimated, like public services. Therefore the determination of the actual GDP cannot be reached, but some statistical approaches offer various ways to estimate that number. One should note that the different approaches do not always provide similar results. Furthermore slight variations in the definition of the GDP may provide different results. Even if the GDP is used since decades everywhere, there are still some discussion about how to measure it. Some details are provided in Wikipedia and in the references therein. But one way, is to use the expenses approach where the GDP is calculated as $$GDP = C + I + X - M + P$$ where $C$ is the consumption, $I$ the investments, $X$ the exports, $M$ the imports and $P$ the public spending. More details can be found here or in the already cited wikipedia. The GDP number is used to indicate the performance of an economy. The higher it is, the more value the country's econmy produced. It is used to Please note that those are only valid with a constant definition and equal estimation approach. One approach It can be noted that the GDP value of a country is often normalised by the number of inhabitants of that country. This, to simplify the comparison between countries like Luxembourg and India, which have US\$54,940,000,000 (US\$111,716 per capita) and US\$2,308,018,000,000 (US\$1,627 per capita) respectively (as per IMF in 2014 and 2015 via the corresponding Wikipedia's lists: nominal and per capita). The higher the GDP/capita, the richer the population. But this, however is assuming a good repartition of the wealth, which is not always granted. Discussions on the validity of GDP The GDP is an indicator of the current state of an economy. However it tends to be used everywhere, every day by policy makers, economic analysts, journalists, etc. This overpowers other complementary indicators. Which, for policy making is probably insufficient. As mentioned before, the GDP is used as a measure of how rich the inhabitants of a country are, but due to (sometimes very) inequal repartition of wealth, the GDP could be misleading. Apart from the document cited above, some claim that neither GDP nor this balance sheet takes account of environmental degradation, insecurity or inequality. -- François Lequiller, OECD And, as can be found, there are some discussions about alternative indicators to replace or complement the GDP, which should indicate the wealth as well as environmental, equality and some other parameters. 1. That is the main difference with GNP, which concerns residents, that is to say where the company is registered. More on the subject can be found here."
How does a national budget differ from a household budget?,"If you think about it, there is really not much difference between a government budget constraint and a household budget constraint.  Both have uncertain income streams, which are labor and capital income for the household, and mostly tax revenue for the government. On the other side of the constraint, you will find consumption and saving for the household. The government will spend most of its income on (public) goods and transfers to households (social system). Both can move income inter temporally using borrowing. However, neither can run a permanent deficit where it is ""not expected to ever pay back"", something that is typically referred to as a No-Ponzi-Game condition. As an implication, the ""life-time budget constraint"" will have to hold for both: I'm pretty informal here, you would typically rather derive ""present value"" of these constraints, but latex is not yet allowed here, and I'm already writing much. What should government debt be used for? We typically assume that households and governments have concave utility, they prefer consumption processes that are second-order dominating. Hence, they want to engage in smoothing of consumption over fluctuations. Since all debt has to be paid back eventually, there is limited use of government borrowing. Most typically, one wants to stabilize consumption and output over business cycles, by financing consumption in busts and paying back in booms. Also, given an interest rate r, investments that promise a higher return than r can be financed through debt, as the government will not violate consumption smoothing. Potential Difference One potential difference is the commitment. Developed governments are typically more 
trustworthy, they are more likely to pay back there debt. Hence, they can borrow at cheaper interest than most households. As a conclusion, they can use debt more aggressively than households for the two purposes consumption smoothing and investment, as emphasized above. tldr: Besides that, they're very much alike. Note also the Ricardian Equivalence, which is not at the heart of this question, but very much related."
Is Basic Economics by Thomas Sowell a good book for a layperson who isn't interested in pursuing economics?,"I heard that Basic Economics by Thomas Sowell is a good book for a layperson to get a feel for how the economy works. I have also heard some criticisms but they seem to be based on Thomas Sowell's political views. I think both of the things you heard about the book are valid. The book is very good at explaining some basic economic concepts such as scarcity and trade-offs, how markets operate and also has a lot of illustrations of very important concept in policy economics of unintended consequences. On the other hand you can definitely see a libertarian slant in the book. For example, the book presents mainly examples of various government interventions that failed due to unintended consequences, but does not always mention more nuanced policies that were successful. This can give reader misleading impression about role of government in the economy. Also, I do not think his treatment of minimum wages (in the fifth edition) provides fair overview of a literature (although it takes stance that has slighter majority support among economists, this issue is controversial with profession not reaching proper consensus yet see IGM poll). This being said the book was written in 2000, and while by 2000 standards it would be fair to say its has political slant by 2022 standards when even AP often interjects some political commentary into their reporting its barely noticeable. Would you recommend this book? That depends on what your objectives are. The book will definitely explain very well basic concepts in economics and also what economics as a subject is. Then it will also give you a lot of illustrations of how some crude interventions in the economy lead to unintended consequences, which is again something that everyone should probably know. But at the same time it has libertarian slant in a way that it nudges reader toward the conclusion government policy is ineffective by mainly presenting failures of government policy. This being said, as long as you read the book keeping in mind that there is more to government policy, then still most of the examples Sowell mentions are valid examples of failures that result from policymakers often ignoring basic economic principles. If you would want some book for a laymen that is more neutral I would recommend: The Economics Book: Big Ideas Simply Explained - I think this is the most neutral laymen economics book on the market. Its essentially like a high-school textbook but written in fun way that is more accessible. The book is still well written, although one has to admit that the basics are probably better explained by Sowell. You can also read both of the books Economics Book is more comprehensive despite basics being less well explained. So despite there being some overlap you would still gain more from reading both."
Are there any states that don't have debt?,"The classic answer here would be Libya and Brunei, but I think Libya now has debt. Brunei is a strange case in that it uses a joint currency with Singapore dollar, controlled by the monetary authority of Singapore, so in effect you can use Singapore debt as a substitute for Brunei dollar investment. Not having any debt, and having a free currency is generally a bad idea, since you cannot control interest rates. Furthermore, if you can afford not to have debt, then people will be begging to lend to you at such favorable rates it would be foolish not to borrow a bit. On the other hand there are many counties with no net debt. Singapore and Norway don't need to borrow but do so to provide bonds for the financial markets. They then invest the money they borrow. Norway can borrow so cheaply they can buy US treasuries and make a profit."
Why is the Cobb-Douglas production function so popular?,"The reason why Cobb Douglas production functions are so popular stem from the fact that the following assumptions are satisfied while remaining statistically rigorous1: Recall the Cobb- Douglas production function form: $$F(K,AL)=K^{\alpha}(AL)^{1-\alpha}$$  where $0<\alpha <1$ (i.e. the share of output that goes to capital) $1)$ Positive marginal products: $${\partial{F(K,AL)}\over{\partial K}}>0 \space , \space\space{\partial{F(K,AL)}\over{\partial (AL)}}>0$$  $2)$ Diminishing Marginal Products (as you already mentioned) $${\partial^2{F(K,AL)}\over{\partial K^2}}<0 \space , \space\space{\partial^2{F(K,AL)}\over{\partial (AL)^2}}<0$$ $3)$ Constant Returns to Scale (this is how most production processes work) $$F(\lambda K, \lambda AL)= \lambda F(K, AL)$$  for any $\lambda \ge 0$ (E.g. ""Double input $\Rightarrow$ Double output"") Hope this helps!! 1 Source: https://en.wikipedia.org/wiki/Cobb%E2%80%93Douglas_production_function"
How could the Euro zone be broken up in an ordered way?,"No Time To Argue Most importantly, there cannot be an ""18 months period of notice"". Tt has to be decided suddenly and there cannot be any period in which individuals could respond to the new plan by withdrawing their money and moving cash from one of the member countries to any of the others. New Currencies Each country has to have its own currency. These currencies should be free floating against each other, which on the other hand means that some will value up and some down. That is, Germany's new currency will probably value up against whatever Greece will have. Capital Controls Therefore, it will be of higher value to have your Euro exchanged for German money than for Greek money. Any bank account will have to immediately be translated into local currency. Also, no one will be allowed to move cash Euro between the member countries.  One way to immediately establish the new currency would be to simply stamp the old money with country specific stamps. Citation Note that Greek's new finance minister implied that their train of thoughts follows exactly these lines of thinking [my own translation]: Greece is not going to leave the currency union, as it cannot go back
  to the path of development on which it was on, hadn't it joined the
  union. An exit would lead to chaos in any case: For the moment, there
  is no new Greek currency as a new Drachm. Preparations for the
  introduction of such a currency would take months and would have the
  same effect as announcing a depreciation several months in advance.
  But you can only have a depreciation over night and unexpectedly,
  otherwise there are massive movements of capital flight/export. From this german article: Die Währungsunion verlassen soll Griechenland nach den Worten von
  Varoufakis aber in keinem Fall. Denn damit könne das Land nicht auf
  den Entwicklungspfad zurückkehren, den es ohne Eintritt in den
  Euroraum genommen hätte. Ein Austritt habe auf jeden Fall Chaos zur
  Folge. Denn gegenwärtig gebe es keine griechische Währung wie eine
  neue Drachme. Vorbereitungen für die Einführung einer solchen Währung
  würden Monate dauern und hätten die gleiche Wirkung wie die
  Ankündigung einer Abwertung mehrere Monate im Voraus. Doch eine
  Abwertung könne es immer nur über Nacht und unerwartet geben, sonst
  gebe es riesige Bewegungen zum Kapitalexport, warnt er."
Is 'full employment' synonymous with 'the natural rate of unemployment'?,"Full employment is a more general term, which has much less implication than natural rate of employment. My experience reading a lot of economics papers is that careful authors do not mix the two concepts. Stricto sensu, full employment should mean 100% of the workforce is employed. But when we are talking about periods such as the trente glorieuses, the term is often used to mean ""very low unemployment"", which Keynesian economists associate with natural rate of employment. Politicians will most certainly never use the later, while economists may not always be very careful about which one they use."
"What is the definition of a ""Stackelberg leader-leader equilibrium""?","A leader-leader Stackelberg is a situation in the Stackelberg model where both firms believe they are leaders. This leads to global production being much higher than expected by both firms, as they anticipate small production by the other firm in response to their high production."
"What is the difference between ""aggregation"" and a ""representative agent?""","(I cannot say if my answer will respond to your questions, which indeed, are a bit unclear).  If one browses through many-many economic papers, one will get the impression that ""representative"" just means identical. Indeed in large chunks of literature this is the case, for historical reasons.   The drive behind the adoption of the ""representative consumer"" modelling framework came from the Lucas critique on the previous-generation macro-models, and the requirement that macroeconomic models are ""micro-founded"". But true theoretical aggregation (with heterogeneity present) requires some skill and knowledge and the bulk of the discipline appeared to have quickly settled for the ""representative means identical"" framework.  The problem is that in such a case, you don't really have a macro-model, just a blown-up version of a micro-model (note: here the words micro/macro are not to be mapped to the partial/general equilibrium concepts). There is nothing to ""aggregate"" here: the whole point of aggregation is to see whether the behavior of the collective differs from the behavior of the individual. And in the ""representative means identical"" approach, no such thing can happen, by construction: Instead of micro-founded macro-models, we ended up having blown-up micro-models posing as macro-ones (this is not opinion, I am just describing). There are some models where the term ""representative"" obtains some intuition -especially in models with more than one classes of agents (say, labor owners and capital owners). Here we model two agents, and each is ""representative"" of its class. In-class are all identical, but here it sounds more appropriate to call the two individuals ""representative"".   The funny thing is, the concept ""representative consumer"" (RC) does have a special meaning: the representative consumer represents all consumers as regards basic structure, not measurement or quantities. E.g. ""all individual maximize utility from consumption (""same structure""), but their utility parameters may differ(""different measure""). All consumers have wealth, but the level of wealth may differ. Etc. RC is still a modelling abstraction, but it does leave room for heterogeneity.   A good source on the matter is Caselli, F., & Ventura, J. (2000). A representative consumer theory of distribution. American Economic Review, 909-926. Apart from their focus on developing a theory of distribution in the context of an RC model, they make a good job in presenting what can be done in an RC framework and what not. An excerpt: The RC is a fictional consumer whose utility maximization problem when
  facing aggregate resource constraints generates the economy's
  aggregate demand functions. The RC assumption does not rule out
  consumer heterogeneity, but only requires that potential sources of
  consumer heterogeneity have sufficient structure to ensure that the
  sum of all consumers behaves as if it were a single consumer."
What are the recent advancements in building a unified theory of bounded rationality?,"The term bounded rationality was introduced by Herbert Simon. He wrote ""The term, bounded rationality, is used to designate rational choice
  that takes into account the cognitive limitations of both knowledge
  and cognitive capacity. Bounded rationality is a central theme in
  behavioral economics. It is concerned with the ways in which the
  actual decision-making process influences decisions."" This passage seems to focus only on incomplete information, limited information processing time, limited knowledge -and not on any ""psychological biases"" like framing effects and the like. But the passage is complemented by a last phrase ""Theories of bounded rationality relax one or more assumptions of
  standard expected utility theory"" which suddenly opens the content of the term to anything. So ""bounded rationality"" has come to signify ""deviations from strict rationality in any way"", making it rather impossible to arrive at a general, all-encompassing, or even most-encompassing, modeling approach, that at the same time, will be specific enough so as to be operational."
Why does any treasury / central bank hold gold?,"I guess it is for the same reason that other countries hold foreign reserves.
The argument is that for some reason foreign markets become suddenly very adverse to take your currency, you should have some other medium of exchange that allow you to finance imports or serve short term external debt.
This is very related to the Guidotti–Greenspan rule."
Economics of forgetting,"Maybe not what you are looking for, but a related concept is regret. Orphanides and Zervos (1995) is a classic paper on rational regret in a health economics concept http://www.jstor.org/discover/10.2307/2138580?uid=3739840&uid=2&uid=4&uid=3739256&sid=21106216442271  There's also plenty of irrational regret papers, mostly just boiling down to hyperbolic discounting models."
Is there a folk theorem for repeated games on networks?,"Yes, there are folk theorems for games on networks, depending on information structure and possible communication. Here are some of the most relevant papers: Thanks @Ubiquitous for basically providing the answer."
Depreciation and second-hand markets for bicycles,"Interesting observation. Perhaps a part of the answer lies in Akerlof's lemons' problem?  Admittedly judging the quality of a 2nd hand bike is easier than that of a car, but from what you're describing in the US there doesn't seem to be a well functioning market for 2nd hand bikes in the US. Moreover, bikes are becoming more and more like cars in that respect with all the gadgets and technology that one cannot fix in their own shed or backyard.  Perhaps it also plays a role that US citizens are not used to bikes and therefore have a harder time judging their quality (just guessing here, and perhaps stereotyping too much). Here in the Netherlands we seem to have a well-functioning market for them that serve all segments (including, I might add, a high theft rate), but everyone rides a bike here, so we should be better at judging their quality."
Why use empirical macroeconomic models when they are not policy invariant (Lucas Critique)?,"The response to the Lucas Critique was the emergence of RBC and DSGE models. Using microeconomic foundations of macro models we can simulate how behavior changes when policy changes and only estimate ""deep"" structural paramateres that are not policy variant. Before microfoundations we were estimating models where the estimation included the actions of people. With microfounded models today we try to seperate the actions. This was not possible in older models as we actually did not consider how people act or react, whereas microfoundations tell you how people would react. However this is a difficult task as once you introduce agents that actively think about their actions you must take into acount their future expectations, which are unknown. One way to deal with this are rational expectations. A further issue is that such models predict reactions that are quicker than in the data. If agents are perfectly rational, have perfect foresight and all information, they react quickly and perfectly. The solution to this today is to add frictions that slow these reactions down. However old models that we used to estimate (think about IS-LM and especially AS-AD models) also have the huge issue that people are too stupid (only adaptive (backward looking) expectations, do not take information into account when forming expectations, don't think about what may come in the future) and this is partially a piece of the Lucas Critique. Now we have the problem that people are too smart or super-rational. Some models (see models where some fraction of agents are ""Rule of Thumb Consumers"") now also assume that some agents are fully rational (many indeed are) while some just are backward looking (as in the normal IS-LM or AS-AD model), as a balance of the two things to generate a more realistic picture. As for the critique of rationality: In many experimental micro studies rationality fails. However this does not tell us how many small deviations from rationality aggregate, which is what macro is interested in. It may occur that from many different deviations in different directions that in aggregate rationality is still a good approximation. Furthermore approaches have been developed now to deviate from rational expectations. These are very difficult to solve though. One interesting way is to assume people are rational, but do not have all the information, which is a reason most people make mistakes. These are models of information frictions. Key words: Rational Inattention (e.g. Sims) and Inattentiveness (e.g. Reis). Another related approach includes Learning Models. To summarize: we try to have models that are immune to the Lucas Critique. These often require rational expectations to be solved, but other approaches are also being developed."
What are estimates of the elasticity of demand for credit with respect to interest rates?,"These papers look relevant, to one degree or the other: Karlan, D., & Zinman, J. (2009). Observing unobservables: Identifying information asymmetries with a consumer credit field experiment. Econometrica, 77(6), 1993-2008.
The authors write:""We estimate the presence and importance of hidden information and hidden action problems in a consumer credit market using a new field experiment methodology. We randomized 58,000 direct mail offers to former clients of a major South African lender along three dimensions (...) These three randomizations, combined with complete knowledge of the lender's information set, permit identification of specific types of private information problems. (...) We find strong evidence of moral hazard and weaker evidence of hidden information problems."" Mandell, L. (1971). Consumer Perception of Incurred Interest Rates: An Empirical test of the efficacy of the Truth-In-Lending Law. The Journal of Finance, 26(5), 1143-1153. ""The purpose of this article is to use national sample survey data to measure
the effect of the Truth-in-Lending Law (enacted in 1969) on the consumer’s knowledge of interest rates that he is actually paying on an installment loan.(...)
Aside from measuring the overall effect of the law on all persons, analysis
of various population subgroups will be made to see whether the law has had
differential effects upon the interest rate perception of persons based upon
personal characteristics such as age, income, amount borrowed, total debt and
education."" Calem, P. S., & Mester, L. J. (1995). Consumer behavior and the stickiness of credit-card interest rates. The American Economic Review, 1327-1336.
The authors present empirical evidence (from a 1989 Survey) that supports the ""stickiness"" of credit card interest rates, attributing it, as they write, to ""(i) consumers facing search costs; (ii) consumers facing switch costs; and (iii) firms facing an adverse-selection problem if they were to unilaterally reduce their interest rates.""  Ausubel, L. M. (1991). The failure of competition in the credit card market. The American Economic Review, 50-81. The original paper to which the previous one provided additional support. It contains both theoretical discussion and empirical evidence, as well as counterfactual calculation (what ""would"" the interest rates be if perfect competition held in this market)."
Real option effect of uncertainty - irreversiblity vs fixed cost,"There exists a post on capital adjustment costs, which gives a good overview of the different types of capital adjustment costs.  http://economictheoryblog.com/2015/08/20/capital-adjustment-costs However, ultimately the post summarizes Cooper, R. W., & Haltiwanger, J. C. (2006). On the nature of capital adjustment costs. The Review of Economic Studies, 73(3), 611-633. Regarding your question, I guess as long as there are any sunk costs associated to investment you destroy future option, regardless the nature of these costs. Consequently the nature of capital adjustment cost should not really change the way uncertainty affect investment through real option effects."
$1.25 PPP per day poverty threshold - What precisely does it mean?,"A partial answer : I m not sure what measurement the World Bank would recommend, but I suspect that in practice, the expenses that are taken into account to measure whether one is below or above the 1.25$ threshold vary from one study to another.  For instance in their famous The Economic Lives of the Poor, Banerjee and Duflo write ""From each of these surveys we identified the extremely poor as those living in households where the consumption per capita is less than \$1.08 per person per day, as well as the merely “poor” defined as those who live under \$2.16 a day using the PPP in year 1993 as benchmark.3 The use of consumption, rather than income, is motivated by the better quality of the consumption data in these surveys (Deaton, 2004)."" The use of ""consumption per capita"" suggest that capital income, such as the benefits one get from owning her own house, would not be taken into account. 
Because data quality considerations seem to enter the picture, it opens the door to much variation from one study to the other.  The above quote however suggests that at equal data quality, income data would be preferred."
Why hasn't innovation in the agricultural industry led to a significant reduction in food prices?,"Why hasn't innovation in the agricultural industry led to a significant reduction in food prices? Mostly (present couple of years excluded) this is because of inflation. Inflation means money loses value. A dollar today does not have the same value as a dollar in 1900. The same holds for almost every currency around the world. More recently, real food prices also increased due to Covid19 supply chain disruptions and the Russo-Ukrainian war. Furthermore, your claim: But if we look at actual food prices since 1960 food actually got more expensive. is not very accurate. First you only show food prices for wheat, not for all food. Second, food did not actually became more expensive over time, but cheaper over time (excluding a recent few years for which I was not able to find good data source for historic comparison, and likely Covid19 and the current Russo-Ukrainian war will affect real price of food). As you can see below real food prices dropped over time (even though there was an increasing trend recently). What happened is that food got cheaper thanks to productivity increases, but money lost its value faster than food got cheaper. As a result value of food in terms of US dollars or Euro rose, but value of food in real terms got cheaper. Overall real food price index around the world from Polaski (2008):  US real food price index for all food and home-cooked food from Christian and Rashad 2007:  Recently, there was increasing trend in real food prices, just anecdotally at conferences you could see a lot of papers worrying about high food prices impact on development. This was due to Covid19 lockdowns supply chain disruptions and the Russo-Ukrainian war. However, I was unable to find any source that would have recent real price index that would allow comparing today's prices with prices in 1900s or 1950s.  Supply chain disruptions decrease overall productivity (e.g. value created divided by inputs), and the Russo-Ukrainian war just means that most of the world cannot (or put restrictions on accessing)  access food production of large food producers, namely Russia and Ukraine."
The Unreasonable Ineffectiveness of Mathematics in Economics,"I think by today the arguments you mention are completely outdated. Nowadays, using combination of psychometry and econometrics companies can predict whether you are pregnant (from your shopping patterns) earlier than it is possible for humans to notice the pregnancy (see here). If companies being able to use data with combination  of social sciences to predict pregnancy does not count as ""unreasonably effective"" I don't know what does. there has been much joking in recent times about ""unreasonable ineffectiveness of mathematics"" I do not know where you heard such jokes, but they must have come from some people who are not up to date on modern research. While economics or other social sciences are not yet as precise as some areas of physics they are not far behind. Such jokes would ring true before the 'credibility revolution' in economics (see Angrist and Pischke 2010). After the credibility revolution such jokes are certainly not accurate. what could be the reasons why mathematics seems to work very well in certain areas (physics or engineering), but is not as useful or accurate in in social sciences such as Economics? In past (pre 80s) social science models were very ineffective mainly for the following reasons: lets take these points one by one: in the past it was very difficult to find high quality data. You will find a lot of social science papers from that era using sample sizes of 40-60, which by modern standards would be laughable. In fact even as recently as in 90s you could see published papers using sample sizes as small as 117 observations (e.g. see Dollar 1992). For example, even in physics it would be difficult if not impossible to predict path of an asteroid if there are no good data on its velocity, position etc available. In connection to the previous point there is usually much more noise in social science data, hence it goes without saying that social science requires much larger data sets to be precise. This is why the stories such as that about the prediction of pregnancy are popping up just now as they are result of large data being employed in social sciences. lack of computing power. Lack of computing power was large issue in the past, and it is still limiting nowadays. For example, analogue hydraulic computer (e.g. MONIAC) was used in economics even after regular computers were invented as early computers did not had computing power to model even simple macro models. Modern computers are better but still computing power is big limiting factor. Recently I took graduate class from Ben Moll on distributional macro where we were building relatively simple macro models with multiple agents (e.g. see examples of the models here), yet even modern PCs have still some trouble running such models (they can take quite long to solve). Nonetheless, outside area of simulations (that are usually very intensive computationally) the computing power now is sufficient to run wide array of statistical models which would be impossible to run in the past. In the past big issue with social sciences was lack of appropriate statistical models. In physics most relationships are exogenous, you have typically very simple chains of causality. In social sciences most relationships are endogenous. Such relationships cannot be as easily analyzed with (comparatively) basic methods that are sufficient for many natural sciences. As a result before the 'creditability revolution' most empirical research was in very bad state. However, the development of new statistical techniques such as diff-in-diff, synthetic control, TSLS, RD etc as well as greater focus on running randomized controlled trials brought credibility and much greater predictive power to social science research (see Angrist and Pischke 2010). Even if social science research is still not as precise as some areas of physics, it would be absurd to say that mathematics is somehow 'unreasonable ineffective' at present day. Is it reasonable to assume that economics or sociology will one day have as intensive and predictive a use of mathematics as is currently the case in physics, or can these sciences as we know them today simply not be formalized to that degree? It is reasonable to assume that. In fact I would say that present day economics can be as accurate as many areas of physics were decades ago. With better data, more computing power, better statistical techniques social sciences can be frighteningly precise.  For example, recently university of Chicago researchers developed algorithm that can predict crimes weeks in advance with about 90% accuracy (see Rotaru et al 2022 or here) which almost sounds like the ""precogs from the minority report."
Textbook for macroeconomics (advanced undergrad),"Advanced Macroeconomics by David Romer, now in its fourth edition. Link contains TOC and a sample chapter. The presentation consists of formal theory models, but with lots of intuition too, followed by light empirical applications."
"Are financial markets ""unique"" for each ""currency pair"", or are they simply ""translated""?","It's not clear what level of answer you're looking for, so here is a much more basic answer. There are indeed many exchanges with many different prices. However, if you have noticed that you could make money by exchanging your BTC for USD, exchanging the USD for SEK, and then exchanging the SEK for slightly more BTC than you started with - then someone else probably has noticed that too, and they'll make free money with the manoeuvre just as you will. As people do this, the prices gradually equalise: you're making BTC more scarce relative to SEK so are driving its price up in SEK (making the last hop less profitable), and you're making BTC more common relative to USD so are driving its price down in USD (making the first hop less profitable). So it is theoretically possible for such an opportunity for arbitrage to exist. In practice, however, in such a simple case, there are powerful forces acting constantly to equalise the price. As long as it's easy and low-fee to make all three transactions in the chain above (that is, as long as the respective markets are sufficiently liquid), people will make those transactions and the opportunity will naturally disappear. If you identified a really unobvious opportunity for arbitrage (so unobvious that nobody else saw it before you) and the markets were liquid enough, then you could make a lot of money off it by exactly this manoeuvre! In fact, very early on, the Bitcoin markets were absolutely full of arbitrage opportunities. The big players were not keen to trade in such risky places (it was routine for an exchange to just close down and run away with large amounts of other people's bitcoin - see the middle point on Forbes's article about this), so there was not such a strong force equalising the prices. As it's become more mainstream, though, more people started taking the free money, and the opportunities have dried up somewhat. So you may very well find that earlier in your dataset, the markets are much more obviously distinct."
Why is pre-specification of punishment order necessary to manipulate compliance?,"Denote the cost of the punishment by $p$, the cost of registering by $r$. In order for the plan to work, you need to have $p > r$. If the order is known, then the person at the top will choose to register. Since rationality is common knowledge, the second person knows this, and thus knows that were he not to register he would be punished with probability 1. And so on, and so on. If the order is unknown, or for example random, every person can believe that the probability of punishment is miniscule, 1 divided by the number of people. It can well be that the expected cost of punishment is less than the cost of registering. This is why criminals in Gotham commit crimes: whoever meets Batman will get beaten up for sure, but since there are many crimes, only a few of them will meet Batman and they don't know in advance who it will be, so the expected costs may be lower than the gain from their crimes."
What is the difference between two stage least squares and instrumental variable regression?,"IV estimators are 2SLS estimators. An IV estimator is the sample analog of the form: $\beta = \frac{Cov(Y, Z)}{Cov(X, Z)}$, where $Y$ is the outcome variable, $X$ is the endogenous variable, and $Z$ is the instrumental variable. It can be shown that the 2SLS is of the above form. The advantage of 2SLS estimators over other IV estimators is that 2SLS can easily combine multiple instrumental variables, and it also makes including control variables easier."
When do supply and demand curves shift?,"Supply and demand curves are a function of price and quantity.  If anything else changes other than P or Q that is relevant to the curve, the curve shifts. For supply, these shifters generally fall into three  categories:   For demand:  If you come up with something that didn't fit into these categories, but is not P or Q, the result is still a shift! You probably just need your imagination to squeeze it into one of these 7 formal categories.  For example, if the supply curve was P=2Q+3, and there was a decrease in the cost of inputs, the demand curve could shift to P=2Q+2.  Note how the price levels are lower at every level of Q. Changes in supply and demand that are not ""shifts"" are called ""slides along the curve"". They are any direct change in P or Q. This is easiest to see with a linear, mathematical example.  Let us say the government wants to set the price of a product. If P=2Q+3 is the supply, then consider that if you set the price (by law) to be 7, then Q is now 2. The curve remains steady, but we slide along it to get to the new P=7, Q=2 position. From wherever P and Q started (P=12 in the example), we now end at point P=7,Q=2, and the curve remains unmoved along the entire line P=2Q+3."
Can the stock market show indefinite exponential growth?,"May I rephrase your question into the broader question ""Can economic growth continue indefinitely?"" (In response to objections that eventually the sun will burn out or the universe will suffer heat death, I take indefinite to mean ""lasting for an unknown or unstated length of time"" (OED). So I am thinking of 100s, 1000s, or even 10000s of years ahead. But I am not thinking of billions of years ahead or the ""infinite future"".) Non-economists commonly believe the answer to be ""no"", giving some reason along the lines of ""resources are finite!""  But the economist's answer to this is ""Yes, of course economic growth can continue indefinitely."" And so to answer also your narrower question, ""Yes, of course the stock market can show indefinite exponential growth."" (By ""can"", I mean that this is at least conceivable. Whether it will is a different matter altogether. After all the world might end tomorrow in a nuclear apocalypse.) I think we can distinguish between two common fallacies at work here. Fallacy #1. ""Economic growth is about making ever more ""stuff"", digging ever more gold and other natural resources out of the ground, burning ever more energy, etc."" (This caricature is perhaps why many non-economists and especially environmentalists are averse to economists and the idea of economic growth.) The fallacy typically continues, ""Resources/the universe is finite. Therefore economic growth must also be finite.""  But this is wrong. Economic growth is about improving human well-being, broadly conceived. It is true that for a long time (the past few centuries), improvements in human well-being were largely through improvements in material well-being and highly-correlated with making ever more stuff and burning ever more energy. After all, it was not two centuries ago that the vast majority of mankind lived at bare subsistence level. (Indeed, even today, many people still do.)  But going forward, it is perfectly conceivable that we make ever less ""stuff"", dig ever less ""stuff"" out of the ground, and burn ever less energy, and yet still become ever more well-off. This is actually already happening today in the rich countries (see e.g. falling energy use, briefly analyzed below). Since the 1930s and 1940s, we've measured economic growth mostly by GDP growth. But economists have always recognized that GDP is a very flawed measure of well-being. Economists are working on alternatives that better capture the notion of improvements in human well-being or equivalently, economic welfare. I do not expect that in 100 years, the current concept of GDP without fundamental modifications will still be used to as the primary measure of economic well-being. (Footnote: Perhaps in the future, we will also include non-human well-being in our conception of economic growth. But for now, we still restrict attention mostly to human well-being.) Fallacy #2. ""Bad things (like the consumption of food or resources) will grow rapidly or even exponentially. In contrast, offsetting good things (like technology) can at best grow arithmetically. Therefore, there are necessarily limits to growth."" This fallacy is not new. Here's an example of doom-and-gloom predictions from each of the past three centuries, all of which proved to be wrong.  2010 commentary: Malthus began with two “fixed laws of our nature.” First, men and
  women cannot exist without food. Second, the “passion between the
  sexes” drives them to reproduce. He explained that, if unchecked, people breed “geometrically” (1, 2,
  4, 8, 16, etc.). But, he continued, the production of food can only
  increase “arithmetically” (1, 2, 3, 4, 5, etc.). “The natural
  inequality of the two powers of population and of [food] production in
  the earth,” he declared, “form the great difficulty that to me appears
  insurmountable [impossible to overcome].” Malthus concluded: “I see no way by which man can escape from the
  weight of this law.” In other words, if people keep reproducing in an
  uncontrolled geometric manner, they will eventually be unable to
  produce enough food for themselves. The future, Malthus argued,
  pointed not to endless improvement for humanity, but to famine and
  starvation.  Writing in the Times of London in 1894, one writer estimated that in 50 years every street in London would be buried under nine feet of manure. Moreover, all these horses had to be stabled, which used up ever-larger areas of increasingly valuable land. And as the number of horses grew, ever-more land had to be devoted to producing hay to feed them (rather than producing food for people), and this had to be brought into cities and distributed—by horse-drawn vehicles. It seemed that urban civilization was doomed. Our attempts to use even the most optimistic estimates of the benefits of technology in the model did not prevent the ultimate decline of population and industry, and in fact did not in any case postpone the collapse beyond the year 2100 (p. 145.). This was a highly-influential best-seller that has sold over 16M copies in over 30 languages. Take their example of gold. On p. 56, they calculate that if gold use continued growing exponentially AND there was 5 times as much gold available as there were known gold reserves (they thought this was a very optimistic assumption), gold would be depleted in 29 years, or in 2001.  Surprisingly, 2001 came and went and gold continued to be mined. Indeed, more than ever. Gold mining graph (source):  Roughly every 5 years since 1972, The Limits to Growth folks (AKA the Club of Rome) have released a new update to their 1972 book, each time explaining why they had been correct all along (of course) and sometimes pushing back their predictions about when the eventual collapse will set in. In their 30-Year Update, they make no mention whatsoever of gold.  The following is the response by two critics to The Limits to Growth, also quoted by Robert Solow in a Newsweek article: The authors load their case by letting some things grow exponentially
  and others not. Population, capital and pollution grow exponentially
  in all models, but technologies for expanding resources and
  controlling pollution are permitted to grow, if at all, only in
  discrete increments. (Footnote: Doomsday-mongering was especially fashionable in the West around the 1970s. See also the famous Simon-Ehrlich wager at around the same time. Predictions at the polar extremes capture the public's attention. Ray Kurzweil comes to mind as someone who makes similar predictions, but at the polar opposite. In contrast, the median economist is cautiously optimistic, believing merely that slow but steady, sustained growth is possible. No doomsday, no stagnation, but no impending Singularity either. Not exactly a position that sells many books.) In 2012, a physics professor wrote a somewhat-influential blogpost: Exponential Economist Meets Finite Physicist, exhibiting both of the above fallacies. That someone as intelligent as a physics professor could commit both fallacies shows that economists must do a far better job at educating the public. There is plenty that is wrong in that blogpost and perhaps I will do a sentence-by-sentence dissection elsewhere, but this is probably not the proper avenue. Here I'll merely point out one obvious factual error that's of particular relevance. He claims as fact that energy growth has far outstripped population growth, so that
  per-capita energy use has surged dramatically over time—our energy
  lives today are far richer than those of our great-great-grandparents
  a century ago [economist nods]. So even if population stabilizes, we
  are accustomed to per-capita energy growth: total energy would have to
  continue growing to maintain such a trend [another nod]. As Tim Harford points out, this is FALSE. In recent decades, energy growth per person in many countries has actually been falling, even as GDP per capita has risen. Graph (data from World Bank, June 1st 2017 update):  In every rich country, per-capita energy use peaked years ago and has been falling ever since. In fact, in some countries, it peaked DECADES ago (peaked in 1978 in the US, in 1979 in Germany, and in 1973 in the UK). (One would've hoped that a physics professor backed up his factual claims with something more than a fictitious and bumbling economist who repeatedly nods.) See also falling energy intensity (energy use per unit of GDP) (source):  The highest per-capita energy use ever attained was the US in 1978. My prediction is that global average human well-being will keep improving, but global per-capita energy use will never hit the US 1978 peak (at least not until we start populating other planets and stars). "
Transversality Condition in neoclassical growth model,"The transversality condition may be more easily understood if we start from a problem with finite horizon. In the standard version, our objective is to 
$$ 
\max_{\{c_t,k_{t+1}\}_{t=0}^T} \sum_{t=0}^T\beta^t u(c_t)
$$
subject to 
$$
\begin{aligned}
f(k_t)-c_t-k_{t+1}&\ge0,\quad t=0,\dots,T &&\text{(resource/budget constraint)}\\
c_t,k_{t+1}&\ge0,\quad t=0,\dots,T &&\text{(non-negativity constraint)}
\end{aligned}
$$
with $k_0$ given. The associated Lagrangian (with multipliers $\lambda_t$, $\mu_t$, and $\omega_t$) is 
$$
\max_{\{c_t,k_{t+1},\lambda_t,\mu_t,\omega_t\}_{t=0}^T}
\sum_{t=0}^T \beta^tu(c_t)+\lambda_t(f(k_t)-c_t-k_{t+1})+\mu_tc_t+\omega_tk_{t+1}
$$
The FOCs are 
$$
\begin{align}
c_t:&& \beta^tu'(c_t)-\lambda_t+\mu_t&=0,\quad t=0,\dots,T \\
k_{t+1}:&& -\lambda_t+\lambda_{t+1}f'(k_{t+1})+\omega_t&=0,\quad t=0,\dots,T-1 \\
k_{T+1}:&& -\lambda_T+\omega_T&=0,\quad T+1 \tag{1}
\end{align}
$$ 
with the Kuhn-Tucker complementary slackness conditions: for $t=0,\dots,T$,
$$
\begin{align}
\lambda_t(f(k_t)-c_t-k_{t+1})&=0 & \lambda_t&\ge0 \\
\mu_tc_t&=0 & \mu_t&\ge0\\
\omega_tk_{t+1}&=0&\omega_t&\ge0\tag{2}
\end{align}
$$
Since resource constraint must be binding in all periods, i.e. $\lambda_t>0$ for all $t$, it follows that at the last period $T$, $\omega_T=\lambda_T>0$, which in turn implies $k_{T+1}=0$. Usually we assume $c_t>0$ for all $t$ (the Inada condition), and this implies $\mu_t=0$ for all $t$. So the consumption FOC becomes
$$
\beta^tu'(c_t)=\lambda_t \tag{3}
$$ Looking at conditions $(1)$ $(2)$ and $(3)$ in the last period $T$, we get
$$\beta^Tu'(c_T)k_{T+1}=0$$
Extending this to the infinite horizon, we get the transversality condition
$$\lim_{T\to\infty}\beta^Tu'(c_T)k_{T+1}=0$$ The intuition of the transversality condition is partly that ""there is no savings in the last period"". But as there is no ""last period"" in an infinite horizon environment, we take the limit as time goes to infinity."
Price discrimination- how much is optimal?,Varian has a paper on Price Discrimination and Social Welfare in which he gives some necessary and sufficient conditions for (third degree) price discrimination to increase welfare. A necessary condition is that the total level of output (i.e. the total number of consumers served) increases as a result of the discrimination.  A sufficient condition is that the profitability of the new output (i.e. after discrimination) exceeds the profitability of the old output (before discrimination) evaluated at the new prices.
What is the difference between ATE and ATT?,"Treatment effects are causal effects of a binary treatment. Because the treatment is binary, individuals are either treated or they are not treated. For the sake of example assume that the treatment is participation in a money making course - the course is claimed to make you better at making money. Obviously, the causal effect of such a course could very well be differ from person to person (this is referred to as treatment heterogeneity). Some people may learn a lot from the course and actually improve at making money while others will be bored by the content of the course and experience a zero effect. As usual when important quantitative measures vary across observational units a canonical summary statistic is the average. The Average Treatment Effect (ATE) is simply that: The average of the individual treatment effects of the population under consideration. And the Average Treatment Effect of the Treated (ATT) is simply the average of the individual treatment effects of those treated (hence not the entire population). To make it formally more clear what the causal effect of treatment is, it is often assumed that for each individual $i$ there exists an amount of money $Y_i^0$ individual $i$ will make without taking the training course. And there also exists an amount of money $Y^1_i$ that individual $i$ will make if she takes course. The causal effect for individual $i$ of participation in the course is then defined as $$\tau_i := Y_i^1- Y^0_i,$$ the difference in outcome with and without treatment. For the sake of example consider the following table for 6 individuals:  It is clear from the table that individual $i=1,2,3$ are treated $D_i=1$ while $i=4,5,6$ are not treated. For those who are treated the observed amount of money made by the individual $Y_i$ is equal to $Y_i^1$. For those not treated the observed amount of money made $Y_i$ is equal to $Y_i^0$. In general this is written as $$Y_i = D_i Y_i^1 + (1-D_i)Y_i^0.$$ An important part of the setup is therefore that while $Y_i^1$ and $Y_i^0$ are assumed to exist they are not assumed to be observed. However, getting back to ATT and ATE. In the above example the ATE can be calculated as $$ATE := \frac{1}{N} \sum_i \tau_i =  \frac{1}{N} \sum_i (Y_i^1 - Y_i^0) = \frac{1+1+1+0+1-1}{6} = 0.5,$$ and the average treatment effect of those treated is calculated as $$ATT := \frac{1}{N_1} \sum_i \tau_i =  \frac{1}{N_1} \sum_i (Y_i^1 - Y_i^0) = \frac{1+1+1}{3} = 1.0,$$ where $N_1 = \sum_i D_i = 3$. In this example ATE and ATT are numerically the same but as you can see they are averages of different sets of individual causal effects. As such they are not necessarily expected to be the same. Try to construct an example yourself where they are different simply by changing the group of treated individuals. The average treatment effect is used when we are interested in the average treatment of the entire population, whereas the average treatment effect is used when we are only interested in the average treatment effect of those treated."
Can econometrics test for correlation or causality between prices and corruption?,"Some related studies: Al-Marhubi, F. A. (2000). Corruption and inflation. Economics Letters, 66(2), 199-202.
The analysis is based on cross-country data consisting of 41 countries from Asia and Latin America for which data is available on four alternative indices of corruption (two from Transparency International, one is the Business International index, and the fourth from another economics study). Note that numerically, these are essentially non-corruption indicators -they range from $0=$ maximum corruption to $10=$  no corruption. The panel covers the period 1980-1995. Four alternative OLS regressions were run (each with one corruption indicator present), with heteroskedasticity-robust standard errors. In all cases a statistically significant positive correlation between corruption and inflation was found.   How large a correlation (to judge its economic significance also)?  The dependent variable was the logarithm of inflation. The value of the coefficient on the corruption variable was on average $-0.22$. Given how the indicators are defined, the minus sign says that  less corruption = less inflation. The actual value roughly says that $1/5$ of inflation was associated with corruption. So if the inflation was, say $10$%, 2 percentage points can be attributed to corruption. Not small. Braun, M. & Di Tella R., (2004). Inflation, inflation variability, and corruption. Economics & Politics, 16(1), 77-100.
They examine the reverse relationship: inflation facilitates corruption. They present a theoretical model, and also empirical evidence. For the theoretical model they write: ""(...)high variability of inflation can make over-invoicing by
  procurement officers and under-invoicing by salespersons easier
  because it makes auditing more expensive to the principal"". So here there is a causative theoretical argument as to why unstable prices increase corruption. Their data relates to 75 countries and for the period 1980-1994: Algeria, Argentina, Austria, Bahamas, Bahrain, Bangladesh, Belgium,
  Bolivia, Botswana, Burkina Faso, Cameroon, Canada, Chile, Colombia,
  Costa Rica, Cote d’Ivoire, Cyprus, Denmark, Dominican Republic,
  Ecuador, Egypt, El Salvador, Ethiopia, Finland, France, Gambia,
  Germany, Ghana, Greece, Guatemala, Haiti, Honduras, Hungary, India,
  Indonesia, Israel, Italy, Jamaica, Japan, Jordan, Kenya, Korea
  (South), Luxembourg, Madagascar, Malaysia, Malta, Mexico, Morocco,
  Myanmar, Netherlands, Niger, Nigeria,Norway, Pakistan, Paraguay, Peru,
  Philippines, Portugal, Senegal, Singapore, South Africa, Spain, Sri
  Lanka, Suriname, Sweden, Switzerland, Thailand, Togo, Trinidad and
  Tobago, Turkey, United Kingdom, USA, Uruguay, Venezuela, Zimbabwe. Here the dependent variable is the corruption index. They use the International Country
Risk Guide (ICRG), that ranges from $0$ to $6$. Here too, a higher score means less corruption. They regress it (among various controls), on inflation variance (since what they want to test is whether an unstable price system introduces noise into transactions that facilitates corruption). They find statistically significant regression coefficients with values around $0.5$. They even find that inflation variability correlates more strongly with corruption, than inflation itself. Lots of references. Dreher, A., & Herzfeld, T. (2005). The economic costs of corruption: A survey and new evidence. Public Economics, 506001.
A more general study on the costs of corruption, looking also at inflation. For their empirical study (71 countries, 1975-2001), they also use the ICRG index. They regress the level of inflation on this index, and they find a positive sign on the coefficient. Since the index is as described above, a non-corruption index, higher values mean less corruption. So what they find is that less corruption is correlated with higher inflation. Irrespective of whether this can stand reason, their findings are also incredibly large: if you move up the ladder by one point in the ICRG scale (less corruption), the corresponding inflation will be 10 percentage points higher(i.e. if it was, say, 5%, it will become 15%). This is too large to be believable even if the direction of association is to be accepted. Despite this, this is a survey, so lots of references.   Blackburn, K., & Powell, J. (2011). Corruption, inflation and growth. Economics Letters, 113(3), 225-227.
This is a theoretical model. as the authors write: ""We present a model in which the embezzlement of tax revenues by public
  officials leads the government to rely more on seigniorage to finance
  its expenditures. This raises inflation which depresses investment and
  growth via a cash-in-advance constraint."" So here we have a causative theoretical argument about why increased corruption causes increased inflation. Bittencourt, M. (2012). Inflation and economic growth in Latin America: some panel time-series evidence. Economic Modelling, 29(2), 333-340.
Four Latin American countries (Argentina, Bolivia, Brazil, Peru) for the period 1970-2007. As the authors note, these four countries account for $\approx 70$% of total GDP and population in South America (for 2009). The dependent variable on their regressions is the growth rate, while both inflation and a composite ""political situation"" variable are included as regressors. The ""political situation"" variable again increases with increased transparency and controls on power, and so it can be expected to correlate negatively with corruption (i.e. the higher its value, the less corruption). For our purpose we need to look not at the regression results but at the correlation matrix of the regressors: there we see that the political situation variable is negatively correlated with inflation, with correlation coefficient $-0.142$ (so again, more corruption is associated with more inflation). Overall, it appears that scholars have empirically detected a positive correlation between the level of corruption and the level of inflation, and also offered theoretical insights on how this can come about, although one theoretical model argues in favor of a causal effect from inflation to corruption, while the other from corruption to inflation. Since both arguments seem reasonable, one could think that it may be the case of a vicious feedback spiral."
Game theory for showing interest and availability when dating,"Sending costly signals may work, at least when the recipient is less attractive than the sender. There's also a nice popular science book by Paul Oyer called Everything I Ever Needed to Know About Economics I Learned from Online Dating that covers some of this ground, including the paper linked above. Another theoretical paper suggests that costly signals that are worthless to the recipient work nicely, because the cost signals to the recipient that the donor has resources and values her highly, but by being worthless, it screens out ""gold-diggers"" that merely want the gift. Perhaps setting a pile of cash on fire would do the trick?"
Can the stock market grow faster than GDP indefinitely?,"There are trends that has allowed stock markets in advanced economies to grow faster than GDP for a long time: Branching out abroad. This gives access to faster growing markets in developing countries. This trend will end when all countries are advanced economies. Fewer private companies. This trend will end when most of GDP is generated by companies listed on the stock market. Multiple Expansion. Reduced interest rates and increased money supply can reduce returns expectations and increases PE multiples. This trend has worked both ways through time but for the last 10-20 years it has generally acted in an expansive capacity. In addition, there is a way for the stock market to generate a higher return  without an increase in stock market value:"
What are the known / alleged problems against using energy as currency?,"The problems with using an energy-back currency are probably the same problems as using gold or anything else.  Some of those mentioned in the link (Wikipedia article) include Mainstream economists believe that economic recessions can be largely mitigated by increasing the money supply during economic downturns. A gold standard means that the money supply would be determined by the gold supply and hence monetary policy could no longer be used to stabilize the economy. The gold standard is often blamed for prolonging the Great Depression, as under the gold standard, central banks could not expand credit at a fast enough rate to offset deflationary forces. Although the gold standard brings long-run price stability, it is historically associated with high short-run price volatility. It has been argued by Schwartz, among others, that instability in short-term price levels can lead to financial instability as lenders and borrowers become uncertain about the value of debt. The money supply would essentially be determined by the rate of gold production. When gold stocks increase more rapidly than the economy, there is inflation and the reverse is also true. The consensus view is that the gold standard contributed to the severity and length of the Great Depression."
Influential Theory in the Economics of Poverty,"To extend @Majoko's comment, you may be very interested in the book Poor Economics which discusses many of the issues you note. It specifically discusses issues with theory, and of course has a lot of empirical work to back it up. Perhaps on the other end of the spectrum is general equilibrium theory applied to poverty and the developing world. you should really look into Robert Townsend's work. His ""Thai Project"" is a massive dive into some of the questions you're asking. A reasonable place to start is probably with his books.  As an aside, I was originally hooked on his work by his fantastic application of microeconomic general equilibrium theory in The Medieval Village Economy, which has sadly gone out of print (although I see there are now some used copies finally selling for less than $70, which is nice). He takes very sparse data and asks what theory might be able to tell us -- then very carefully and purposefully walks the reader from a most basic model through many extensions to explore important possible variations of the medieval experience. It's really an excellent example of applied general equilibrium work, and how theory might still be tested/explored with little data. This isn't my area of research, but I'm very interested to see other answers appear here."
"Are there any economic peer-reviewed journals dedicated to, or at least committed to publishing, replication studies?","A notable journal that has a dedicated replication section is the Journal of Applied Econometrics, which contains a detailed description on their webpage. Hope this helps!"
What are the arguments against the rational expectations hypothesis?,"The Rational Expectations Hypothesis (REH) is an hypothesis about aggregate expectations. I believe it is illuminating to post here a lengthy quote (part 2) from Muth (1961) paper where REH originated (bold letters are our emphasis): 2. THE ""RATIONAL EXPECTATIONS"" HYPOTHESIS
Two major conclusions from studies of expectations data are the following:
  1. Averages of expectations in an industry are more accurate than naive models and as accurate as elaborate equation systems, although
  there are considerable cross-sectional differences of opinion.
  2. Reported expectations generally underestimate the extent of changes that actually take place.     In order to explain these phenomena, I
  should like to suggest that expectations, since they are informed
  predictions of future events, are essentially the same as the
  predictions of the relevant economic theory (We show in Section 5 that
  the hypothesis is consistent with these two phenomena). At the risk of
  confusing this purely descriptive hypothesis with a pronouncement as
  to what firms ought to do, we call such expectations ""rational."" It is
  sometimes argued that the assumption of rationality in economics leads
  to theories inconsistent with, or inadequate to explain, observed
  phenomena, especially changes over time (e.g., Simon 1959). Our
  hypothesis is based on exactly the opposite point of view: that
  dynamic economic models do not assume enough rationality.   The hypothesis can be rephrased a little more precisely as follows: that
  expectations of firms (or, more generally, the subjective probability
  distribution of outcomes) tend to be distributed, for the same
  information set, about the prediction of the theory (or the
  ""objective"" probability distributions of outcomes).   The hypothesis asserts three things: (1) Information is scarce, and the economic
  system generally does not waste it. (2) The way expectations are
  formed depends specifically on the structure of the relevant system
  describing the economy. (3) A ""public prediction,"" in the sense of
  Grunberg and Modigliani (1954), will have no substantial effect on the
  operation of the economic system (unless it is based on inside
  information). This is not quite the same thing as stating that the
  marginal revenue product of economics is zero, because expectations of
  a single firm may still be subject to greater error than the theory.   It does not assert that the scratch work of entrepreneurs resembles
  the system of equations in any way; nor does it state that predictions
  of entrepreneurs are perfect or that their expectations are all the
  same. ... I believe that it should be clear from the above that:
1) REH  is not an assertion about each separate individual, but about the properties of the ""prevailing"" expectation produced by the black-box combination of individual expectations.  In other words the REH is assumed, without really making any assumptions about individual rationality.   2) It has as much to do with  the ""internal consistency"" of the economic model itself, because by construction and without any economic assumptions, $E(X\mid I) = X+ e,\; E(e\mid I) =0 $.   The fact that the predominant economic model framework has been that of the ""representative"" (identical) consumer, nevertheless blurred the distinction between the aggregate expectation, and individual expectations on aggregate variables. This provided shallow ""micro-foundations"" to the REH, (shallow because it is not really micro-founded, that which essentially assumes away the need to aggregate), but also, it moved the debate into the arena of individual expectations formation and whether individuals use information efficiently or not, which raised valid objections as those mentioned in the answer by @EnergyNumbers.   But really, at the individual level, the hypothesis that individuals use the mathematical expected value comes essentially from Expected Utility theory, that predates the Rational Expectations, and has a debate on its own (also here in Economics.SE) Another set of ""arguments against"" the REH (which gave very interesting literature), was collected early on in the book ""Individual forecasting and aggregate outcomes - Rational Expectations examined"" 1983 R. Frydman and E. Phelps (ed). Of which I mention two:   1) Being an equilibrium concept, REH requires co-ordination of expectations formation (which is really not that realistic) or properties of Nash-equilibrium: this last insight gave us ""Eductive Expectations"" and some really thoughtful works by Roger Guesnerie.   2) The second one, which became rather more widely spread than Eductive Expectations, is ""Adaptive Learning"" (see ""Learning and Expectations in Macroeconomics"" By Evans and Honkapohja, 2001).
Adaptive Learning pointed out that REH assumes that economic agents know the structure of their environment perfectly. So in Adaptive Learning models we have the first systematic approach to model uncertainty : as economists, so economic agents do not know the environment perfectly, and they have to estimate it and learn it gradually (hence ""adaptive learning""). In this strand of literature, ""learning"" is done through econometric methods, mainly least-squares (which is a very intuitive least-distance mathematical approximation method). Roughly speaking, here agents' expectations are not the expected values, but the estimated expected values. This creates much more interesting and realistic dynamics, that some times may converge (someday) to an REH equilibrium (which makes Adaptive Learning a ""selection mechanism"" for the sometimes multiple REH equilibira), or to some other point, not predicted by REH.   Research into the issue of aggregate expectations formation and modeling is currently exploding, see for example another Frydman & Phelps (ed.) book, ""Rethinking Expectations"" (2012), in parallel with the emerging ""Post-Walrasian"" direction in Macroeconomics (see D. Colander (ed). Post-Walrasian Macroeconomics 2006).  "
What would be the effects of an expiration date on currency?,"A bill with an expiration date does not become worthless suddenly. Clearly its value would decrease over time until, just before the expiration date, it was next to worthless. Think about it: how much would you pay for a $1 bill that expires in 5 minutes? Certainly a lot less than you would for one that expires in 6 months. This constantly changing value would make it hard to use as a medium of exchange. Whenever buying or selling something, one would have to look not just at the face value of the bill, but also at the expiration date, to be able to calculate its real value. People would probably start carrying calculators next to their wallets. Cattle (which obviously ""expire"") were once used as currency, so it certainly could work. But that does not mean it is ideal."
Did previous researchers fail to detect the hot hand simply because of a statistical fallacy?,"(This answer was completely rewritten for greater clarity and readability in July 2017.) Flip a coin 100 times in a row. Examine the flip immediately after a streak of three tails. Let $\hat{p}(H|3T)$ be the proportion of coin flips after each streak of three tails in a row that are heads. Similarly, let $\hat{p}(H|3H)$ be the proportion of coin flips after each streak of three heads in a row that are heads. (Example at bottom of this answer.) Let $x:=\hat{p}(H|3H)-\hat{p}(H|3T)$. If the coin-flips are i.i.d., then ""obviously"", across many sequences of 100 coin-flips, (1) $x>0$ is expected to happen as often as $x<0$. (2) $E(X)=0$. We generate a million sequences of 100 coin-flips and get the following two results: (I) $x>0$ happens roughly as often as as $x<0$. (II) $\bar{x} \approx 0$ ($\bar{x}$ is the average of $x$ across the million sequences). And so we conclude that the coin-flips are indeed i.i.d. and there is no evidence of a hot hand. This is what GVT (1985) did (but with basketball shots in place of coin-flips). And this is how they concluded that the hot hand does not exist. Punchline: Shockingly, (1) and (2) are incorrect. If the coin-flips are i.i.d., then it should instead be that (1-corrected) $x>0$ occurs only about 37% of the time, while $x<0$ occurs about 60% of the time. (In the remaining 3% of the time, either $x=0$ or $x$ is undefined — either because there was no streak of 3H or no streak of 3T in the 100 flips.) (2-corrected) $E(X) \approx -0.08$. The intuition (or counter-intuition) involved is similar to that in several other famous probability puzzles: the Monty Hall problem, the two-boys problem, and the principle of restricted choice (in the card game bridge). This answer is already long enough and so I'll skip the explanation of this intuition. And so the very results (I) and (II) obtained by GVT (1985) are actually strong evidence in favor of the hot hand. This is what Miller and Sanjurjo (2015) showed. Further analysis of GVT's Table 4. Many (e.g. @scerwin below) have — without bothering to read GVT (1985) — expressed disbelief that any ""trained statistician would ever"" take an  average of averages in this context. But that is exactly what GVT (1985) did in their Table 4.
See their Table 4, columns 2-4 and 5-6, bottom row. They find that averaged across the 26 players, $\hat{p}(H|1M) \approx 0.47$ and $\hat{p}(H|1H) \approx 0.48$, $\hat{p}(H|2M) \approx 0.47$ and $\hat{p}(H|2H) \approx 0.49$, $\hat{p}(H|3M) \approx 0.45$ and $\hat{p}(H|3H) \approx 0.49$. Actually it is the case that for each $k=1,2,3$, the averaged $\hat{p}(H|kH)>\hat{p}(H|kM)$. But GVT's argument seems to be that these are not statistically significant and so these are not evidence in favor of the hot hand. OK fair enough. But if instead of taking the average of averages (a move considered unbelievably stupid by some), we redo their analysis and aggregate across the 26 players (100 shots for each, with some exceptions), we get the following table of weighted averages. The table says, for example, that a total of 2,515 shots were taken by the 26 players, of which 1,175 or 46.72% were made. And of the 400 instances where a player missed 3 in a row, 161 or 40.25% were immediately followed by a hit. And of the 313 instances where a player hit 3 in a row, 179 or 57.19% were immediately followed by a hit. The above weighted averages seem to be strong evidence in favor of the hot hand. Bear in mind that the shooting experiment was set up so that each player was shooting from where it had been determined he/she could make roughly 50% of his/her shots. (Note: ""Strangely"" enough, in Table 1 for a very similar analysis with the Sixers' in-game shooting, GVT instead present the weighted averages. So why didn't they do the same for Table 4? My guess is that they certainly did calculate the weighted averages for Table 4 — the numbers I present above, didn't like what they saw, and chose to suppress them. This sort of behavior is unfortunately par for the course in academia.) Example: Say we have the sequence $HHHTTTHHHHH…H$ (only flips #4-#6 are tails, the remaining 97 flips are all heads). Then $\hat{p}(H|3T)=1/1=1$ because there is only 1 streak of three tails and the flip immediately after that streak is heads. And $\hat{p}(H|3H)=91/92 \approx 0.989$ because there are 92 streaks of three heads and for 91 of those 92 streaks, the flip immediately after is heads. P.S. GVT's (1985) Table 4 contains several errors. I spotted at least two rounding errors. And also for player 10, the parenthetical values in columns 4 and 6 do not add up to one less than that in column 5 (contrary to the note at the bottom). I contacted Gilovich (Tversky is dead and Vallone I am not sure), but unfortunately he no longer has the original sequences of hits and misses. Table 4 is all we have."
Why do some news say China’s economy is bad yet still predicting its 2023 growth to be around 5 per cent?,"The only time I read the word bad in the linked news article is refering to The government has set a modest GDP growth target of around 5% for
this year after badly missing its 2022 goal. The Chinese economy is nowhere near as mature as the US economy (per capita income, factor productivities etc). A developing economy has a lot of room for improvements and growth. The Chinese economy grew around 8% to 10% for a long time. True, you cannot expect to grow like this forever, but cutting forecasts and lower expectations are still bad news, no matter at what level they happen."
Why isn't the cost of shoes affected by their size?,"Most of the price of creating a shoe is the cost of labour to make it and the cost to ship it to the store. The cost of the materials needed to make the shoe is negligible. So, they can ignore the difference in material costs without losing too much money. But why would they want to lose any money at all? Logistics. Keeping track of the different prices per shoe, printing the different labels, making sure the shoes are sold at the right price, paying people to determine how much of a price difference the company should charge are all things that cost money. So the company is looking at creating a giant logistical nightmare for themselves, costing them who knows how much money, all for a penny difference between a size 8 and a size 13. Trying to charge a different price would be a case of being 'penny smart; dollar stupid'."
Collusion and number of firms,"Iet's say we have n identical firms and an infinite horizon of time. The n firms sustaining the collusion, will find optimal to fix the same price $p_m$ where $p_m$ is the price of the monopoly level and we define $\frac{\Pi^m}{n}$  as the profits each firm is obtaining by sustaining the collusion in each moment t. Now, of course each firm can betray the others by fixing a price lower than $p_m$, namely $p_m-ε$, where ε is small, and by doing so, the firm will capture the entire demand because in this market the firms are doing the Bertrand competition. In other words, the firm by betraying the others, will get almost π_m at the time T=t. We will also assume that in all t > T no firms will make any profits, because they will punish the firm, by fixing the price in Bertrand competition. The firm will defect if: $π_m/n + δπ_m/n + δ^2π_m/n.... < π_m+0+0....$ Where δ is the discount factor. This can rewritten as: $(\frac{π_m}{n})(\frac{1}{(1-δ)}) < π_m$ We can now see that if n, the number of firms, increases then the profits by sustaining the collusion will decrease, so the above inequality will be more likely to be true. This means that a firm has less incentives to sustain a collusion when there are too many participants, because the profits will be divided among too many firms and the punishment will be seen as less heavy."
Will printing more money during COVID cause hyperinflation?,"Will printing more money during COVID cause hyperinflation? Likely not for several reasons. First, the article from economicshelp.org you mention is oversimplifying the economics, which is understandable as it is article written for non-economists, but to understand this issue we need to go little bit further. Inflation, is simply just positive change in price level and price level of an economy is determined by the equilibrium at money market. The money market equilibrium, in its simplified form (more complex models include expectations of the quantities as well but this should suffice for this answer) is given by equation of exchange (See Mankiw Macroeconomics pp 87) as: $$MV=PY.$$ Where $M$ is the money supply, $V$ velocity of money, $P$ price level and $Y$ output. Solving for price level and log-linearizing (so % changes in right hand side variables give us the % change in P) we get: $$\ln P=\ln M+\ln V−\ln Y.$$ Hence, it is true that inflation depends on increase in money supply but it also depend on changes in velocity of money and real output. If money supply increases by $30\%$ and real output decreases (due to pandemic) by $10\%$ and velocity drops by $40\%$ these effects cancel out and even $30\%$ increase in money supply would not lead to any inflation. In fact one of the main reasons why we did not see much inflation recently is that velocity of money dropped significantly as you can see from Fred data. It is not clear if velocity will pick up in near future and as long as it remains low it will be offsetting considerable amount of increase in money supply despite of significant drop of real output. Next, it is important to understand what hyperinflation is. Following Cagan The Monetary Dynamics of Hyperinflation, hyperinflation can be defined as an increase in price level that exceeds $50\%$ per month. This means that in terms of the simple model $\Delta M + \Delta V - \Delta Y  = 50\%$ per month.  There is simply not enough money creation going on  in most countries to hit that $50\%$ per month value to classify as hyperinflation. For example, in the US the money stock as measured by $M2$ increased during last year by $\approx 19\%$ (see Fred data). While, that is without doubt large increase relative to previous years, it is far cry from increase in money supply that would lead to hyperinflation, even taking into account drop in output due to covid. Hyperinflation would not be caused by changes in velocity either because even though it is possible it would pick up later velocity usually does not increase fast month by month. Lastly, if there would be any signs of hyperinflation independent central banks would start acting. In countries like Zimbabwe central banks are not independent and often they are forced to expand money supply rapidly by politicians. In most developed countries central banks are independent and for most part run by technocrats. Central banks have plethora of tools to sharply reduce money supply. Even though people often just label all money creation money printing this is really just analogy as it is easier to explain money supply increases to people that way, but in reality most money are created by banking system via lending and central banks control the amount of money that can be created by changes in interest rates on its reserves (see for example McLeay, Radia, & Thomas; 2014). Next money are also created when Fed borrows money to the government via its open market operations. Only very small fraction of money are actually printed each year and similar picture holds for most developed nations. Consequently, if Fed or any other modern independent central bank would spot signs of not just hyperinflation but even high inflation (like $10\%$ per year) they would quickly start hiking interest rates which would slow money creation and eventually even destroy part of new money, they would stop buying government bonds or even sell them again destroying part of money supply and so on. Independent central banks are quite good 'inflation fighting machines' and empirical literature, for the most part, shows that there is negative relationship between central bank independence and inflation (e.g. see Jácome & Vázquez; 2008). Of course, it is not literally impossible that the situation will be mishandled, and heads of central banks are still typically appointed by politicians in most countries  so there is possibility of political capture of central banks, but as the things look like now I would say that hyperinflation in most developing countries is astronomically unlikely in near future."
Macroeconomics for Mathematicians,"I really think a great place to start in general is Olivier Blanchard's ""Macroeconomics"". However, this is an economics undergraduate text and may not be optimal for a mathematician, especially given the structure you expect. More to your taste may be the following standard texts: ""Introduction to modern economic growth"" by Daron Acemoglu. ""Recursive Macroeconomic Theory"" by Lars Ljungqvist and Thomas J. Sargent ""Recursive Methods in Economic Dynamics"" by Nancy L. Stokey, Robert E. Lucas and Edward C. Prescott  ""Foundations of international macroeconomics"" by Maurice Obstfeld and Kenneth Rogoff. A bit old by now, but a classic and a good place to start for international economic topics. Alternatively, try: ""Open Economy Macroeconomics"" by Martín Uribe and Stephanie Schmitt-Grohé."
Open access datasets for teaching IV regression,"I can recommend this paper as an example: Daron Acemoglu, Simon Johnson, and James A. Robinson This example is famous not only thanks to the creative use of instrumental variables, but also because of the subsequent discussion about the validity of the instruments. And relevant discussions: Secondly, Angrist, Joshua D., and Alan B. Krueger We discuss the mechanics of instrumental variables and the qualities
  that make for a good instrument, devoting particular attention to
  instruments derived from ""natural experiments."" Examples follow."
"Does risk aversion cause diminishing marginal utility, or vice versa?","I think I've found an answer to my question, in this excerpt from Nobel laureate John C. Harsanyi's 1994 paper ""Normative validity and meaning of von neumann-morgenstern utilities"", presented at the Ninth International Congress of Logic, Methodology and Philosophy of Science.  Harsanyi starts by proving the same lemma that Alecos proved in his answer, namely that if $u$ is a vNM utility function of an individual, then $u(10) - u(5) < u(5) - u(0)$ if and only if they would prefer a guaranteed 5 dollars compared to a 50% of 10 dollars and a 50% chance of 0 dollars.  In the comments section I said that was insufficient to demonstrate that the vNM utility function represented intensity of preferences, because what if the individual's actual pleasure and pain was accurately described by some other utility function $v$, which is a monotonic transformation but not an affine transformation of $u$?  In that case couldn't $v$ fail to satisfy the expected value property, and couldn't $v(10) - v(5) = v(5) - v(0)$? Harsanyi has a clever argument dealing with this issue. Let $L_1$ be the lottery where you get 5 dollars guaranteed, let $L_2$ be the lottery where you have a 50% chance of 10 dollars and a 50% chance of 0 dollars, and let $L_3$ be the lottery where you have a 50% chance of 10 dollars and a 50% chance of 5 dollars.  Then obviously the person prefers $L_3$ to both $L_1$ and $L_2$.  And Harsanyi argues that $L_3$ is preferred to $L_1$ less strongly than $L_3$ is preferred to $L_2$ if and only if $v(10) - v(5) < v(5) - v(0)$.  That's because in the choice between, $L_3$ vs $L_1$, 50% of the time they get 5 dollars, and 50% of the time they have to make a choice between 10 and 5.  Similarly in the choice between $L_3$ and $L_2$, 50% of the time they get 10 dollars, and 50% of the time they have to make a choice between 5 and 0.   Now here comes the master stroke: $L_1$ is preferred to $L_2$ if and only if $L_3$ is preferred to $L_1$ less strongly than $L_3$ is preferred to $L_2$.  Therefore, $L_1$ is preferred to $L_2$ if and only if $v(10)-v(5) < v(5) -v(0)$.  And thus we reach the grand conclusion that $u(10) - u(5) < u(5) - u(0)$ if and only if $v(10)-v(5) < v(5) -v(0)$. Thus Harasanyi reaches the conclusion that the vNM utility function represents preferences intensities.  So the answer to my question seems to be that diminishing marginal utility in the vNM utility function reflects genuine diminishing marginal utility when it comes to intensity of preferences, and thus (assuming the vNM axioms are true) diminishing marginal utility really is the cause of risk aversion. By the way, on a side note I wonder whether we could identify the set of all functions $v$ that satisfy the constraint that $u(x) - u(y) < u(z) - u(w)$ if and only if $v(x)-v(y) < v(z) -v(w)$ (and similarly for greater than and equal to).  (EDIT: I asked about this on Mathematics.SE here.)"
Why is Roy's Identity so important?,"It is not that surprising if you have the right intuition, but let's make sure we consider it unsurprising for the right reasons. Roy's identity can be rewritten as $$x^*_{i}(\text{p},m)\frac{\partial v}{\partial m}=-\frac{\partial v}{\partial p_{i}}.$$
The right-hand side is the marginal utility loss from a price increase. The left-hand side is the amount of good $i$ consumed times the marginal utility of money. If the consumer were forced to consume exactly $x^*_{i}(\text{p},m)$ units of good $i$, this would be obvious.  The consumer has to pay proportionally more of the income on buying good $i$, and cannot spend it on other things whose marginal utility equals the marginal utility of money.  Of course, the consumer can make adjustments and is not forced to hold the amount fixed. But, similar as in Hotelling's lemma, the consumer who is at an optimum need not do any adjusting, indirect effects are negligible. And showing the latter is more than just evaluating utility at the appropriate Marshallian demands."
Gross substitutes vs. net substitutes,"Intuitively, a higher price for pears means that I have to give up more apples to be able to afford an extra pear (or, conversely, if I give up one pear then the number of extra apples that I can afford increases). This is going to make me want to reduce my pear consumption and increase my apple consumption (in orther words, to substitute away from pears towards apples). Graphically, this corresponds to a change in the slope of the budget constraint and is known as the (Hicksian) substitution effect. This substitution effect is, however, moderated by a second consideration. If I increase one or more prices then the total amount of ""stuff"" that I can afford to buy decreases, so it is as if my income has decreased (which would correspond to a shift of the budget constraint towards the origin). This is known as the income effect and will usually mean that my consumption of apples decreases if the price of pears rises. When the income and substitutes effects are put together, you get the total effect of an increase in the pear price upon the demand for apples. This total effect gives rise the the notion of gross substitutes: apples and pears are gross substitutes if the following is true increasing the price of pears causes the consumer to demand more apples. Formally, this is written as $$\frac{\partial X_a}{\partial p_P}>0,$$ where $X_A$ and $p_P$ are the demand for apples and the price of pears respectively. To arrive at the notion of net substitutes we simply take a price change and shut-down the income effect. The hypothetical exercise works like this: Two goods are net substitutes if, after making this adjustment, we find that the demand for apples has increased. Thus, we say that two produces are net substitutes if Increasing the price of pears while compensating the consumer for the resulting decrease in his real income causes the consumer to demand more apples. Formally, $$\left.\frac{\partial X_a}{\partial p_P}\right|_{\text{constant }U}>0,$$ where $X_A$ and $p_P$ are the demand for apples and the price of pears respectively, and $U$ is the consumer's utility. It is not true to say that ""a competitive equilibrium [does] not exist if the products are net-substitute"". Indeed, most of the time, products that are gross substitutes will also be net substitutes as well. Thus, most examples of gross substitute preferences supporting a competitive equilibrium will also be examples of net substitutes doing the same. The reason why it doesn't make sense to state the existence condition for competitive equilibrium in terms of net substitutes is that net substitutes is a purely hypothetical construction in which a fictitious agent intervenes to shut down the income effect and keep the consumer's utility constant. The whole point of a competitive equilibrium is that there is no such intervention: the equilibrium is entirely decentralised and is sustained purely by finding prices such that the market clears when consumers pick their optimal demand."
Wouldn't abolition of cash give rise to a substitute currency?,"""Cash"" is an emergent phenomenon of human economic organization. It exists for lots of reasons, as a provider of economic anonymity, a low transaction cost solution to the double-coincidence of wants, a portable medium of exchange, and a tool of economic accessibility for all including those in the informal economy, foreigners, the unbanked, and those with poor credit, among others.  A country can make it difficult to use cash in the interest of thwarting anonymity and convenience, usually to prevent money laundering and tax evasion. They can do so by taxing the use of cash, failing to enforce contracts calling for cash, refusing to make more, even criminalizing the possession of cash.  But cash is unlikely to die easily. We know from the experience with drug and alcohol prohibition as well as the international ivory and rhino horn trades that while the regulation can can certainly influence prices and thereby quantities demanded, it is awfully difficult to destroy a market for a valuable product. For a more specific example, possession of money is forbidden to US convicts. Nevertheless, they have developed internal monetary economies based around canned fish, shelf-stable pastries, as well as cigarettes and stamps. Cigarettes also emerged as money in WWII P.O.W. camps, showing that even the Nazis couldn't keep a monetary economy from forming. Sweden in particular has a significant additional obstacle to keeping currency out of their economy. They share boarders with Norway and Finland, each with their own currency. Go to practically any boarder region or tourist spot around the world and you'll see tellers happy to take dollars, euros, or yen in addition to the local currency. It is especially difficult to ditch cash when other sources of cash are available. "
When can one safely talk about decreasing marginal utility?,"The concept of ""marginal utility"" (and therefore of decreasing such) has meaning only in the context of cardinal utility. Assume we have an ordinal utility index $u()$, on a single good, and three quantities of this good, $q_1<q_2<q_3$, with $q_2-q_1 = q_3-q_2$.
Preferences are well behaved and satisfy the benchmark regularity conditions, so $$u(q_1)< u(q_2) < u(q_3)$$ This is ordinal utility. Only the ranking is meaningful, not the distances. So the distances $u(q_2) - u(q_1)$ and $u(q_3) - u(q_2)$ have no behavioral/economic interpretation. If they don't, neither do the ratios $$\frac {u(q_2) - u(q_1)}{q_2-q_1},\;\; \frac {u(q_3) - u(q_2)}{q_3-q_2}$$ But the limits of these ratios as the denominator goes to zero would be the definition of the derivative of the function $u()$. So the derivative is devoid of economic/behavioral interpretation, and so comparing two instances of the derivative function would not produce any meaningful content.   Of course this does not mean that the derivatives of $u()$ do not exist as mathematical concepts. They can exist, if $u()$ satisfies the conditions needed for differentiability. So one can ask the purely mathematical question ""under which condition the  function representing ordinal utility has strictly negative second derivative"" (or negative definite Hessian for the multivariate case), trying not to interpret it as ""decreasing marginal utility"" with economic/behavioral content, but as just a mathematical property that may play some role in the model he examines.   In such a case, we know that:
1) If preferences are convex, the utility index is a quasi-concave function
2) If preferences are strictly convex, the utility index is strictly quasi-concave But quasi-concavity is a different kind of property than concavity: quasi-concavity is an ""ordinal"" property in the sense that it is preserved under an increasing transformation of the function.   On the other hand, concavity is a ""cardinal"" property, in the sense that it won't necessarily be preserved under an increasing transformation.
Consider what this implies: assume that we find a characterization of preferences such that they can be represented by a utility index which is concave as a function. Then we can find and implement some increasing transformation of this utility index, that will eliminate the concavity property."
"Intuitive explanation of $S(p,w)\cdot p=0$","This is a general mathematical property of the second derivative/Hessian matrix of multivariate functions that are homogeneous of degree one.   The Expenditure function $E$ is homogeneous of degree one in prices. Why?
If all prices change in the same proportion (which is how we check for the mathematical property of homogeneity), relative prices do not change. If relative prices do not change, the quantitative composition of the minimum-cost compensated consumption bundle in order to achieve a given utility does not change at all. Then, since all prices have increased by the same proportion, budget shares remain the same, and the Expenditure needed to achieve the same utility, rises in that same proportion: homogeneity of degree one.  By duality, the Hicksian demand vector is the gradient of the Expenditure function, $H = \nabla_p E$. The Hicksian demand vector, gives us minimum-cost quantities demanded. Due to the homogeneity of degree one of the Expenditure function, the inner product of the Hicksian demand vector times the price vector equals the Expenditure function. This also should be intuitive: we just multiply each quantity demanded by the unit price that must be paid for it, and by summing these products we obtain the total Expenditure we must incur in order to acquire the minimum-cost bundle for given utility. So we have  (simplifying differentiation notation) $ E = H\cdot p$ while also $\frac {\partial}{\partial p}E = H$. Therefore also  $$\frac {\partial}{\partial p}(H\cdot p) = H \implies H + \frac {\partial H}{\partial p}\cdot p = H$$ and it must be the case that  $$\frac {\partial H}{\partial p}\cdot p = 0$$ So the Hicksian demand vector is homogeneous of degree zero in prices (mathematically, this is a consequence of Euler's theorem for homogeneous functions, i.e. that if a function is homogeneous with degree of homogeneity $k$, its gradient has degree of homogeneity $k-1$). But the 1st derivative (Jacobian) of Hicksian demand (which is the Hessian-matrix of second derivatives of the Expenditure function) is the Slutsky matrix, $\frac {\partial^2 E}{\partial p^2}=\frac {\partial H}{\partial p} = S(p,w)$. So $S(w,p) \cdot p = 0$. So the result stems from the homogeneity of degree one of the Expenditure function. Is there an intuitive explanation, analogously with the intuition behind the homogeneity of degree one of the Expenditure function? Well, the former comes directly from the latter, so it is difficult to come up with a ""separate"" intuitive argument. One could informally say that compensated quantities demanded are ""independent"" of (not affected by) price variation when relative prices remain the same. Then in geometric terms, this means that the vectors of rates of change of compensated quantities demanded (which is what each row of the Slutsky matrix contains), are orthogonal to the price vector."
Can the Machina Paradox be solved by expanding the choice set?,"No, I would not say that this resolves the Machina paradox, because it is exactly the same as the Machina paradox: the paradox does indeed require from you to look at the three possible outcomes. The M-C/W/G book discuss only the $B$ and $C$ outcomes because it is there where the paradox focuses on whether a violation of the axiom of independence may happen.   But most importantly, Machina did not argue that all people will have preference ordering $A>C>B$. He argued that it is reasonable, for evident psychological reasons, to expect that some people may... So some others will have the ordering $A>B>C$, which does conform to expected utility framework. The first will say ""I cannot watch a movie about Paris after losing the trip - I will smash the TV!"" The second will say ""Well, tough luck. At least I will see it on screen and keep dreaming about it"". Both seem like behaviors that could be anticipated by ""usual"" human beings.  The point of the paradox is not to show that Expected Utility (EU) is invalid for all people -only that it may be violated in reasonable situations, i.e. situations that may characterize a lot of people and may happen often.   What paradoxes like this examine and contemplate, is the degree to which EU represents adequately the ""majority"" of people in some sense, and so whether it is valid/useful/not-misleading as a core theoretical assumption in economic models, or not.
And this is a matter of degree, a quantitative matter. This is true for almost all assumptions in theoretical models in social sciences. "
Inflation and economic growth,"Macro regressions, especially annual ones, have in general two flaws: In order to circumvent problem #1, people often assume that the DGP process behind different countries are the same, increasing observations from perhaps 60 to 600. In order to attack #2, many people add timing assumptions. However, this is still no clear identification, these are still assumptions. To see the issue: Imagine high inflation at $t-5$, and low growth at $t$. Using a standard timing assumption, the former caused the latter. However, we like to think about agents as forward looking. Can we be certain that expectations w.r.t. the latter didn't influence the former? Especially because of #2, this type of regressions have lost popularity. Also, there is not really much more to do.  tl;dr: No important new results that I know of. "
What are some reasons for decline of georgism?,"Warren Samuels addresses this issue in his article ""Why the Georgist Movement Has Not Succeeded: A Speculative Memorandum."" In summary he argues first that Georgism did not succeed because of the conflation of the ideas of income and productivity, leading people to view a tax on land as a tax on productivity even though the tax is meant to equate income with productivity as referenced in the question. This idea, he says, was reinforced by self-interested beliefs of land owners as well as traders in other markets (he names equities) including preference for passing property through inheritance and viewing the proposed tax system as a threat to income through speculation (such as through the stock market), since it would eliminate speculative gains in land prices. The rise of mortgages and home-ownership increased the number of people with these self-interests. Second, Samuels addresses the fact that George was viewed by many, including mainstream economists, as a radical and admits that relative to the status quo of changes in tax policy being incremental, the proposed single tax system is radical. The Bolshevik revolution in the 1910's and subsequent red scare caused people to distance themselves from ideas that could be viewed as radical leftist.  Samuels continues on to discuss possible internal reasons, such as lack of leadership amongst George' successors, but notes that he is less confident in discussing these factors.  In addition to this article, four responses and Samuel's response to the respondents can be found in the Jul 2003 issue of American Journal of Economics and Sociology."
The Case for Basic Income in developed and underdeveloped countries,"Thanks to densp for identifying this paper. It refers to a major pilot project undertaken in Namibia Background: The Basic Income Grant (BIG) pilot project took place in the Otjivero-Omitara area, about 100 kilometres east of Windhoek. All residents below the age of 60 years receive a Basic Income Grant of N$100 per person per month, without any conditions being attached. The grant is being given to every person registered as living there in July 2007, whatever their social and economic status. Before the pilot project, the area was characterised by unemployment, hunger and poverty.  Conclusions: Since the introduction of the Basic Income Guarantee (BIG), household poverty has dropped significantly. Using the food poverty line, 76% of residents fell below this line in November 2007. This was reduced to 37% within one year of the BIG.  There was a dramatic increase in economic activity. The rate of those engaged in income generating activities (above the age of 15) increased from 44% to 55%. Thus the BIG enabled recipients to increase their work both for pay, profit or family gain as well as self-employment. The grant enabled recipients to increase their productive income earned, particularly through starting their own small business, including brick-making, baking of bread and dress-making. The BIG contributed to the creation of a local market by increasing households' buying power. This finding contradicts critics' claims that the BIG would lead to laziness and dependency. Huge reduction in child malnutrition from 42% to 17% in 6 months. Increase in school attendances (non attendance due to financial reasons dropped 42%) Drop in crime (theft down 42%) Estimated cost for a nationwide program in Namibia would be 2-3% of GDP. Not cheap, not unaffordable either. All thing considered, the author of this report seemed pretty happy with the outcome. There were some issues impacting the data. Since it was a localised study, there was a fair bit of migration towards the treatment area. The long term impacts are not yet known as well. Additional resources: Haarmann, Claudia; Haarmann, Dirk; Jauch, Herbert; Mote Hilma et al 2008. Towards a Basic Income Grant for all. Basic Income Grant Pilot Project. First Assessment Report, September 2008. Windhoek Kameeta, Zephania; Haarmann, Claudia; Haarmann, Dirk; Jauch, Herbert 2007. Promoting employment and decent work for all - Towards a good practice model in Namibia. - Research Paper - Presentation to the United Nations Commission for Social Development. Windhoek  Haarmann, Claudia; Haarmann, Dirk (ed.) 2005. The Basic Income Grant in Namibia. Resource Book. Windhoek"
Difference-in-differences in 2SLS regression,"Well, if you believe that treatment is endogenous (which depends on the problem at hand here and is not an inherent feature of the model), then using eligibility as an instrumental variable will help you to get rid of the biases due to the safe selection in treatment. (Incidentally, DID is intended to do the same, but won't do as good a job as a well chosen instrument, so there is some doubts whether applying both of them is better then resorting to only one). However it is up to you to decide whether eligibility is exogenous, as it well may be, that those who are expecting higher return to treatment made sure to be eligible. Taking that we believe that there are some biases that arenot eliminated by DID and that eligibility can help us, there is still considerations of efficiency. In many cases eligibility may happen to be a weak instrument and then the reduction is bias will come at a cost of significant efficiency loss. And taking a look at the particular specification that you have sugested, it seems not very reasonable in general setting. You may choose when you believe that eligibility is changing quickly, or the interaction term in second equation will be generally unhelpful. Inclusion of time After in that equation can have even more drastic consequences, as it is likely to be endogenous and will weaken the bias reduction effect. If not endogenous, it is likeliy to be negligible as well as interaction, unless Treatment is rapidly changing on it's own. So in this case I would recommend leaving only the eligibility as an instrument in the first equation and specifying the third one in a DID form. With respect to interpretation, my specification does not allow for a nice interpretation of difference in changes in two subgroups and should be interpreted as a difference in changes in two hypothetical subgroup where each person is divided between them with some weights. Your specification, however, loses all interpretation as DID, because you do not use the resulting interaction coefficient, but just employ more variables as instruments for treatment. Unfortunately, probably due to the aformentioned reasons, I was unable to recall or find any appropriate paper, sorry about that."
Are There Giffen Inputs?,"I believe the answer is true. Giffen goods are goods where the income effect overpowers the substitution effect. $$\begin{align}
\max_{\vec x} \ \ \ & U(\vec x) \\
& \text{s.t.} \ \ \ \vec p \cdot \vec x \leq I
\end{align}$$ To start, if you think about the consumer's problem (for example utility maximization, here), a change in a good's price affects both relative substitutability of goods through the marginal rate of substitution AND it affects purchasing power through the budget constraint. Let us consider a profit maximizing firm with a constraint on how much they can spend. For simplicity let us use a single output technology, with differentiable production function $f(\vec z)$. Let $\vec z$ be a vector of inputs (expressed as negative values), $\vec w$ a vector of input prices, and $p$ the output price. $$\begin{align}
\max_{\vec z} \ \ \ & pf(\vec z) + \vec w \cdot \vec z \\
\text{s.t.} & \ \ \ \vec w \cdot \vec z \leq B \\
& \ \ \  z_i \leq 0 \\
\end{align}$$ Normally we would have a constraint on production, but instead we have a ""budget"" constraint. What happens if we form the Lagrangian here? $$\mathcal{L} = pf(\vec z) - \vec w \cdot \vec z - \lambda(\vec w \cdot \vec z - B) + \vec\mu \cdot \vec z$$ Take first order conditions: $$\frac{\partial \mathcal{L}}{\partial z_i} = pf_{z_i}(\vec z) - w_i - \lambda w_i + \mu_i = 0 \tag{1}$$ $$\frac{\partial \mathcal{L}}{\partial f(\vec z)} = p = 0 \tag{2}$$ $$\frac{\partial \mathcal{L}}{\partial \lambda} = \vec w \cdot \vec z - B = 0 \tag{3}$$ At an interior solution where the budget constraint binds, we should have the optimum $\vec z^*$ to solve the FOCs $$p \frac{\partial f(\vec z^*)}{\partial z_i} = w_i$$ but instead you solve out (1): $$p \frac{\partial f(\vec z^*)}{\partial z_i} = - \frac{\mu_i}{1 + \lambda}w_i$$ and (3) does not provide any help to solve the Lagrangian multipliers. (2) is nonsense. A better constraint would be something like $y - f(\vec z) \leq 0$, where $y$ represents the scalar of output. Without an ""income effect"", there isn't much to study Giffen behavior. Producer theory doesn't use a budget constraint to solve these sorts of problems. Increasing input price will always decrease use of that input except with corner solutions, where there might be no change. So there can't be a Giffen input."
Under what conditions is a monopoly undesirable?,"Firstly, suppose we take a utilitarian welfare standard that is linear in money. That is to say, suppose that both utility and profits are linear in the amount of money that consumers and firms have (but not necessarily linear in anything else). In that case, the Pareto standard and the utilitarian social welfare criterion coincide exactly! You can see a nice video of Jeff Ely talking about the intuition for this result here (look at the video titled ""efficiency""). Intuitively, if both the utility and profits are linear in money then we can always maximise utilitarian welfare by implementing the Pareto optimum and then constructing side-payments to support it. Now, the answer to the question of when a monopolist is undesirable depends on the richness of the model that one has in mind. In a very basic textbook model of a monopolist, FooBar's criterion is a good one. We know that an ordinary competitive market equilibrium maximises total welfare (the shaded area in the below figure) with price equal to marginal cost (n.b. the supply curve and the marginal cost curve are essentially the same thing):  Since the profit that a monopolist makes from a unit is equal to difference between its price and its marginal cost, the monopolist will tend to set price above marginal cost. This results in a higher price (the green $p$) and a reduction in total welfare (given by the shaded area in this figure) and consumer surplus (the triangle between the price line and the demand curve):  Since this kind of analysis is standard, it makes sense to think of the case in which a monopolist reduces consumer surplus and welfare as being a kind of 'default' and instead ask ""when might a monopolist be desirable""? Here are some situations in which having a monopolist might be better than having very intense competition: Those are a few of the most common ways in which a monopolist might be preferable to competition. I am sure there are others that don't come to mind right now. The main message is that you have to look at the specific market context to evaluate whether there might be any problems associated with intense competition. Most often the answer will be that there are not, and the usual assumption is therefore that monopolists are undesirable unless we have good reason to think otherwise."
Dynamic Optimization: What if the second order condition does not hold?,"There is not a single answer, it will depend on the particulars of each problem. Let's look at a standard example.   Consider the benchmark intertemporal optimization problem for the Ramsey model $$\begin{align}
&\max_u \int^{\infty}_0{e^{-\rho t}u(c)dt}\\
\\
& \text{s.t.}\;\; \dot{k} = i-\delta k\\
& \text{s.t.}\;\; y = f(k)=c+i
\end{align}$$ The current value Hamiltonian is  $$\tilde H = u(c) +\lambda [f(k)-c-\delta k]$$ Maximizing over $c$ alone we have $$\frac {\partial \tilde H}{\partial c} = u'(c) - \lambda =0 \implies u'(c^*) = \lambda \implies c^* = (u')^{-1}(\lambda)$$ and the 2nd-order condition will hold if the utility function is concave, 
$$\frac {\partial^2 H}{\partial c^2} = u''(c^*) < 0$$ Moreover, from the first-order condition with respect to consumption, $\lambda >0$ if local non-satiation holds. Assume that we do have such ""usual"" preferences. The maximized over consumption Hamiltonian is $$\tilde H^0 = u[(u')^{-1}(\lambda)]+\lambda [f(k)-(u')^{-1}(\lambda)-\delta k]$$ The partial derivatives with respect to the state variable, $k$ are $$\frac {\partial \tilde H^0}{\partial k} = \lambda[f'(k) - \delta],  \;\;\;\; \frac {\partial^2 \tilde H^0}{\partial k^2} = \lambda f''(k)$$ So here, the Arrow-Kurz sufficiency condition boils down to whether the marginal product of capital is decreasing, constant, or increasing (which will depend on the sign of the second derivative of the production function). In the standard case $f''(k) < 0$ and we have the sufficient condition. In the most famous case of deviation, Romer's $AK$ model that initiated the Endogenous Growth literature, $f''(k) =0$, and the marginal product of capital is a positive constant. So what can we say in this case?   Here, 
Seierstad, A., & Sydsaeter, K. (1977). Sufficient conditions in optimal control theory. International Economic Review, 367-391. provide various results that can help us. In particular, they prove that if the Hamiltonian is jointly concave in $c$ and $k$, it is a sufficient condition for a maximum.
The Hessian of the Hamiltonian is (we can ignore the discount term)  $${\rm He}_H = \left [ \begin{matrix} u''(c) & 0\\
0 & \lambda f''(k)\\
\end{matrix} \right]$$ In the standard case with $u''(c) <0, \; f''(k) <0$ this is a negative definite matrix and so the Hamiltonian is jointly strictly concave in $c$ and $k$.   When $f''(k) =0$, checking that the matrix is negative-semidefinite is straightforward using the definition. Consider a vector $\mathbf z = (z_1, z_2)^T \in \mathbb R^2$ and the product $$\mathbf z^T{\rm He}_H\mathbf z = z_1^2u''(c) \leq 0$$ this weak inequality holds $\forall \mathbf z \in \mathbb R^2$, and so the Hessian is jointly concave in $c$ and $k$. So in the $AK$ model of endogenous growth, the solution is indeed a maximum (subject to the parameter constraints needed for the problem to be well-defined of course)."
How does GNU software development sustain economically?,"I would like to start by saying that I'm not a programmer and I have never contributed to any open source project. However, I have been interested in open source for a long time and I believe that I understand the general concepts of open source and how it works.  To start of, I would like to say that open source does not mean that you cannot make money on the software. It just means that the code has to be publicly available. Companies like Red Hat and Canonical make money not by selling the software, but by selling their expertise. If i wan't my company to run a Linux server, I can get the software for free. But I need somebody to install it, set it up and give support. This is where specialist from e.g. Red Hat comes in and makes money. For the company it makes sense, because hiring their own specialist would probably be much more expensive. This also gives these companies an incentive to contribute the the code. They want their product to be good so people will use it and by their services.  But lets talk about your points about scalability.  The cool thing about open source is that you do not have to develop everything from scratch. An operating system like Ubuntu has not been build by a single person. Instead a lot of people has contributed to different parts of the system (actually I think it would be hard to find one person with all the skills to make and effective operating system). For example, the Ubuntu people does not develop the Linux kernel. They just use one developed by others. So what was without open source probably impossible, is now possible,because you can build on other peoples work.  Reading and understanding others code it not more time consuming than writing your own. At least not in many cases. Beyond that, you do not have to understand all the code you use. If I want to write a program for Linux, I do not have to understand how all the parts in that program works in detail. I just have to know what they do. I can then take these parts and put them together with other parts to create my program. Or I can take an existing program and modify it for my needs. tools like git and github makes it incredibly easy to collaborate. You just get the code and make modifications. You then submit them to the person in charge of the project. If it is good, it will be accepted.   people go in and out of projects all the time. But if the project is popular, enough will be working on it.  Here are some reasons why open source works.  I think the main reason that open source software has become so good is that the  large number of people working on the a project, insures a level of expertise that i hard to archive in a small team of developers. While it might seem weird, this single fact, seems to outweigh all the negative problems that can arise in open source.  In commercial programming, the project dies with the firm. Lets say you  by some software from a company that then closes. Then your screwed, as you will not receive updates and bug fixes, and you will need to by new software to keep up. With open source you can just find another company to support you software or develop it yourself.  If you are still interested, I suggest you read 
The Cathedral and the Bazaar"
Calculus and Indifference Curves in an Urban Economics Example,"The utility function under consideration is $v(c,q)$ and then $$MRS(c,q) = \frac{\partial v/\partial q}{\partial v/\partial c} = v_2/v_1$$ make the functional denpendency of on $u$ explicit then you have $$\frac{\partial}{\partial u}MRS(c(u),q(u)) = \frac{\partial MRS(c(u),q(u))}{\partial c} \frac{\partial c(u)}{\partial u} + \frac{\partial MRS(c(u),q(u))}{\partial q} \frac{\partial q(u)}{\partial u} = \frac{\partial p(u)}{\partial u},$$ where the last identity follows because you know that $p = MRS$. Now simply rearrange the identity $$\frac{\partial MRS(c(u),q(u))}{\partial c} \frac{\partial c(u)}{\partial u} + \frac{\partial MRS(c(u),q(u))}{\partial q} \frac{\partial q(u)}{\partial u} = \frac{\partial p(u)}{\partial u},$$ to get $$ \frac{\partial q(u)}{\partial u} = \frac{\left[\frac{\partial p(u)}{\partial u} - \frac{\partial MRS(c(u),q(u))}{\partial c} \frac{\partial c(u)}{\partial u}\right]}{\frac{\partial MRS(c(u),q(u))}{\partial q} },$$ then use that Brueckner has defined $\eta := \left[\frac{\partial MRS(c(u),q(u))}{\partial q}\right]^{-1}$ in footnote(3) to get $$ \frac{\partial q(u)}{\partial u} = \left[\frac{\partial p(u)}{\partial u} - \frac{\partial MRS(c(u),q(u))}{\partial c} \frac{\partial c(u)}{\partial u}\right] \eta ,$$ and finally apply the rule that $\frac{\partial c(u)}{\partial u} = \frac{1}{\partial v/\partial c} = 1/v_1$ to get $$ \frac{\partial q(u)}{\partial u} = \left[\frac{\partial p(u)}{\partial u} - \frac{\partial MRS(c(u),q(u))}{\partial c} \frac{1}{v_1}\right] \eta.$$"
second order stochastic dominance without the same mean,"Let $u(x) = x$ which is increasing and concave. Then the defining condition of SOSD reads $$\int x\mathrm dF(x)\ge \int x\mathrm dG(x) \implies E_F(X) \geq E_G(X) \tag{1}$$ ..which would contradict the case  $E_F(X) < E_G(X)$ that would be permissible under a general ""different means"" postulate. On the other hand, we see that the ""same mean"" condition can be violated by the defining condition for SOSD itself. What does that tell us? 1) That $E_F(X) \geq E_G(X)$ is a necessary condition for $F$ to SOSD $G$.   2) ...And so that the requirement ""$F$ and $G$ have the same mean"" wrongly restricts the application of the concept of SOSD."
Why does savings equal investment (scenario)?,"To clarify the first scenario: you assume Joe can buy this car. However, Joe has to either earn an income (and therefore, produce something himself) or he has to dissave. In the first case, Joe has produced 500 output and earned an income of 500. He spends his income on a car, but Amanda saves the money. As is clarified in other answers, Joe's output goes to inventories (because not purchased by Amanda), which are counted as (unplanned) investments. S = I = 500, C = 500, Y = S + C = 1000. You mention in a comment that most consumers do not produce something. In fact, they do. At work, when earning their income (with which they buy the car), they created added value (this is the production (500) that goes to the inventory when Amanda does not buy it). Think of the income they earn at work as their part in the created added value, their contribution to real production. Consider the second case when Joe does not work. In the case Joe dissaves to buy the car instead of working and using his income (he just drives around, without producing), 500 savings have to be deducted from your final result for the first scenario, so that S = I = 0, C = 500, Y = S + C = 500. In the second scenario, forget about Joe's 500 for a while. Assume Amanda invests 250 (one car) in inventory, which means she has produced 250 (one car) first (and sold nothing), so that I = Y - C = 250, C = 0. Consumption is zero because Amanda sold nothing and Joe bought nothing.  An income can only be earned when value added was produced. Therefore, the  500 Joe has received cannot be seen as an income, and since only income can lead to consumption or saving, Joe had neither consumed nor saved. The 500 Joe received is an expansion of money supply (and has no real interpretation) and will increase prices proportionally (assume V and Y to be constant in the fisher equation (MV=PY)). The 250 that Amanda produced has now increased in value, but in real terms her investment in inventory will still refer to the 250 (one car).  This actually illustrates the importance of distinguishing nominal and real variables in the economy. The macro-economic identity you refer to is only about real variables."
Examples of Applied Micro Paper with R (!) Code and Data in Public Repository,"You could try searching the Harvard Dataverse for fileType:""R Data"" like this: https://dataverse.harvard.edu/dataverse/harvard?q=fileType%3A%22R+Data%22  I think your use case of wanting to search for data in specific formats such as RData is a common one so I just created an issue about improving Dataverse to support this use case better: https://github.com/IQSS/dataverse/issues/2707"
Why plug deficits with bonds rather than printing money?,"There are several issues with this approach. One is that any changes in the quantity of money - including those considered 'acceptable amounts of inflation' - act to distort the price signal being communicated to all economic participants which is an extremely undesirable side effect.  The other is a little more insidious. Governments can only print cash or asset money. The vast majority, over 98% in most modern banking systems of money being used is liability money - money that is represented as deposits in the banking system. Were the government for example to print physical cash and deposit it in a commercial bank the book keeping operation would be [debit cash, credit deposit]. In almost all cases, it is the deposit money that is actually spent as the government pays salaries etc. So if a government prints money and its banking regime relies on a framework where assets act as a regulatory control on lending (and consequent deposit creation), and there is no other regulatory control, the result is hyper-inflation. The problem is not just the money created by the government, it is the consequent multiplication that results from banks increasing their credit/money creation. (This form of regulation is generally referred to as the reserve requirement in economic literature.) Typically the resulting inflation then leads the government to print more money, and the result is a rapid spiral that quickly destroys the usefulness of the currency concerned for any economic transaction. Most modern banking systems use a combination of reserve requirements and capital requirements, and this is why the quantitative easing interventions have so far had no inflationary effect on the economies using them. Even though the US government printed a huge amount of money for the TARP intervention, the capital controls intervened to prevent the runaway hyperinflation that would have occurred under earlier regimes. However it's still a dangerous thing to do, and the long term behaviour of banking systems that rely on basel capital controls is poorly understood. Finally, it's worth noting that the alternative available to any government besides borrowing, is raising taxes or controlling its' expenditure. Ultimately the government's use of social resources has to be regulated by something, and keeping a more or less balanced budget isn't a bad place to start."
When should a receiver randomize across actions in a signaling game?,"Perhaps I have a counterexample! Let there be three messages, $m_1, m_2,$ and $m_3$, and three sender types $t_1,t_2,t_3$ where $\Pr(t=t_3)=\frac{1}{2}-\epsilon$, $\Pr(t=t_2)=\frac{1}{4}$ and $\Pr(t=t_1)=\frac{1}{4}+\epsilon$. Sending $m_3$ results in a payoff $0$ for senders, we can think of it as exiting the game.  The set of receiver responses to a message $m=m_1,m_2$ is $\{a,r\}$ $u_t(a,m_1)=1 > u_t(a,m_2)=\beta>u_t(r,\cdot)=0$ $u_R(t_1,m_1,a)=u_R(t_2,m_2,a)=2$, $u_R(t_3,m_i,a)=1$, $u_R(t_2,m_1,a)=u_R(t_2,m_1,a)=0$, $u_R(t_3,m_i,r)=2$, $u_R(t_1,m_i,r)=u_R(t_2,m_i,r)=1$. Then in equilibrium, all senders must obtain the same utility, correct?. Otherwise, one will imitate the other's strategy.  So, the only pure strategy equilibrium is for all senders to choose $m_3$. In a pooling equilibrium on $m_1$ or $m_2$, the best response is to choose $r$. There is no pure strategy separating equilibrium except if $t_1$ and $t_2$ send $m_2$, and the receiver responds with $r$. Then $t_3$ is indifferent between all messages, because he will surely be met with payoff $0$. All of this gives the receiver payoff $\frac{3}{2}-\epsilon$ Then consider the case where $\sigma_R^{m_1}(a)=\beta$ and $\sigma_R^{m_2}(a)=1.$ Now, the senders are indifferent between sending those two messages. Then, let $\sigma_{t_3}(m_1)=\frac{\epsilon+1/4}{-\epsilon+1/2}=1-\sigma_{t_3}(m_1)$ and $\sigma_{t_i}(m_i)=1$ for $i=1,2$. Then the receiver strategy is rational.  The receiver's expected utility from $m_1$ given $a$ or $r$ is 1.5. The expected utility from $m_2$ is slightly above 1.5, given $a$. So the ex ante expected payoff is above $\frac{3}{2}-\epsilon$, better than the pure equilibrium described above. Furthermore, this separation is only maintained by mixing. Any other pure strategy taken by the receiver will induce sender pooling, meaning the only a pure strategy equilibrium is when the receiver chooses $r$. I should have $\beta$s in the picture below for the lefthand side sender payoffs to $a$. I think the $\beta<1$ is the key ingredient. "
Definition of capitalism,"Though there may not be a set definition of ""capitalism"" chisled in stone somewhere, there are general principles that economists agree are implied in capitalism and that capitalism implies. Take, for example, the two below definitions of ""capitalism"": [A]n economic system characterized by private or corporate ownership of capital goods, by investments that are determined by private decision, and by prices, production, and the distribution of goods that are determined mainly by competition in a free market -- Merriam-Webster Dictionary Capitalism is an economic system in which private individuals or businesses own capital goods. The production of goods and services is based on supply and demand in the general market—known as a market economy—rather than through central planning—known as a planned economy or command economy. -- Investopedia Both definitions provide two elements necessary for ""capitalism"":
(1) private individual / corporate business ownership of capital goods and
(2) private decisions based on the given market's supply and demand curves (and the factors influencing those curves)."
Are there fundamental reasons why (exponential) economic growth is highly desirable?,"Because according to utility theory: more is better- or at least not worse (nonsatiation- or free disposal). Your question seems to actually be: why is growth good?  Firstly, It's axiomatic. Utility theory very formally assumes the law of nonsatiation/free disposal.  Things that result in the death of the cosmos are bad not because we are dead, but because we cannot consume and improve our happiness afterwards. Evidince: We observe that consumption improves happiness. Second, in cases of limited resources, we have situations where optional growth paths are negative-But this is only because it allows for increased overall consumption. "
Effect of a permanent increase in government expenditure in an open economy?,"The intuition for this result is pretty straightforward, and I think one can think about it in terms of saddlepoint stability in a phase diagram, although you don't need any serious technical apparatus - it's all conceptual. Krugman and Obstfeld posit a model in which government expenditure does not affect the ""full employment"" level of output $Y^f$ (to which the economy must tend in the long run - note that proving this requires you to look at a phase diagram and rule out the explosive paths that fail to converge to $Y^f$). The money supply $M^s$ is also assumed to be constant. Together, these assumptions imply that the steady state price level $P$ doesn't change. Now Krugman and Obstfeld, critically, also assume that you're already at the steady state price level $P$ when the permanent increase in government expenditure occurs. And even once the increase hits and the phase diagram changes somewhat, this $P$ is still the steady-state price level (i.e. the price at the unique fixed point of the implied system of differential equations).  In their world, $P$ is the only state variable. (Prices are sticky, but quantities and exchange rates are not; they're jump variables.) Since you're already at the fixed point's value of $P$, the other variables will immediately jump to their fixed point levels and stay there. (This is always true for models where the fixed point is a saddle and equilibrium is unique.) And since the fixed point for output is unaffected by the expenditure shock, this means we stay at the same output even in the short run. Conceptually, the point here is that we only get nontrivial dynamics from a permanent shock if that shock changes the fixed point values of state variables. This happens, for instance, in the neoclassical growth model following a permanent TFP shock, because an increase in TFP raises the steady state level of capital above the current level of capital. But it doesn't happen here, since under Krugman & Obstfeld's particular set of assumptions there is only one state variable and it's unaffected by permanent changes in government expenditure."
How does Google price the items on Google Play?,"The simple answer is they estimate the demand curves for each product and, using their cost structure and market characteristics (competition structure, etc.) set price to maximize profits. This is standard for any firm, though. How Google in particular and these big firms in general (Amazon, Microsoft, etc.) estimate demand curves is somewhat different than the usual economist might do it. For usual demand estimation, a researcher would have to make use of market idiosyncrasies to identify demand. For example, using supply shifters with 2SLS for basic demand estimation, BLP for discrete choice with heterogeneous products, etc. Identification is such a big issue for demand estimation because a researcher generally just observes equilibrium (p, q) combinations, not the actual demand curve. We are also often constrained purely by the amount of data available. For a big firm like Google, however, they 1) have the ability to enact exogenous perturbation in price to see how sales change and 2) have access to tons and tons of data. Using 1) they are constantly running little experiments to see how consumer behavior changes. They can then use the results to actually trace out the demand curve. In these experiments, the firm could easily take into account things like movie popularity, genre, etc. With respect to 2), Pat Bajari, chief economist at Amazon and one of the biggest names in modern empirical IO, has a (at this point) working paper with Nekipelov, Ryan, and Yang on how to use machine learning to estimate demand curves across products with lots of sample points bunches of characteristics (think thousands of product characteristics). As a ""fledgling computer science researcher,"" you'd probably be in to this. This approach is especially relevant for people/firms with access to tons of data (like Google, Amazon, etc.)"
Is elasticity of substitution between goods empirically constant?,"Constant elasticity of substitution (CES) Utility functions imply demand functions that are linear in (i.e. conditional on prices they are constant fractions of) income (see Rutherford's Lecture Notes on Constant Elasticity Functions). However, there is empirical evidence for both superior goods (demand that increases faster than income) and inferior goods (goods that decrease in income), this does not actually hold for real preferences.  The fact that household budget shares on each consumption good (or even category) are not constant is probably additional evidence against CES. In principle it is explainable if different consumers face different costs for goods but shouldn't explain why some people choose to do entirely without certain categories of consumption (no TV rather than an inexpensive one). I say probably because:  But, as @regressforward suggests, it may still be an acceptable approximation. "
What are the economic impacts of different professions?,"Enrico Moretti (U.C. Berkeley) works on a related question: The multiplier effect. He finds that high tech industries have the largest multiplier. For each new high-tech job in a city, five additional jobs are created outside high-tech in that city over the next 10 years. A quote from his book, The New Geography of Jobs: With only a fraction of the jobs, the innovation sector generates a
  disproportionate number of additional local jobs and therefore
  profoundly shapes the local economy. A healthy traded sector benefits
  the local economy directly, as it generates well-paid jobs, and
  indirectly as it creates additional jobs in the non-traded sector.
  What is truly remarkable is that this indirect effect o the local
  economy is much larger than the direct effect. My research, based on
  an analysis of 11 million American workers in 320 metropolitan areas,
  shows that for each new high-tech job in a metropolitan area, five
  additional local jobs are created outside of high tech in the long
  run. [And] it gets even more interesting. These five jobs benefit a diverse
  set of workers. Two of the jobs created by the multiplier effect are
  professional jobs—doctors and lawyers—while the other three benefit
  workers in nonprofessional occupations—waiters and store clerks. Take
  Apple, for example. It employs 12,000 workers in Cupertino. Through
  the multiplier effect, however, the company generates more than 60,000
  additional service jobs in the entire metropolitan area, of which
  36,000 are unskilled and 24,000 are skilled. Incredibly, this means
  that the main effect of Apple on the region’s employment is on jobs
  outside of high tech. There is also a Wikipedia page on the Local multiplier effect."
Neo Keynesian Modelling and the Lucas Critique,"Before gravity was conceived as a physical phenomenon and studied by Isaac Newton, gravity was not microfounded. So before Newton, modelling the real world should assume away gravity, because it was not microfounded. We should instead create models into which humans could walk on air. When a phenomenon appears stable, while attempting to understand and decipher it (to micro-found it), we should be wise enough to accept its existence and include it in our models, of course in an ""ad hoc"" manner since we haven't decipher it yet. Whether firms ""cannot"" or ""choose optimally not to"" change their prices everyday, makes little difference, as long as we haven't settled on the optimization framework into which such an optimal choice could come about. It makes little difference because in the absence of knowledge it can be seen as a semantic issue: I can re-interpret ""cannot"" as ""an optimizing agent cannot do what is suboptimal -it goes against every inch of its existence"". Nevertheless, bashing for micro-foundations of an observed regularity (and it is a prevailing observed regularity across time, space, cultures and economic systems that firms don't changes their prices continually), is a good thing, in that it keeps up the pressure to finally find those microfoundations.
I just hope the bashers don't argue in addition, that in the meantime, we should only be modelling people as walking also on air -sorry, be modelling firms only as being able to change their prices continually.   And to send the ball back, I have never seen in my life any convincing ""micro-foundation argument"" for the assumption that prices are perfectly elastic."
Why does economics escape Godel's theorems?,"The Incompleteness Theorems apply to computable, first-order, deductive systems.  That means that there must be both a computable set of axioms and a computable inference system.  In other words, you must be able to write a computer program that can answer the following question: Given a finite sequence of sentences, is it the case that each statement is either an axiom or follows inferentially from previous statements? Furthermore, the Incompleteness Theorems require that the system be able to interpret Peano arithmetic (i.e., talk about the non-negative integers with plus and times). Given these constraints, I'm sure it wouldn't be hard to construct such a first-order, axiomatic system for economics (the Austrian concept of praxeology would be a good starting point).  Now, would such a system be able to interpret Peano arithmetic?  If so, then all the Incompleteness Theorems would tell you is that there would be certain statements (specifically, about integers) that could neither be proved nor refuted.  Would such statements be relevant to someone studying economic theory?  Godel doesn't tell us.  The system would only be incomplete because it could talk about integers and the integers are what creates the incompleteness.  The fact that the system could also talk about economics would be merely incidental. I do think that Godel's theorems (and not just the ones about incompleteness; keep in mind that his dissertation was proving the Completeness Theorem) are fascinating.  However, I fear that many people exaggerate their significance beyond their original bounds and try to make some grand philosophical epiphany out of them."
Why does quantity supplied increase with price in economics?,"Not all (current and potential) production has the same costs. Some production has very low additional cost: maybe all the factories and workforce are already in place, they're close to where the product is sold, and it's very little effort to start production and get new product to market. Other production has higher costs. When the price is very low, then in general only the lowest-cost production will happen, as any other production would generate a loss, not a profit. As the price rises, then additional forms of production become profitable. It becomes worthwhile for new investors to move into the sector, and for workers to re-train into that industry, for new factories to get built even on more expensive land, and so on and on. So, when the price is high, all the lowest-cost production happens, as before. AND lots of the higher-cost production happens, too. So the quantity supplied, increases. In a well-functioning market, no one is a price-setter - no supplier, no demander; the price arises automatically from the collective responses of all of the participants. So if a producer has only a small quantity to sell, they can't just set a high price, and reap excess profits. If they tried to do that, then someone else would see the excess profits on offer, and go in and undercut the incumbent supplier, driving them out of business. Sometimes, we do see cases where a supplier can set an excessively high price. Then, either new investors do indeed come in, maybe after a year or two; or the industry gets investigated for anti-competitive practices, and measures are taken to restore the market's competitiveness. It's important to remember that this happens in theory and in practice. It's been observed countless times over many centuries, for just about every product and service that has a functioning market."
Thesis-advisor/student pairs that won the nobel prize,"This seemed like a fun exercise, so using just the info on Wikipedia, I was able to compile this:  Honourable mentions, who I classify as economists who have at least two students who have won the Nobel include: Another interesting fact worth mentioning is that, while Krugman’s advisor, Dornbusch, did not win the Nobel, Dornbusch’s supervisor, Mundell, did. A similar comment applies so Lovell, who was Leontief's student, and had two students who won the Prize. More interesting observations: predictably, for the vast majority of these pairs, the advisor won the prize before the student. Exceptions are Leontief and Samuelson; Fama and Scholes; and Hurwicz and McFadden. Other triplets:  We even have a quadruplet: Kuznets-Friedman-Markowitz-Sharpe."
Why is economic growth measured exponentially rather than linearly?,"Growth as is meant here ""must"" be nothing in particular. It is a specific metric, the percentage change in yearly GNP/GDP, and it is what it is.
In Blanchard and Fischer 's ""Lectures on Macroeconomics"", in the introductory chapter 1, page 2, Figure 1.1, the logarithm of USA GNP 1874-1986 is graphed: and it is impressively linear , bar a disturbance around World-War II (a dive before it that was roughly equally compensated immediately after). But this means that $$\ln Y \approx at \Rightarrow Y \approx e^{at}$$ (for the US Economy, $a \approx 0.030\;\; \text{to} \;\;0.037$ for the period). It is the data that told us that ""growth was exponential"" during this period.
(Note that ""exponential growth"" usually includes the concept of constant growth rate, while  in informal language, ""exponential"" may also refer to exploding paths, paths with increasing growth rate).
And so economic models were deemed relevant if they could replicate to a respectable degree the observed data. The question ""can this go on forever?"" is an altogether different issue, starting with the meaning of the word ""forever""."
"What is the consensus (if any) on Peters ""The ergodicity problem in economics"" (2019)?","Well there is no opinion poll among economists on specifically this problem, but what can be judged from reaction of economists the consensus is that the Ole Peters paper is misguided and irrelevant at best. I think the economists' consensus was already very well and succinctly summed up by Doctor, Wakker and Wang you cite (and is an example of this phenomena). For starters, on twitter R. Thaler (Nobel Prize) called it hogwash, if a Nobel Prize winner for work on decision making and behavioral economics so readily dismisses your idea about how humans make decision under uncertainity its a red flag. This work was also criticized by other notable economists such as Farmer. However, even more can be judged by the non-reaction of economists. The Ole Peters paper was so widely circulated that it is safe to assume that majority of economists know about (I would bet that if you will ask at your university department about ""that ergodicity paper"" most of them will know what you are talking about). It is unbelievable but this paper got so much free press that it should an case study in marketing (it was covered and uncritically cited by major media outlets such as Bloomberg and it even got a TED talk). So it is safe to assume at very least most economists are aware of the Peter's work existence. Yet, if you look at which articles cite the Peters work, you will see most are either A) criticisms, B) not even in the field of economics and C) save for the criticisms most are not published in any reputable journal. The Peters work is now already 2 years old, given that it is so widely known, and given that the work basically claims that our whole expected utility framework even in its general and behavioral applications is both wrong and not useful, you would expect that people would jump at the idea and start widely applying it, or at least testing it. This is because expected utility is widely used workhorse model, and even though one can criticize it on a behavioral grounds it remains useful, the same way as Newtonian Physics, remains useful in presence of general relativity. In the areas where it is not useful we have behavioral models that still build upon the idea of expected utility or alternative theories that do not require ergodicity (e.g. like prospect theory etc., see Kahneman Thinking Fast and Slow for discussion). Yet we do not see people at mass abandoning either expected utility or other more generalized concepts amass. Rather the paper was met by a silence occasionally interrupted by cricket's chirp. Now either there is some conspiracy going on in our profession, or simply most economists do not even consider the paper worth while to respond to or engage with. Given how large our profession is conspiracy is unlikely (given that likelihood of keeping conspiracy secret declines drastically with number of people involved in e.g. see work of Grimes 2016 on this). So really the most straightforward explanation for the utter lack of influence of the paper on profession is that the general consensus is that it is not even worth discussing. One could also argue that the idea is being suppressed by the 'old guard'. There are several historical examples of new ideas in a field being suppressed, for some time, by established scientists e.g. like this example. While it is not impossible that is happening to the ergodicity idea as well, one has to remember that for any good idea that is too eagerly suppressed by the 'old guard' there is always a large number of ideas that were dismissed by the old guard and actually also turned out to be bad or irrelevant. A good example are ideas like EM drive or think of all the 'theories' like ancient astronaut theory. So while it could turn out that ergodicity is being suppressed, it is more likely that it is actually not. Of course, an important caveat is that absence of evidence is not necessary evidence of absence. Ideally, you would want some poll among economists, or at least well known economists.   This being, said the evidence and lack of thereof that we currently have strongly points toward the conclusion that the consensus is that Peters work is irrelevant."
Estimating CES utility (not production) function parameters,"It may be interesting to exploit the homothetic separability of the CES utility function in $x$. It implies that
$$\frac{x_i}{x_j} = \left( \frac{\alpha_i}{\alpha_j}\frac{p_j}{p_i} \right)^\sigma $$
and after $log$-transformation:
$$\ln(x_i) - \ln(x_j) = \beta_{ij} + \sigma (\ln(p_j) - \ln(p_i)). $$ After adding a random term, this specification could be used to estimate the $\beta_{ij} \equiv \sigma (\ln(\alpha_i)-\ln(\alpha_j))$ and $\sigma$ parameters by OLS and in a second step identifying the $\alpha_i$ by minimum distance: $$ \widehat{\alpha} = \arg \min_\alpha \Big(\widehat{\beta}-\widehat{\sigma}(\ln(\alpha)-\ln(P\alpha)) \Big)'\Omega^{-1} \Big( \widehat{\beta}-\widehat{\sigma}(\ln(\alpha)-\ln(P\alpha)) \Big), $$
where $P$ represents the adequate permutation matrix. Some references estimating parameters of CES preference parameters include Diewert and Feenstra (2017) or Redding and Weinstein (2020), with a different approach, however, based on the unit expenditure function. Most empirical contributions reject the validity of homothetic utility functions (like the CES). There are some propositions on how to built nonhomothetic CES production functions that could easily extended to utility functions, see Shimomura (1999) and the references therein. A simple extension of the above specification is: $$\ln(x_i) - \ln(x_j) = \beta_{ij} + \sigma (\ln(p_j) - \ln(p_i)) + \gamma_{ij} M/p + \varepsilon, $$
where $p$ denotes a consumer and time specific aggregate price index (over all commodities). Notation $ij$ stands for commodities $i$ and $j$ and not for the observations (I skipped the $n,t$ subscripts for clarity). This relationship is compatible with nonhomothetic preferences. The homothetic case is obtained for $\gamma_{ij}=0$ which can be tested. Regarding the computer software... I fully switched to R few years ago, it would ""easily"" allow to code the minimum distance estimator, and the link between the utility and demand functions. References: Diewert, Erwin and Robert Feenstra (2017), “Estimating the Benefits and Costs of New and Disappearing Products,” mimeo, University of California at Davis. Redding Stephen J and David E Weinstein (2020), ""Measuring Aggregate Price Indices with Taste Shocks: Theory and Evidence for CES Preferences,"" The Quarterly Journal of Economics, 135, 503-560. Shimomura, K., 1999, ""A simple proof of the Sato proposition on non-homothetic CES functions,"" Economic Theory, 14, 501–503."
Help understanding Lagrangian multipliers?,"A constrained optimization function maximizes or minimizes an objective subject to one or more constraints. As I understand it, the Lagrangian multiplier approach transforms a constrained optimization problem (I) into an unconstrained optimization problem (II) where the optimal control values to problem II are also the optimal control values to problem I. Additionally, the the objective functions in problems I and II take the same optimal values. The trick is a clever way of putting the constraints into the objective function directly rather than using them separately.  I agree with your presentation of the consumer's maximization problem: 
$\Lambda(x,y,\lambda) = x^{\alpha} y^{1-\alpha} + \lambda ((xp_x+yp_y)-w)$.  Now we take the partial derivatives with respect to x an y, set them equal to zero, and then solve for x* and y*. $0=\partial\Lambda / \partial x  = \alpha x^{\alpha -1 } y^{1-\alpha} + \lambda p_x = (\alpha / x ) x^{\alpha  } y^{1-\alpha} + \lambda p_x$  $\Rightarrow -\lambda = (\alpha / (x p_x)) x^{\alpha  } y^{1-\alpha}$  $0 =\partial\Lambda / \partial y  = (1 - \alpha) x^{\alpha} y^{-\alpha} + \lambda p_y = ((1 - \alpha) / y ) x^{\alpha  } y^{1-\alpha} + \lambda p_y$  $\Rightarrow -\lambda = ((1- \alpha) / (y p_y)) x^{\alpha  } y^{1-\alpha}$ $\Rightarrow (\alpha / (x p_x)) x^{\alpha  } y^{1-\alpha} = -\lambda = ((1- \alpha) / (y p_y)) x^{\alpha  } y^{1-\alpha}$ $\Rightarrow (\alpha / (x p_x)) = ((1- \alpha) / (y p_y))$ $\Rightarrow ( y p_y ) / (1- \alpha) = (x p_x) / \alpha$ (eqn 1) Recover the budget constraint equation by taking the partial derivative $\partial\Lambda / \partial \lambda = 0$. $0 = \partial\Lambda / \partial \lambda = xp_x + yp_y - w \Rightarrow xp_x /  w + yp_y /w = 1$ (eqn 2) We now have two equations and two unknowns (x,y) and can solve for x* and y*. $\Rightarrow yp_y / w  = xp_x/w \cdot (1 / \alpha - 1) = xp_x/w / \alpha - xp_x/w$ $\Rightarrow 1 = yp_y / w + xp_x/w = xp_x/w / \alpha$ $\rightarrow \alpha = xp_x/w$ (result 1) $\Rightarrow \alpha = xp_x/w = 1 - yp_y /w$ $\rightarrow 1-\alpha =yp_y/w$ (result 2)  Results 1 and 2 form the famous constant expenditure shares result for the Cobb-Douglas utility and production functions. Which can also be explicitly solved for x* and y*: $x^* = \alpha w /p_x$ and $y^* = (1-\alpha) w /p_y$ which are the optimal values for both the Lagrangian and the original problems. "
Macroeconomics Textbook on New-Keynesian models,"Michael Woodford's book Interest and Prices, while it may not be explicitly New Keynesian, may have some of the rigor you're looking for applied to this class of models. A more direct alternative would be New Keynesian Economics edited by Mankiw and Romer. While it's a collection of papers not a textbook, if you're looking for underpinnings of New Keynesian models this'd be a good place to start. Also ""The Science of Monetary Policy: A New Keynesian Perspective"" by Clarida, Gali and Gertler."
Why are utility functions typically assumed to be concave?,"More or less, yes. Making the right assumption on the shape of the utility function allows you to prove existence or uniqueness of the equilibrium. The exact assumption you need depends on what exactly you are trying to prove and how general you want your result to be. In the case of concavity, it also makes the equilibrium easier to find using the first-order conditions of the utility maximizer, because it makes sure that the local maximum that you find by setting the derivative of the Lagrangian to zero is also a global maximum."
What is money really?,"Money is, in essence, debt. More broadly, it's a system of clearing and credit. That entry in the computer means that the bank owes you \$1000, which is worth whatever others are willing to exchange for that sum— one way of thinking about it is that if you have \$1000, you have a general claim on the rest of society for \$1000 worth of whatever society produces.  The fact is that most money is in the form of loans (debt), not currency. This is well-explained in a recent Bank of England paper. "
Is scalping tickets harmful?,"There is a good Planet Money episode on ticket scalping; I recommend it. The reason for banning ticket scalping has nothing to do with economic harm, and everything to do with making the arts (or sports, whatever) accessible to people of more-modest means. Consider the fact that artists could, if they wanted, just auction off all the seats to their shows, capturing all the surplus and putting scalpers out of business.*  If artists did this, however, lots of people wouldn't be able to afford to see popular artists perform. Many artists (the Planet Money episode uses Kid Rock as an example) want to try to make sure that their fans have a reasonable shot at attending, which means that they want to price the tickets low... but the only way this can work is if resale is illegal. It's been suggested that artists (again, the same goes for sports teams) might benefit from making low-priced tickets available to fans for two reasons: because they plan on selling stuff to people once they're inside, and because an artist-fan relationship is hopefully not just a one-shot transaction. The idea is that fans who can attend the occasional show might be more likely to buy recordings, merchandise, and tickets in the future, providing the sort of long-running support that can keep an artist viable over many years. [P]erformers who undercharge their fans can paradoxically reap higher profits than those who maximize each ticket price. It’s a strategy similar to the one employed by ventures like casinos and cruise ships, which take a hit on admission prices but make their money once the customers are inside. Concert promoters can overcharge on everything from beer sales to T-shirts, and the benefits of low-priced tickets can accrue significantly over the years as loyal fans return. *For the most part— there's some amount of resale that is sort of frictional, which arises from people not being able to attend a show and unloading tickets so they don't just eat the full cost, etc."
Are there real world examples of a shoe event horizon?,"The dutch disease is when an economy is so dominated by a single export industry that the success of that industry drives up the currency and makes the exports of other industries un-competitive. This drives some of the firms making other goods out of business.  Continuing this cycle can lead in the short term to even greater concentration of the economy in that industry.  If later the export industry experiences a significant shock, the broader economy also experiences that shock. This happens for two reasons: Further, if the industry responds by boosting output (like sometimes happens when oil prices fall and producers become desperate) this can make export prices fall further still, worsening the shock. "
How is it possible that all currency exchange rates are fixed w.r.t. to each other?,"International currency markets are highly liquid, with near-instant transfer of knowledge between trading centres. This means that any arbitrage opportunities tend to get resolved within seconds. To put it simply, the thing you tried to do, of trading two currencies via a third, is something that lots of real-world currency traders, and their automatic trading programs, are trying to do all the time; so any discrepancies are quickly mopped up by the market."
Schumpeterian Business Cycles,"I will toot my own horn and cite - Phillips and Wrase. (2006) ""Is Schumpeterian ‘Creative Destruction’ a Plausible Source of Endogenous Real Business Cycle Shocks?,"" Journal of Economic Dynamics and Control, vol. 30 no. 11 pp. 1885-1913. We found it difficult to match the volatility using Schumpterian mechanisms alone.  The business cycle asymmetries from our model were also exactly backward.  That is, a sudden discovery leads to a jump in output which slowly peters out.  While recessions tend to suddenly jump downward and then slowly recover. The mechanism in our model was that an increase in productivity due to the discovery, leads to a reallocation of resources away from innovation toward production of output and this slows down the innovation process."
"In the US, do high wages in regions like New York and California offset the high cost of living?","TL;DR Wages offset the high cost of living in most states.  In NY and CA specifically, wages are insufficient to offset the high cost of living.  In New York City specifically, wages do not even come close to offsetting the high cost of living. Having now researched this a bit, I can give a very concrete answer to the Lou/Nue hypothetical, and I think a pretty decent answer to the general idea.  Let me just say up front, I am not an economist, I'm taking a best guess based on the data published by real economists.  These are layman's conclusions, and, as BKay pointed out, can't possibly capture the complexity of the real world.  That said... Nue runs out of money first.  Here's why: The regional price parity (RPP) in NYC is 136.  That means, a purchase which costs \$100 for an average American will cost \$136 for an average resident of NYC.  The median income in NYC is \$50,711.  This is 97% the national median income.  This means that a days work which pays \$100 for an average American pays \$97 dollars for an average resident of NYC. The regional price parity in Louisiana is 91.  A purchase which costs \$100 for an average American will cost \$91 for an average resident of Louisiana.  The median income in Louisiana is \$40,462.  This is 77% the national average.  This means that a days work which pays \$100 for an average American pays \$77 dollars for an average resident of Louisiana. In addition to Lou and Nue, also let us consider Medie.  Medie earns exactly the national median income, and her cost of living is exactly the national average, i.e. RPP = RWP = 100.  (Sidebar: to the extent that such a person exists, they probably live in Pennsylvania, which comes the closest to conforming to both wage and price averages).  So let's compare a hypothetical wage and price for Medie, Lou, and Nue: Medie earns \$100 per day
Apples cost \$1 each in Medie's home town
Medie can buy 100 apples per day Lou earns \$77.81 per day
Apples cost \$0.91 each in Lou's home town
Lou can buy 85 apples per day Nue earns \$97.63 per day
Apples cost \$1.36 in Nue's home town
Nue can buy 71 apples per day Note, this effect only holds for New York City proper.  If you compare New York State with Louisiana, they are dead even. To answer the question more generally, all other things being equal it pays to be in a high wage, high cost of living state, though there is major regional variation.  To try and answer this more fully, let's look at the data BKay supplied above, and expand the hypothetical from the original question.  Regional Price Parity data is fairly easy to come by (i.e., ""for every dollar spent by an average american, how many dollars must a person from region X spend to get the same result?"").  Regional Median Wage data is also pretty easy to come by.  From the Regional Median Wage we can trivially calculate Regional Wage Parity (i.e. ""for every dollar earned by an average American, how many dollars can a person in region X expect to earn?"").  This is a term I'm making up, so I know it's not exactly scientific, but I think it give a good gestalt for the situation.  This gives us: $$
\frac{Regional Wage Parity}{Regional Price Parity} = HowMuchStuffYouCanBuy
$$ ""How much stuff you can buy"" is really the figure that this question is trying to get at. The state with the highest ""apples per day"" value is New Hampshire (126) followed by Virginia (123).  As I said, New York and Louisiana do the worst.  DC and Georgia conform exactly to the national average.  Here are these value's graphed:  Note, while local wages seem to predict well for this ""how many apples"" measure, prices do not seem to predict at all.  Goes to show, cheap != affordable.  Here's the raw data I used for this if you want to play around with it: Sources:
Wage data (Direct XLS Download)
Price Data (PDF)"
Which are the different Schools of Economic Thought?,"Post-Keynesian Post-Keynesianism (PK) is based on the criticism of the so called ""Keynesianism"", which according to PKs is not loyal to core Keynes ideas. As such, this school of thought aims to be called the ""true"" Keynesians. The criticism starts with the workhorse model of Keynesianism, the IS-LM model, developed by Hick in a 1937's article, right after Keynes magnum opus. According to Minsky (a prominent PK), this is an article which ... misses Keynes’ point completely’ (Minsky, 1969, p. 225) Later in life Hicks acknowledge this, by stating that his model was Walrasian rather than Keynesian in origin (Hicks, 1981, p. 142) Given this background, PK has the following core features in its method: Some early authors of this discipline are Michael Kalecki, Joan Robinson, Nicholas Kaldor, Luigi Pasinetti, and Piero Sraffa. More recent authors include Wynne Godley, Steve Keen, Frederic S. Lee, and Marc Lavoie. A recent introduction to PK can be found in Godley and Lavoie (2007). There is also a very comprehensive two-volume Oxford Handbook of Post-Keynesian Economics. Sources: own research, and Keen (2013). "
What are the causes of negative real interest rates?,"If there is inflation, what is your alternative? If you do not lend, your money loses even more of its value. A numerical example: 
If inflation is 5% and you can lend at 2% nominal interest rate, you can make the loan and lose 3% of your money's purchasing power OR you can not make the loan and lose 5% of your money's purchasing power.

Poor choices, but one is better than the other."
Money in the animal kingdom,"The only use of money (other than by humans— who, it should be noted, lived for most of human history without it) in the animal kingdom that I'm aware of has been when researchers have taught other primates (Capuchin monkeys) how to use it. While the introduction of currency was apparently successful (there was Capuchin prostitution), the research did not go so far as to show that equilibrium prices would arise endogenously through exchange."
Impact of auction systems that allow 'sniping',"Firstly, it's right to say that the hard ending rule on ebay seems to be behind sniping. Here's a figure from Roth & Ockenfels' AER paper:  On eBay (fixed end time) bids arrive much later than on the now defunct Amazon auction platform (which used an extendable end time). One explanation (see here and here) for this is that people want to hide their private information. If you think that you discovered a 'hidden gem' then the last thing you want to do is start bidding and let everyone else know that it's valuable! Note, from the figure, that sniping seems to be more acute in antiques auctions, where private information is likely to be more important. The other leading explanation is that some bidders do not understand that they can use the automatic proxy system to take care of the bidding, and 'naively' sit incrementing their bid manually. Sniping is then a way to take advantage of such bidders' poor reaction speed. An experiment by Ely & Hossain that appeared in AEJ Micro verified that sniping indeed gives you an advantage over such naive bidders. This marketing Science paper by Zeithammer and Adams also provides evidence that manual incremental bidding is common and that it is therefore wrong to think of eBay as a simple implementation of a second price auction. So, two perspectives on what is going on here: concealment of private information and exploitation of naive bidders. Both of these have one thing in common: they imply that Sniping leads to lower prices, not higher. So why does eBay persist in the use of such a mechanism? Only they know. But here is what I suspect: eBay is a two-sided market. It must attract both sellers and buyers. Usually, optimal pricing in a two-sided market involves subsidizing one side of the market to get them onto the platform before milking the other side. But buyers already pay $0 to eBay; what if a bigger subsidy is needed? One way to implicitly subsidize buyers and attract them to the platform is to choose a mechanism that results in a lower selling price. In the end, if the result is indeed to attract more bidders, this might not even be that bad for eBay. Bulow and Klemperer have shown that adding bidders to an auction is almost always more profitable for the seller than redesigning the auction but keeping the number of bidders the same."
Optimality of Zero Capital Taxation,"There is quite a bit of work being done in that area. One very recent example is Straub and Werning's working paper ""Positive Long Run Capital Taxation: Chamley-Juff Revisited."" 
The point seems to be that we need to consider the rate of convergence to the steady state. Also, there is other literature that gives some competing results (e.g., ""the new dynamic public finance"" literature). For a more complete summary of mainstream objections, see Diamond and Saez, ""The Case for a Progressive Tax: From Basic Research to Policy Recommendations,"" (JEP, 2011). They devote a whole section to why they think the Chamley-Judd result is not policy relevant."
OLS bias in demand estimation: the bias always underestimate the demand's elasticity?,"Usually, $\hat{\beta_1^{IV}} = \beta_1 + \frac{cov(z,u)}{cov(z,x)}$.  The denominator will go to zero.  That is true unless there is some correlation between the instrument and the error term, and the nominator is the strength of the relationship between the instrument and the endogenous variable. The smaller the denominator gets, the greater the bias $\left[\frac{cov(z,u)}{cov(z,x)}\right]$. In addition,  weak instrument will have no precision, so that the variance will have a big upward bias. 
\begin{eqnarray} var(\hat{\beta_1}) &\to_p& \frac{\sigma^2}{n \sigma^2_x} \nonumber \\
\hat{\beta_1^{IV}} &=& \frac{\sum (z_i - \bar{z})y_i}{\sum(z_i - \bar{z})x_i} = \beta_1 + \frac{\sum (z_i - \bar{z})u_i}{\sum(z_i - \bar{z})x_i} \nonumber \\
var(\hat{\beta_1^{IV}} &=& var \left( \frac{\sum (z_i - \bar{z})u_i}{\sum(z_i - \bar{z})x_i} \right) \nonumber\\
var(u | z) &=& \sigma^2 \nonumber\\
var(\hat{\beta_1^{IV}}) &=& \frac{\sigma^2 \frac{1}{n} \sum (z_i - \bar{z})}{n[ \frac{1}{n} \sum (z_i - \bar{z})(x_i - \bar{x})]^2} \nonumber
\end{eqnarray} As $n \to \inf$
\begin{eqnarray} var(\hat{\beta_1^{IV}}) &\to_p& \frac{\sigma^2 \sigma_z^2 }{\sigma^2_{zx}} \nonumber\\
var(\hat{\beta_1^{IV}}) &\to_p&  \sigma^2 \frac{1}{n \sigma^2_x} \frac{1}{\rho^2_{xz}}\nonumber \\
\rho^2_{xz} &=& \frac{[\sigma^2 _{xz}]^2}{\sigma_x^2 \sigma_z^2} \textit{for} \rho \in [0,1]\nonumber
\end{eqnarray} That is why if your instrument is weak, then you  may be better off running an OLS regression.    "
Nobel prize for empirical work,"In 2011, Tom Sargent and Chris Sims were jointly awarded the Nobel Memorial prize for empirical work. The Royal Swedish Academy of Sciences has decided to award The Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel for 2011 to Thomas J. Sargent and Christopher A. Sims ""for their empirical research on cause and effect in the macroeconomy."" Dr. Sargent's contribution was related to the use of structural macroeconometrics in policy and analysis and Dr. Sims' contribution was to the use of vector autoregressions (VAR) to analyze temporary changes in policy and other factors. Source: Nobel Prize Website"
Intuition behind risk premium,"The name for the amount $56.25 is certainty equivalent. The expected utility for the individual from taking the bet is calculated as follows:
$$E[U]=\frac12U(100+125)+\frac12U(100-100)=75$$
Suppose the individual can pay an amount of money $x$ so that she can avoid taking the bet (which leads to expected utility $75$). What's the maximum amount of money $x$ she's willing to pay? Well, she would pay up to a point where she's indifferent between taking and not taking the bet. If she takes the bet, expected utility is $75$. If she pays, her utility is $U(100-x)$. We want her to be indifferent, so that $U(100-x)=75$. Reading off from the blue curve in your graph (the curve describing $U$), we see that 
$$U(56.25)=75$$
which means $100-x=56.25$, or $x=43.75$.  So we can interpret 43.75 as the maximum amount of money that an individual is willing to pay in order to avoid the (risky) bet. "
How to show that a homothetic utility function has demand functions which are linear in income,"I think what you need is that if $U(x,y)$ is homothetic then
$$
\forall \alpha \in \mathbb{R}_{++}, \forall (x,y) : \hskip 6pt
\frac{\frac{\partial U(x,y)}{\partial x}}{\frac{\partial U(x,y)}{\partial y}} = 
\frac{\frac{\partial U(\alpha \cdot x,\alpha \cdot y)}{\partial x}}{\frac{\partial U(\alpha \cdot x,\alpha \cdot y)}{\partial y}}
$$
and love."
Is it possible to have a modern economy without a central bank?,"It surely is possible: For a long while, Gold was the basis for the value of money in many ""modern"" countries, leaving central banks with little room for policies. "
What is the calculable effect of counterfeiting on an economy?,"So, is it correct to say that in an economy with X currency units, counterfeiting and spending Y fake units is equal to theft of Y/(X+Y) of the wealth of the economy? (Emphasis added.) No.  It is correct to say that spending Y fake units is equal to the theft of Y/(X+Y) of the money holdings of the economy, which is not at all the same thing."
"Why hasn't Quantitative Easing surged inflation, or caused hyperinflation?","High-powered money is another term for the monetary base or MB.  MB represents all money created by the government.  However, that is not all the only source of money.  Banks are allowed to create money too.  Which is why higher aggregates like M2 and M3 are greater than MB. Because bank money is greater than government money, it (and not MB) is more of a primary determinant to monetary inflation.  Pretend there is an economy of blue dollars and yellow dollars.  The blue are created by the government and the yellow by banks.  There are many more yellow dollars than blue dollars.  So when the supply of blue dollars goes up it doesn't matter as much because of the number of yellow dollars. Bank money was being destroyed in the crisis which was actually creating deflationary pressures.  The collapse in higher monetary aggregates was offset somewhat by the surge in the monetary base.   Now typically base money is used to create and leverage higher aggregates, but this was not the case during the crisis.  Because of liquidity concerns, banks curtailed monetary creation so this is why inflation did not happen."
Simulating Real Business Cycle,"Explosiveness The paper contains an error, which causes the explosive dynamics in your simulation (although presumably the underlying computations in the paper were correct). The equilibrium condition derived from eigenvalue decomposition is contained in the third row of matrix $Q^{-1}$ on page 12 of the paper, with variables ordered as $(c,k,h,z)$ (I'll drop tildas, so all lowercase variables are to be understood as log-deviations). Comparing with eqn. (16) on p. 13, we see that coefficients for $k$ and $h$ are switched, and so the correct condition is $$
c_t = 0.54 k_t + 0.02 h_t + 0.44 z_t
$$ Simulation First, we can express consumption and labor as linear function of state variables (no need to solve the system at each step of the simulation). The intertemporal and intratemporal equilibrium conditions can be written as $$
\begin{bmatrix}1 & -0.02 \\ 2.78 & 1 \end{bmatrix} \begin{bmatrix} c_t \\ h_t\end{bmatrix} = \begin{bmatrix} 0.54 & 0.44 \\ 1 & 2.78 \end{bmatrix} \begin{bmatrix} k_t \\ z_t\end{bmatrix}
$$  so after multiplying by an inverse we get $$
\begin{bmatrix} c_t \\ h_t\end{bmatrix} = \begin{bmatrix} 0.53 & 0.47 \\
-0.47 & 1.47 \end{bmatrix} \begin{bmatrix} k_t \\ z_t\end{bmatrix}
$$ Next, transition for states can be written as $$
\begin{bmatrix} k_{t+1} \\ z_{t+1} \end{bmatrix} = \begin{bmatrix} -0.07 & 0.06 \\ 0 & 0 \end{bmatrix} \begin{bmatrix} c_t \\ h_t\end{bmatrix} + \begin{bmatrix} 1.01 & 0.1 \\ 0 & 0.95 \end{bmatrix} \begin{bmatrix} k_t \\ z_t\end{bmatrix} + \begin{bmatrix} 0 \\ \epsilon_{t+1}\end{bmatrix}
$$ which can be reduced by substuting for control variables to $$
\begin{bmatrix} k_{t+1} \\ z_{t+1} \end{bmatrix} = \begin{bmatrix} 0.94 & 0.16 \\ 0 & 0.95 \end{bmatrix} \begin{bmatrix} k_t \\ z_t\end{bmatrix} + \begin{bmatrix} 0 \\ \epsilon_{t+1}\end{bmatrix}
$$ Now the simulation should be trivial, here's a Matlab/Octave example:  Of course in practice, you should probably recompute the whole solution, including the eigenvalue decomposition, so that you would be able to change parameters, etc."
Is it possible to derive indifference curves given marshallian demand function?,"Yes, under some conditions. This is the classic integrability problem: for detailed discussion, see some excellent notes by Kim Border. Several other technical conditions are required, but the most economically substantive condition is that the Slutsky matrix must always be symmetric and negative semidefinite. To be concrete, if we define the $ij$th element of the Slutsky matrix at $(p,m)$ to be
$$\sigma_{ij}(p,m)=\frac{\partial D_i(p,m)}{\partial p_j}+D_j(p,m)\frac{\partial D_i(p,m)}{\partial m}$$
then we must have $\sigma_{ij}(p,m)=\sigma_{ji}(p,m)$ for all $(p,m)$, and also for any vector $v$ we must have for all $(p,m)$
$$\sum_i \sum_j \sigma_{ij}(p,m)v_iv_j \leq 0$$
The necessity of these conditions follows immediately from basic consumer theory, which shows that if Marshallian demand is derived from constrained maximization of a utility function, then the Slutsky matrix is symmetric and negative semidefinite. But the sufficiency of these conditions (in conjunction with some other technical assumptions) for us to back out a utility function is a more complicated matter, and to get the details I recommend Border's notes or some other advanced micro source. If, assuming that the Slutsky conditions hold, you want a rough practical way (ignoring the technical subtleties) to back out indifference curves in the typical two-good case, the simplest way is probably to use your knowledge of demand to determine the compensating change in expenditure that is necessary to adjust for a given change in prices. Specifically, for either $i=1,2$ use the identity
$$\frac{\partial e(p,u)}{\partial p_i} = h_i(p,u) = D_i(p,e(p,u))$$
which, given knowledge of the Marshallian demand function $D$, is a differential equation in the expenditure function $e$. Starting with some initial values $(\bar{p},\bar{m})$ that produce some unknown utility $\bar{u}$, we know that $e(\bar{p},\bar{u})=\bar{m}$. Then, varying $p_1$, we can integrate the above differential equation for $i=1$ to obtain $e(p_1,\bar{p}_2,\bar{u})$ for any $p_1$. And then we can get the Hicksian demand vector
$$h(p_1,\bar{p}_2,\bar{u})=D(p_1,\bar{p}_2,e(p_1,\bar{p}_2,\bar{u}))$$
for any $p_1$.  Since these Hicksian demands all correspond to the same utility $\bar{u}$, they are on the same indifference curve. By varying $p_1$, we will be able to trace out many different points on this indifference curve. In fact, if demand is sufficiently well-behaved, then we can trace out the entire indifference curve by varying $p_1$ enough in either direction. (By the way, ""tracing indifference curves"" is all we can do in any event: since the cardinality of utility is irrelevant to Marshallian demand, we can only retrieve ordinal properties like indifference curves and their ordering.)"
What is the purpose of measuring GNP?,"In 1991, when the Bureau of Economic Analysis made the switch from GNP to GDP as ""their primary measure of U.S. production,"" they indicated the continued importance of GNP this way: GNP, however, continues to be a useful concept. Because it refers to the income available to U.S. residents as the result of their contribution to production, it is appropriate for analyses related to sources and uses of income. For example, saving rates are normally expressed as a percentage of income, and GNP is the more appropriate measure for this propose. In addition, GNP is better than GDP for analyses that focus on the availability of resources, such as the Nation’s ability to finance expenditures on education."
Do a group of economic agents really act as if they are rational?,"The literature is full of examples in which either You are asking: In my view, there is a statistical law behind many ""regularity through aggregation results"", which is best illustrated by these equations (in standard notations): \begin{align}
    y_n &= f_n(p) + u_n \\
    \frac{1}{N}\sum_n^N y_n &= F(p) + \frac{1}{N}\sum_n^N u_n
\end{align} The disaggregate error term $u_n$ represents a gap wrt rational behavior. If this term is iid, then the variance of the aggregate ""mean"" term is much smaller than the variance of $u_n$. So, while ""irrationality"" can be large empirically at the individual level, there is hope for a smaller importance of ""irrationality"" in the aggregate (due to compensations by summation). The ""rational"" (demand or supply) function $F(p) \equiv \frac{1}{N} \sum_n^N f_n(p) $ may become the driving force for explaining the aggregate ""mean"" level $Y$, whereas rationality may be a small component in disaggregate behaviors. If economic structure (for instance a simple budget constraints, or market equilibrium condition) is added to the above statistical explanation, then some properties can be reinforced in the aggregate. For a reference illustrating this type of reasoning, see: Becker,  G.  S., 1962, “Irrational  Behavior  and  Economic  Theory,” Journal of Political Economy, 70, 1-13.
Heiner, R. A., 1982, ""Theory of the Firm in Short-Run Industry Equilibrium,"" American Economic Review, 72, 555-62. Well, there are many aggregate results which are not ""rational"" (because they fall into category 2 above). Regarding non-experimental evidence for category 4, there is: Hildenbrand, W., 1994, Market Demand: Theory and Empirical Evidence, Princeton University Press."
Was the Concorde project an example of the sunk-cost fallacy?,"I doubt that the sunk-cost fallacy was a major reason why the Concorde project was pursued for so long (approved by the UK government 1962, entered commercial service 1976). Any ""emotional impact on British citizens"" that it was feared cancellation would cause would more likely have been due to a loss of national prestige (this was a time when many in Britain still - unrealistically - thought of their country as a great power).   An account of the project (Myddelton They Meant Well: Government Project Disasters pp 107-127) identifies the following factors which may have contributed to decisions at various times to keep the project going (see especially Conclusion p 127):"
Is money a conserved quantity?,"Νominal money could be seen as a ""conserved quantity"" in a similar/analogous sense that we have in physics, under some qualifications and restrictions.  I stress that we are talking about the nominal value of money, which is an indisputable quantity and legal tender (the concept of ""real"" value is an estimated value and hence subject to disagreements etc.) Then if we exclude
1) The central bank/government with the authority of the law
2) The commercial banking system as regards Transactions that have to do with taking or repaying loans
3) Actions that just destroy the physical carrier (say, burn the paper bill) ...then transactions do not alter the amount of nominal money, and in that sense, the nominal value/amount of money is conserved. We exclude 1) because the central bank / government have the power by law to create new money (and destroy existing money, for example ""cutting zeros"" after a hyperinflation episode).   We exclude 2) because commercial banks create money under the ""fractional reserve banking"" system, while money is destroyed whenever a loan is repaid (the part that has to do with the principal, not the interest). The OP should ask a different question if the OP wants to know about that. We exclude 3) because we can argue that these are not ""normal economic transactions"". So the system in which nominal money can be considered a ""conserved quantity"" is a subset of a real world economic system."
Should Costs of Travel to Buy Goods be Regarded as Transaction Costs?,"Since it was mentioned in an another answer let's clear this first: whether the transportation (and its time and monetary costs) should be associated with the intended consumption of the good you are going to purchase, or it can be considered as consumption on its own, depends on your subjective view of it: do you derive any form of pleasure by the trip itself? If yes, at least part of it should be considered consumption per se. The consensus among economists appears to be that most of such travel is not considered by the consumers as utility-enhancing per se (although trends like ""family-shopping on Saturday"" may say a different story), and so it should be interpreted in a different way.  In the field of Industrial Organization, the good's distance from the consumer has been often treated as an aspect of product differentiation.  You could certainly treat it as a ""transaction cost"", by suitably define the scope of the concept. Personally I prefer to think of it as an access cost. I hit upon this concept in a little side-research I did in hedonic-price analysis. If you start to think about it, all packaging and transportation costs from the supplier to the shop are also ""access costs"" from the point of view of the consumer. They don't provide any direct utility to him -they are obligatory costs that end up increasing the price, so that the consumer is able to acquire the good and enjoy the services/utility of the good itself. Think computers: only the materials themselves and the technology embodied in them provide utility to you (plus maybe the brand). But the price includes all shorts of overheads, like the access costs I mentioned, or marketing costs (that can be seen as information costs or as the price to pay for competition and the innovation and product variety that brings along), etc. "
Why is the derivative used to represent marginal cost instead of the difference?,"The derivative is used in some contexts, but not all, when the cost function is differentiable. In those contexts, it tends to be assumed that supply is continuous, not discrete. This is a matter of convention and of analytic convenience. It has the advantage of being consistent, whether you're approaching the supply point from above or from below. But in other contexts, given your cost function, assuming that the thing being supplied is discrete and not continuous (that is, it is possible to supply 2 units or 3 units, but not 2.9 or 3.5 or any other fractional unit) then the marginal cost of the third item is indeed 5, not 4."
Applications/generalizations of a theorem of Debreu,"This result is indeed a version of Berge's maximum theorem. If there is a continuous function $u:M\times H\to\mathbb{R}$ such that $x\preceq_e z$ if and only if $u(e,x)\leq u(e,z)$, one can derive the result directly from Berge's maximum theorem. If $H$ is locally compact, as it is the case if $H=\mathbb{R}^n$, then such a function can always be found, this follows from Theorem 1 in Mas-Colell's On the Continuous Representation of Preorders (at least if $M$ is metrizable, I'm not sure on that point). More on such ""jointly continuous utility functions"" can be found in chapter 8 of Representations of preference orderings, 1995, by Bridges & Mehta. Now Debreu did not have such a result available, so he worked with preference relations and essentially reproved Berge's maximum theorem (the generalization is mathematically straightforward). Why did he do so? To understand that, one needs to understand the point of Debreu's paper, which is finding a topology on preference relations that has nioce properties and makes economic behavior continuous. The need for such a result comes from the literature on economies with a continuum of agents.  What does it mean that a continuum of agents economy is the limit of a sequence of finite eonomies? One answer is that the distribution on characteristics of agents converges to the distribution of characteristics in the continuum economy, so the notion of convergence is convergence in distribution. To make this idea operational, one needs to topologize the characteristics of agents. Now an agent is characterized by her endowment and by her preferences (and in more general models by her consumption set). There is a natural topology on endowments, the Euclidean topology, but it is less straightforward to topologize preferences, and that is what Debreu did in his paper. An exposition of this distributional approach can be found in Hildenbrand 1974, Core and equilibria of a large economy. Now, there are cases where one would like to apply Berge's theorem for non-compact sets of choices. This can be important when studying economies with infinite dimensional commodity spaces, in which being closed and bounded does not imply compactness. One way to deal with this problem is to find a compact set so that the correspondence is compact-valued and nonempty-valued when restricted to this set. There is a large, very technical, literature on ""generalized games"" or ""abstract economies"" (basically normalform games in which strategy spaces depend on the actions of others), and they implicitely often contain non-compact generalizations of Berge's theorem. If you can get your hands on the book, check chapter 4 of Xian-Zhi Yuan 1999, KKM Theory and Applications in Nonlinear Analysis. My impression, however, is that these results proved to be not that useful in economic applications. To prove existence of Walrasian equilibria in models with infinite dimensional commodity spaces, one usually uses different methods."
Karush-Kuhn-Tucker for series,"Yes, Bachir et al. (2021) extend the Karush-Kuhn-Tucker theorem under mild hypotheses, for an infinite number of variables (their Corollary 4.1). I give hereafter a weaker version of the generalization of Karush-Kuh-Tucker for sequence spaces: Let $X\subset\mathbb{R}^{\mathbb{N}}$ be a nonempty convex subset of
$\mathbb{R}^{\mathbb{N}}$ and let $x^{*}\in Int\left(X\right)$. Let
$f,g_{1},g_{2},...,g_{m}:X\rightarrow\mathbb{R}$ be convex functions
continuous at $x^{*}$ and term-to-term differentiable at $x^{*}$, i.e.
such that the functions
$f_{n,x^{*}}\left(x_{n}\right):=f\left((x_1^{*},...,x_{n-1}^*,x_n,x_{n+1}^*,...)\right)$ and
$g_{j,n,x^{*}}\left(x_{n}\right):=g_{j}\left((x_1^{*},...,x_{n-1}^*,x_n,x_{n+1}^*,...)\right)$ are
differentiable at $x_{n}$ for all $n\in\mathbb{N}$ and
$j\in\left\{ 1,2,...,m\right\}$. (Qualification condition) Suppose that for all $k\in\mathbb{N}^{*}$
and for all $x\in X$,
$$x^{*}+P^{k}\left(x-x^{*}\right)=\left(x_{1},...,x_{k},x_{k+1}^{*},x_{k+2}^{*},...\right)\in X$$
If there exist
$\left(\lambda_{j}^{*}\right)_{j}\in\left(\mathbb{R}_{+}\right)^{\mathbb{N}}$
such that $$\lambda_{j}^{*}g_{j}\left(x^{*}\right)  =0,\:\forall j\in\left\{
1,2,...,m\right\} \quad \quad \quad \quad \quad (1)$$
$$f_{n,x^{*}}^{\prime}\left(x_{n}^{*}\right)+\sum_{j=1}^{m}   \lambda_{j}^{*}g_{j,n,x^{*}}^{\prime}\left(x_{n}^{*}\right)=0,\:\forall
n\in\mathbb{N} \quad \quad (2)$$ (Sufficiency) Then $x^{*}$ is an optimal solution on $\Gamma:=\left\{
\left(x_{i}\right)_{i}\in
X\,:\,g_{1}\left(x\right)\leq0,...,g_{m}\left(x\right)\leq0\right\} :$ $$f\left(x^{*}\right)=\underset{x\in\Gamma}{\inf}f\left(x\right)$$ (Necessity) Besides, if $x^{*}$ is an optimal solution on $\Gamma$ and
if the Slater condition $Int\left(\Gamma\right)\neq\emptyset$ is
verified, then there exist unique
$\left(\lambda_{j}^{*}\right)_{j}\in\left(\mathbb{R}_{+}\right)^{\mathbb{N}}$
which verify the (Karush-Kuhn-Tucker) conditions (1) and (2). The number of constraints has to be finite, but simple constraints like non-negativity constraints can be replaced by an equivalent restriction on the domain of the variables. For example, instead of the constraints $\forall n \in \mathbb{N},\;x_n \geq 0$ on the domain $\mathbb{R}^{\mathbb{N}}$, one can take $X=(\mathbb{R}_+)^{\mathbb{N}}$, and the theorem applies. Note that the (sufficiency) result is easy to prove when one further assumes that the convex Lagrangian $\mathcal L(x, \lambda)=f(x)+\sum_{j=1}^m\lambda_j g_j(x)$ is Gateaux differentiable, with a Gateaux derivative equal to 0 at $u=(x^*, \lambda^*)$. Indeed, a function $h: V \rightarrow \mathbb{R} $ convex and Gateaux differentiable on $V$ verifies $h(v)-h(u) \geq h^\prime(u; v-u), \forall u,v \in V$, where $h^\prime(u; v)$ is the directional derivative of $h$ at $u$ in the direction $v$. (One can see that from the definition of convexity: $h(u)+\theta \left( h(v) -h(u) \right) \geq h\left(u+\theta (v-u)\right)$; subtracting $h(u)$, dividing by $\theta$, and taking the limit when $\theta \rightarrow 0^+$; see this for more details). Applying that inequality to the Lagrangian at $u$ proves that the Lagrangian admits a minimum at $u$, which solves the minimization program: $ f(x^*) =L(x^*, \lambda^*) \leq f(x)+\sum_{j=1}^m\lambda_j g_j(x) \leq f(x), \; \forall x\in \Gamma$. However, in general, it is not easy to prove that the Gateaux derivative of a convex series (such as an infinite Lagragian) (exists and) equals 0 at some point $u$, unless one uses the result (Theorem 3.14 in Bachir et al. (2021)) that the Gateaux derivative is thus equal to the sum of derivatives of each term in the series."
Regression over the whole population,"I had initially flagged this question for moderators to examine whether it would be better to migrate over to the statistics SE site Cross Validated. But since the OP introduced a very specific econometrics example, I believe the (very deep) concept of ""population/sample"" can be usefully discussed for the purposes of this example. A first issue is that discussed in @AdamBailey answer: if one considers ""all the countries in the world"" for a given year or years, and it labels the data as ""population"", then the next year should belong to a different population. If it belongs to a different population, then how are we to use results from one population to make inference for another population? So indeed, here our ""population"" is two-dimensional, country and time period -and in that sense, with the time horizon open-ended, we only have a sample in our hands. The second issue (partly implied in @luchonacho answer) is the following: our population is not the actually observed realizations of the random variables ""$GDP_i, i=1,..n$. This is the data. Our population is the collection of random variables themselves, which are functions, not values.  So our data is just one of the possible combined realizations of these random variables. These realizations came about not only as a result of deterministic/engineering relations/causality (reflected in the coefficients), but also under the effect of inherently random factors. In that sense, the data is not a ""pure/typical"" image of the ""population"" -it contains noise, non-structural disturbances, one-off shocks etc.  Then this uncertainty will carry over to the estimation of the coefficients we are trying to estimate, because we assume that these coefficients describe causality or co-movement prior to the random elements affecting the final value of the dependent variable. Due to both aspects above, talking about ""standard error of estimates"" is totally valid, in this case too, and then apply statistical tests as usual."
CES: Production function: Elasticity of substitution $\sigma = 1/(1 + \rho)$,"The production function is:
$$q = (l^\rho + k^\rho)^\frac{1}{\rho}$$
The MPL and MPK are respectively:
$$q_l = \frac{\partial q}{\partial l} = \frac{1}{\rho} \cdot (l^\rho + k^\rho)^{\frac{1}{\rho}-1} \cdot \rho\cdot l^{\rho-1}$$
$$q_k = \frac{\partial q}{\partial k} = \frac{1}{\rho} \cdot (l^\rho 
+ k^\rho)^{\frac{1}{\rho}-1} \cdot \rho\cdot k^{\rho-1}$$
What is the rate that l can be substituted for k? Where $f$ is a differentiable real-valued function of a single variable, we define the elasticity of f(x) with respect to x (at the point x) to be
$$\sigma(x) =  \frac{x f'(x)}{f(x)}\equiv \frac{\frac{df(x)}{f(x)}}{\frac{dx}{x}}$$ Now let's tackle your elasticity problem.  $$ ln(\frac{q_k}{q_l})= log(\frac{\frac{1}{\rho} \cdot (l^\rho + k^\rho)^{\frac{1}{\rho}-1} \cdot \rho\cdot l^{\rho-1}}{\frac{1}{\rho} \cdot (l^\rho 
+ k^\rho)^{\frac{1}{\rho}-1} \cdot \rho\cdot k^{\rho-1}}) = ln (\frac{l}{k})^{\rho-1} = (\rho-1) ln (l/k) = (1 - \rho) ln (k/l)$$
$$ \Rightarrow ln (k/l) = \frac{1}{1-\rho} \cdot ln(\frac{q_k}{q_l})$$ So $\sigma = \frac{1}{1-\rho}$"
Are options a form of insurance?,"No, the primary purpose of options is not to provide insurance against changes in the price of the underlying instrument: options don't have a primary purpose, they don't have an agenda, and they don't have a plan. They're just another tradeable instrument. Some people buy them as means of insuring a position. Some people write them as a means of insuring a position. And as Aurigae notes, in a crisis all correlations tend to one; so even if, in theory, your options insure the rest of your position, in reality, in a crisis, the counterparty risk can become very high, so the insurance is least available when it's most needed."
Envelope Paradox,"Here is an ""expected utility maximization/ game theoretic"" approach to the matter (with a dash of set-theoretic probability). In such a framework, the answers appear clear. PREMISES We are told in absolute honesty that, for $x$ a strictly positive monetary amount, the following two tickets were placed in a box :  $\{A=x, B= 2x\}$ with assigned identification number $1$ and $\{A=2x, B= x\}$ with assigned identification number $0$. Then a draw from a Bernoulli  $(p=0.5)$ random variable was executed, and based on the result and the event that has occurred, the amounts $x$ and $2x$ were placed in envelopes $A$ and $B$. We are not told what the value of $x$ is, or what amount went to which envelope.  First CASE: Choose an envelope with the option to switch without opening it The first issue is how do we choose an envelope? This has to do with preferences. So assume that we are expected utility maximizers, with utility function $u()$. We can model the probabilistic structure here by considering two dichotomous random variables, $A$ and $B$ representing the envelopes, and the amount in them. The support of each is $\{x, 2x\}$. But they are not independent. So we have to start with the joint distribution. In table form, the joint distribution, and the corresponding marginal distributions are \begin{array}{| r | r |  }
  \hline                       
\text{A} \;/ \;\;\text{B} \rightarrow  & x & 2x & \text {Marg A} \\
  \hline 
  \hline                       
  x & 0 & 0.5 & 0.5\\
  \hline                     
2x & 0.5 & 0 & 0.5 \\
\hline
\text{Marg B} & 0.5 & 0.5 & 1.00 \\
  \hline  
\end{array} This tells us that $A$ and $B$ have identical marginal distributions. But this means that it doesn't matter how we choose envelopes, because we will always get the same expected utility,  $$0.5 \cdot u(x) + 0.5\cdot u(2x)$$ What we are facing here is a compound gamble (how to choose an envelope) over two identical gambles (each envelope). We can choose $A$ with probability $1$, $0$, or anything in-between (and complementarily for $B$). It doesn't matter. We will always get the same expected utility. Note that our attitude towards risk doesn't play a role here. So we do choose an envelope, say $A$, and we are looking at it. What is now our expected utility? Exactly the same as prior to choosing. Picking an envelope in whatever way, does not affect the probabilities of what's inside. We are allowed to switch. Say we do, and now we are holding envelope $B$. What is now are expected utility? Exactly the same as before.  These are the two possible states of the world for us: choose $A$ or choose $B$. Under any choice, both states of the world imply the same value to our chosen/assumed driving force (i.e. maximize expected utility).  So here, we are indifferent to switching., and in fact we could also randomize. 2nd CASE: OPENING THE ENVELOPE with the option to switch after  Assume now that we have picked $A$, opened it, and found inside the amount $y \in \{x, 2x\}$. Does this change things?   Let's see. I wonder, what is $$P(A = x \mid A \in \{x, 2x\}) = ?$$  Well, $\{x, 2x\}$ is the sample space on which random variable $A$ is defined. Conditioning on the whole sample space, i.e. on the trivial sigma-algebra, does not affect neither the probabilities, nor the expected values. It is as though we wonder ""what is the value of $A$ if we know that all possible values may have been realized?"" No effective knowledge has been gained, so we are still at the original probabilistic structure.   But I also wonder, what is  $$P(B = x \mid A \in \{x, 2x\}) = ?$$  The conditioning statement, properly viewed as a sigma-algebra generated by the event $\big \{A \in \{x, 2x\}\big\}$, is the whole product sample space on which the random vector $(A,B)$ has been defined. From the table of the joint distribution above, we can see that the probability allocation of the joint is equivalent a.s to the probability allocation of the marginals (the ""almost surely"" qualification due to the presence of two events of measure zero). So here too we essentially condition the probabilities for $B$ on its whole sample space. It follows that our action to open the envelope did not affect the probabilistic structure for $B$ also. Enter game theory, alongside decision making. We have opened the envelope, and we have to decide whether we will switch or not. If we don't switch we get utility $u(y)$. If we switch, then we are in the following two possible states of the world  $$y = x, u(A) = u(x) \implies u(B) = u(2x)$$
$$y = 2x, u(A) = u(2x)\implies u(B) = u(x)$$ We do not know which state actually holds, but per the above discussion, we do know that each has probability $p=0.5$ of existing.   We can model this as a game where our opponent is ""nature"" and where we know that nature plays with certainty a randomized strategy: with $p=0.5$ $y=x$ and with $p=0.5$, $y=2x$. But we also now that if we do not switch, our payoff is certain. So here is our game in normal form, with our payoffs: \begin{array}{| r | r |  }
  \hline                       
\text{We} \;/ \;\;\text{nature} \rightarrow  &y= x & y=2x  \\
  \hline                       
  \text{Switch} & u(2x) & u(x) \\
  \hline                     
\text{Don't Switch} & u(y) & u(y)  \\
  \hline
\end{array} We should resist the temptation to substitute $u(x)$ and $u(2x)$ for $u(y)$. $u(y)$ is a known and certain payoff. The payoffs for the ""Switch"" strategy are not actually known (since we do not know the value of $x$). So we should reverse the substitution. If $y=x$ then $u(2x) = u(2y)$, and if $y=2x$ then $u(x) = u(y/2)$. So here is our game again: \begin{array}{| r | r |  }
  \hline                       
\text{We} \;/ \;\;\text{nature} \rightarrow  &y= x & y=2x  \\
  \hline                       
  \text{Switch} & u(2y) & u(y/2) \\
  \hline                     
\text{Don't Switch} & u(y) & u(y)  \\
  \hline
\end{array} Now all the payoffs in the matrix are known. Is there a pure dominant strategy?   The expected payoff of strategy ""Switch"" is $$E(V_S) = 0.5\cdot u(2y) + 0.5 \cdot u(y/2)$$ The expected payoff of strategy ""Don't Switch"" is $$E(V_{DS}) = u(y)$$ We should switch if $$E(V_S) > E(V_{DS}) \implies 0.5\cdot u(2y) + 0.5 \cdot u(y/2) > u(y)$$ And now, attitude towards risk becomes critical. It is not difficult to deduce that under risk-taking and risk neutral behavior, we should Switch. As regards risk-averse behavior, I find an elegant result:    For ""less concave"" (strictly above) utility functions than logarithmic (say, square root), then we should still Switch. For logarithmic utility $u(y) = \ln y$, we are indifferent between switching or not. For ""more concave"" than (strictly below)  logarithmic utility functions, we should not Switch. I close with the diagram of the logarithmic case  Assume $y=4$. Then $y/2 =2, 2y = 8$. The line $Γ-Δ-Ε$ is the line on which the expected utility from ""Switch"" will lie. Since nature plays a $50-50$ strategy, it will actually be at point $\Delta$, which is the middle point of $Γ-Δ-Ε$. At that point with logarithmic utility, we get exactly the same utility from ""Don't Switch"", i.e. $\ln(4)$ for this numerical example."
Are there papers in evolutionary game theory that are influential in economics?,"Kandori, Michihiro & Mailath, George J & Rob, Rafael, 1993. ""Learning, Mutation, and Long Run Equilibria in Games,"" Econometrica, Econometric Society, vol. 61(1), pages 29-56, January. Young, H Peyton, 1993. ""The Evolution of Conventions,"" Econometrica, Econometric Society, vol. 61(1), pages 57-84, January. I disagree with your assertion that EGT is seldom applied or that it has not been  influential.  Both of the papers referenced above appear in RePEc's list of the
top 1% of citations. Allowing preferences to 'change' can generate just about any equilibrium you want, so the premise behind EGT is a tough sell in many economic models - not to say compelling research hasn't done so.  The bottom line is that it isn't a part of the atmosphere where mere mortals can tread. EGT has, in fact, been applied to many problems in economics as well as those traditionally parsed by the other social sciences - particularly the evolution of social norms, political representation, voting, and distributive justice."
What's quantitative easing?,"Before the edit, you wrote ""qualitative easing"", but I think you refer to quantitative easing. I'll discuss both. Quantitative easing corresponds to the central bank (CB) expanding its balance sheets by ""buying"" assets. This is typically done in secondary markets. It mainly injects liquidity into the system. To the extent that there is an additional buyer of assets now, the price of assets/investment (interest rates) decreases. However, the quantitative impact should be negligible: Through its demand, the CB increases value and liquidity in the markets it is operating. However, the scale of its operations should be too small  to affect the aggregate interest rate. Qualitative easing is a relatively new expression and refers to the riskiness of the stocks that the CB is investing in. In contrast to quantitative easing, which is about the magnitude of assets on the CB's balance sheets, qualitative easing is about the riskiness on the CB's balance sheets, and hence the decrease in aggregate risk (on the banks' balance sheets). When the CB holds assets, it is interested in their value. This may lead it to commit policies that infract its primary directive (e.g. inflation stability). Even if it does not do so, effectiveness of a CB comes from its capacity to control expectations. It suffices for households and firms to expect the CB to commit ""bad"" policies, to decrease the effectiveness of the CB. I don't have sources on this, but I seem to remember that central banks with one clear goal (i.e. monetary stability) are more effective than those with a basket of goals (i.e. monetary stability, GDP growth, decrease of unemployment rate). A general criticism can be that QE are not operations that help with the important margin, monetary stability - and that the central bank should focus on that instead. Note that these are not just esoteric points. In fact, the academics and central bankers at the ""Rethinking Macro Policy"" conference agreed that (quoting Blanchard) Throughout the conference, e.g., in Gill Marcus’ talk, and actually throughout the various meetings which took place during the IMF meetings in the following days, policy makers remarked and complained about the heavy burden placed on monetary policy in this crisis, and the danger of a political backlash against central banks. Even as the crisis recedes, it is clear that central banks will end up with substantially more responsibilities—whether they are given in full or shared—for financial regulation, financial supervision, and the use of macro prudential tools. While even the use of the policy rate has distributional implications, these implications are much more salient in the case of regulation or macro prudential tools, such as the loan-to-value ratio. The general consensus was that these distributional implications could not be ignored, and that while central banks should retain full independence with respect to traditional monetary policy, this cannot be the case for regulation or macro prudential tools."
Price dispersion in online retail,"Yes, this has actually been quite an active area for research within the consumer search literature. As a starting point, I would recommend looking at the following: BAYE, M. R., AND J. MORGAN (2001): “Information Gatekeepers on the Internet and the Competitiveness of Homogeneous Product Markets”, American Economic Review, 91(2), 454–474. These authors have a model that is interesting because it explains the persistence of price dispersion, even in environments like price comparison sites where one would ordinarily think that all consumers will buy from the lowest-priced merchant. The intuition is closely related to that of Varian's classic model of sales, if you know that. The idea is that some firms price low to sell to the consumers on the price comparison site, whilst others price high to sell to less-informed consumers who do not use the price comparison site and therefore do not compare as many prices. BAYE, M. R., J. MORGAN AND P. SCHOLTEN (2004): “Price Dispersion in the Small and in the Large: Evidence From an Internet Price Comparison Site”, Journal of Industrial Economics, 52(4), 463–496. (older working paper link). This is interesting because they test various models (including the one mentioned above) against data from a price comparison site. De Los Santos, B., A. Hortaçsu, and M. R. Wildenbeest (2012): ""Testing Models of Consumer Search Using Data on Web Browsing and Purchasing Behavior."" American Economic Review, 102(6), 2955-80. Not directly related to the question, but an interesting take on a similar issue. There are two classic models of consumer search: sequential and fixed-sample. In sequential search, consumers visit a merchant, evaluate its offer and decide whether to search again; repeating his process until they find an offer they are willing to accept. In fixed-sample search consumers first decide the number $n$ of merchants they will visit; they then visit precisely $n$ merchants and buy the best offer among them. Given dispersed prices, sequential search is optimal and the fixed-sample strategy is not. These authors test the two models against data from online retailers and find evidence to suggest that consumers use a strategy closer to the fixed-sample end of the spectrum. A couple of extra references to finish up:"
Situations where revelation principle may not hold,"You might want to be a little more precise about what you mean by ""Revelation Principle"" as there are many formulations of the ""Revelation Principle"" out there, some of which are stronger than others. Each of these formulations makes a different claim and relies on a particular set of assumptions. Of course, the claim will often fail to be true if some of the assumptions are false. (The following is from notes I got from a microeconomics class.)  Consider for instance the following version of the revelation principle, which is from Repullo (1985), Review of Economic Studies:  Repullo's Revelation Principle : Let $g$ be a dominant strategy mechanism for the game $\Gamma \equiv ( g, U_1, \dots, U_n)$, where $g$ is some game form. For each equilibrium selection function $s : \Theta \rightarrow S$, there exists an equivalent direct dominant strategy mechanism $h$ to $g$ (where $\Theta$ is the set of types). If in addition the equilibrium selection function $s^* : \Theta \rightarrow S$ is surjective, then the dominant equilibrium outcome under $h$ are a subset of the dominant equilibrium outcome under $g$ for all $\theta \in \Theta$.  The bold part is important. If it is not satisfied, there may still be a non-truthtelling  equilibrium in the equivalent direct mechanism. An example is provided in Repullo (1985), Review of Economic Studies pp 223-229.  $$A \equiv \{a,b,c,d\}$$
$$ \Theta_1 \equiv \{ \theta_1', \theta_1''\}$$
$$ \Theta_2 \equiv \{ \theta_2', \theta_2''\}$$ $$ \begin{array}{c |c c c c}
 & a & b & c & d \\ \hline
u_1(\cdot, \theta_1') & 2 & 4 & 2 & 4\\ 
u_1(\cdot, \theta_1'') & 1 & 0 & 2 & 4\\
u_2(\cdot, \theta_2') & 2 & 2 & 4 & 4\\
u_2(\cdot, \theta_2'') & 1 & 2 & 0 & 4\\
\end{array} $$ $$S_1 \equiv \{s_1',s_1'',s_1'''\}$$
$$S_2 \equiv \{s_2',s_2'',s_2'''\}$$ The game form is  $$ \begin{array}{c |c c  c}
 & s_2' & s_2'' & s_2''' \\ \hline
s_1' & a & b & b \\ 
s_1'' & c & d & c \\
s_1''' & c & b & a \\
\end{array} $$ On can check than the following is an equivalent direct mechanism $$ \begin{array}{c |c c  }
 & \theta_2' & \theta_2''  \\ \hline
\theta_1 & a & b  \\ 
\theta_1 & c & d \\
\end{array} $$ Yet, when the types are $(\theta_1',\theta_2')$, although telling the truth is a dominant strategy, any other report of preferences is also a dominant strategy. This may be quite bothersome as it means that for some configuration of the types, saying the truth is only one equilibrium among others. As a consequence, we have no real guarantee that ""telling the truth""  will be played (it may even be that truth-telling is Pareto dominated by another equilibrium. Using a focal point argument, this may further undermine the relevance of the truth-telling equilibrium). The above issue is due to the fact that in the original game, some strategies are never played, which is ruled out if $s^*$ is surjective. So Repullo's version of the Revelation Principle (requiering that every dominant strategy equilibrium outcome of the equivalent game be among the equilibrium outcome of the initial game for every possible configuration of the types) only holds if the equilibrium selection function be surjective, and fails otherwise. "
Has the assumption that individuals' tastes do not change over time been rigorously challenged?,"Stigler and Becker's argument is methodological, not philosophical. They do not try to convince us that preferences are indeed identical across individuals and invariant across time as a matter of reality (the ""Rocky Mountains"" metaphor is an ""as if "" approach). Their point is that any outcome can be rationalized by assuming that ""it was preferences that made it so"", since ""De Gustibus Non Est Disputandum"", and also they are unobservable.  But then, we could ""explain everything"" in this way, and so explain nothing. Their goal is to defend in terms of useful modelling the other extreme: assume immovable preferences and try to find explanations for the observed outcomes based on observable, quantifiable concepts, like prices. I believe the following passage from the first page of the paper summarizes the approach ""On the traditional view, an explanation of economic phenomena that reaches a difference in tastes between people or times is the terminus
  of the argument: the problem is abandoned at this point to whoever
  studies and explains tastes (psychologists? anthropologists?
  phrenologists'? sociobiologists?). On our preferred interpretation,
  one never reaches this impasse: the economist continues to search for
  differences in prices or incomes to explain any differences or changes
  in behavior. The choice between these two views of the role of tastes
  in economic theory must ultimately be made on the basis of their
  comparative analytical productivities."" Bold my emphasis.   So the question of the OP appears to be mistargeted: people's tastes may very well change over time, and I don't think that Stigler and Becker would deny that. 
The question is, can we arrive at more useful economic models by assuming changing tastes, compared to the models where tastes are fixed (while avoiding the ""explain everything and so explain nothing"" trap)? But this would be a whole research program, not some rigorous argument in a paper."
Why does immigration boost the economy? (or does it not...?),"You’re right, the migration of workers boost the economy. It’s a stylized fact. It may have political consequences, though, but it is not the part of the analysis of economics. Let’s consider $A$ the set of active labour force of an economy before, and $A'$ after the migration. Then $A$ is the subset of $A'$:
$$A \subseteq A'$$
This notation means that the social planner of the economy is able to optimize on a greater number of workers. We can assume that opening the door for migrant workers is at least as good as it was before migration:
$$A \preceq A'$$
If this condition is not true (e.g. the migrants doesn't want to work), the authorities of the economy won't open borders for migrant workers. You're considering the effect when the nation itself does not have full employment. This condition is not important. The nation could have 0% unemployment, however, the migrant workers would still get jobs. Immigrants from developing nations tend to be happier to take on unattractive jobs, which citizens of the developed country may refuse
  to do. Absolutely, these people have got different preferences. Here're the utility functions of residents ($r$) and immigrants (i):
$$u_r=C-\alpha_r L$$
$$u_i=C-\alpha_i L$$
where C and L denotes consumption and work. We can conclude that $\alpha_r$ must be higher than $\alpha_i$. The disutility of work is higher in the case of the residents.  That means migrant workers are more cost-efficient for firms. Higher immigration creates more competition for jobs, causing wages to
  decrease and allowing companies to pay their employees less. Therefore, the nation's companies make more money, and this causes the economy to grow. Growth and R&D may also improve by means of brain-drain effect, because there are migrant scientists and white-collar workers, too. The 5% unemployment rate is actually due to people who are sick /
  disabled / otherwise unable to work. By definition it is not correct. In Economics, active workers and job-searchers are on the labour market only. The main concept of migration being good is the increase of the horizon of labour optimization. See also: Krugman & Obstfeld: International Economics Theory and Policy"
Will a Guaranteed minimum income not eventually just be crowded out by inflation?,"I will not discuss fairness (for example employers gains versus employees gains and bargaining power) principles as you did not ask about those. Is ""crowding out"" the right phrase here? Even if inflation lessens the effect of a minimal income there may still be an effect. Imagine that there are two people, $A$ with an income of 0 $B$ with an income of 60, so total income is 60. The government issues a minimal income of 30 so $A$'s income is changed to 30, total income is changed to 90. If this has no real effect on the economy then all prices rise accordingly, that is they rise by 50% because that is by how much nominal incomes haved changed. However if you adjust for this 150% price level, $A$ still can buy goods in value of $\frac{30}{1.5} = 20$ while $B$ can buy goods in value of $\frac{60}{1.5} = 40$. So there could at least be a redistribution effect as previously $A$ could not buy anything and $B$ could buy more things. One concern is that inflation may not be even. Only the income of poor people will be raised hence demands for goods poor people buy will rise but demand for goods consumed by rich folks will not change. As a result the poor may experience larger inflation than the well to do, making the redistribution of incomes one between only poor people. The lines between poor and well to do here are murky. If (due to inflation) a pop-tart costs \$10 then people will buy quality food unless it is even more expensive. If a pop-tart's price rises to \$1,000 people will buy caviar unless its price is also raised. A hope of governments implementing such policies is that (aside from fairness and solidarity principles) a minimal wage will also help minimum wage earners be more financially secure. Some studies have shown that in extreme poverty you make worse decisions and are less productive simply as a result of stress. If this is indeed a main effect then a minimum wage (or a higher minimum wage) can increase overall productivity. Thus inflation will not be the only effect of the policy but also an increased number of goods will be produced."
CES Production Function with $\rho>1$,"The problem with $\rho>1$ is that it means the marginal product of factors is not decreasing ($\rho<1$) or constant ($\rho=1$) but increasing, which is an odd assumption. Such functions yield isoquants that are concave, and might lead to only one factor being used (as BKay said). As in any generic CES, the marginal product of factor $x_i$ is $$ MP_i = \left(\frac{y}{x_i}\right)^{1-\rho} $$ The derivative of this MP with respect to $x_i$ is, after some rearranging, $$ (\rho-1) \left(\frac{y}{x_i}\right)^{1-\rho}\left(\frac{x_{-i}}{x_iy^{\rho}}\right) $$ For $\rho>1$, this expression is positive, which means that the productivity of a factor increases as more of that factor is used.  Regarding isoquants, you can find these by rewriting the production function as $x_2=g(y,x_1)$. In the generic CES, this is $$ x_2 = \left(y^{\rho} - x_1^{\rho}\right)^\frac{1}{\rho} $$ These are linear in the case of $\rho=1$, convex in the case of Cobb-Douglas (where the function above is $x_2=\frac{y}{x_1}$, a hyperbole), and concave in the case of $\rho>1$. For example, select $\rho=2$ and you have: $$ x_2^2 = y^2 - x_1^2 $$ which is the formula of a circle centered at $(0,0)$, with radius $y$. Normally, for production theory only $x_i \geq 0$ is interesting, which gives you the concave isoquants for different levels of $y$. The figure below shows an example, were for a given factor prices ratio, there is a corner solution (point A): $\hskip3cm$  (Code for reproducing figure here)"
Lab experiments for a course in standard contract theory,"There are a couple of papers by Hoppe-Schmitz in GEB that might be useful: I am most familiar with the first paper. It illustrates the hold-up (underinvestment) problem, shows that it cannot be solved by fixed-price contracts, but finds that it can be solved by option contracts (given full commitment). There is also a treatment on renegotiation; however, that is more relevant to the behavioral theories you mentioned. There actually is a website with classroom games for teaching economics where one can find a single-player and a two-player online implementation of this experiment. The second paper shows that in agency problems in which the agent has private information (adverse selection) it is indeed helpful to offer a menu of contracts; it also finds a simple version of the ""no distortions at the top"" result. The third paper illustrates some basic aspects of moral hazard models where the agent must be incentivized to choose a hidden action and contracts may be conditioned on an outcome that stochastically depends on the action."
"The 2015 Economics ""Nobel"": Angus Deaton [closed]","Deaton's ""Understanding Consumption"" (1993) was one of the major sources I used when doing seminar work for my master's. In the end I decided to write my thesis about a different (somewhat related) subject after realising the empirical difficulties with what I planned on doing. I have since chosen a different research subject, but Understanding Consumption is a book I have spent many hours with. It's a bit dated now, but still a decent read if you want to know about consumption and saving."
"The Sonnenschein-Mantel-Debreu results, what are the implications for macroeconomics?","This is somewhat broad and argumentative question, but I'll try to provide one possible answer (disclaimer: I'm not a GE theorist). SMD theorem states that imposing only the abstract structure of general equilibrium has few empirical implications. Fair enough. But it's incorrect to say that  we cannot hope to recover the underlying preferences from an aggregate demand setup as much of the cross-country empirical macro or calibration macro seems to try. because once we impose additional assumptions about preferences, technologies and endowments, we do obtain additional empirical predictions (and of course, different sets of assumptions will lead to different predictions). This is precisely what economists are doing most of the time! It's not 1950's anymore - these days, there is very little research that tries to arrive at general results from abstract axioms. Most theoretical work in macroeconomics will present specific model, derive its qualitative and quantitative implications and often will include an empirical component. Another often-heard argument, present e.g. in the linked piece, is that SMD theorem shows GE models are in general unstable or indeterminate, and thus we must resort to unrealistic restrictions, such as representative agent assumption, to guarantee that our models are well-behaved. But this confuses necessary and sufficient condition: yes, assuming representative agent is sufficient to obtain well-behaved aggregate demand function, but that doesn't mean all, or even most, models with heterogeneity are somehow automatically unstable. And a logical possibility of unstable equilibrium says nothing about whether such situation is empirically relevant (maybe it is, but critics usually provide no evidence for such claims)."
How is wealth created? [duplicate],"First we need to discuss what wealth is. Is it just the amassment of valubles? In the 16th-18th century, rulers thought so. This notion is called mercantilism. This idea was rejected by Adam Smith in book IV of The Wealth of Nations. The current meaning of wealth is not the amount you own, but rather how much you can consume (sustainably). Formally, the definition is Total of all assets of an economic unit that generate current income or have the potential to generate future income. This is of course the same amount as the amount that you can sustainably consume. This means that we can measure wealth either as production or as consumption. Thus, there is not just how much riches you own, but also how much you can create. Also, remember that you cannot consume money. You can only consume goods and services. Amassing riches has little value in itself. It is mainly a way (for an individual or group) to postpone consumption to some point in the future. Thus, wealth is measured not just by the current riches you own, but also by the ability to create more riches in the future. A man with 100,000 and no income is poorer than a man with no money but with an income of 50,000 a year.  These things [...] don't generate new wealth.  But they do.  Labor Let's look at how labor increases wealth by looking at a couple of examples: Example 1. I work at a farm. The result of my work is some farm produce worth X. For that I am paid Y. The farmer that hired me will receive X-Y.
The net wealth gain for the society is Y + (X-Y), or X. Example 2. I work as a barber. I charge X for a haircut. The customer gets a haircut worth X. Net wealth gain for the society is X + (X-X), or X. Since the haircut is consumed immediately, the amassed riches don't increase. Note that the customer didn't lose out - to him a haircut was worth at least X or he wouldn't have gotten one. In macroeconomic terms the GDP increases by X and net savings is unaffected. Trade Trade increases wealth by transferring a good or a service to someone that values it higher.  Example 1: I buy a used car from a neighbour for X amount of money. I get a car that is worth Y to me (Y>=X, or I wouldn't have bought it). My neighbour gets X for a car that was worth Z to him (Z<=X, or he would have kept the car). The wealth produced is (Y-X) + (X-Z), or Y-Z, which is >= 0 because Z<=X<=Y. In GDP terms the wealth produced is X - X, or 0, because GDP only measures how much is paid.  Example 2: An electronics company imports batteries from China for 10 cents each. They sell the batteries to customers for \$3 each. The wealth increase in GDP terms is \$2:90 per battery. The wealth produced is shared by the electronics company, the shipping company, the store salesman and any others that provide services to the electronics company. Innovation Innovation is both the invention of new and the improvement of the existing. Actually, most innovation is small improvements in existing technology or design. Example: I improve a machine that makes nails so that it can make twice the number of nails in the same time. This means that more nails can be produced (and thus consumed) using less labor."
The median as a welfare function,"John Rawls, in his book, A Theory of Justice, discusses the Maxi Min Principle, where he essentially says that societies can be compared based on how well they maximize the welfare of those with the lowest welfare (subject to caveats about certain basic human rights).  If the typical welfare function is a weighted average of individual welfare $U_i$ like: $$\sum a_i \cdot u_i$$ over all $i$, then this welfare function is $$\min u_i$$.
Since the minimum is the 0th percentile, I think this would just barely qualify as an answer to your question. I suspect that's not exactly what you had in mind, but it might be helpful.  This metric has appeared in a number of economics papers including Amartya Sen 's 1977 Econometrica paper,  On Weights and Measures: Informational Constraints in Social Welfare Analysis, The Aggregation of Climate Change Damages: a Welfare Theoretic Approach (Fankhauser, Tol, and Pearce (1997)), and Optimal Redistributive Taxation When Individual Welfare Depends Upon Relative Income (Boskin and Sheshinski (1978)) One more thing:
I've also seen a blog post that compares Swedish and American incomes at all income deciles and shows the level of percentiles at which a poor person would be better off in Sweden:
 The Economist's View has a related figure for more countries. 
"
"Banking Regulatory Frameworks, between the 1973 collapse of Bretton Woods and the introduction of the Basel Accords in 1988?","I found two of two papers discussing this period, the second at length. Throughout the 1970s, the capital position of many banking
  institutions declined significantly. To address this decline, in
  December 1981, the bank regulators issued explicit minimum capital
  standards for banks and bank holding companies. These standards
  required banks to hold capital at least equal in amount to a fixed
  percentage of their assets. While these standards have been given
  credit for increasing bank capital ratios, the 1980s saw an increase
  in both the number and cost of bank failures. A weakness of the
  minimum capital standards is that they failed to take into account the
  risk in a bank's portfolio of assets; high-risk assets required the
  same amount of capital as low-risk assets. Risk-based capital, portfolio risk, and bank capital: A simultaneous equations approach by K Jacques and P Nigro (1997) In 1972 the Fed capital standard was revised again. Asset risk was
  separated into “credit risk” and “market risk” components. In
  addition, banks were required to maintain a higher capital ratio to
  meet the test of capital sufficiency. Further, the Fed reintroduced
  both the capital to total asset and capital to total deposit ratios.
  This time, however, the former ratio was based on total assets less
  cash plus U.S. government securities, a rough “risk asset” adjustment.
  In practice, bankers and analysts used the FDIC and Fed standards more
  than those of the OCC. None of the agencies established a firm minimum capital ratio.
  Instead, the capital positions of banking institutions were evaluated
  on an individual bank basis. Particular attention was directed toward
  smaller banks whose loan portfolios were not as diversified and whose
  shareholders were fewer in number than those of larger institutions.
  It was reasoned that small or “community banks” might have a hard time
  raising capital in times of difficulty and therefore should be more
  highly capitalized at the start than larger institutions. Table 1
  shows the banking industry’s capital-asset ratios from 1960 to 1980.
  The table shows that there was a steady downward drift in the ratio,
  which can be explained by a number of factors. Chief among these would
  be the attractiveness of increased leverage in banking and reliance on
  other techniques to manage balance sheets, e.g., liability management. In late 1981 the three Federal bank regulatory agencies announced a
  new coordinated policy related to bank capital. The policy established
  a new definition of bank capital and set guidelines to be used in
  evaluating capital adequacy. The new definition of bank capital
  included two components: primary and secondary capital. Primary capital consisted of common stock, perpetual preferred stock,
  surplus, undivided profits, mandatory convertible instruments (debt
  that must be convertible into stock or repaid with proceeds from the
  sale of equity), reserves for loan losses, and other capital reserves.
  These items were treated as permanent forms of capital because they
  were not subject to redemption or retirement. Secondary capital
  consisted of nonpermanent forms of equity such as limited-life or
  redeemable preferred stock and bank subordinated debt. These items
  were deemed nonpermanent since they were subject to redemption or
  retirement. In addition to the new definition of capital, the agencies also set a
  minimum acceptable level for primary capital and established three
  zones for classifying institutions according to the adequacy of their
  total capital. International risk-based capital standard: History and explanation by MC Alfriend (1988) In the United States it isn't just Basel that is changing. FDICIA act of 1991 is also an important regulatory change for regulatory capital standards. My understanding is that Basel I only worried about credit risk but FDICIA introduced capital requirements for interest rate risk as well, perhaps in response to the contemporaneous savings and loan crisis where many thrifts failed as a result of losses from interest rate exposures. "
What makes a company too big to fail?,"In economics Too Big To Fail (TBTF) can have slightly different meaning depending on what research you are looking at but generally speaking literature seems to agree that what matters is how interconnected or systemically important firm is (e.g. see Bernanke 2010;  Zhou, 2009). Systemically important firms are such that other firms critically depend on them. In fact in recent literature you will often see people arguing that TBTF is a misnomer and better terms would be “too complex to fail” or  “too interconnected to fail” (see Kaufman, 2014), because really TBTF has much less to do with actual size than interconnectedness of a firm. This is the reason why you usually hear the term TBTF being applied to banks or financial institutions. Financial, institutions are an economic equivalent of infrastructure as they help to allocate capital to its most efficient uses and many other firms depend on them (see Mishkin & Eakins Financial Markets and Institutions). Consequently, whether General Motors or Snapchat are TBTF depends on how interconnected are with the rest of an economy. If there would be significantly many  'influencers' and 'content creators' that they would become significant part of an economy snapchat might be TBTF. This being said although I was not able to find specific papers on examining how interconnected GM or Snapchat are I am going on a limb to say that probably neither of them are TBTF. As argued by Strahan (2013) TBTF in most cases really applies mainly to financial sector due to reasons already mention above. In fact Strahan specifically argues in that paper that non-financial firms, using General Motors as an example of non-financial firm that would not be TBTF, can only be TBTF in rare cases. Although the paper makes just analytical argument for this and draws only on general literature without actually examining networks of General Motors specifically (importantly note just because firm is bailed out that does not mean it was TBTF - every company has incentive to argue its TBTF to get bailout and policymakers often cannot examine networks in real time so they have to make an educated guesses about whether firm is actually TBTF or not - if a non-financial firm claims to be TBTF its good to be skeptical given literature but I would not dismiss it out of hand either)."
Homogenous of degree one in utility function.,"The way you show that $v(p,m)$ is homogeneous of degree one in $m$ is correct, but the reason why this implies that, $e(p,u)$ is homogeneous of degree one in $u$, is not very precise in your argument. For example, duality tells us
$$v(p,e(p,u))=u,$$
where $u$ is just a target utility level, but should not be $u(x)$ as in your proof. Here is one possible way to proceed: Since $v(p,m)$ is homogeneous of degree one in $m$, it can be written as
$$v(p,m)=mv(p,1)=m\tilde v(p).$$
Applying the equality $v(p,e(p,u))=u$ gives
$$e(p,u)=\frac{u}{\tilde v(p)},$$
which clearly implies that $e(p,u)$ is homogeneous of degree one in $u$. You can use a similar argument to prove homogeneity of the Hicksian demand. With all that said, I would suggest you prove the original statement directly using the definitions of expenditure function and Hicksian demand. For instance,
\begin{align*}
e(p,\lambda u)&= \min p\cdot x ~~\text{ s.t. } u(x)\geq \lambda u\\
&=\lambda\min p\cdot\frac{1}{\lambda}x ~~\text{ s.t. }\frac{1}{\lambda}u(x)\geq u\\
&=\cdots
\end{align*}"
Does 'ethical investing' have any effect?,"In a world where capital markets are infinitely deep there shouldn't be any price response to capital and so we wouldn't expect any consequences on firm behavior. There is some good empirical evidence for slow moving capital so that's probably not true. Ethical investing is likely to drive up capital costs at least some. In which case your question boils down to two questions, ""can an non-ethical company become an ethical one"" and one that may seem quite different, ""what is the elasticity of demand for capital with respect to the price of capital?"" Tobacco and munitions companies can shift into other business but they can't get ethical investment funds without abandoning that business. Other businesses like fair-trade coffee and certified conflict-free diamonds, may allow ethical investing in the current business with higher costs. The former group really only have one choice, to shrink in response to expensive capital. The latter group have a second option, to shift into ethical investing business models that may be more expensive but less expensive than the increase in the cost of capital.  Let's ignore that first channel for a minute. Let's just consider a world with some ethical industries and some non-ethical ones. One day some people wake up and become ethical investors and this causes a change in the demand for non-ethical securities. If firms have an inelastic demand for capital (lower right graph, moving from $S_2$ to $S_1$) then the quantity of capital demanded changes relatively little from the ethical preference shock and so the  total capital invested in the non-ethical industry changes very little ($Q_2$ to $Q_1$). Instead returns go up a lot ($P_2$ to $P_1$) for those investors who remain invested in non-ethical industries.  On the other hand, if capital demand is quite inelastic (lower left graph, moving from $S_2$ curve to $S_1$)), the return on capital changes very little ($P_2$ to $P_1$) but the total capital invested in  non-ethical industries falls a lot ($Q_2$ to $Q_1$). In that world the ethical industries crowd out the non-ethical ones because the profits aren't there to preserve their capital supply. Now consider production shifting. If substitution into ethical practice is more expensive than paying a higher cost of capital, this won't be an important channel. Firms are better of taking their licks in the capital markets. However, if substitution is cheaper (like when capital demand is inelastic and and substitution easy) then behavioral adaption will become the dominant channel. In practice, according to Does Ethical Investing Impose a Cost Upon the Firm? A Theoretical Perspective by James J. Angel and Pietra Rivoli (1997), the actual effects on capital costs and quantities are quite small, suggesting that neither industry shrinking nor industry transformation are important under prevailing circumstances. If more money shifts into ethical investing this could easily change."
Does economic theory support the notion that the wealth of the wealthy is based on the poverty of the poor?,"Let me preface this answer with a word of caution: your question is a very good and important one but it is also one that depends tremendously on the definitions of the terms it uses. I'm going to attempt to answer it in the most unassuming and non-technical way. You could pose it in technical terms and get a more precise answer. The wealth of the rich can be attributed to the poverty of the poor. In North Korea, one of the poorest countries in terms of per-capita GDP, there are few rich people and many poor people. It is well-known that almost all of the rich people work either in the highest levels of government or the military. The poor people who are lucky enough to make any income largely do so at the behest of the rich people, and those who refuse are imprisoned or worse. Since almost all economic activity in North Korea is centrally planned via force, it follows that the rich derive their wealth from the suffering of the poor. However, in societies with more private ownership of production and capital - and less centralization - force is still useful in general but less of a factor in determining who is wealthy and who is poor. Regardless of our relative lot in life, if you and I freely trade with each other, we only do so when both of us feel like we are ""better off"" as as a result. So, if you are a wealthy industrialist and I am a poor banana farmer, and I freely decide to trade 100 of my bananas for 100 of your dollars, you necessarily think a banana is worth at least 1 dollar to you, and I necessarily think a banana is worth at most 1 dollar to me. Excluding outright fraud (misrepresentation either of your dollars or my bananas) we must conclude that each of us either thinks this is a fair trade or that the other guy is a dope. For instance, you may know something about the value of bananas that I don't. Maybe you've just discovered that bananas have special healing powers or that my bananas are particularly good. On the other hand, maybe I know something you don't. Maybe I know that there are about to be many more banana producers in the area or that I can buy two bananas for 1 dollar somewhere else. Usually, however, it's just a matter of what each party in the trade is able to do with the traded goods that determines what he/she thinks they are worth. If you get rich from selling my bananas at a higher price, you are not doing so ""at my expense"" as long as you are not preventing me from doing the same thing. There may be reasons you have the ability to sell bananas at a higher price that have nothing to do with force - maybe you own a distribution system or retail outlets, and I don't - but my choice isn't between selling retail bananas and selling wholesale bananas, but rather selling bananas at the highest possible price vs not selling bananas at all."
Fair voting procedure when there are many issues,"That's interesting: the flavor of the frequentist approach to probability used for a socio-political fairness criterion: if my measure as a population group is $0<p<1$, and known, then my opinion should be accepted by the whole at the same measure, as number of issues goes to infinity. In other words, current observed acceptance rate should be a consistent estimator of theoretical acceptance rate, and equal to my measure.   Then it is very easy to create such a decision rule, while saving public money: no need to hold one referendum after another, just construct a die, with as many sides as there are ""uniform groups"", with the die's weight distributed in such a way that the side representing uniform group $i$ will have probability of turning up equal to $p_i$. It won't be difficult to construct, and publicly and objectively test it for the desired properties. Then, wherever an issue comes up for voting, just roll the die. And ok, spend some money for a suitable public ceremony. Whenever there is a census, the relative size of each uniform group can be re-measured and a new die can be constructed. Why do I have the feeling though that no uniform group is likely to ever accept such a scheme? (This of course puts aside the importance of each issue, in general, for each uniform group, etc, but I took that from the OP which concentrates on number of issues, irrespective of what the issues are about, and to whom they matter and how much they matter, and how do we measure that etc)."
What are the economic perspectives regarding the game of salary negotiations?,"I will provide a simple game-theoretic modelling of the situation.
A new year starts and a company wants to make a wage-increase offer to an existing employee. Let $e$ be the employee's current efficiency and the corresponding wage $h(e)$ (which, represents an increase over previous wage). Let $v$ be the premium observed in the market for new hires (so if the employee goes to another employer he will earn $h(e) + v$). Let $c$ be churn costs (recruitment plus loss of efficiency etc) to the current employer, if the employee leaves and needs to be replaced.  This is a sequential game so we have to use the extensive form.  A) Firm offers a wage  First, the case where the firm ($F$) offers a wage and the employee ($E$) decides what to do:  The first outcome refers to the firm's cost, the second to the employee's wage. We have assumed that if the employee is offered the new hire premium it stays with the firm.
Let $p_l$ be the probability that the employee will leave if he is offered $h(e)$ only. The firm faces the following expected costs: $$EC [h(e)] \equiv EC_{A1}= (1-p_l)h(e) + p_l[h(e)+v+c] = h(e)+p_l[v+c]$$ $$EC [h(e)+v] \equiv EC_{A2} = h(e)+v$$ Then in order for the firm to nevertheless offer $h(e)$ it must be the case that $$EC_{A1} < EC_{A2} \implies h(e)+p_l[v+c] < h(e)+v$$ $$\implies p_l < \frac {v}{v+c}$$ and it should offer $h(e) + v$ if the inequality points to the other direction.  Let's move now to the OP idea, to tell the employee to ask for a wage. Here we have B) Employee asks for a wage   Here too the first outcome is the firm's cost. We have allowed for the possibility that the employee asks only the efficiency wage. This is crucial. Since we are in the ""decision about the process structure"" phase, we assign some probability $p_e$ that the employee may actually ask for just the efficiency wage. This is important.
It is also important to note that, assuming that the employee has asked for $h(e) + v$, while the criterion for whether the firm should accept or counter-offer only $h(e)$ has exactly the same expression as before, we are looking at a different probability. Here the firm must decide whether the employee is ""bluffing"" (he does not have an offer from another firm), or not. This is a different probability than the previous one. Here, the firm has additional information (for better or for worse), and so it has to make a difference assessment. Call the probability of not bluffing $p_c$.   We have that  $$p_l < p_c$$  because the former is a conditional probability of the same event (""the employee leaves""), while the latter is the unconditional probability. Keep this inequality for later.   Assume that the firm has estimated somehow (""if the employee asks for $h(e)+v$ there is $p_c$ probability that he will leave if I counter offer $h(e)$ only).
If this is estimated, then the firm already knows what it will do if the employee asks for the new hire premium -and it will depend on the specific values of the various quantities here.   So given an estimate for $p_c$ we are looking at two possible expected costs for the firm
B.1. The firm will counteroffer $h(e)$
here the expected cost is $$EC_{B1} = p_eh(e) + (1-p_e)\cdot [(1-p_c)h(e) + p_c(h(e)+v+c)]$$ $$=  h(e) + (1-p_e)p_c(v+c)$$ B.2. The firm will accept $h(e)+v$ $$EC_{B2} = p_eh(e) + (1-p_e)(h(e) +v) = h(e) + (1-p_e)v$$ WHAT STRUCTURE TO CHOOSE? Now we want somehow to compare the two structures and select the one that is more profitable for the firm. This requires to examine various cases characterized by the relation between the various probabilities. CASE 1 : $p_e = 0 , p_l < p_c < v/(v+c)$ Here the firm will offer $h(e)$ in structure $A$ (so expected cost $EC_{A1}$), and will counter-offer $h(e)$ in structure $B$ (so expected cost $EC_{B1}$). 
Given the assumed values of the probabilities we have that  $$EC_{A1} = h(e)+p_l[+v+c] < h(e) + p_c(v+c) = EC_{B1}$$ an so we should stick with the traditional structure $A$ where the firm offers first a wage. CASE 2 : $p_e = 0 , p_l  < v/(v+c) < p_c$ Here the firm will offer $h(e)$ in structure $A$ (so expected cost $EC_{A1}$), but will accept $h(e)+v$ in structure $B$ (so expected cost $EC_{B2}$). 
Given the assumed values of the probabilities we have that $$EC_{A1} = h(e)+p_l[v+c] <  h(e) + v =EC_{B2}$$ and again we should stick with structure $A$. CASE 3 : $p_e = 0 ,  v/(v+c) < p_l  < p_c$
Here we compare $EC_{A2}$ with  $EC_{B2}$
$$EC_{A2} = h(e)+v  = EC_{B2}$$ No winner here, but overall we see that 
the incentive to adopt structure $B$ hinges on whether the employee may after all ask only for $h(e)$. ($p_e>0$ is necessary but not sufficient condition to adopt structure $B$). CASE 4 : $p_e > 0 , p_l < p_c < v/(v+c)$ Here too we compare $EC_{A1}$,with $EC_{B1}$ but with $p_e>0$ so $$EC_{A1} = h(e)+p_l[v+c] < >h(e) + (1-p_e)p_c(v+c) = EC_{B1}$$ We stick with structure $A$ if $p_e < (p_c -p_l)/p_c$, and we adopt structure $B$ if the inequality runs the other way. CASE 5 : $p_e > 0 , p_l  < v/(v+c) < p_c$ Here we compare  $EC_{A1}$, with $EC_{B2}$ but with $p_e > 0$  $$EC_{A1} = h(e)+p_l[v+c] <  h(e) + (1-p_e)v = EC_{B2}$$ as one can verify. So here we stick with structure $A$. Finally CASE 6 : $p_e > 0 ,  v/(v+c) < p_l  < p_c$ Here we compare $EC_{A2}$ with  $EC_{B2}$
$$EC_{A2} = h(e)+v > h(e) + (1-p_e)v = EC_{B2}$$ and we should go with structure $B$. VERBAL SUMMARY  1) If we expect that employees will always ask for the new hire premium if they get to ask first, then we should stick with the structure where the firm offers first a wage.  (Cases (1,2,3) 2) If there exists a positive probability that the employees may just ask for $h(e)$ then :
2a) If the firm will stand by $h(e)$ in any case and structure, we should keep the structure where the firm offers first a wage if $p_e < (p_c -p_l)/p_c$ (Case 4)
2b) If the firm will go for $h(e) + v$ in any case and structure, we should choose the structure where employees ask first (Case 6).
2c) If the firm will play differently in the two situations, we should keep the structure where the firm offers first a wage. (Case 5).  As is usually the case reality is more complex than that: negotiations may have more rounds, and the firm and the employee may not even agree on $h(e)$ although such disagreement is less common than ""accepted wisdom"" would have it.   But the general feeling I get from all the above analysis is that the main reason I would consider implementing a structure where employees ""ask first"" is if I thought that there exists a high enough probability that they won't ask for the new hire premium -and still, if at the same time I think that they won't try to bluff (i.e. I expect $p_c$ to be close to unity), again it would be likely preferable to stick with the traditional model."
Are obstructed views a negative externality?,"In general, yes it's an externality: it's a cost borne by others. How do we know it's a cost with real economic value? Because in general, properties with good light and better views tend to attract higher purchase prices and higher market rents. And there's a difference between ""view/light might be obscured some time in the future"" versus ""they are obscured now"" - in sufficiently sophisticated and liquid markets, that would be captured by the price of an option."
Perfect Bayesian Equilibrium,"Let the strategy of player 1 be represented by $(x1_1,xDD_1,xDC_1,xCD_1,xCC_1)$ where $x1$ is the first round action of player 1, $xDD_1$ is the action taken at the information set where both players have defected in the first round, $xDC_1$ is the action taken at the information set where player 1 has defected and player 2 has cooperated in round 1, etc.. Note that something like $(x1_1,x2_1)$ (with $x2_1$ being the action taken in round 2) is never a full specification of the strategy of player 1, since we need to specify behavior at each information set separately. Define the strategies of player 2 similarly. However, a perfect Bayesian equilibrium must also specify the beliefs of the player, $\mu_1,\mu_2$. This is an important part of the specification of an equilibrium. As we will see below, the question is geared towards understanding that a different equilibrium does not require the strategies to differ. A difference in beliefs is sufficient to count as a different equilibrium. The perfect equilibrium is given by: $((D,D,D,D,D),\mu_1)$ for player 1 and $((D,D,D,D,D),\mu_2)$ for player 2, where $\mu_1$ and $\mu_2$ are consistent beliefs at all information sets. As has been noted in the comments, since ""defect"" is a dominant strategy irrespective of beliefs, even in a weak perfect Bayesian equilibrium the strategy profiles must be $(D,D,D,D,D)$ for both players. However, the following is now also a weak perfect Bayesian Nash equilibrium: $((D,D,D,D,D),\mu_1')$ and $((D,D,D,D,D),\mu_2')$ with $\mu_1'$, $\mu_2'$ consistent on the equilibrium path. Thus, the question is not wrong, it simply shows that two weak perfect Bayesian Nash equilibria can have identical strategies as long as they differ in beliefs off the equilibrium path."
Causes of trade deficit: Multiple views,"Often a good answer consist the most of shedding light on some aspects of the terms of the question itself. I hope a couple of remarks could be useful. About causality, the fact is that current deficit implies reserves exit from USA, which only can be supported by reserves entering, otherwise resulting in a diminishing stock of reserves, eventually exhausting it. A capital-account superavit --net inflow of capital-- represents an inflow of reserves, in that sense financing trade deficit (see Economic Report of the President, 2004). Letting aside causality --which can be very complex and call for other matters-- both imbalances are bond to present themselves simultaneously. In other terms, the fact that trade deficit and capital superavit pop up strongly correlated doesn't mean a causality. I suspect --sorry again for the lack of evidence-- that there's no causality there indeed. Instead, those paths suggested in the question about rates of interest or foreign exchange rates could be more promising sources of explanations, probably besides productivity and cost structures (again, see Conclusions in Economic Report of the President, 2004). It would be an example of third -unknown- causal factor. About repaying the debt with capital inflows, we have to keep in mind that those capital inflows are new debt. For example, see Kirabaeva, Koralai & Razin, Assaf. (2010). Composition of Capital Flows: A Survey, NBER Working Paper Series. Aside with debt-instruments, capital inflows also comprise equity-instruments, i.e. Foreign Direct Investments and equity purchases. Depending on the context, even the last ones could also be considered national 'debt', as it represents foreign ownership of 'local' firms. Interestingly, the ""US's control to decide what to do with the capital we get"" reflects the conflation in an unique agent ""US"" which in practice is a set of --millions of-- firms, consumers, authorities... There is no central control on the employment of all those flows, even though all those agents are acting, and controlling, in the USA, in that sense all them constitute ""US's control""."
"Do large public companies ever ""quit while they're ahead""?","Assume a company decides to ""wind up"". What would happen? Its competitors would want to get hold of its tangible and intangible assets (like customer base, patents etc). How is this different from ""fighting to a buyout""? In this second scenario, the company has always available the option/threat of continuing competition (possibly harmful to the competitors), while in the ""wind-up"" scenario it does not -it has committed to leave the business. So ""fighting to a buyout"" gives the company better position in negotiations, and so chances to increase its selling price compared to the ""wind up"" scenario.   You could spice this with theories about management, and how ""management will not let go"" etc, but the above provides an interpretation that appears more widely applicable. Compared to smaller companies, large companies have more market power and so the ""threat"" of continuing competition weighs more heavily with competitors."
"Straub and Werning, 2014, on zero capital taxation","The non-mathematical answer I believe is well described in Straub & Werning's last paragraph: ""In quantitative evaluations it may well be the case that one finds a
  zero long-run tax on capital, e.g. for the model in Judd (1985) one
  may set $\sigma  < 1$ ($\sigma$ is the reciprocal of the intertemporal elasticity of substitution), and in Chamley (1986) the bounds may not bind forever,
  depending on parameters. In this paper we stay away from making any
  such claim, one way or another. We confined attention to the original
  theoretical results, widely perceived as delivering zero long-run
  taxation as an ironclad conclusion, independent of parameter values.
  Based on our analysis, we find little basis for such an
  interpretation."" Just above their Proposition 2, there is also another illuminating passage: S&W write:   ""(...)This shows that the solution cannot converge to the zero-tax
  steady state. Indeed, it actually proves the solution cannot converge
  to any interior steady state, since, we argued, the only possible
  interior steady state is the zero tax steady state."" In other words, it appears that Judd and  Chamley did not fully solved their models, but provided results conditional on the parameters being such that the steady state will be an interior one. S&W argue (I have not checked mathematical correctness), that, depending on the parameter values, the optimal solution may lead to a corner steady state in some aspect (see below for an example) -and in a corner steady state, the capital tax-rate will be positive.  This needs checking because Judd explicitly considers different values for $\sigma$ (so a plain mistake may be involved after all). Now if you ask me, this mostly indicates that the tools used may not be well suited after all (or we have somehow ""misused"" them) to solve the specific theoretical problem, and so they are unreliable to affect policy one direction or the other (which, by the way, reminds me of my question...) Because, exactly of what value is (for either theory or policy) to, say, empirically determine that the elasticity of intertemporal substitution is lower than unity, and then declare ""the optimal taxation solution for a government that cares only about the workers entails positive tax on capital and zero consumption of workers"" at the steady state?  (see Proposition 3, p. 11 of S& W). Who is going to contemplate seriously such a proposition for purposes of real-world policy? ADDENDUM   $\sigma >1 \Rightarrow$ the intertemporal elasticity of substitution ($1/\sigma)$ is lower than unity. $\gamma =0\Rightarrow$ the planner puts $0$ weight on capitalists' utility. $c_t$ is the workers' consumption, $C_t$ is capitalists' consumption, and $g$ is governments own consumption (i.e. it is not the part of government budget that goes to the workers as transfers).   Again, I have not checked the mathematics here. Apart from what kind of economic content and relevance one could conceivably provide for workers consumption going to zero, another issue here is that, if the limiting tax rate is going to unity as government own consumption goes to zero, then what happens to the Tax revenues (which are not given to the workers as transfers because then their consumption could not go to zero?)"
How does a bank convert one currency to another?,"On the international foreign exchange market, many millions are traded at a time. Otherwise, with smaller amounts of money, the costs of transferring that money (e.g. via SWIFT) would be too high for the amount transferred. That is why investors and small businesses do not trade in the forex market themselves—they go through the financial intermediary. The rest of my answer is copied from https://kevinkotze.github.io/if-2-forex/ , which comes from Kevin Kotzé's web page https://www.economodel.com/international-finance 1.1 Communications and funds transfers The enormous volume of trade in the foreign exchange market requires an extensive communication network between traders and a sophisticated settlement system to transfer payments in different currencies between the buyers and sellers of different currencies. Traders are able to obtain information that is provided by major commercial distributors such as Reuters and Bloomberg. The traders are then able to contact each other, to obtain actual prices and negotiate deals. In addition, they could approach a foreign exchange broker to broker a deal, or they can trade on an electronic brokerage system, where quotes on a screen are transactable. When a trade is agreed upon, banks communicate and transfer funds electronically, using systems such as the Society of Worldwide Interbank Financial Telecommunications (SWIFT), which confirm trades and facilitate payment. As Cross-Currency transactions may involve the simultaneous exchange of currencies, there is a risk that only one leg of the transaction may be completed, due to the possibility that parties use different systems in different countries that operate out of different time zones. This is known as cross-currency settlement risk, or Herstatt risk. Recently, foreign exchange dealers, encouraged by the BIS, have developed a number of practices to limit settlement risk. These measures include: firstly, banks now have strict limits on the amount of transactions they are willing to settle with a single counterparty on a given day. Secondly, banks have started to engage in a variety of netting arrangements, in which they agree to wire the net traded amounts only at the end of a trading day. Thirdly, settlement risk is eliminated if the exchange of the two monies occur simultaneously in a process known as payment versus payment (PvP). More recently, we have witnessed the foundation of the Continuous Linked Settlement (CLS) Bank, which is owned by the world’s largest financial groups. CLS is the largest multi-currency cash settlement system, eliminating settlement risk for over half of the worlds foreign exchange payment instructions and its members include central banks, large commercial banks and other large corporations. The CLS daily settlement cycle operates with settlement and funding occurring during a five-hour window when all real time gross settlement systems are able to make and receive payments. This enables simultaneous settlement of the payments on both sides of a foreign exchange transaction. Each member holds a single multicurrency account with CLS, which has a zero balance at the start and the end of trading day. The settlement of the payment instructions and the associated payments are final and irrevocable."
Why don't profitable firms use previous profits to offset current loss,"The primary goal of most companies is to make money for its shareholders. They put money in, and they expect to either get dividends, or be able to resell their shares for a higher amount. It's the shareholders who own the company, and they are the ones deciding. And they want money. Otherwise they would give their money to charity. So when a company makes a profit, they will do a combination of one or more of these: Investors usually don't want the company to hoard profits. A safe reserve so it can go through difficulties, but that's it. Cash in the bank makes very little money, it's not a useful use of those funds for the investor: they'd rather get the money and invest it themselves according to their own risk profile. If the company doesn't do what the investors want, then the shareholders will usually change management to make sure it complies. So keeping employees on the payroll in tough times is not a primary goal of most companies. Of course, it's not a good idea to fire everybody at the first sign of trouble (it has an impact on the morale and performance of the other employees, it may cost a lot in severance, and when business picks up it will take time and cost money to hire and train new people), but if things are too bad, then it's the only option the company has to fulfil its obligation to shareholders. There may be exceptions: a company and its shareholders may decide that their social responsibility is more important than making money. Cue all the ""fair"", ""equitable"", ""social"", ""responsible"", etc. keywords. But those are really the exception rather than the norm, and there's a limit to what can be done. Even if it made profits consistently and kept those as reserves, those reserves can't last forever while the company is burning through cash. Remember that many companies have very small profits compared to the revenue and costs. Consider a company with usual revenue of 1 billion pounds, but with tight margins: their expenses could be 900 million pounds, so the profit is 100 million pounds. They keep the profit year after year for 5 years, so they now have 500 million pounds in the bank, and the shareholders haven't complained yet (as if). Now Covid strikes, and the revenue dwindles to 100 million pounds. If they keep all their costs the same, they're haemorrhaging 800 million pounds a year. They won't last a year like that! So yes, they will have to reduce costs, and part of this is laying off staff. Don't think those figures are realistic? In 2019, Easyjet has revenue of about 6.385 billion pounds, but expenses of over 6 billion pounds! With revenue slashed to a fraction of what it was, they obviously can't keep spending like they did. They would have needed to keep profits of the last 20 years just to be able to get through this year."
"Piracy/File sharing - Why aren't songs, movies or ebooks given for free (+ads) like TV?","It's called a Principal-Agent Conflict. The RIAA/MPAA act as agents on behalf of the people who actually produce content (and consequently end-consumer value). To maintain relevance to their principals', the RIAA/MPAA must signal value to them (i.e. claim loudly and repeatedly that they do something good for them [regardless of the validity of that claim]). Firstly, this signaling is demonstrably an example of the Principal-Agent problem, in that it diverts resources away from actually advancing the principals' interests, but more so in that it creates a perverse incentive for lobbying groups to fight imaginary fires. The impact of piracy on the broad economy is believed to be near zero, so IP laws are effectively just rent-seeking $^{[2]}$ anyway. Whether or not that particular type of rent-seeking is always a drain on the broad economy (in that it distorts markets producing deadweight loss in excess of the externalities corrected) is up for discussion, though it is known to be possible (insofar as the existence of an optimal tax [or government induced market distortion] is necessarilyy proof of the existence of an excessive tax)."
Mathematical Micro/Macro Economics Textbook Recommendation,"You should also try Advanced Microeconomic Theory (3rd Edition) by (Jehle, Reny, 2011).
Note that, imho, Mas-colell-Whinston-Green (1995!) is the best choice for those with initial background in Math switching to Economics. When it comes to Economics majors the former seems to be more appropriate (and more modern).
For modern Macro with strong math see Introduction to Modern Economic Growth (Acemoglu, 2009).
To integrate the micro/macro problems with econometrics and math see Economic Modeling and Inference (Christensen & Kiefer, 2009). Hope, that helps."
Mortgage loans from foreign banks at lower interest rates,"The low interest rate will be in a different currency. If your domestic currency falls in value, the value of the mortgage in terms of the domestic currency goes up. Entities borrowing in a foreign currency and then running into difficulties is a standard features of financial crises over history."
Does recycling lead to fewer jobs?,"Any invention that replaces human labor puts an end to that specific task. Glass recycling eliminates (or decrease) the need for silica-gathering task. Typewriter eliminates the need for printing press typesetter. Etc. Those people whose tasks are eliminated will get reallocated to their most productive use. This might be in the form of job change (silica miner move to coal miner), or might be in the form of task redefinition (the book Prediction Machines describe how self-driving school bus might shift the main task of a school bus driver to an adult who oversees and ""teaches"" the schoolchildren.) Going back to your question, recycling also creates new jobs. They need people to sort bottles, maybe drive a recycling truck, etc. So whether an invention leads to more or less job is ambiguous. Added: While the effect on a specific industry might be quantifiable, if you take into account job mobility, etc. then the effect on the whole labor market is ambiguous. Regardless, an invention that increases productivity should increase the size of the pie. We can produce more from the same resources. How that bigger pie is divided, however, is another very important question altogether."
Why is the rouble collapsing?,"The answer is very clear when you look at Russia's monetary statistics. The Central Bank of the Russian Federation has a very good site, and you can see them here: Russian Money Supply (M2) or courtesy of the St. Louis Federal Reserve:  They provide the annual expansion rate which is nice. Historically, Russia's money supply has always been an extreme outlier compared to other European countries. Compared to the USA's money supply, which roughly doubles every 10 years, Germany which was down at 1.3x/decade last time I checked, Russia's typically increases by 20x a decade. The open research question is why currencies tend to collapse suddenly, rather than over time, but the underlying reason is always found in their relative expansion rates. I would suspect that this particular episode has also been triggered by the recent drop in oil prices, since that will put additional pressure on oil exporters like Russia, due to the drop in income from exports."
Is the Federal Reserve issuing money in a fair way?,"The Fed introduces money in the economy through the banks via the mechanism of fractional-reserve banking, as you mentioned in your article. 
This means the Fed is then allowing the banks to provide credit to anyone who they find adequate. The reason to do so is that banks will favor investment opportunities which lead to long term growth, such as build a house or a factory, rather than immediate consumption of goods, which would lead faster to inflation. The alternative would be to transfer the money to the government, which would distribute it through investments (like build roads) or subsidies. Your question is on fairness: whilst there would be several ways to look at what ""fairness"" means (give the same to everyone OR give more to the ones who deserve/need more), the key point is that the Fed's job is to make the US dollar stable, and not to work on individual bank gains. You can read on the Fed's mission: (...) to foster the stability, integrity, and efficiency of the nation's monetary, financial, and payment systems so as to promote optimal macroeconomic performance Their approach to distributing the money is to keep the inflation low, which according to them and the US Congress, is on the best interest of the whole country and not just the banks."
What kind of economic freeware exists for economics students?,"I never understood why discussion about specialized software should be off-topic in the specialty's website. And of course I don't agree. So: Before diving into R, which indeed appears to be the dominant (and rich) freeware for statistical computing, one can try Gretl. It is an Econometrics freeware, with a lot of functionality, a very good Random Number Generator, and has both menu-driven implementation but also code-writing by the user. It is easier for beginners - but it is serious stuff. The wikipedia page for Gretl lists some reviews about Gretl.   The other kind of ""economics"" (not mathematics) software apart from econometrics packages would be specialized simulation software, for say Dynamic Stochastic General Equilibrium Models, or micro-applications.  For DSGE, one such is Dynare.  A lot of DSGE-code for various other software can be found at International Network for DSGE Modeling, Monetary and Fiscal Policy."
Is Marx considered an important classical economist?,"There is a major disciplinary specification problem here: “who is an economist?” At the time Marx was active as an author (including posthumously with Engels) the field of knowledge was known as “political economy,” so as to distinguish it from the domestic economy of household management—both from the Greek oikos.  Political economy was, and still can, be distinguished as a scholarly discipline as the study of the formation of “value,” or the question of “what is something worth?”  At the time three methods of dealing with this were present in the discipline: absolute labour content, proto-marginalisms, and utilitarianisms (subjective or moralistic evaluations). Marx primarily engaged with political economy through a critique of the labour theory of value, positing instead a realised (ie sold), valorised (ie applied to production, not wasted) socially-necessary (technical organisation), labour power (skill organisation), averaged (exertion) theory of value with the price of buying actual living labour to convert to labour power politically determined by the balance of class conflict (that is, the price of labour may be below life time replacement or day to day replacement: starvation wages may be what the politics bear). He also spent his time viciously heckling utilitarians over the incommensurability of internal desires and proto-marginalists. Economics, as a discipline, is founded in the reaction to the incommensurability problem of subjective prices, and instead posits marginal differences in effective demand instead of human desires. In the sense that Marx’s work is hard to reconcile as a system with marginalism, it is hard to posit Marx as an economist rather than a political economist. However, in the sense that Marx specifies a number of useful problems, represents the most highly developed outcome of labour theories of value, and that a whole load of scholars trained in Marx’s work work in economics departments he is of continuing influence in that discipline. "
"Lack of skilled IT workforce, but they do not raise wages","The factual observations you've listed fit neatly with the law of supply.  You've seen that wages (price) have fallen, and during the same period, supply has gone down. The fact that companies publicly complain that there aren't lots of cheap, highly skilled workers available is not the same as companies being willing to convert that into effective demand by raising wages. One common reason companies moan about things publicly, for instance, is that they want the government to intervene to help them.  In this case, perhaps they want government to provide more subsidised IT training, or permit more immigration in the sector, either of which could lower wages further or prevent them from rising."
What are some good graduate-level econometrics books for someone with a strong mathematics background?,"""Adult"" Wooldridge is great intro to various microeconometrics topics. For time series, Hamilton's Time Series and Lutkepohl's Introduction to Multiple Time Series Analysis are both nice, though Hamilton is a bit dated and Lutkepohl is more focused. As far as more foundational, rigorous material, Herman Bierens has a short Introduction to the Mathematical and Statistical Foundations of Econometrics. Gourieroux and Monfort have a whole flock of graduate econometrics texts, but to quote an anonymous reviewer, they are ""in the French tradition of excellent precision and terrible pedagogics,"" though they have their champions."
R or Python for private sector economist/strategist,"If you already know how to code in MATLAB then python is more similar to it than R so I would say you will have easier time to transitioning there.  Otherwise, both R and Python are programming languages so you will be able to do all those things in both of them as you can always program your own functions. The strength of R is that the language has a large following in the research community, which then in turn means that people write a lot of packages that focus on exactly the models that are commonly used in research. For more than two decades academics and statisticians basically helped to develop R so you can now find library for anything there save for really the most cutting edge econometric techniques - and you can bet that the first place where those will get their packages will be R. For most of the things you describe in your post you will have some excellent libraries and they are relatively more easier to estimate with R than with Python.  Python is more of an general purpose programming language which strength is very intuitive syntax. Writing in Python can sometimes feel like writing in pseudo code - its very intuitive and natural. However, since its more general purpose language its not as tailored to statistical analysis as R. You will definitely be able to do all you want to do there but more routine statistical analysis will be a bit more time consuming to code.   I actually prefer using R for any standard statistical analysis and Python for numerical analysis & machine learning & mining data through web-scraping. "
"What did ""18/9"", ""25/"", and ""30/"" mean in this 1800 British document?","Yes your guess is correct. In fact, the forward slash punctuation mark ""/"" actually comes to us through its being the abbreviation for the English shilling. From Humez and Humez (2008): ... forward slash (/), which is also variously known as a solidus, virgule, or just plain slash, when the deletion of a single character is to be marked. The solidus was a Roman coin and is ultimately the basis for English soldier, the idea being that a soldier is someone who fights for money, or, as we might say today, a mercenary (from the Latin merces ‘wages.’) The solidus as slash is historically a straightened-out S, the abbreviation for the English shilling (as in 2/3—two shillings thruppence), the step from one coin to another being relatively easy. How we get from shilling to slash in its various other uses—as proofreader’s strike-out mark, arithmetical sign of division, general separatrix, and so on—is rather more murky."
"When countries industrialise, why do jobs from agriculture shift to manufacturing?","This is a common area of study in Development Economics. There is for example the Dual-sector model, first developed in 1954. It is very well explained in the link provided, but basically: [the] agricultural sector is typically characterized by low wages, an abundance of labour, and low productivity through a labour-intensive production process. In contrast, the capitalist manufacturing sector is defined by higher wage rates as compared to the subsistence sector, higher marginal productivity, and a demand for more workers. Also, the capitalist sector is assumed to use a production process that is capital intensive, so investment and capital formation in the manufacturing sector are possible over time as capitalists' profits are reinvested in the capital stock. [...] The primary relationship between the two sectors is that when the capitalist sector expands, it extracts or draws labour from the subsistence sector. This causes the output per head of labourers who move from the subsistence sector to the capitalist sector to increase. [...] The agricultural sector has a limited amount of land to cultivate, the marginal product of an additional farmer is assumed to be zero as the law of diminishing marginal returns has run its course due to the fixed input, land. As a result, the agricultural sector has a quantity of farm workers that are not contributing to agricultural output since their marginal productivities are zero. This group of farmers that is not producing any output is termed surplus labour since this cohort could be moved to another sector with no effect on agricultural output. [...] The end result of this transition process is that the agricultural wage equals the manufacturing wage, the agricultural marginal product of labour equals the manufacturing marginal product of labour, and no further manufacturing sector enlargement takes place as workers no longer have a monetary incentive to transition. In other words,, low productivity in agriculture due to unlimited land and workers and low use of capital means low agricultural wages, whereas high productivity in new capital intensive industries means high wages, thereby leading to a migration process that continues until wages equalise. It might be worth noticing that this model was also used by Simon Kuznets to explain why industrialised countries saw a non-monotonic evolution of wage inequality between 1870 and 1950 (i.e. an increase and then a decrease in inequality), pattern that came to be known as the Kuznets Curve. As the article above states: The Kuznets curve implies that as a nation undergoes industrialization – and especially the mechanization of agriculture – the center of the nation’s economy will shift to the cities. As internal migration by farmers looking for better-paying jobs in urban hubs causes a significant rural-urban inequality gap (the owners of firms would be profiting, while laborers from those industries would see their incomes rise at a much slower rate and agricultural workers would possibly see their incomes decrease), rural populations decrease as urban populations increase. Inequality is then expected to decrease when a certain level of average income is reached and the processes of industrialization – democratization and the rise of the welfare state – allow for the trickle-down of the benefits from rapid growth, and increase the per-capita income."
Getting up to Date on Macro,"A good site, with high quality online lectures, is the MIT Open Course Ware.  You will find courses going from Principles of Macroeconomics, Economic Growth, Macroeconomic Theory I, II, III to Advanced Macroeconomics I or II."
What is the evidence that econometrics has empirical value?,"To (slightly) paraphrase the OP:  Economies (Human Bodies) are extremely complex systems with many variables, not to mention the fact that they emerge from the
  interactions of complex
  beings (factors). I agree that economies (human bodies) have certain underlying principles, but I remain skeptical of the
  overall value of econometrics (medicine) as a science. A truly useful econometrics (medical science) would be a
  valuable tool in predicting the future behavior of the economy
(human body), particularly in predicting shocks like the recent financial crisis and Great Recession (severe illnesses or near
  death). But it did not happen. If econometrics (medical science) can't predict the future, how
  do we know it even effectively describes the past? What is the
  evidence that it has empirical value? Comment: There is a Present that needs to be dealt with, just like with human bodies. To (slightly) paraphrase Milton Friedman:  I don't care how I predict, as long as I predict adequately. Comment: But what do we do when our predictions are not adequate ? Econometrics is based on Mathematical Statistics on the one hand, and on the assumptions made by Economic Theory on the other, which in turn are based on (imperfect) empirical observation, and are then led to their logical conclusions. In other words, Econometrics uses rigorous mathematics, induction, deduction and all the words dear to an epistemologist. Its epistemological foundations are solid as a rock.   What hangs on the balance is the observed validity of its abstractions. So the question is not whether Econometrics is a science, in the sense of whether it follows the scientific method or not. It does, fully.  The question is whether its results are useful.   But what are the criteria in order to determine ""useful""? Is it just adequate predictions? That would be a matter of disagreement between humans.   And is it a matter of a ""yes/no"" answer?  Or is it a matter of degree to which it is useful? In which case, we have to somehow measure this degree (after we have agreed on the criteria), which brings us back to where we have to collect, analyze, assess, and debate the evidence."
Why do low-budget films charge the same amount at the box office as super-high budget films?,"Opportunity Cost of the Seats Once the movie is made the cost of production is sunk and irrelevant to the proper pricing of tickets. Only the marginal costs of serving an additional customer and the opportunity cost of showing a different film would enter into ticket pricing. Since cinemas should be setting the number of screens for each movie so that the opportunity costs of the seats are equated across films, this makes them want to charge the same for all films. In support of this idea, I offer that movies do vary in price by time of day. This is a rational response to the perishability of seats (once the movie starts an empty seat for that show is worthless) and time (of day) varying demand for film-going which varies the opportunity cost of the seats by time of day. The theater can't easily equate the opportunity cost across time the way it can across films.  At the Movies: The Economics of Exhibition Contracts (Filson, Switzer, and Besocke (2004)) provides the following explanation: Practitioners provide several explanations for inflexible ticket
  prices. Exhibitors want to avoid menu costs and eliminate consumer
  uncertainty about what the movie will cost. Exhibitors do not increase
  prices of hits because they are engaged in repeat business with local
  consumers, and the potential loss of goodwill from increased prices
  outweighs the potential gain. Charging different prices for different
  movies at multiplexes necessitates employing monitors to ensure that
  consumers see the movies they pay for. Even offering mid-week
  discounts may lead to more time shifting than new demand. Not all
  analysts or practitioners agree that inflexible prices are optimal
  (see Orbach and Einav 2001), although it seems unlikely that such an
  easy-to-exploit profit opportunity would persist. Some practioners
  have experimented with non-uniform prices in  the U.S. in the recent
  past but inflexible prices remain the norm."
Are terrorists rational?,"I guess you might already know this, but I wanted to add a little detail to the other answers for the sake of any layman who comes here and gets the wrong end of the stick. It is important to begin by saying that when economics use the term rational they have in mind a fairly precise definition that does not perfectly coincide with the way the word is sometimes used colloquially: We say that a decision maker is rational if On the first bullet: suppose some one has to make a decision. We say that they have transitive and complete preferences if (i) they are able to rank the alternatives among which they choose from ""most preferred"" to ""least preferred"", and (ii) that ranking is internally consistent. On the second bullet: A person acts in an optimal manner given their preferences if they choose the alternative that is 'most preferred' among the set of all feasible alternatives. As others have noted, the literal answer to the question is that we can't really (empirically) verify that terrorists are rational. This is because observing one rational decision does not rule-out the possibility that the individual concerned made an irrational decision at some point in the past (or that they will make an irrational decision in the future). The best we can manage is to look for behavior that violates the two conditions above and take observations of such violations to be evidence of irrationality. But, to address the spirit of the question: just because voluntarilly committing suicide and murder in the name of your beliefs may seem irrational in the colloquial sense does not mean that it is necessarily so. Indeed, if a person has carefully considered the alternatives available to them and decided that the option they find most attractive is to conduct a suicide attack then this behaviour is entirely consistent with rationality. It is important to stress that there is no value judgement implicit in an eocnomist describing behaviour as rational. Just because something is rational behaviour does not mean that it is good or desirable or can be condoned. Rather, it just means that we think we have a systematic way of understanding why people might choose to behave in a certain way. Building a systematic understanding of a phenomenon is an important first step in deciding upon the best way to respond to it."
Game Theory Book,I will also recommend The Art of Strategy by Dixit and Nabebuff. But do also check out Game Theory: A Very Short Introduction by Ken Binmore. I am not sure how much game theory you will learn from the recommended books by Kahneman or Roth (the Kahneman book is not about game theory; the Roth book is about matching and touches on game theory rather tangentially.)
Which capital accumulation is right? $K_t = (1-\delta)K_{t-1} +I_t$ or $K_t = (1-\delta)K_{t-1}+I_{t-1}$?,Both are economically sound. The notation is just a question of convention. The reason behind the ambiguity is that capital is a stock and investment is a flow variable. You are looking at capital in two different instants. Investment happens during the time between the two instants and its index is either the starting or the ending instant. 
"Statistical models which represent the impact of strict measures vs no strict measures (and less economic impact), enforced due to Covid-19","""The Benefits and Costs of Using Social Distancing to Flatten the Curve for COVID-19"", Thunstrom et al. We examine the net benefits of social distancing to slow the spread of COVID-19 in the United States. Social distancing saves lives but imposes large costs on society due to reduced economic activity. We use epidemiological and economic forecasting to perform a rapid benefit-cost analysis of controlling the COVID-19 outbreak. Assuming that social distancing measures can substantially reduce contacts among individuals, we find net benefits of about $5.2 trillion in our benchmark case. We examine the magnitude of the critical parameters that might imply negative net benefits, including the value of statistical life and the discount rate. A key unknown factor is the speed of economic recovery with and without social distancing measures in place. A series of robustness checks also highlight the key role of the value of mortality risk reductions and discounting in the analysis and point to a need for effective economic stimulus when the outbreak has passed. ""A Simple Planning Problem for COVID-19 Lockdown"" (NBER) Alvarez, Argente, and Lippi We study the optimal lockdown policy for a planner who wants to control the fatalities of a pandemic while minimizing the output costs of the lockdown. We use the SIR epidemiology model and a linear economy to formalize the planner's dynamic control problem. The optimal policy depends on the fraction of infected and susceptible in the population. We parametrize the model using data on the COVID19 pandemic and the economic breadth of the lockdown. The quantitative analysis identifies the features that shape the intensity and duration of the optimal lockdown policy. Our baseline parametrization is conditional on a 1% of infected agents at the outbreak, no cure for the disease, and the possibility of testing. The optimal policy prescribes a severe lockdown beginning two weeks after the outbreak, covers 60% of the population after a month, and is gradually withdrawn covering 20% of the population after 3 months. The intensity of the lockdown depends on the gradient of the fatality rate as a function of the infected, and on the assumed value of a statistical life. The absence of testing increases the economic costs of the lockdown, and shortens the duration of the optimal lockdown which ends more abruptly. Welfare under the optimal policy with testing is higher, equivalent to a one-time payment of 2% of GDP. Also  ""The Macroeconomics of Epidemics"" by Eichenbaum, Rebelo, and Trabandt. Published in NBER in March when it got NYT coverage; there's an ""April update"" now. The abstract of that paper isn't very revealing (as it only describes one scenario), but they actually consider several scenarios in the paper, with various exogenous constraints like treatments being found effective or not, vaccines being discovered etc. And then they consider a lockdown sequence optimization problem [in each scenario] as a Ramsey problem; see section 5 in the paper... and then some ""exit strategies"" from each. (Even that can be formulated as an optimization problem, which they call ""smart containment"" but it requires knowing the immunity status of everyone--which is a bit unrealistic as they admit.) Teaser figure from the paper (I won't try to explain it here):
 ""Macroeconomic Dynamics and Reallocation in an Epidemic"", (NBER, also in CEPR with somewhat more copyediting) Krueger, Uhlig, Xie In this paper we argue that endogenous shifts in private consumption behavior across sectors of the economy can act as a potent mitigation mechanism during an epidemic or when the economy is re-opened after a temporary lockdown. Extending the theoretical framework proposed by Eichenbaum-Rebelo-Trabandt (2020), we distinguish goods by their degree to which they can be consumed at home rather than in a social (and thus possibly contagious) context. We demonstrate that, within the model the ""Swedish solution"" of letting the epidemic play out without government intervention and allowing agents to shift their sectoral behavior on their own can lead to a substantial mitigation of the economic and human costs of the COVID-19 crisis, avoiding more than 80 [percent] of the decline in output and of number of deaths within one year, compared to a model in which sectors are assumed to be homogeneous. For different parameter configurations that capture the additional social distancing and hygiene activities individuals might engage in voluntarily, we show that infections may decline entirely on their own, simply due to the individually rational re-allocation of economic activity: the curve not only just flattens, it gets reversed. ""Internal and External Effects of Social Distancing in a Pandemic"" (NBER) Farboodi, Jarosch, Shimer We use a conventional dynamic economic model to integrate individual optimization, equilibrium interactions, and policy analysis into the canonical epidemiological model. Our tractable framework allows us to represent both equilibrium and optimal allocations as a set of differential equations that can jointly be solved with the epidemiological model in a unified fashion. Quantitatively, the laissez-faire equilibrium accounts for the decline in social activity we measure in US micro-data from SafeGraph. Relative to that, we highlight three key features of the optimal policy: it imposes immediate, discontinuous social distancing; it keeps social distancing in place for a long time or until treatment is found; and it is never extremely restrictive, keeping the effective reproduction number mildly above the share of the population susceptible to the disease. Not strictly about Covid-19, but also recent paper from FRB members (and one MIT faculty): Correia, Luck and Verner. ""Pandemics Depress the Economy, Public Health Interventions Do Not: Evidence from the 1918 Flu"",  what are the economic costs and benefits of non-pharmaceutical interventions (NPI)? Using geographic variation in mortality during the 1918 Flu Pandemic in the U.S., we find that more exposed areas experience a sharp and persistent decline in economic activity. The estimates imply that the pandemic reduced manufacturing output by 18%. The downturn is driven by both supply and demand-side channels. Further, building on findings from the epidemiology literature establishing that NPIs decrease influenza mortality, we use variation in the timing and intensity of NPIs across U.S. cities to study their economic effects. We find that cities that intervened earlier and more aggressively do not perform worse and, if anything, grow faster after the pandemic is over. Our findings thus indicate that NPIs not only lower mortality; they may also mitigate the adverse economic consequences of a pandemic. A 2nd (recent) paper from a FRB author on the 1918 pandemic, albeit a bit more descriptive: ""What Happened to the US Economy During the 1918 Influenza Pandemic? A View Through High-Frequency Data"" (and the part most relevant to the question here is left in teaser form in the abstract...) Interventions to hinder the contagion were brief (typically a month) and there is some evidence that interventions made a difference for economic outcomes."
Why can't countries print another country's currency?,"Why assuming that it isn't done in the first place? Turns out, it was actually done by Nazis in WWII https://en.wikipedia.org/wiki/Operation_Bernhard Also, suppose that somebody actually made dollars that are indistinguishable from genuine, Fed-printed dollars. How would you find out in the first place? How sure are you that nobody is doing it? :)"
Why is 10% the necessary upper bound for a negative interest rate?,"You're right to ask; the sentence you bolded is unclear in its wording. The effect of randomly invalidating 1/10th of the currency outstanding in one year is that in expectation, cash would have an interest rate of -10%. So when Mankiw says the policy ""might enable central banks to set negative interest rates provided the rate was less than 10%"", he means ""provided the rate was less negative than a -10% rate"". This is equivalent to saying that a central bank could set negative rates just so long as the bank set the rate $r$ such that $ -10\% \leq r < 0\% $. Of course, the effects of such a policy in the real world would be much more complicated. If there were certainty about the date of the invalidation, for example, people could just hold cash as usual until just before the invalidation date, then deposit the cash in a bank for a couple days (thus not exposing themselves to negative deposit rates for long periods), and withdraw (valid) currency after the invalidation date."
What would happen if the world switched to a single currency?,"The Euro was always conceived by most Economists as a political goal, not an economic one.  Prologue: There is the theory of Optimum Currency Areas (OCA) which characterizes properties that a larger area would need to have if it were to operate on a single unit of currency. Since the announcement of the Euro around 1990, there were many papers that looked into whether the European Area actually satisfied the broader range of these criteria, and they mostly agreed that it did not.  Ex-post, we can now see this playing out as the ECB has a hard time setting up an inflation rate that is improving conditions of countries that are hit very badly by the crisis such as Greece and Spain, and countries which are not, such as France and Germany. Basically, a country gives up the tool of monetary policy when it subordinates into such a currency union. In order to properly use monetary policy for a set of different economies (countries), you need these to be very similar in nature: If all countries react similarly to a housing bubble / oil shock / etc, you can easily improve outcomes for all countries with the same monetary tool. If the countries respond differently, it is much harder to do so. To the extent that all countries in the world are very different in nature, your experiment is similar to the gold standard (see Bretton Woods system). We think about that system mostly as a failure. While there is no hard data on the causal relationship, countries that abandoned the gold standard earlier tended to do better. We observed similar trends for countries that abandoned the dollar standard earlier. As a short intuition, among other explanations, when you abandon monetary policy and fix your exchange rate, you make yourself more vulnerable to inflation/deflation of other regions."
"In log-linearized New Keynesian model, what do $Y$, $Y_t$, $y_t$ actually mean?",The following post explains in a somewhat easier way what exactly is happening when we log linearize a model.  http://economictheoryblog.com/2012/06/22/latexgx_t/ Going through the provided example should make it clear what the single steps are. 
Existence of utility representation of a rational but discontinuous preference,"I think a basic problem is that any utility function defines a preference, and discontinuous utility functions can be used to define discontinuous preferences. Hence there are many discontinuous preferences that can be represented by utility functions.
An example: Let $U(x,y)$ be a continuous utility function that maps from $\mathbb{R}^2$ to $(0,1)$. This latter may seem arbitrary, but the strictly monotononicaly increasing function $\frac{x}{x+1}$ maps from $\mathbb{R}_{++}$ to $(0,1)$, so it should be fine. Also define a closed set $H \subset \mathbb{R}^2$. Let
$$
\hat{U}(x,y)
=
\left\{
\begin{array}{ll}
U(x,y) & \mbox{if} (x,y) \notin H \\
\\
U(x,y)+1 & \mbox{if} (x,y) \in H.
\end{array}
\right.
$$
Obviously $\hat{U}(x,y)$ is not continuous and neither are the preferences defined by it. (The boundary of $H$ is prefered to anything outside $H$.) But the way these preferences were generated seems fairly general. So a large class of discontinuous preferences exists for which there is a utility representation. Future question: How 'large' is this class, what measure can be used? EDIT: As @NicolasPinto points out in his answer it is also necessary to specify that $H$ is such that
$$
\exists x \in H, \exists y \notin H: y \succ x,
$$
so $H$ is not the upper contour set of some point,
otherwise $U(x,y)$ and $\hat{U}(x,y)$ would in fact represent the same continuous preferences."
"Meaning of Additively Separable, Linear in X","A function is additively separable in its arguments if it has the form $$f(x,y) = g(x) + h(y)$$ This means that the cross partials are zero, and so there is no ""cross"" effect of the one argument over the marginal effect that the other has on the value of the function. Since marginal effects are at the very heart of Economics (see here), assuming additive separability greatly simplifies the analysis. In dynamic problems, where the intertemporal utility function is assumed to be additively separable, it permits us to transform an infinite horizon problem into a recursive two-period one. Functions that can be transformed into something additively separable (by usually considering their logarithms), are sometimes called ""multiplicatively separable"". The most famous example here is the Cobb-Douglas production function: $$Q = K^aL^{1-a} \implies \ln Q = a\ln K + (1-a)\ln L$$ As for linearity, it is a unique (structurally) relationship, while non-linear relationships are many, perhaps too many.
A mathematician once said that ""the whole field of Analysis, is essentially the study of linear approximation of non-linear relations"".  Again, mathematical tractability is the drive here, supported by the fact that a linearity assumption is a ""first-order"" approximation to the true relation (see Taylor expansion)."
Nash Equilibrium and Pareto efficiency,"Nash Equilibrium (N.E) is a general solution concept in Game Theory. N.E is a state of game when any player does not want to deviate from the strategy she is playing because she cannot do so profitably. So, no players wants to deviate from the strategy that they are playing given that others don't change their strategy. Thus, it is a mutually enforcing kind of strategy profile.  'Pareto optimality' is an efficiency concept. So no state will be Pareto Optimal if, at least one of the players can get more payoff without decreasing the payoff of any other player. There are many many examples of Nash Equilibria which are not pareto optimal. The most famous example could be the N.E in prisoner's dilemma. "
Why absolute value in elasticities and marginal rate of substitution?,"I think there are pedagogical advantages to discussing both the raw numbers and the absolute values and I think the benefits of both explain why they both show up (sometimes in the same text, even). Each elasticity number gives two bits of information. First, the absolute value with respect to 1 and second, the sign. Now, clearly, if you had a negative elasticity, you could compare it to -1. However, it becomes somewhat difficult to teach when using phrases like ""greater than"" or ""less than"" -1 to discuss a good being (in)elastic, since ""greater than -1"" is actually inelastic if the elasticity is negative. It is much more intuitive to be able to discuss the ratios of percent changes if ""greater than"" does in fact mean that the top is bigger than the bottom and vice versa for ""less than"". Of course, there is also a bunch of information tied up in the sign of the elasticity. We get the Law of Demand out of own-price elasticity, we get compliments/substitutes from cross-price elasticity, etc. So it is important to still make sure students understand the importance of the sign. When I am teaching, I try to discuss both parts explicitly, but make clear that the elasticity itself includes the appropriate sign. I think most books are trying to capture these two bits of information in one way or another. In any case, the formal definition of elasticity should include the sign, but if one is just talking about how elastic a good is, the absolute value could be reported (with the note that it is the absolute value of the elasticity, not the elasticity itself).  As for MRS, it's usually not the absolute value, per se, that we report, but rather the negative of the derivative dy/dx. This is quite standard, since it has the intuitive interpretation of the consumer being willing to give up so many units of x for so many units of y. Since indifference curves are usually convex, this derivative is negative, thus changing the interpretation (and intuition) somewhat if we don't negate it."
Economics Online Seminars,"I have two here to add to your list.   Brookings Insitute https://www.brookings.edu/events/ St. Louis Fed  Economic lowdown
(FREE) https://www.econlowdown.org/ There is also the National Association of Business Economics (NABE)
But to get access to their webinar resources you have to register on their site.
I know this won't qualify as it is not free but they are a good source of up-to-date information.  I felt that this would be a good share.  www.nabe.com Others that I just found:
Peterson Institute for international Economics https://www.piie.com/events National Bureau of Economic Research (www.nber.org - click on videos) 
or  https://www.nber.org/video_archives.html Organisation for Economic Co-operation and Development (OECD) http://video.oecd.org/ World Bank
https://www.worldbank.org/en/about/archives/past-events"
How can nominal interest rates be negative?,"There was only one reason to ever think that nominal interest rates couldn't go negative, which is that the nominal return on both forms of base money (electronic reserves, and paper currency) had a floor of zero -- and investors wouldn't accept a below-zero nominal return when they could get a higher one by holding base money. But for electronic reserves, there's certainly no practical need for a floor of zero: the central bank can easily pay negative interest on reserves. (For instance, the Swiss National Bank is now paying -0.75% on most ""sight deposits"", which is their term for electronic bank reserves.) It's quite easy: they just tell the computer to charge a certain amount of interest on reserve accounts, just as they would normally tell it to pay interest. The potentially serious problem is paper currency, which by its very nature generally pays a nominal interest rate of zero. (If you have a \$5 bill, it will still be worth \$5 in a year - zero nominal interest.) The concern is that if we try to set nominal interest rates below zero, people will just massively switch to paper currency paying 0%, and the traditional financial system would disappear and be replaced by paper. This is where we reach a very important point, which is that return isn't all that matters in an asset. Other factors, like convenience, play a role as well; different assets are convenient in different ways, and aren't perfect substitutes for each other. This is why in normal times, people are willing to hold paper currency even though it pays less than electronic accounts (because paper is useful in some ways that electronic accounts aren't); inversely, it's why in these abnormal times, when Swiss cash pays more than Swiss deposits, investors don't flock exclusively into cash. Hence, when the SNB pays -0.75% on electronic reserves, rates in Swiss-denominated money markets (like the SARON overnight rate or LIBOR CHF) also drop to near -0.75%, and not everyone converts their electronic assets into paper. Indeed, there has been very little response on that front: if you look at SNB balance sheet column 16, banknotes in circulation, it hasn't budged very much thus far since negative rates began in December and the rates fell to 0.75% in January. Again, this is just a matter of imperfect substitutability: paper currency simply doesn't have the same kind of transactional and liquidity value that electronic accounts do. (And then there are the costs and risks of paper currency storage, as mentioned in the article you cite.) This is the feature of the world that wasn't adequately captured, before recent events, in the simple zero lower bound models used by many economists. The one quasi-mystery, to me, is why banks haven't stockpiled paper. After all, even though an electronic account is more useful than paper to an end user, this doesn't apply to a bank that's already overflowing in electronic reserves. (Indeed, conceivably banks could be large-scale intermediaries here, stockpiling paper to back electronic deposits held by their customers - effectively transforming paper into a more convenient asset.) Yet the quantity of banknotes held by Swiss banks has remained low (see the 4th column at the bottom of the 2nd page here).  There are two possibilities here. One is that the logistical costs of holding cash are still higher than the 0.75% spread, so that it's still not worthwhile for banks to hoard. Another is that banks could make a profit by embarking on some large-scale cash hoarding program, but they know that the central bank would quickly get angry and create new rules (like a 0.75% tax on excess cash holdings by banks) to stop this, so they don't bother -- and indeed, maybe there has been under-the-table pressure from the central bank already!  Ultimately, when the government has the authority to regulate financial institutions, it can always disallow organized cash hoarding schemes that threaten the negative interest rate policy, so none of this is a long-term threat. The real issue is cash hoarding by end users - and there, the disadvantages of cash apparently still loom large at -0.75%. (One complication, by the way, is that I think banks are generally not charging negative interest on small-scale retail deposits. At least superficially, then, the relevant interest rate isn't negative for the typical consumer; but I suspect that banks are making up the difference by charging fees, etc., which is already necessary to recover their costs even when interest rates are at 0%. For large-scale deposits, on the other hand, banks are charging clients to hold cash - and, of course, this must be true, since money market rates wouldn't be negative if it was always possible to find some bank that would give you a zero rate.)"
"Why not talk about utility functions on the surreal line when preferences are lexicographic, etc?","The first thing that comes to mind is that some economic models have more than one agent. If one agent has a utility function with values in ${\mathbb R}$, then two agents have a pair of utility functions with joint values in ${\mathbb R}^2$.  If one agent has a utility function with values in $S^*$, then two agents have a pair of utility functions with joint values in ...what?  We can't just form a product of $S^*$ with itself, because $S^*$ is not a set. There are probably ways around these difficulties, but they're going to be clumsy and they're going to require lots of mucking around with foundations that are almost surely not interesting to most economists. Set theory lets us form unions, products, coproducts and power sets.  It lets us talk about subsets and supersets without having to worry about whether they exist.  Economists use these constructions all the time without stopping to think about them.  I doubt that we want to switch to a new foundation that forces us to think about them constantly."
"Inflation, cause or result of monetary emission?","The problematic part of the statement, is the ""because of other reasons not important here"" part . In other words: ""ignore general equilibrium"" -which is an unacceptable statement to make when discussing government policy and actions.   Consider the naive quantity theory of money: $$PQ = VM \tag{1}$$
$P$ is the price level, $Q$ is output produced (measured in quantity), $M$ is money supply, and $V$ is ""velocity of money"", an indicator of the ""transactions technology"" in the economy, how fast money circulates around to settle transactions.   Assume now that we are talking for a ""small"" country that needs to import basic factors of production like raw materials or energy. ""Small"" here means ""with no market power"". Such a country is a price-taker in the international market. More over, substitution possibilities for these factors are usually small to non-existent. Competitive markets or not, the economy's output will be distributed to factors of production and for our purposes, it doesn't matter whether there will be ""capital rents"" and ""profits"", or only capital rents. Use for convenience three factors of production and write $$PQ = rK + wL + p_fE \tag {2}$$ where $r$ and $w$ are nominal, and $p_fE$ is the nominal cost of imported factors. Denote $s_f$ the foreign exchange rate (units of local currency per one unit of foreign currency), $c_f$ the price of the imported factors in foreign currency, so $p_f = c_fs_f$.
Use this and substitute $(2)$ in $(1)$ $$rK + wL +c_fs_fE= VM \tag{3}$$ If something happens to the international market and $c_f$ goes up to $c_f' > c_f$, this will tend to increase the left hand side. This ""something"" in the international production factors market does not relate to the level of domestic output $Q$, or to domestic money transactions technology, $V$. More over, at least in the short run, factor substitutions will not happen, wages do not move that easily, and firms will maintain their output level while increasing selling prices, to cover the increased production costs. And since the reasons for the increase affect more-or less the whole economy, it is not that likely that competition will stop firms from doing so: they all want to cover their increased costs, they all know that the cost-rise is general and comes from abroad, so they don't need to actually collude in order to sustain a price increase. ""Common knowledge"" suffices.   So in order to preserve the equality in $(3)$ it appears that we must have $$rK + wL +c_f's_fE = VM',  \;\; M' > M\tag{3}$$ You see? This is the phenomenon called ""imported inflation"". Whatever the reasons were for the price increase (the ""not important"" reasons), inflation was not caused by the expansion of the money supply (that's indeed true), and what else the government could do than raise the money supply to service the higher nominal level of output? Of course what the story above does not say, is that foreign factors of production will want a ""money"" that they accept, and most likely this won't be the local currency of this small country. And by increasing the money supply the exchange rate $s_f$ will suffer (increase), because $s_f = h(M), \;\; h' >0$, increasing in this way further the costs of imported factors in terms of local currency, and making the increase in the money supply equivalent to ""shoot oneself in the foot"". And this is only one more step towards the road to general equilibrium. The essence here is that
a) it is trivial that there are many other factors that may tend to affect prices upwardly, except money supply expansion   b) in the presence of these other influences, increasing the money supply is not necessarily the appropriate government response."
Why is capital often not included in New Keynesian models? Is there a reason other than modeling difficulty?,"Capital is included in all the big estimated New Keynesian models (Smets-Wouters, Christiano-Eichenbaum-Evans), etc. But you're absolutely right that the stylized core NK model does not have capital - which is hard to defend on empirical grounds, since capital investment is a very important part of business cycle fluctuations and the response to monetary policy. Ultimately, the reason does basically boil down to the ""modeling difficulties"" that you mention. First, there is an obvious way in which capital makes the NK model more complicated: at an absolute minimum, it introduces at least one additional backward-looking state variable $K$. In contrast, the two core equations (the intertemporal Euler equation and New Keynesian Phillips curve) of the ordinary log-linearized NK model are completely forward-looking. Adding $K$ to the mix eliminates this nice analytical feature. Still, on its own, this is not such a compelling reason to leave $K$ out of the standard presentation of the model, since the increase in complexity would still be tolerable and possibly justified by the added realism. The additional complications that make capital much more difficult to include are the following. Capital adjustment costs are needed to avoid absurd results. Suppose that there are no capital adjustment costs, and that firms rent capital each period on competitive markets. Suppose also that there are no shocks today. The real rental cost of capital today will be approximately $r+\delta$, where $r$ is the real interest rate that was expected yesterday and $\delta$ is depreciation. For each firm $i$, we have $MC(i)=(r+\delta)/MPK(i)$: real marginal cost equals the real cost of producing another unit of output by increasing capital. Now, suppose for simplicity that firms are identical and there is no price dispersion. (Moving away from this, everything I say will still be approximately true.) Then we can eliminate the $i$ and just write $MC=(r+\delta)/MPK$. Furthermore, real marginal cost is just the inverse markup, so we can rewrite this in terms of markup $\mathcal{M}$ as $MPK=\mathcal{M}(r+\delta)$. Finally, if we assume that production is Cobb-Douglas with capital share $\alpha$ this becomes
$$\frac{K}{Y}=\frac{\alpha}{\mathcal{M}}\frac{1}{r+\delta}$$
Pick some reasonable parameters: say, $\alpha=0.3$, $\mathcal{M}=1.33$, $r=0.04$, and $\delta=0.06$. Given these we have $K/Y\approx 2.26$. Suppose that we exist in this steady-state world for a while, and then the Fed pushes down the projected path of interest rates such that the expected real rate declines to $r=0.02$. Then leaving $\mathcal{M}$ constant, the value of $K/Y$ given by the expression above increases to $K/Y\approx 2.82$. The NK model implies that this value should hold in the period after the shock. This is an immense increase in $K/Y$, and the model tells us that it should happen in one period. If our period is a quarter, and $Y$ was not expected to take a sudden dive, then we'd have to invest well above the entire usual level of GDP to accomplish this. (In the continuous time limit, it becomes simply infeasible.) And although the assumption than $\mathcal{M}$ is constant is not right ($\mathcal{M}$ is determined endogenously in the NK model from the relationship of sticky prices to costs), relaxing this could easily make the puzzle more extreme: if $r=0.02$ is expected to prevail for a while, then it will imply higher-than-usual output $Y$ and lower-than-usual markups $\mathcal{M}$, which raise the implied $K$ even further. The model simply doesn't work in this form: you need some form of capital adjustment costs. And these, of course, make the model still more complicated. (By the way, the problem here isn't so much the NK model as the fact that an assumption of no adjustment costs is generally absurd: seemingly small changes in the real interest rate must be accompanied by massive swings in the capital-output ratio, which we never see in practice. The NK model simply brings this absurdity, which is found in the basic RBC model as well, into sharper relief because exogenous interest rate shocks are such an important feature of the NK environment.) Firm-specific capital is needed for strategic complementarity. Even if we fix the problem above by including capital adjustment costs of some form, we run into another awkward feature of NK models: taken alone, the Calvo price rigidity is not plausibly large enough to make the NKPC as flat as we think it is.  The most popular fix is some form of strategic complementarity, where firms try not to set prices too far from the aggregate price level. And the most popular way to get strategic complementarity is to assume that firms face both a high elasticity of demand and a steeply upward-sloping marginal cost curve. That way, for instance, any firm that sets its price too far below the average price will receive a flood of demand that causes its marginal cost to spike - and this discourages the firm from setting such a low price in the first place. (Yes, this sounds a little ridiculous, but it's how the models work.) When the model excludes capital altogether, it's easy to just write a declining-returns-to-scale production function for each firm with labor as the only input. This makes each firm's marginal cost curve slope upward. But when we include both capital and labor in the model, the firm's production function probably should be much closer to constant-returns-to-scale. And this means, if the firm can rent any amount of capital from a competitive market on demand, that the firm's marginal cost curve is much closer to flat. This limits strategic complementarity. To get around this, you need to dispense with the assumption of a common rental market for capital, and start talking about firm-specific capital accumulation. But then the model becomes much more complicated, and you're reduced to opaque quantitative exercises like ACEL. Given all this, you can imagine how economists in insight-building mode often just dispense with capital altogether."
Thin indifference curves,"I don't think continuity alone is enough to guarantee thin indifference curves. Consider preferences such that, for any $x$ and $y$ in the choice set, the consumer is indifferent between $x$ and $y$. This seems like it must fit any definition of a thick indifference curve because the whole choice set lies on a single indifference curve! But these preferences also satisfy your definition of continuity. Thus, it seems like continuity only implies thin indifference curves if it is paired with some other assumption."
Imperfect vs incomplete information,"You are at a decision point and you don't know what strategy the other player has taken. But you still know that which game you are in, you still know the other players' strategy sets etc. 
Under incomplete information you might not even know what game you are into.  For example: Two firms are taking up R&D projects sequentially. One is the first mover. It decides whether to take up R&D or not. Now the second firm has to decide but does not know what first firm has decided. This is imperfect information.  Now imagine ypu are playing a card game and need to throw a card but you don't know whether you are playing poker or sweep. This is incomplete information. "
Why does the Brexit cause a fall in crude oil prices?,"I'd like to extend Lasse's excellent answer. Fear and uncertainty are driving markets - but they can drive prices in either direction of course. Specifically what's happening here is that the oil markets are pricing in at least two effects. Firstly, the UK is an oil producer, and Sterling's slide means that its oil just got cheaper for other countries. This is a small effect, but relevant, as UK short-run marginal extraction costs are close to current prices, so some UK fields are marginal producers and thus price-setters. Much more importantly, the markets are now pricing in the possibility of a further slump in oil demand, as a result of poor general economic performance. That would be driven not only by a soon-to-be-declining UK economy, but also driven by a (much larger) destabilised European economy; and also driven by the possibility that this vote will mark the turning point, where globally we move away from a consensus on the value of tariff-free international trade, and back towards protectionism and isolationism, which would leave the global economy worse off, as well as leaving most participants individually worse off too."
Continuity Axiom in Expected Utility Theory,"It is.
Prior to continuity, which is a property of the preference relation,  the preference relation $\succsim$ itself has been defined to be a binary relation that is characterized by transitivity, and, to begin with,  by completeness.
  Then if $S_1\cup S_2 \neq [0,1]$, it means that there exist some values of $\alpha$ somewhere in $[0,1]$, call them $\tilde \alpha$ for which   neither  $$\{\tilde \alpha L+(1-\tilde \alpha)L'\succsim L''\}$$ nor  $$\{L''\succsim \tilde \alpha L+(1-\tilde \alpha)L'\}$$ In words, for these $\tilde \alpha$'s, the pair cannot be ordered at all. But this contradicts the completeness foundation that is needed to even obtain a preference relation (as of course used in our theory. Psychologists I guess would disagree).  Also, note that completeness is defined over all conceivable pairs, even if, in a specific situation, we chose to restrict the space of lotteries to something smaller. Whether the lotteries under consideration belong to the specified lottery space, is really irrelevant. The person having the preferences has to be able to order them in any case, even as a ""hypothetical"" scenario (although strictly speaking, for a specific problem we have the ""luxury"" to impose completeness only as regards the lotteries available, while ""remaining agnostic"" as regards completeness if we expand the lottery space. Still this ""weakening"" on the imposition of the completeness axiom, does not really bring any gain)."
Where to start with social networks?,"The best introduction is, in my opinion, the book by Matthew Jackson. It has a pretty nice introduction (even if you know nothing about social networks) and chapters about many of the applications of graph theory in economics. It sounds like you try to model a game on a network (i.e., using game theory with players on a network without changing the network). If this is true, then the following paper might be even more relevant:"
Aggregation of the closure property of a production set,"Doing this more abstractly, let $Y_j\subseteq\mathbb{R}^n$ be a production set for $j=1,\ldots,J$ and let
$$Y=Y_1+Y_2+\cdots+Y_J=\{y_1+y_2+\cdots+y_J|y_j\in Y_j, j=1,\ldots,J\}$$
be the aggregate production set. The standard result on when the aggregate production set is closed is the following: Theorem: Let $Y_j$ be closed and convex sets containing $0$ for $j=1,\ldots,J$ and assume that $Y\cap -Y=\{0\}$. Then $Y$ is closed. Now, this result uses an assumption (irreversibility and possibility of inaction) on the aggregate production set, but this follows from $0$ being in every production set and the input and output commodities being separate in your setting. The assumption that $0$ is in each individual production set is not needed, but the proof becomes messier (though not fundamentally different) without it. The answer of tdm points you to a proof via asymptotic cones. Here is a more direct argument: Proof:
Let $(y_m)\to y$ be a convergent sequence in $Y$. For every $m$, let $(y^1_m,\ldots,y^J_m)\in Y_1\times\ldots\times Y_J$ be such that $y_m=\sum_{j=1}^J y^j_m$. If this sequence is bounded, a compactness argument guarantees that $y\in Y$. We show that the sequence has to be bounded. Suppose not. W.l.o.g., assume that none of the terms in this sequence is zero and that the norm of the terms is increasing and unbounded. Define a sequence $(x^1_m,\ldots,x^J_m)$ by $x^j_m=y^j_m/\|(y^1_m,\ldots,y^J_m)\|$. The sequence has a converging subsequence $(z^1_m,\ldots,z^J_m)\to(z^1,\ldots,z^J)$ because it lies in the compact unit-sphere and $(z_1,\ldots,z^J)\in Y_1,\ldots, Y_J$ because each $Y_j$ is convex and contains $0$. Also, $\|(z^1,\ldots,z^J)\|=1$, so at least one coordinate must be nonzero. W.l.o.g. let $z^1\neq 0$. We have $$\sum_{j=1}^J z^j=\lim_{m\to\infty}  \frac{y_m}{\|(y_m^1,\ldots,y_m^J)|\|}=0$$
because $\|(y^1_m,\ldots,y^J_m)\|$ is unbounded and the convergent sequence $(y_m)$ is bounded.
So $z^1-\sum_{j=2}^J z^j=0$, but $z_1\in Y$ and $\sum_{j=2}^J z^j\in Y$ because each $Y_j$ contains $0$, so this contradicts $Y\cap-Y=\{0\}$."
References for particular definitions of risk and uncertainty,"Knight's 1921 essay was not written in formal mathematics (and trying to formulate a direct translation into modern mathematics may be quite problematic). Since Knight's time, a formal decision theory literature has developed which makes distinctions that are at least reminiscent of Knight's. Lars P. Hansen (2012), writes ""Motivated by the insights of Knight (1921), decision theorists use the terms uncertainty and ambiguity as distinguished from risk."" Hansen references Gilboa et. al. (2008) who write: In economics, Knight (1921) is typically credited with the distinction
  between situations of ""risk” and of “uncertainty.” In his formulation,
  “risk” designates situations in which probabilities are known, or
  knowable in the sense that they can be estimated from past data and
  calculated using the laws of probability. By contrast, “uncertainty”
  refers to situations in which probabilities are neither known, nor can
  they be deduced, calculated, or estimated in an objective way. Gilboa and Schmeidler (1989) introduced a max-min utility theory where instead of simply maximizing expected utility, agents solve a max-min problem where to model ambiguity aversion, the objective is minimized over different priors. This work has been expanded on by Hansen and Sargent in their work on robustness.  Sliding away from Knight and into the broader topic of decision theory, it would be mandatory to reference Leonard Savage's classic, Foundations of Statistics where he introduces the notion of subjective probability. You have a Bernoulli likelihood function for winning the lottery. To a frequentist statistician, $p$ is a scalar value, a parameter (albeit unknown). There is only one possible outcome for $p$ and so there's no randomness. If you're a Bayesian in the spirit of Savage, you're willing to extend the tools of probability to model uncertainty in your own mind; you will treat $p$ as a random variable! If we put a prior on $p$ (eg. the beta distribution is a conjugate prior to the Bernoulli likelihood), we can compute posterior probabilities and make decisions based upon standard expected utility. In terms of how we behave though, there's no difference between a 20% probability that's objective vs. subjective. Under the ambiguity aversion, max-min model though, we may maximize our control variable over expected utility, taking into account that it will then be minimized over multiple priors: we choose control variable $x$ to maximize utility and then (after observing our choice $x$) a mean guy chooses the prior to minimize utility. Hansen, Lars P., 2012, ""Challenges in Identifying and Measuring Systemic Risk"", NBER Hansen, Lars P. and Thomas Sargent, 2001, ""Robust Control and Model Uncertainty,"" American Economic Review Gilboa, Itzhak and David Schmeidler, 1989, ""Maxmin Expected Utility with Non-unique Prior,"" Journal of Mathematical Economics Gilboa, Itzhak, Andrew W. Postlewaite, and
David Schmeidler, 2008, ""Probability and Uncertainty in
Economic Modeling"", Journal of Economic Perspectives Savage, Leonard Jimmie, 1954, Foundations of Statistics"
Limits to Growth: Is environmental collapse by 2030 a likely scenerio?,"We know one core element of the Guardian article is nonsense - the discussion of Peak Oil. The Peak Oil lament, fashionable a decade ago, was that we didn't have enough oil, and that was going to become a major problem very soon. However, we know from climate science that the reverse is true - we have far too much fossil fuel reserves (disclosure - that's a paper by my colleagues). Our challenge isn't that we won't extract enough to continue economic growth - it's that we'll extract too much and cause a major environmental collapse. Turner's report makes a significant show about the age of cheap and easy oil being over (pp 11,12) - and that's a prediction that was made to look very silly, quickly, as the report's publication in August 2014 happened just as the 18-month oil-price crash caused by huge global over-supply was beginning. Is environmental collapse possible? Yes. Are we causing huge ongoing damage to the environment? Yes. Were all of the significant predictions of the Club of Rome correct? No. Not only has geometric population growth stopped, but even arithmetic growth is in decline - and the growth wasn't capped by disaster, but by better healthcare, better access to contraception, and more education and emancipation of women. Food production per capita has way exceeded predictions. And the renewable alternatives to fossil-fuel energy are already economic and scalable for static applications, and becoming so for transport too. Are we still on a trajectory of yesterday's brown economy, that will require a trajectory change if we are to avoid environmental collapse?
Yes.
Do we have enough evidence to say that the date for that collapse is 2030?
No.
Do we have the technical means to transition to a modern, clean economy and avoid that collapse?
Yes."
Bayesian Nash Equilibrium - Mixed Strategies,"I believe that the answer given by @denesp is incorrect. The second method involves simply writing the game in strategic of ""normal"" form. I believe
the first method is better (easier to use), but I think that they can both be used.
In the answer given by @desesp, the following explanation is given. The reason why method two is flawed is that the probabilities $a$, $b$
  and $c$ are not  independent as $$ a = p \cdot q, \hskip 20pt b = p
 \cdot (1 - q), \hskip 20pt c = (1 - p) \cdot q, \hskip 20pt  1 - a - b
 - c = (1 - p) \cdot (1 - q). $$ Here, it appears that mixing is occurring over L in game 1 (with probability $p$) and L in game 2 (with probability $q$). This interpretation does make sense. I believe that @denesp is confusing conditional and unconditional probabilities. To better understand this, I'm going to start with a discussion of actions versus strategies. Then I'll discuss how the set of strategies considered in methods 1 is included in method 2. I'll note that method 2 contains a larger strategy set, which may or may not be useful. I'll conclude with an example of how both methods can produce the same answers. In the explanation given above, it may appear that mixing is occurring over actions. This is not a
correct interpretation. It is technically incorrect because the player is not mixing over actions but mixing over strategies. This is because a player chooses strategies, not actions. A strategy is a plan
that denotes that actions that a player takes in any and every contingency.
We can think of it as mapping information sets to actions. This is important because we would like player 1's actions to depend on the state of nature---we want them to depend on which game he/she is playing. For reference, we can find definitions of actions and strategies in the first chapter of Rasmusen's book, Games and Information (4th edition). The relevant text is given here: 

 In the case of the game that you have given, the pure strategies available can be written succinctly (LL, LR, RL, RR), as you have already done in method 2.
Therefore, the method that you described in method two mixes over the pure strategies, with probabilities: $a$, $b$, $c$, and $1 -a-b-c$. What strategies, then, are we mixing over in method 1? Suppose that $p$
is the probability of choosing L is game 1 and $q$ is the probability of choosing L in game 2. Then in method 1, we can see that we are choosing 
the conditional probability of taking each action in each contingency.
Suppose that game 1 is denoted $G_1$ and that game 2 is denoted $G_2$.
When we specify $p$ and $q$, we are really specifying
$$
p=P(L|G_1)\\ q=P(L|G_2). 
$$
Now, in order to show that these two methods are equivalent, we need to show that the sets of strategies represented by each of these sets is the same. First, note that the pure strategies LL, LR, RL, and RR can be represented in method 1 by setting $p$ and $q$ to zero or 1. Suppose that we are using method 2 and that we choose a particular $a$,$b$, and $c$, as defined above. This can be represented in method 1
with
\begin{align*}
p &= a + b \\
q &= a + c.
\end{align*} However, suppose we choose a particular $p$ and $q$ in method 1. It can be represented in method 2, but not uniquely.
Suppose $p=1/2$ and $q=1/2$. Then two possibilities are $(a,b,c) = (1/2,0,0)$
or another is $(a,b,c)=(0,1/2,1/2)$.  Method 2 contains more strategies because it allows more flexibility
to specify off-equilibrium behavior. This can end up capturing non-credible
threats. Depending on which equilibrium concept you're using, you may or may not want to include these. If you're only interested in Bayesian Nash equilibria, then you want to include these. If you're interested in sub-game perfect Nash equilibria or Bayesian sequential equilibria, then you don't want them. The following game is again take from Rasmusen's book. Suppose that in this game
Smith moves first.   Then, Jones must choose among 4 strategies. In each of these strategies, he specifies his actions in each contingency. The 4 strategies are listed here and the game is represented in strategic or ""normal"" form.  There are three equilibria, denoted $E_1$, $E_2$, and $E_3$.
If we were simply interested in the Nash equilibria of this game,
we would include all of these equilbria. However, if we are interested
in only the subgame perfect equilibria, we would only want $E_2$.
That is because $E_1$ and $E_3$ involve non-credible threats. I believe that if we were to try to solve this game using method 1, we would not be able
to identify all three of these equilibria. In the question you've given, method 2 is essentially transforming this
into a static game in which we consider all the strategies. This means that we are considering the ""normal"" form of the game. This method is easy and appropriate if you're interested in finding the pure strategy equilibria. It can probably also used to find the mixed strategy BNE, but is perhaps more complicated then what is described in methods 2.
For reference, 
here are some notes on the topic. 
These notes give instructions on how to solve for the pure strategy Nash equilibria using the transformation that you've given. It also demonstrates how to solve the mixed strategy equilibria using method 1. (See http://www.sas.upenn.edu/~ordonez/pdfs/ECON%20201/NoteBAYES.pdf .) You can also use this online tool to test how the methods can give you the same answers. This is a tool to solve for the Nash equilibria of n by n games. I would recommend using this tool on the examples given in the previous section. 
I found this tool referenced in this other question."
"Finding demand function given a utility min(x,y) function","No, you should not use Lagrange multipliers here, but sound thinking. Suppose $x\neq y$, say for concreteness $x<y$. Let $\epsilon=y-x$. Then $\min\{x,y\}=x=\min\{x,x\}=\min\{x,y-\epsilon\}.$
So the consumer could reduce her consumption of good 2, without being worse off. On the other hand for all $\delta>0$, we would have $\min\{x+\delta,y-\epsilon/2\}>x=\min\{x,y\}$, so the consumer could be better of by reducing the consumption of the second good and spending the freed money on the first good. In an optimum, a consumer cannot improve so optimality requires $x=y$. It is also clear that consumers improve along the $x=y$ 45° ray. So you can simply use $x=y$ as an optimality condition to be substituted into your budget constraint and bypass Lagrange multipliers."
Whatever happened to efficiency wage theories?,"I don't believe that Search Unemployment (MP) actually ""won"" over efficiency wages. The whole discussion of search literature would be too long for this post, so I'll just skim the most important parts. First, to the extent that the MP model actually needs exogenous wage rigidity to get the moments in the data right, it has not won over efficiency wages, as these could be understood as a complementary theory (as being the reason for wage ridigity, as an alternative to (ii)). Second, (iii) kind of goes against both MP and efficiency wages at the same time - at least to the extent that they explain fluctuations in hirings. Note that this is the relevant margin for unemployment fluctuations: [I think it was Shimer who] showed that the most fluctuations in labor come from volatility in hiring, and not from firing. Finally, a personal opinion. I was interested in the story myself, but I wouldn't know how you could find efficiency wages in the data we have. Also, there's not really more to the theoretical component than was written in the past century. The trend in the past decade has gone to microfounded, self-consistent models that are found using empirical data. I don't believe that this is really possible at the moment with the efficiency wages story. We believe efficiency wages exist, but we can't detect them and they (arguably) don't explain the Shimer puzzle, which makes them less interesting these days."
Why did the Federal reserve balance sheet capital drop by 32% in Dec 2015?,"The Fixing America's Surface Transportation Act (FAST), which was enacted on December 4, 2015, requires that aggregate Federal Reserve Bank surplus not exceed \$10 billion. The amounts of the line items ""Other liabilities and capital"" on table 1, and ""Surplus"" on tables 5 and 6 reflect the payment of approximately \$19.3 billion to Treasury on December 28, 2015, which was necessary to reduce aggregate Reserve Bank surplus to the \$10 billion limitation in the FAST Act. Source: Factors Affecting Reserve Balances - H.4.1"
Is there an alternative metric to GDP that measures a country's assets rather than flows?,"You are correct in pointing out the flaws of GDP. But honestly, it's the best we have came up with; measuring the stock of wealth is just impossible.  You say you can estimate the value of your own assets, and of course it seems that way. We all feel that way, until we actually try. How much is your cellphone worth? Easy to know, go to ebay and look for similar phones in similar conditions and you get a price for that phone, right? Why don't we do that for all the country's cellphones? Well, because if we put all the cellphones in the market prices would drop down dramatically. Now, we're not obviously sending all the cellphones to the market, we just want to know how much its value is, but the point is that in order to have an estimation, we have to make assumptions regarding the market. The value of one cellphone is easy to calculate because it doesn't really affect the market.  Additionally, who says that you have to sell it in your local market? Why don't you offer it in Australia or Japan or Algeria or wherever the price is higher? And why not sell it another day when the price is up? The same goes for your car, house, computer, clothes, books, etc. Think now of your financial assets. You can know that, right? All one needs to do is see what the bank says, and it says that my investment portfolio is worth \$100.... at least today... and exactly at the time I checked. But remember: the value of any financial portfolio is going up and down with high volatility even in the same day. Now try to figure out the value of the financial assets of all the country. Well, you can do that. But again: just today, and just at the time you checked.  Even though it doesn't seem like it, the same goes for goods. Take oil. Say that there is an oil reserve with x millions of barrels just waiting to be extracted and sent out to the market. How much is the value of that reserve? It depends on when you ask, whom you ask to, and who else is asking the same question (other sellers). What about gold? silver? copper? coal? wood? You can estimate a value in a given time, but that can be very different in another time. What about land? Well, if I sow corn it will give me a given amount, but if I sow rice it will yield a different amount. Besides, would it be the same if my neighbor sows corn and I sow rice than if we both sow the same (competition)? What if I want to use it to generate electricity instead of growing food? Or what if I use it for livestock? What if I sell it? What if we all decide to do the same thing at the same time? Or what if we decide to do completely different things? Indeed to value farm land, you have to assume what use will it have. Infrastructure is even more complicated. How much is a railroad valued? And what does value even mean in this context? There is no secondary market for railroads is there? There are opportunity costs, though. And we have the construction value too. But the we would have to make assumptions on its depreciation, and say how much of the revenue of the country is due to this specific segment of the railroad. To estimate this, one has to say how much the country would loss was the railroad not there (which is a fictitious value). What about parks? Don't even get me started on natural resources (rivers, lakes, coast, jungles, biodiversity, etc). But even then, those are the easy ones. We still have left education, experience, skills and the like. But this post is long already, the point has been made and now I have a headache for thinking all this.  Indeed, there are many serious and very clever attempts to make estimations of these. But they are used to answer very specific questions. GDP, on the other hand is easy: just sum all the sells of goods and services. We (economists) don't like it very much either, but that's all we have. Just try to accept it, and don't take it to seriously."
Would healthcare costs increase (in the long term) if all smokers quit?,"It is a reputable journal and the results are surely correct on a ""total dollars"" sense. I guess the point they are making is that because health care expenditure of old age individuals is very likely to be high, there is some 'social benefit' of dying before old age.  However, it seems somewhat meaningless:  A) First, by this logic, murders also decrease health care costs. We're not trying to keep smokers form smoking solely because they will be a health care burden. We also want these people to live long, rewarding lives. B) People contribute to society, their families, their friends and so on. They also save and, specifically, they pay health insurance premiums while they are alive. So, while they might require more care in the long run, non-smokers might well be able to pay or it with the taxes on their income and the productivity they bring to society. Similarly, we all invest through our taxes, on public education, and it goes to waste when somebody that has received public education dies. C) There are enormous variations in the amount of healthcare spending that old people receive across countries, so the estimates of how much health are old people will receive has got to be very imprecise.  D) Investments pay a positive rate of return, so if we saved the money now we could pay for a lot more care later. Put differently, all the resources that would not be devoted to the care of smokers now would be used to improve our lives meaningfully, potentially even helping us do research into how to take care of old people effectively, which would again reduce the future expected cost of caring for the smokers."
Mathematical open problems that (when answered) might unlock MAJOR mathematical (micro)economics/finance/econometrics discoveries,"People have argued that if $P \neq NP$ then efficient markets are impossible and certain equalibria may not exist. However, they may hold approximately, so I'm not sure if this qualifies. Additionally, if it turns out that $P=NP$ then certain economic optimization problems (e.g. in logistics) become easily solvable. On the other hand, if $P \neq NP$ then it might imply that encryption is very safe and therefore certain forms of economic arrangements will still be possible in the face of technological advance.  "
Inflation without Increase in Money Supply?,"It all depends on what you mean by inflation and by money supply. Technical questions and answers need specific definitions, otherwise everyone ends up talking at cross-purposes. Is it possible to have an increase in general price levels without any changes to the amount of money in circulation? Yes: if the velocity of circulation of money increases, and the amount of goods and services available to buy does not increase by as much. Is it possible to have an increase in general price levels without any changes to the amount of money in circulation or the velocity of circulation? Yes: if the amount of goods and services available to buy, decreases, so that there's more money chasing fewer goods. Is it possible to have an increase in general price levels without any changes to the amount of money in circulation or the velocity of circulation, and with no decrease in the amount of goods and services available to buy? Yes, if the demand curve changes so that the same amount of money is now used to buy a smaller quantity of stuff at higher prices. Is it possible to have an increase in general price levels without any changes to the amount of money in circulation or the velocity of circulation, and with no change in the amount of goods and services bought? No, because the velocity of circulation is by definition total transaction value divided by the amount of money in circulation, so if velocity, quantity and money supply are constant, then prices must be too, because total transaction value equals prices times quantity."
"Why wouldn't competition prevent ""usurious"" payday loan rates?","The posted quote is economic nonsense. If a lender chooses to innovate and reduce cost to borrowers in order to secure a larger share of the market, the competing lenders will instantly do the same, negating the effect. This applies to any industry without intellectual property protection -- it is hardly unique to the payday loan industry. By this logic, we'd expect to encounter massive price gouging across dozens of industries. Besides, if payday loan innovation is in the form of software that better predicts default (the most likely path), it will be protected under copyright law and potentially software patents. And while business model innovations are not patentable, there's still a first mover advantage. The payday loan business is not highly profitable. Profit margins of payday-loan corporations are publicly available, and lower than most other industries. One study found that ""despite the common belief, payday lending firms do not always make extraordinary profits. In fact, when compared to many other well-known lending institutions, payday lenders may fall far short in terms of profitability."" This is not surprsing, since the payday loan market is highly saturated, which suggests substantial competition. ""Usurious"" APR rates are misleading. A typical payday loan charges \$17 for a two-week \$100 loan.
Expressed as an annualized rate, this is an ""outrageous"" 390% APR.
But the loan's short-term nature means transaction costs will likely prevent a large profit.
(The source of this information is potentially biased. Be sure to read critically.)"
Are the figures quoted in this liblabcon's blog post accurate? (topic: UK banks),"Source:- Bankers cost this country £456.3bn in fraud In the UK, where the government bailed out Royal Bank of Scotland Group Plc (RBS) and Lloyds Banking Group, the total outstanding support explicitly pledged to Britain’s banks stood at 456.3 billion pounds ($730 billion) at the end of March, or 31 percent of GDP, the National Audit Office said in a July report. The amount was down from a peak of 1.16 trillion pounds.  Source:- Politicians had to bail the bankers out with £1.2tn, leaving the next generation with 3 times more tuition fees to pay. The Treasury has pledged to spend £1.2 trillion on the bail-out since the crisis began. But the real outlay has been much smaller. By March it was committed to spending £456.33bn: £123.93bn in loan or share purchases, which required an actual cash injection from the government to the banks, and £332.4bn in guarantees and liabilities. It costs taxpayers up to £5bn a year just to service the loan that the crisis incurred.  The idea seems to be more to create noise rather than putting facts into perspective. The 2 lines in isolation are damning, but when you read the whole thing it seems to make a bit more sense. Seems more at fear mongering rather than putting the facts into perspective."
Concave production function implies convex cost function,"Given the fixed input price $w$, the cost function can be written as
$$
C(q)=f^{-1}(q)\times w
$$
where $f^{-1}$ is the inverse of the production function $f$. From the discussion here, one can conclude that the inverse of a concave strictly increasing function is convex. Thus, $C(q)$ is convex as well.  Going back to your approach, you might like to have this clearly stated. Let $q''=\alpha q + (1-\alpha)q'$, $f(z)=q$, $f(z')=q'$, and $f(z'')=q''$ Then 
$$
\begin{align}
f(z'')&=&q''\\
&=&(\alpha q + (1-\alpha)q')\\
&=&\alpha f(z)+(1-\alpha) f(z')\\
&\leq&f(\alpha z +(1-\alpha z'))
\end{align}
$$
Then $f(z'')\leq f(\alpha z +(1-\alpha z'))$ implying that $\alpha z +(1-\alpha z')\geq z''$ since $f$ is (strictly) increasing. Hence, 
$$
z''w=C(q'')\leq \alpha C(q)+ (1-\alpha) C(q')
$$"
Lagrangian: How to understand the No-Ponzi Condition,"The condition is mostly referred to as the No-Ponzi (-scheme) [NP] condition. It is one additional constraint, that prevents Ponzi-schemes: Paying debt with new higher debt, ad infinitum.  By the way: The NP condition is one condition, hence the associated multiplier should be $\psi$ instead of $\psi_t$. While certainly nothing is lost repeating the same condition over and over again (for any $t$), we don't need it more than once, and it is being imprecise. Think about optimization for finite $T$ periods. Then, you have the condition that $B_T \geq 0$. The Lagrangian optimization gives you the local optimization between $0, 1, 2$... There are many solutions that are locally optimal, but you will only allow solutions that in the end lead to $B_T > 0$. Your example is much too messy to think about these core issues. Look instead at the problem  $$ \max_{\{c_t, a_{t+1}\}_t} \sum_t \beta^t U(c_t) + \lambda_t (a_{t+1} + c_t - Ra_t)$$ That is, a household that choses assets $a$ and consumption $c$ to maximize his utility. You can summarize the FOC as  $$ \beta^t U'(c_t) = \lambda_t \\
\lambda_t = R\lambda_{t+1}\\
\Leftrightarrow U'(c_t) = \beta R U'(c_{t+1})
$$ Look for a moment at the special case where $\beta R = 1$ (what does that imply?). With most preferences, this necessarily leads to $c_t = c_{t+1}$. This is the local optimization that I was referring to, which is what the Lagrangian gives you. There are, however, infinitely many solutions that satisfy $c_t = c_{t+1}$. Next, we try to use the budget constraint: $$ a_{t+1} + c_t = R a_t\\
\Leftrightarrow R a_0 = \lim_{T\to\infty}\sum_{t=0}^T \frac{c_t}{R^t} + \frac{a_{T+1}}{R^T}$$ This is as far we get using the (infinite) set of local budget constraints, where I have used forward iteration (hopefully correctly), assuming any start date $t=0$. Now, if the household also has to satisfy the NP condition, this boils down to  $$R a_0 = \lim_{T\to\infty}\sum_{t=0}^T \frac{c_t}{R^t}$$ which, as we showed $c_t$ to be constant, we can solve easily and receive a single budget constraint. The unique solution to the problem that satisfies the NP condition is the solution where $c_t$ is a constant and this last equation holds."
"What does Battigalli really mean by ""Players can not choose strategies, they can only choose actions.""?","The idea is precisely that players do not chose actions, but only chose one action at the time at every node at which they play, based on their beliefs about the way other players and themselves will play at future nodes in the game (where beliefs are conditional on the history that led to that node). The interpretation is letting players choose full-fetched strategies is equivalent to letting players rely on a computer program to plays the game in their place. That is, they can commit via this computer program to playing a given action at each node. Such games with commitment devices are in essence very different from games in which the actual players have to repeatedly chose an action at each of their decision nodes. When actual players play at nodes, players have to form beliefs about the way other players and themselves will play at future nodes, and these beliefs may depend on the history that led to future nodes.  For instance, in a Stackelberg game, the leader could believe that the follower will be rational (i.e., utility maximizing) if the leader plays ""Low production"", but will be irrational (i.e., non utility maximizing) if the leader plays ""High production"". Maybe the leader anticipates that the follower will be angry if the leader plays ""High production"", and that, blinded by her anger, the follower then then want to retaliate.  If the follower could have committed through a strategy, the game would have been completely different. Maybe the follower could have committed not to retaliate before she gets angry, and she cannot help her desire to retaliate anymore. But here the idea is that the actual follower has to choose an action later in the game given what the leader chose at the root node. Therefore, the behavioral rule through which the follower chooses an action at a node (e.g., utility max vs. non utility max), and the beliefs of the leader about these procedures may depend on the history that led to that node too.  This opens the way for many new outcomes of the game that would not have emerged from classical game theory. From a conceptual point of view, it also switches the focus from solution concepts to epistemic and behavioral assumption (i.e., from classical game theory to epistemic game theory). Instead of identifying a set of reasonable outcomes (e.g., Nash equilibrium outcomes) and look at the strategies that match these outcomes, one identifies reasonable properties of players' behaviors and beliefs (about each others' beliefs and behaviors), and derives the conclusions of these epistemic and behavioral assumptions for the outcome as the game unfolds. Now, this is just to give some meat and intuition to Battigalli's framework, and it is does not do justice to the richness of the framework (in part because I don't know much about his work other that the video you linked to). If you haven't done it yet, I strongly recommend that you watch the whole video. I think Battigalli does a great job at making his framework accessible. He also present helpful and intuitive examples to connect his epistemic approach to ""classical"" game theory by identifying simple conditions on players beliefs and behaviors that allow to recover classical solutions to games such as backward induction."
Causes of income inequality in the US,"The reason you bring forward belongs to technological directed change, which is regarded one of the main explanations for wage growth differentials. Keep in mind it's not exactly the way you're phrasing it: The growth in tech companies rewards people who are skilled well for their kind of jobs (as opposed to ""wealthy people""). Card and DiNardo have a nice summary article on the issue: The recent rise in wage inequality is usually attributed to skill-biased technical change (SBTC), associated with new computer technologies. We review the evidence for this hypothesis, focusing on the implica- tions of SBTC for overall wage inequality and for changes in wage differentials between groups. A key problem for the SBTC hypothesis is that wage inequality stabilized in the 1990s despite continuing ad- vances in computer technology; SBTC also fails to explain the evo- lution of other dimensions of wage inequality, including the gender and racial wage gaps and the age gradient in the return to education."
Alternative way of deriving OLS coefficients,"The $\mathbf M = \mathbf I-\mathbf X(\mathbf X'\mathbf X)^{-1}\mathbf X'$ matrix is the ""annihilator"" or ""residual maker"" matrix associated with matrix $\mathbf X$. It is called ""annihilator"" because $\mathbf M\mathbf X =0$ (for its own $X$ matrix of course). Is is called ""residual maker"" because $\mathbf M \mathbf y =\mathbf {\hat e}$, in the regression $\mathbf y = \mathbf X \beta + \mathbf e$.   It is a symmetric and idempotent matrix. It is used in the proof of the Gauss-Markov theorem.   Also, it is used in the Frisch–Waugh–Lovell theorem, from which one can obtain results for the ""partitioned regression"", that says that in the model (in matrix form) $$\mathbf y = \mathbf X_1\beta_1 + \mathbf X_2\beta_2 + \mathbf u$$ we have that $$\hat \beta_1 = (\mathbf X_1'\mathbf M_2\mathbf X_1)^{-1}(\mathbf X_1'\mathbf M_2)\mathbf y $$ Since $\mathbf M_2$ is idempotent we can re-write the above by $$\hat \beta_1 = (\mathbf X_1'\mathbf M_2\mathbf M_2\mathbf X_1)^{-1}(\mathbf X_1'\mathbf M_2\mathbf M_2)\mathbf y$$ and since $M_2$ is also symmetric we have $$\hat \beta_1 = ([\mathbf M_2\mathbf X_1]'[\mathbf M_2\mathbf X_1])^{-1}([\mathbf M_2\mathbf X_1]'[\mathbf M_2\mathbf y]$$ But this is the least-squares estimator from the model $$[\mathbf M_2\mathbf y] = [\mathbf M_2\mathbf X_1]\beta_1 + \mathbf M_2\mathbf u$$ and also $\mathbf M_2\mathbf y$ are the residuals from regressing $\mathbf y$ on the matrix $\mathbf X_2$ only.   In other words:
1) If we regress $\mathbf y$ on the matrix $\mathbf X_2$ only, and then regress the residuals from this estimation on the matrix $\mathbf M_2\mathbf X_1$ only, the $\hat \beta_1$ estimates we will obtain will be mathematically equal to the estimates we will obtain if we regress $\mathbf y$ on both $\mathbf X_1$ and $\mathbf X_2$ together at the same time, as a usual multiple regression.   Now, assume that $\mathbf X_1$ is not a matrix but just one regressor, say $\mathbf x_1$. Then $\mathbf M_2 \mathbf x_1$ is the residuals from regressing the variable $X_1$ on the regressor matrix $\mathbf X_2$. And this provides the intuition here: $\hat \beta_1$ gives us the effect that ""the part of $X_1$ that is unexplained by $\mathbf X_2$"" has on ""the part of $Y$ that is left unexplained by $\mathbf X_2$"". This is an emblematic part of classic Least-Squares Algebra."
Why is CRRA utility often used in macroeconomics DSGE model?,"Models at Dynamic Stochastic General Equilibrium level must be able to replicate real economies to an acceptable degree. One of the features of real economies has been a relatively stable growth rate (see also this post), $\dot x/x=\gamma$, where the dot above a variable denots the derivative with respect to time. So one would want a model that admits a constant growth rate at its steady-state. In the benchmark deterministic/continuous time ""representative household"" model, the Euler equation  takes the form $$r = \rho - \left(\frac {u''(c)\cdot c}{u'(c)}\right)\cdot \frac {\dot c}{c}$$ This is the optimal rule for the growth rate of consumption. The rate of pure time preference $\rho$ is assumed constant. The interest rate $r$ has its own way to become constant at the steady state. So in order to obtain a constant consumption growth rate at the steady state, we want the term $$\left(\frac {u''(c)\cdot c}{u'(c)}\right)$$
to be constant too. The Constant Relative Risk Aversion (CRRA) utility function satisfies exactly this requirement: $$u(c) = \frac {c^{1-\sigma}}{1-\sigma} \Rightarrow u'(c) = c^{-\sigma} \Rightarrow u''(c) = -\sigma c^{-\sigma-1}$$ So $$\frac {u''(c)\cdot c}{u'(c)} = \frac {-\sigma c^{-\sigma-1} \cdot c}{c^{-\sigma}} = -\sigma $$
and the Euler equation becomes $$\frac {\dot c}{c} = (1/\sigma)\cdot (r-\rho)$$ Barro & Sala-i-Martin (2004, 2n ed.), extend the required form of the utility function when there is also leisure-labor choice (ch. 9 pp 427-428).
These fundamental property extends to the case of stochastic/discrete time. XXXX To compare, if we have specified a Constant Absolute Risk Aversion (CARA) form, we would have  $$u(c) = -\alpha^{-1}e^{-\alpha c} \Rightarrow u'(c) = e^{-\alpha c}\Rightarrow u''(c) = -\alpha e^{-\alpha c}$$ and the Euler equation would become $$\dot c = (1/\alpha)\cdot (r-\rho)$$ i.e.here we would obtain a constant steady-state growth in the level of consumption (and so a diminishing growth rate). "
"Perfect Competition, Zero profit rule and General Equilibrium","Parallel to Arrow and Debreu, there is the approach of Lionel McKenzie, in which no ownership is specified and all technology has constant returns to scale. In such a model, firms can make no profit. If all firms in an Arrow-Debreu economy have constant returns to scale, equilibrium profits must necessarily be zero. A firm then always has the option to produce nothing without any input, so profits cannot be negative. If a firm were to make positive profits, it could double the profit by doubling the production plan, which would contradict profits being maximal. So the Arrow-Debreu approach is seemingly more general than the McKenzie approach. However, there is a way to represent the Arrow-Debreu approach within McKenzie's approach. Let the commodity space be $\mathbb{R}^l$. If $Y\subseteq\mathbb{R}^l$ is a convex production set, then there exists a convex production set with constant returns to scale and an additional factor, $Y'\subseteq\mathbb{R}^{l+1}$ such that
$$Y=\big\{y\in\mathbb{R}^l:(y_1,\ldots,y_l,-1)\in Y'\big\}.$$
Indeed, the set $Y'$ must be given by $$Y'=\big\{\alpha(y_1,\ldots,y_l,-1):y\in Y, \alpha\geq 0\big\}.$$
So one can always think of decreasing returns to scale as constant returns to scale with an unmodeled factor whose net supply is $1$. Now, in terms of profits, let $p=(p_1,\ldots,p_{l+1})$ be a price system for the extended commodity space. The firm with the production set $Y'$ must make zero profits in equilibrium. So if $(y_1,y_2,\ldots,y_l,-1)$ is a profit-maximizing production plan we must have, by the mentioned zero profit condition, that
$$p_1 y_1+p_2 y_2+\cdots p_l y_l + p_{l+1}(-1)=0$$
$$p_1 y_1+p_2 y_2+\cdots p_l y_l=p_{l+1}.$$
So profits for the firm with the original production set $Y$ can be interpreted as returns to the unmodeled production factor. The economic relevance of these results is that one can always view profits as returns to unspecified factors in an economy that actually satisfies a zero-profit condition."
Why is bargaining more common in poor countries?,"In first world countries, the price of a bottle of water is set by a well-established market. There are millions of prospective buyers of a bottle of water and thousands of prospective suppliers, and while many suppliers are able to achieve brand differentiation, for the most part their water is fungible. Moreover, the supply and demand curves of water are reasonably stable, and water companies have the excess capital to hold inventory stockpiles that further reduce the volatility of prices. There is little information asymmetry. Customers can easily comparison shop, and price discrimination is very difficult. Suppliers are generally large corporations, which means that allowing bargaining would introduce massive headaches, such as agent-principal issues. There's also a feedback effect: once an economy and culture are built around fixed prices, it's more difficult to introduce bargaining. Water companies are in a low-margin business where their money comes from high volume. Spending resources bargaining would destroy those margins."
No Ponzi game condition and transversality condition are the same?,"Is it right to interpret the no Ponzi game condition as a finite
  horizon version of the transversality condition? No. The ""No-Ponzi-Game"" or ""solvency"" condition is an external constraint imposed on the individual by the market/other participants. The individual would very much like to violate it. The Transversality condition must be satisfied in order for the individual to maximize indeed its intertemporal utility. It is an optimization condition. So they are conceptually very different aspects of the problem. Finally the No-ponzi-game/solvency condition is not inherently of finite horizon -it extends to the infinite horizon also."
Ricardo's theory of comparative advantage,"The solution concept used in Ricardo's modell is the competitive equilibrium. Let the set of countries $N$ be defined as $N = \left\{E,P\right\}.$ (England, Portugal) Then the competitive equilibrium is a vector
$$
\left(p,\left(q_{x,i},q_{y,i}\right)_{i\in N},\left(c_{x,i},c_{y,i}\right)_{i\in N}\right),
$$
where $p$ is the equilibrium price ratio of the goods $x$ and $y$, so $p = \frac{p_x}{p_y}$, and $\left(q_{x,i},q_{y,i}\right)$ and $\left(c_{x,i},c_{y,i}\right)$ are the production and consumption vectors of country $i$. The equilibrium vector has the following properties: Let us examine what these properties imply. The set $T_i$ is a triangle. As goods have positive value in equilibrium so all the labor is used up and the production vector is chosen from the production possibility frontier. Which industry ($x$ or $y$) can employ labor more lucratively? In industry $x$ a unit of labor produces value $\frac{p_x}{a_{x,i}}$. Similarly the value produced in industry $y$ is $\frac{p_y}{a_{y,i}}$. If $\frac{p_x}{a_{x,i}} > \frac{p_y}{a_{y,i}}$ only good $x$ is produced, if $\frac{p_x}{a_{x,i}} < \frac{p_y}{a_{y,i}}$ only good $y$ is produced, if $\frac{p_x}{a_{x,i}} = \frac{p_y}{a_{y,i}}$ it does not matter how labor is allocated among the industries as long as all labor is used. So the profit maximizating quantities are
$$
\left(q_{x,i},q_{y,i}\right) = \left\{
\begin{array}{cc}
\left(\frac{L_i}{a_{x,i}},0\right) & \frac{a_{x,i}}{a_{y,i}} < p \\
\alpha \cdot \left(\frac{L_i}{a_{x,i}},0\right) + (1 - \alpha)  \cdot \left(0,\frac{L_i}{a_{y,i}}\right)
 & \frac{a_{x,i}}{a_{y,i}} = p \\
\left(0,\frac{L_i}{a_{y,i}}\right) & \frac{a_{x,i}}{a_{y,i}} > p. 
\end{array}
\right.
$$
The optimum condition for the utility maximization problem is
$$
MRS_i(c_{x,i},c_{y,i}) = \frac{c_{y,i}}{c_{x,i}} = p.
$$
Because the utility functions in England and Portugal have the same form we can take this further. From
$$
\frac{c_y^A}{c_x^A} = p = \frac{c_y^P}{c_x^P}.
$$
we get
$$
\frac{c_{y,A}}{c_{x,A}} = p = \frac{c_{y,P}}{c_{x,P}}.
$$
we get
$$
c_{y,P}  = \frac{c_{y,A}}{c_{x,A}} \cdot c_{x,P}.
$$
Using this
$$
\frac{c_{y,A}+c_{y,P}}{c_{x,A}+c_{x,P}} = \frac{c_{y,A}+\frac{c_{y,A}}{c_{x,A}} \cdot c_{x,P}}{c_{x,A}+c_{x,P}} =  \frac{c_{x,A}}{c_{x,A}} \cdot \frac{c_{y,A}+\frac{c_{y,A}}{c_{x,A}} \cdot c_{x,P}}{c_{x,A}+c_{x,P}} =
\frac{c_{y,A} \cdot c_{x,A} + c_{y,A} \cdot c_{x,P}}{c_{x,A} \cdot \left(c_{x,A}+c_{x,P}\right)}.
$$
so
$$
\frac{c_{y,A}+c_{y,P}}{c_{x,A}+c_{x,P}} = \frac{c_{y,A} \cdot c_{x,A} + c_{y,A} \cdot c_{x,P}}{c_{x,A} \cdot \left(c_{x,A}+c_{x,P}\right)} = \frac{c_{y,A}}{c_{x,A}} = p.
$$
What this says is that the relative demand ($\frac{c_{y,i}}{c_{x,i}}$) is not only equal to the price ratio for individual countries but also the relative aggregate world demand is equal to the price ratio. (Again, this is only true if the individual countries have Cobb-Douglas utility functions with identical parameters.) We now have a relatively easy way to find the equilibrium price ratio: we calculate relative aggregate supply. As aggregate supply equals aggregate demand in equilibrium, relative aggregate supply will equal relative aggregate demand, and as we have just shown it will also equal $p$. We get relative supply from the profit maximizing productions of the individual countries. Let us first discuss the aggregate of the profit maximizing productions, which I will denote by $(q_x,q_y)$. So $(q_x,q_y) = (q_{x,E} + q_{x,P},q_{y,E} + q_{y,P})$ which means
$$
(q_x,q_y) = \left\{
\begin{array}{cc}
\left(\frac{L_E}{a_{x,E}} + \frac{L_P}{a_{x,P}},0\right) & \frac{a_{x,E}}{a_{y,E}} < \frac{a_{x,P}}{a_{y,P}} < p \\
\left(\frac{L_E}{a_{x,E}} + \alpha \cdot \frac{L_P}{a_{x,P}} , (1 - \alpha) \cdot \frac{L_P}{a_{y,P}} \right) & \frac{a_{x,E}}{a_{y,E}} < p = \frac{a_{x,P}}{a_{y,P}} \\
\left(\frac{L_E}{a_{x,E}} , \frac{L_P}{a_{y,P}} \right) & \frac{a_{x,E}}{a_{y,E}} < p < \frac{a_{x,P}}{a_{y,P}} \\
\left(\alpha \cdot \frac{L_E}{a_{x,E}}, (1 - \alpha) \cdot \frac{L_E}{a_{y,E}} + \frac{L_P}{a_{y,P}} \right) & \frac{a_{x,E}}{a_{y,E}} = p < \frac{a_{x,P}}{a_{y,P}}  \\
\left(0, \frac{L_E}{a_{y,E}} + \frac{L_P}{a_{y,P}} \right) & p < \frac{a_{x,E}}{a_{y,E}} < \frac{a_{x,P}}{a_{y,P}}  .
\end{array}
\right.
$$
The relative aggregate supply is the ratio $\frac{q_x}{q_y}$. It is perhaps best described by this image:  The relative aggregate demand is the ratio $\frac{c_x}{c_y}$. As we have discussed $\frac{c_y}{c_x} = p$ so 
$$
\frac{c_x}{c_y} = \frac{1}{p}.
$$
As a result one can draw the relative aggregate demand in the previous graph as  hyperbole. The intersection with the relative aggregate supply curve will give yield the equilibrium price ratio and will also yield information about the production of individual countries. Where this intersection occurs depends on the parameters $L_E,L_P,a_{x,E},a_{y,E},a_{x,P},a_{y,P}$. I will distinguish between three types of equilibria, each represented in the following figure:  In the 1. equilibrium the price ratio is $p = \frac{a_{x,E}}{a_{y,E}} < \frac{a_{x,P}}{a_{y,P}}$. Thus Portugal specializes and only produces good $y$, but England does not specialize but produces both good $x$ and $y$. Producing either good gives her the same value. The exact equilibrium quantities England produces are determined by the value the aggregate demand curve takes at price $p$, because
$$
\frac{q_x}{q_y} = \frac{c_x}{c_y} = \frac{1}{p}
$$
and
$$
q_x = q_{x,E} + q_{x,P} = q_{x,E} + 0 \hskip 20pt q_y = q_{y,E} + q_{y,P} = q_{y,E} + \frac{L_P}{a_{y,P}}.
$$
In this case England will still not achieve a net export of good $y$. The preferences tell us that Portugal will consume both goods $x$ and $y$. But the only way it can pay for the goods $x$ consumed is by trading some of its goods $y$, so Portugal, not England, will be a net exporter of good $y$ while England is a net exporter of good $x$. 
In the 2. equilibrium both countries specialize: England produces only good $x$, Portugal produces only good $y$. This is usually presented as the textbook case. 
The 3. equilibrium is like the 1. equilibrium, but here England specializes and produces only good $x$ while Portugal does not specialize and produces both. So to answer my original questions: 
Given that England has a comparative advantage in producing good $x$,"
How does the money supply behave when bank loans are repaid?,"The money is removed when the loan principal is repaid. The actual point in the loan this occurs depends on the loan terms. For a typical compound interest rate loan, this means a small portion of the principal is repaid every month, and a matching liability deposit (money in the customers account) is removed. For an interest only loan, this occurs at the end of the loan - assuming all the principal is repaid then. Interest payments essentially circulate through the monetary system. They're deducted from the customer's account, recognised as income by the bank, and then paid out as some form of expense, e.g. salaries, rent, taxes, etc. They may also be moved into an internal account to provide required loss provisions on loans.  Loan defaults are also an expense, and this is the achilles heel of the banking system. If defaults on loans exceed the bank's loss provisions and profits from interest, then the bank will have to write off against its capital - and this interferes with the regulatory controls on lending, causing the money supply to shrink. In practice, central bank or government intervention is inevitable if this occurs. As far as the relationship of principal to the money supply. Essentially the banking system relies on new lending always being sufficient to replace the money being removed. In most banking systems, new lending is typically in excess of loan repayment, and so we see the money supply more or less continuously expanding. "
Can Dynare solve general equilibrium (GE) models with non-convex adjustment costs?,"Short answer: no. Dynare, and linearization/perturbation methods in general, are designed for solving A model with fixed cost is typically non-smooth, and its behavior away from the steady state may be very different, if e.g. the firm switches from investing to not investing. On the most practical level, a model with fixed cost will typically include equation such as $$
V = \max \left\{ V^{\text{invest}}, V^{\text{not invest}} \right\},
$$ which cannot be entered into Dynare, because max operator is not supported. On the other hand, first order conditions for convex (e.g. quadratic) adjustment cost are still smooth (one simply adds additional terms to Euler equation for investment) and thus can be easily solved with Dynare. To actually compute optimal policy with fixed costs, one has typically to use global method, e.g. value function iteration. I'm not aware of any standardized toolbox for solving such problems, so you may need to code your own. PS: there are some modelling tricks that make the problem smoother, typically in a setting with many, possibly heterogeneous agents/firms. For example, Thomas (2002) keeps track of number of firms depending on how long they didn't invest, and solves the model with standard linearization on this extended state space. Khan & Thomas (2007) assume that the fixed cost is random and iid over time and across firms, so one can average over the realization of fixed cost to obtain smooth value functions. Miao & Wang (2014) use a similar approach in a model with constant returns to scale and show how it aggregates to a version of representative-firm model with only convex adjustment costs."
Did free markets cause the industrial revolution?,"It is fair to say that free markets played a non-trivial role in industrialization and that protectionism in Britain hindered industrialization. However, the Industrial Revolution was caused by many other factors than having relatively free markets. It required Britain to generally adopt set of inclusive institution (of which relatively free markets are just one example), and it also required some historic luck. In economics, it is accepted that countries with good 'inclusive' institutions, such as strong property rights, are more productive and able to develop faster (or even develop at all) than countries with bad 'extractive' institutions, such as forced labor (see Acemoglu 2008, Acemoglu & Robinson 2000a, 2000b, 2001, 2006, 2008; Olson 1984, Bates 1981, 1983, 1989 and sources cited therein) An industrialization, is just a period of a rapid economic development where the share of national income from manufacturing also increases (Bagchi  2016), so it requires that country predominantly accept inclusive institutions which are thought to be the main precondition for an economic development. Inclusive institutions are institutions that allow people broadly to participate in the economy, so free markets and free trade are generally inclusive institutions. However, they are not the only inclusive institutions and not even the most important ones.  For example, a far more important institutions are strong and secure property rights, without which people would be afraid of making large capital investments (see Acemoglu & Robinson Why Nations Fail) which are obviously pre-requisite for industrialization to naturally occur. Next, having a central government that can impose peace and security from violence and coercion is also extremely important, forced labor or other types of coercive extractive institutions are antithesis to economic development (again see Acemoglu & Robinson). Protectionism, is also an extractive institution, and Britain indeed was protectionist during its main industrialization phase, but in the grand scheme of things having few extractive institution, while relaying mostly on inclusive institutions won't hinder economic development. But generally it is argued and it was already recognized by contemporary economists that the Britain's  protectionist policies (e.g. corn laws) hindered the industrialization and that it happened in spite of them rather than thanks to them, since its protectionist policies primarily advantaged agriculture  (see Hirst, (1925) From Adam Smith to Philip Snowden). In addition, it was likely also due to some historical luck. Other countries at that time, such as the Netherlands, had by all accounts more advanced economic system to that of Great Britain (see Mokyr 2000). In fact, while industrialization proper, started in the Britain, the Netherlands was already experiencing proto-industrialization at small scale far earlier (Bavel 2003). However, in the end the industrialization first took of in England, and it is argued that was because  the Industrial Revolution at its inception requires a close cooperation between scientists, engineers and businessmen (see Mokyr 2000) . Mokyr argues it was the ""closeness of natural philosophers, engineers, and entrepreneurs was a key to success in Britain."" Consequently, it would be fair to say that free markets played an important role in Britain's industrialization, but they were certainly not the only cause, far from it. Industrialization required a system built predominantly of inclusive institutions where private property, security, fair set of laws (at least relatively fair for their time)  etc dominate. In addition, there already were countries that were undergoing proto-industrialization for much longer time than UK, but those countries did not undergo proper industrialization because share of income generated from manufacturing was still small. This was because for manufacturing to be really productive you need to apply some technology to it and UK happened to be a place where cooperation between scientists, engineers and business people was more common. Hence you should not just try to pin the industrialization in the UK on one single factor."
Rothschild-Stiglitz working paper?,"(Edited)
Here is the working paper. Enjoy. Rothschild, M., and J. E. Stiglitz, ""Equilibrium in Competitive Insurance Markets,"" Technical Report No. 170, IMSSS Stanford University, 1975. https://drive.google.com/file/d/1MXb3OcOQc_lxNYC4CzsNTTax9_3Q_4UN/view?usp=sharing"
"Why, in supply and demand curves, does price go on the y-axis?","This objection never made too much sense to me. In the standard model of perfect competition, firms take the price as given and respond by choosing their quantity. So you have a model in which a bunch of actors choose quantity and the market price emerges as a consequence of all of those decisions. This makes it sound awfully like price is the ""dependent"" variable, which by convention is always placed on the vertical access. Indeed, this seems to be how Alfred Marshall (who originated the modern form of the Demand-Supply diagram) thought about things. Here's a quote from An Introduction to Postitive Economics, Seventh ed. by Richard G. Lipsey (as quoted here): ""Readers trained in other disciplines often wonder why economists plot demand curves with price on the vertical axis. The normal convention is to put the independent variable on the X axis and the dependent variable on the Y axis. This convention calls for price to be plotted on the horizontal axis and quantity on the vertical axis. ""The axis reversal - now enshrined by nearly a century of usage - arose as follows. The analysis of the competitive market that we use today stems from Leon Walras, in whose theory quantity was the dependent variable. Graphical analysis in economics, however, was popularized by Alfred Marshall, in whose theory price was the dependent variable. Economists continue to use Walras' theory and Marshall's graphical representation and thus draw the diagram with the independent and dependent variables reversed - to the everlasting confusion of readers trained in other disciplines. In virtually every other graph in economics the axes are labelled conventionally, with the dependent variable on the vertical axis."" See also this post on Greg Mankiw's blog."
How do I calculate price elasticity of demand using historical price and quantity data?,"You've fallen into a really common pitfall -- the spurious regression. The parameters you chose to include can't be chosen 'willy nilly' by throwing data into a regress command. Ultimately this can't be answered in so little words, without data, while maintaining accuracy. That said I can try to answer your question as a reference point. Before even attempting this I couldn't emphasize enough caution. Just remember that a model that's almost correct -- for practical purposes is 100% wrong.  So it seems you've settled on a linear log-log specification. I would do a quick Google Scholar search on papers forecasting consumer demand for a product similar to the one of interest. The functional form along with parameters you choose to include must be derived from theory. You'd be able to just copy their equation and just modify it to your data. Otherwise, you're doing blind statistics. This is the main reason you're getting inconsistent signs.  What you're trying to do is specify a derived demand for the category of goods you're focussing on. Your general model is going to look like this: $ logQ_t = \alpha + \beta_llogP_t + X'_t\gamma + \epsilon$ where:
Q sales    P selling point
  X vector of factors other than selling price  
$\epsilon$ random component in demand Have you included any economic indicator parameters? This would be things like GDP, income, population growth, unemployment, interest rates... etc. Depending on the good -- for your purpose -- there's usually theory providing necessary parameters regardless of statistical significance. I would do this first. Add some macro indicators and re-check the F-statistic for the model. Your model would then provide not only own price elasticity of the good, but you'd get an income elasticity, as well as cross-price elasticities for competing/complementing goods. If your company has spent money on advertising this would be necessary to include as well. Have you added dummy variables for the 'category' of the good itself? And trend variables? Lags of seasonal effects, their quadratic effects? Interaction effect variables?  Any cursory approach would lead to omitted variable bias over your misspecified parameters. On most circumstances you'll have to verify the multiplier effect isn't changing through time.  Have you checked for normality, autocorrelated errors? This is as broad and general a reference I can think to give you. The functional specification here might be better calculated through an autoregressive form. But this is beyond the scope I think.  I'm not sure as to your level of econometric/demand analysis so I kept things devoid of much math. I hope this was somewhat helpful. It was great intuition to stop what you're doing when you saw incorrect signs. Keep to that rule of thumb. "
Why do stock exchanges not operate at a fixed frequency?,"As long as trading faster is not explicitly forbidden, there will be people who want to trade faster if they can take advantage of it.  If you are able to trade fast enough, you can take advantage of arbitrage situations.
For instance, if you can detect a fall in the price of soy on the New-York exchange, buy soy on the New-York Exchange, and sell it on the Chicago exchange before the price in Chicago adapts, you will profit from it.  So, on the positive side, the answer to your question in the title ""Why do stock exchanges not operate at a fixed frequency?"" seems to be ""because it's not impossible/forbidden to operate faster and people benefit from it"". Now, on the normative side, there are certainly a fair amount of people who believe that stock exchanges would perform ``better"" if they operated at a fixed frequency. Roughly, increasing the frequency of trades is good if it allows prices to adapt faster to new economic information. Some people argue, however, that past a certain threshold, increasing the speed of trade cannot foster more effective price adjustments. The argument is that relevant economic information only arrives every so often, and there is no point in trading faster than the fastest stream of relevant information.
Past this threshold -- so the argument goes, increasing the speed of trade only allows for increased high-frequency speculation in which traders benefits from arbitrage situations that are unrelated with ``actual"" economic fundamentals.  I don't think they are the first to propose it, but Budish, Crampton and John have a recent pair of papers in which they advocate for a cap on the speed of transactions for similar reasons : The High-Frequency Trading Arms Race: Frequent Batch Auctions as a Market Design Response, and Implementation Details for Frequent Batch Auctions: Slowing Down Markets to the Blink of an Eye."
How infinite Nash equilibria are possible in a game?,"Well the intuition is quite straightforward: as you have mentioned yourself, player 2 is indifferent between playing any action in pure strategies, or randomizing in any possible way. So, for a mixed strategy equilibrium to exist, player 2 needs to play L w/ probability 4/7. If this is the case, it is irrelevant what Player 1 player---player 2's outcome is the same. As a result, in pure strategies the Equilibria are L,L and R,R and, in Mixed strategies, q=4/7 and p can take any value between 0 and 1. Hence, there exist infinite possible Nash Equilibria (p just has to obey the fundamental laws of probability). "
Generalization of the Heckscher-Ohlin Model,"The HO model has been generalised. Vanek does a good job of it.  Instead of only two countries, there is an index of countries. There are many industries. Identical technology Identical, homothetic tastes. The HOV theorem states that if a country is abundant in a factor,
its factor content of trade in that factor should be positive, and
negative otherwise. Empirically, this model is not that successful. Here is a good paper discussing the applications and results. See below for that extension:
Vanek, Jaroslav, The Factor Proportions Theory: The N-Factor Case,” Kyklos, October 1968, 21, 749-755."
Why are ecology and economics seemingly disjunct topics?,"I think you are right to say that an economy is always a subsystem of a larger ecosystem, but it needs not always be modeled as such. It may be interesting to have a precise and elaborate model of the solar system taken in isolation, independently of its interactions with the rest of the galaxy, other galaxies, etc. In very much the same way, there are many good reasons to build a model of an economic system without accounting for how it fits in a more general ecosystem.  The same kind of question applies to many other topics. One could for instance ask ""why partial equilibrium and not always general equilibrium?"", ""why small open economies instead of replicating the actual size structure of the global economy?"", etc. I guess the only answer is that the smaller, less general models are considered useful in one way or another. This is not to say that the opposite endeavours (the ecological economics attempts to model economic systems as part of an ecosystem) are worthless, quite the contrary. But there is only so much you can do and there is always a trade-off (in terms of estimability, tractability, additional assumptions, you name it ) to increasing the generality of a model."
The Savage sure thing principle and Subjective utility representation,"In Kreps' (1988) book ""Notes on the Theory of Choice"", the issue is dealt with in chapter 9 ""Savage's Theory of Choice Under Uncertainty"", after discussing subjective probability in chapter 8. As usual, Kreps' style helps: he has the ability to seamlessly inject his -always formal- approach with very down-to-earth comments and examples that are strong in intuition (and he does it better than Savage, I might add). But also, here ""formal"" does not translate into ""complete exposition"": he explicitly refrains from formally proving parts of the whole apparatus, mentioning that ""this is a two-page proof"", and ""this is another two-page proof"", and ""if you want to prove this, good luck"". For these parts he falls back on Fishburn's (1970) ""Utility Theory for Decision Making"" book, chapter 14 ""Savage's Expected Utility Theory"". And Fishburn is formal alright (more symbols than words in a page).   My impression is that combining these two sources can be beneficial."
How was the year 1925 chosen as a dividing point in the history of economic thought?,"It is most likely an arbitrary date because they had to set some year. In fact the year 1925 is not even a strict guideline but only loose criterion. As such the year was not set due to some special significance. The JEL classification also responds not just to the needs of academicians but also grant agencies, government, university HR departments etc. and so classifications might be arbitrarily set in ways that do not necessary always correspond to what economists believe. However, there are some subtle hints it could have something to do with the advent of Neoclassical Economics. The JEL codes are set by American Economic Association (AEA) as they were originally developed for their Journal of Economic Literature. As such AEA is really the only authoritative source on this issue. Normally classification codes come with guidelines but in this case JEL guidelines for ""History of Economic Thought through 1925"" just read: B1 History of Economic Thought through 1925:
Guideline: Not Specified.
Keywords: Economic Thought. However, B1 is just general category that is further subdivided into smaller sub-categories B10-B19. When you look at each of the B10-B19 guidelines individually most of them include the following caveat: Caveats: The demarcation year of 1925 is used as a loose criterion. When we look at the classification guidelines for History of Economic Thought since 1925"" we find the following guidelines: B2 History of Economic Thought since 1925:
Guideline: Studies that are also relevant to contemporary economics or
economies should be cross-classified here and under the other
appropriate categories.
Keywords: Economic Thought, Neo Classical. Again quite vague but the keyword 'Neo Classical', is suggestive and suspiciously inserted there especially since neoclassical economics (although technically possible to trace prior to 1900s) first gained on prominence in early 1900s (see the History of Economic Thought by Brue and Grant). Although this is really not enough to be certain. Also as in the previous case B2 is split into multiple sub-categories ranging from B20-B29 and most of them again include the following caveat: Caveats: The demarcation year of 1925 is used as a loose criterion. Furthermore, there is actually paper published by JEL about history of it's classification system and how it developed to it's present form. According to Classifying Economics: A History of the JEL Codes by Beatrice Cherrier (2017): ... Yet the AEA classification
does not provide a pure image of the discipline. Instead, it is a compromise between
looking forward and looking backward,
between AEA officials’ sometimes conflicting visions of their science and the multiple
and contradictory demands they face: editors
needed a way to select reviewers and referees; recruitment committees needed a way
to classify job candidates and their output;
the government wanted a system to draft
economists into the war effort, and later to
recruit specialists into the various bureaus
concerned with monitoring and managing
economic affairs; librarians needed help in
indexing papers and books; and the National
Science Foundation (NSF) needed a classification to quantify and evaluate national
scientific expertise ... Author further states: JEL codes reflected changes in
the ways theory and applied work interacted.
Second, the codes point to the transformation of the subject matters of the discipline
and the rise and fall of different approaches
to economics. Third, they reflect changes in
the external pressures on the discipline and
information technology Furthermore, as the article explains the classification codes undergo 4 revisions and were originally set up 1911. Consequently, the year 1925 was not originally even part of classification and might very well change in the future. The article also tells a fascinating story of how these classifications came to be and most of them were results of compromise between various stakeholders and economists holding different opinions on how to classify economics thinking. It is a result of a messy process rather than some unanimous academic consensus. Lastly, I actually written an email to AEA requesting clarification. They confirmed that the year 1925 is not strict criterion and the person I corresponded with simply stated they do not know how the criterion was set. I am not publishing the e-mail as I don't know what would legal ramifications of such act be, and my e-mail address contains my real name, but I expect that anyone making the same inquiry would receive the same answer. So what can we conclude from the above?"
Would a fair distribution of wealth from the super rich increase the purchasing power/life quality of the average person?,"tl;dr: In the hypothetical you set in the body of your question redistribution cannot help the poor. However, this is not because redistribution could not significantly raise the welfare of the poor but rather because in your question 'the rich' actually don't have any resources to share with the rest. In fact, in your hypothetical example's set up, what you call 'redistribution' is equivalent to monetary expansion. However, in real life redistribution, even redistribution of money, is not equivalent to monetary expansion and can help to raise material welfare of people especially of the poor and to the extent quality of life depends on material welfare their quality of life as well. The full answer will be structured in three parts. I will first try to answer your question in body of the text, then explain the problems with your assumptions and finally try to answer the question in your title with small changes that will be still true to the spirit of the question and make it actually answerable in objective fashion (I do this because I think that most of the attention this question is driving comes from the question in its title). In order to answer your Q let's try to model it. In your example you talk about a hypothetical economy when nobody works; there are just people with some given wealth, so we are in some endowment economy where people are endowed with some output $Y$ which constitutes their 'wealth', but rich people have only cash/money $M_r$. What's more, you assume that the cash was not used in any way, so for all practical purposes it simply is not even part of the economy. Moreover, as per the question I will assume money is equally distributed among everyone. Taking the assumptions above seriously, we can just analyze the effect of such money transfer the same way as just an expansion of money supply using simple equation of exchange (this is not normally an appropriate model for evaluating redistribution, but in your case redistribution is just monetary expansion in disguise and this model is a simple textbook example of a model that can be used to analyse effect of monetary expansion): $$MV=PY$$ where, $V$ and $P$ which were not yet introduced, are velocity of money and price level respectively. In this case $Y$ is fixed as there is no production, just everyone has some certain level of wealth. Hence when $M$ increases by adding the $M_r$ either $P$ increases or $V$ must drop (which would happen if the money people get from the rich end up 'buried' and not used), or some combination of thereof. In either case, whether prices or velocity changes, people can't be made better off by the transfer of cash in your hypothetical scenario, as in your scenario the rich $1\%$ really has nothing of value to give to the $99\%$. Their endowments $Y$ remain fixed. Again, I would normally never recommend using this model for analyzing redistribution but in your case you are not really talking about redistribution. The main problem to your question is that if really taken to its logical conclusion it boils down to the following: does creating more money make people any richer? Well, the answer to that is no. What makes people rich is the amounts of goods and services they have access to. In fact, from an economic perspective, the $1\%$ of population in your question must be miserable bunch because by your assumptions, they only have money, while the other $99\%$ actually have some other 'wealth' which presumably in your question implies some goods and services to enjoy. However, the above situation is clearly absurd and does not correspond to reality. Rich people do not sit upon piles of sterile cash that is not used in economy. If their money circulates in the economy either because the rich spend it, or put it in bank into their accounts or invest it into some assets they will already be part of the $M$ and redistribution, even redistribution of money, will not expand $M$. Redistribution might decrease output due to the fact that it distorts incentives, however at the same time there are arguments that it might boost output, as high levels of inequality can make harder for poor people to become entrepreneurs or realize their full potential in different ways. Empirical studies actually show that the relationship between economic growth and intensity of redistribution is mixed, suggesting that on net it has no impact on economic growth save for cases of extreme redistribution that would be well above what we typically see in developed nations nowadays (see Ostry, Berg, and Tsangarides 2014 and sources cited therein). When it comes to velocity redistribution can slightly increase it as some research shows that people with lower savings rate contribute to higher velocity (see Wang & Ding 2005), however the effects are nowhere near strong enough so any transfer will have no net effect on welfare of recipients.  As a matter of fact it is generally agreed by the profession that over long time periods it is an increase in $M$ which leads to inflation (see pretty much any conventional textbook for example Blanchard et al Macroeconomics an European Perspective or Mankiw Principles of Economics). As a consequence of the above, redistribution can be more or less viewed not just as a transfer of money, but as a transfer of resources, and in fact this is how redistribution is treated in vast majority of the literature in public economics (see models presented in any public economics textbook). Many redistribution models will not even explicitly include money at all, and this is not due to lack of oversight but deliberate simplification (the same way as physicists often may assume that some part of space contains a perfect vacuum even if it has some particles) reflecting the view that redistribution is not simply some money creation especially when financed by taxes (even though relative prices might change which can still affect welfare but this is accounted for in the models). The question in your title: Would a fair distribution of wealth from the super rich increase the purchasing power/life quality of the average person? is actually far more sensible but I will still make 3 changes that I think are true to the spirit of what you are actually interested in. First, I am going to change the word fair distribution to redistribution to get rid of the word 'fair'. Well, what is 'fair' or 'fair distribution'? That's a question that human race has pondered ever since civilization emerged from the fertile crescent, and perhaps even in prehistory, without any generally agreed upon answer. Discussion of fairness belongs to moral philosophy, not economics. Second, I will change wealth to wealth/income. The reason for this is that I feel that like many other non-economists, you do not distinguish between wealth and income and treat them jointly. For example, who is in your opinion a richer person? A old retiree whose net assets are worth $\\\$1,000,000$ because his house happened to be in gentrified area and went up in value, but with a measly salary of $\\\$500$ working as shop greeter, or a superstar which might have no net assets (i.e. living in fancy hotel instead of owning a house) but with monthly paycheck of $\\\$70,000$? The retired person has more wealth, but I think many would actually consider person in that situation poor, whereas the second person has no wealth, but I think most reasonable people would consider such a person rich. Third, I will replace 'average person' with 'low income people'. The reason for this is that in any right skewed income distribution (and income distributions in our world are right skewed) an average person will actually be better off than more than half of the population, and optimal redistribution systems are designed to help people at the bottom, not people who are better off than most (even though unfortunately in practice governments often engage in such 'perverse' redistribution as well). Hence I will try to answer the following question: Would a redistribution of wealth/income from the super rich increase the purchasing power/life quality of the low income people? the answer here when it comes to income is a resounding yes, when it comes to wealth the answer is not clear or rather the answer would be maybe. The literature on optimal income tax shows that optimal top marginal tax rates for the richest can be somewhere in rage of $50-75\%$ in the US (see Saez 2001) an generally similar estimates are also found for other developed countries. The simulations for these tax rates are already made with an explicit goal of maximizing actual welfare of the recipients and society overall  - not to maximize any monetary transfer per se - but to maximize their underlying utility. They are based on the following optimal non-linear tax function which comes from the seminal works of Mirrlees (1971) - who in fact got Nobel Prize in Economics for this contribution, Diamond (1998) and Saez (2001). Furthermore, the optimal tax rate even takes into account all the labor supply response and other factors in fact the formula is given by: $$ \frac{T'(z_n)}{1-T'(z_n)} = \left( 1 + \frac{1}{\epsilon_{lT}} \right)\frac{\int (1-b_m)f(z_m)dz_m}{1-F(z_n)} \frac{1-F(z_n)}{z_nf(z_n0)}$$, with $b_n \equiv \frac{\Psi'(u_n)u_c}{\eta}+ nT'(z_n) \frac{\partial l_n}{\partial \rho} $. I won't go over every single term in the formula as it would turn this answer into a book, but broadly speaking the first part $\left( 1 + \frac{1}{\epsilon_{lT}^*} \right)$ is given by elasticity of labor supply to income taxes and you can think of it as an 'efficiency' parameter, the second part $\frac{\int (1-b_m)f(z_m)dzm}{1-F(z_n)}$ tells us what the marginal benefit of redistribution is and this marginal benefit factors in the underlying actual welfare which is captured by $b_n$ which depends on both utility of consumers and the societal utility function, and finally $\frac{1-F(z_n)}{z_nf(z_n0)}$ is the part that captures the relative magnitude of distortions created by this taxation. Again, since we are talking about improvements in actual underlying utility, what the aggregate price level is does not matter. If redistribution would not be able to improve welfare in the poor this formula would give you zero marginal tax rates. Since simulations based on real world parameters (Saez 2001) show that top marginal tax rates could be as high as almost $80\%$, clearly redistribution can improve the welfare of the poor. An important caveat is that this optimal taxing formula does not take into account general equilibrium effects, and general equilibrium effects generally (no pun intended) result in lower taxes, but no reasonable estimate of the magnitude of general equilibrium effects would push the top marginal tax rates to zero. Even if we would say that in general equilibrium these taxes would be half of what partial equilibrium analysis suggests, they would still result in redistribution that would be able to significantly help the low income individuals. When it comes to wealth taxes the literature is much smaller, as studying wealth taxes is exponentially more difficult than income taxes due to the fact that they are rare and we don't have good data. As a consequence the discussion on wealth taxes to some extent takes a form of Twitter fights between various economists (I am referring to Summers-Saez & Zucman infamous Twitter battle) rather than in some proper research.  This being said, some academic work was already done on this topic, but it remains highly inconclusive. I will try to present both views fairly starting with the no wealth tax view and then presenting the pro wealth tax view. Anti wealth tax view: Some economists argue that optimal wealth tax is simply $0$. If this is so then obviously wealth tax cannot help improve welfare of the poor. There are several reasons why this would be so which are all summed nicely in this article from Larry Summers. Here is the digest: Wealth taxation also raises practical concerns — for example, issues of valuation: Is a partnership in a law firm wealth? How will illiquid assets — like football teams or newspapers — be valued? And issues of liquidity: If someone owns 1 percent of Uber — still a private company — she will owe roughly $20 million in taxes each year, but it’s unclear where she can get this money. She can’t sell shares and, if involved with the operation of the company, is likely to be barred from borrowing against the value of her stock. There are also family unit issues: If a couple files separately or gets divorced, do they get two $50 million exemptions? And issues of gaming: There will be incentives to use legal structures to relinquish direct ownership of assets while maintaining control of them. For example, owning assets in a trust or a nonprofit to benefit from wealth while avoiding tax liability. Granting that capital income should be taxed more heavily than it now is, and that unrealized capital gains going untaxed is a serious problem, there is also a question of just how a punitive tax is appropriate. It is important not to be misled by the 2 percent annual rate: A 50-year old who has accumulated a substantial fortune can expect to pay more than half of it in taxes before she dies. Imagine that a wealthy person invests in 10-year treasury bonds, with a 2.4 percent return. The wealth tax would extract 2 of the 2.4 percent return. Combined with income taxes levied at a 40 percent rate, the wealth tax could make the effective tax rate on capital income well over 100 percent. And then at the end of life would come the estate tax. While we are not aware of formal estimates of the loss in economic efficiency from wealth taxes, we suspect that if levied without concomitant reductions in income tax rates or estate tax rates, the ratio of burden on the economy to revenue raised would be far higher than with the base-broadening measures we advocate. .... The Organization for Economic Cooperation and Development recently assessed wealth taxation and concluded that “from both an efficiency and equity perspective, there are limited arguments for having a net wealth tax.” Of the three countries with a wealth tax, two — Norway and Spain — raise an average of 0.305 percent of GDP. These taxes generate less than one-third of what the wealth tax estimates despite having a much broader base: While precise data are hard to come by, we suspect that less than 10 percent of this revenue — or 0.03 percent of GDP — comes from those in the top 0.1 percent of the wealth distribution. Pro wealth tax view However, Saez and Zucman the main proponents of wealth tax  would argue that the worries mentioned above are overstated (see here). According to Saez and Zucman the main reason why the European wealth taxes failed is that Europe tolerates (and arguably even encourages) tax competition. In this article they argue that: The specific form of wealth taxation applied in a number of European countries had three main weaknesses. First, European countries were exposed to tax competition and tax evasion through offshore accounts, in a context where until recently there was no cross-border information sharing. Second, European wealth taxes had low exemption thresholds, creating liquidity problems for some moderately wealthy taxpayers with few liquid assets and limited cash incomes. Third, European wealth taxes, many of which had been designed in the early 20th century, had not been modernized, perhaps reflecting ideological and political opposition to wealth taxation in recent decades. These wealth taxes relied on self-assessments rather than systematic information reporting. These three weaknesses led to reforms that gradually undermined the integrity of the wealth tax: the exemption of some asset classes such as business assets, a preferential treatment of others such as real estate, or a repeal of wealth taxation altogether. A modern wealth tax can overcome these three weaknesses. First, offshore tax evasion can be fought more effectively today than in the past, thanks to recent breakthrough in cross-border information exchange, and wealth taxes could be applied to expatriates (for at least some years), mitigating concerns about tax competition. The United States, moreoever, has a citizenship based tax system, making it much less vulnerable than other countries to mobility threats. Second, a comprehensive wealth tax base with a high exemption threshold and no preferential treatment for any asset classes can dramatically reduce avoidance possibilities. Third, leveraging modern information technology, it is possible for tax authorities to collect data on the marketvalue of most forms of household wealth and use this information to pre-populate wealth tax returns, reducing evasion possibilities to a minimum. We also discuss how missing market values could be obtained by creating markets. In brief, the specific way in which wealth was taxed in a number of European countries is not the only possible way and it is possible to do much better today This being said, even the most extreme serious wealth tax proposals are somewhere in ballpark of $5\%$ - this is much less then the top marginal taxes that are optimal for income redistribution. It can still be argued to be somewhat well-being enhancing but clearly it is debatable of how significant would this effect be. Nonetheless as correctly pointed by Brian in his +1 answer/comment the argument can be made that wealth tax will help to erode the political power of the rich which will ultimately help the poor. There are also counterarguments; in fact Summers argues that wealth tax might actually increase the political power of the rich, but this debate is something that is outside the field of economics so feel free to read their papers and articles I linked here in full to make up your mind about this. I personally am agnostic about this power argument as there is not much research done on this issue. I suspect that even Summers and Saez and Zucman would agree that on both sides of this argument evidence is mixed. Hence when it comes to wealth taxes it is hard to say if they can improve the lives of low income people significantly, but this is not because of a high inflationary effect redistribution via these taxes would have but rather due to the fact that optimal wealth taxes are either zero or low. The only exception of the above is land or similar type of property which can still be counted as wealth and which is actually easy to be taxed even at high rates without generating some of the above issues (although the liquidity and difficulty of valuation argument would remain). However, this being said most of the wealth around the world does not really consist of land so this is more or less side note. PS: The above is just a tip of an iceberg. I did not discuss how factoring in the effect on incentives for education change the result, what is the role of capital, inheritance, consumption taxes in all of this, I also did not discussed in great lengths general equilibrium analysis, or fully factored in all dynamic effects, but all I have written still holds in spirit even if the issues mentioned here could change the actual numbers presented. If your intent is to actually learn about the field then in addition to sources already presented here and sources cited therein I wholeheartedly recommend reading the Mirrlees review which is probably the best literature review on optimal taxation there is and also the book by Inequality Atkinson who before his death was one of the top experts in the area of inequality and redistribution."
Weakly monotone preferences with singleton indifference curves: do any of them admit a utility representation?,"What you're asking for is equivalent to finding an injective function $f:\mathbb R^n\to\mathbb R$ that is monotone in the sense that if $x$ is coordinate-wise at most as big as $y$, then $f(x)\le f(y)$.
As a first step, notice that this is equivalent to finding such a function from $(0,1)^n\to\mathbb (0,1)$.
And this is easy by interleaving the digits of the coordinates of $x$, i.e., if $x=(x^1,x^2,\ldots,x^n)=(0.x^1_1x^1_2\ldots,0.x^2_1x^2_2\ldots,\ldots,0.x^n_1x^n_2\ldots)$, then let $f(x)=0.x^1_1x^2_1\ldots x^n_1x^1_2x^2_2\ldots x^n_2x^1_3x^2_3\ldots x^n_3\ldots$.
This is clearly injective and we prove monotonicity as follows.
If $f(x)>f(y)$, then they differ first at some $x_i^h>y_i^h$ for which $x_j^h=y_j^h$ for all $j<i$.
But then $x^h>y^h$, so $x$ cannot be coordinate-wise at most as big as $y$. ps. Note that $f$ is almost surjective and almost continuous; the only issue is with numbers that have a finite decimal expansion. To see that an injective $f$ cannot be continuous, consider $f^{-1}(\mathbb R_-)$, $f^{-1}(0)$ and $f^{-1}(\mathbb R_+)$ where wlog. $0$ is an inner point of the image of $f$.
By continuity, these are two open sets and a point, so they cannot form a partition of $\mathbb R^n$ for $n\ge 2$. (Here we didn't even use monotonicity.) It is, however, possible to make $f$ into a monotone bijection.
This can be achieved as follows.
First we show the statement for $f:[0,1)^n\to [0,1)$.
Write every number in its finite decimal form, if it has one, i.e., it shouldn't end with $999\ldots$.
The number $f(x)$ will start with the first few decimal digits of $x^1$.
If $x^1_1\ne 9$, then $f(x)$ will start with $x^1_1$, and then we take the first few digits of $x^2$.
If $x^1_1=9$ but $x^1_2\ne 9$, then $f(x)$ will start with $x^1_1x^1_2$, and then we go to $x^2$.
And so on, we always go until the first non-$9$ digit of $x^1$ before we start taking digits from $x^2$.
Then we repeat this for $x^2$, then for $x^3$, etc. (in a circular order).
This finishes the construction for $f:[0,1)^n\to [0,1)$.
To make it into a function $\mathbb R^n\to\mathbb R$, just apply a monotone bijection $g:\mathbb Z^n\to\mathbb Z$ to the integer part of the numbers.
(Such a $g$ is easy to construct by induction and by taking larger and larger cubes around the origin, $0^n$.)
Such a $g$ gives a partition of $\mathbb R^n$ into cubes that are isomorphic to $[0,1)^n$.
We can combine $f$ and $g$ to obtain a final function that will map $x$ to $g(\lfloor x\rfloor)+f(x-\lfloor x\rfloor)$."
What happens to a country with lot of debt?,"What happens to a country with debt depends on several factors.  Debt is not exactly free money. It is 'free' money (apart from interest) only if you can be depended on to pay back that debt. In other words, investors are willing to lend money to a government if they are sure that the government will pay them back. Failing to pay back your debt would require your country to declare bankruptcy (or drop out of the global financial system altogether).  The reason why governments like the U.S. can keep borrowing money is because investors believe that the U.S. can easily make enough money to pay them back in the future. This is based on the debt to GDP ratio. If your debt to GDP ratio is too high (say, 10 to 1) that means it might be quite difficult for the country to pay back all that debt. The current U.S. debt to GDP ratio is between 0.7 and 1.0, which is relatively high (a result of the Great Recession).  Another smaller country might not be able to easily borrow with such a high debt to GDP ratio, but the U.S. also has credibility - it has not defaulted on its debts before, unlike some other sovereign states (Argentina). Combined with the fact that the U.S. has large financial clout and is relatively important, investors feel that the U.S. government is unlikely to default on its debts.  If the U.S. government were to keep on borrowing money, eventually people would get a little concerned with the amount of debt it has taken on.  It's possible to think of sovereign debt in a similar way to personal (your) debt. For example, if you own a credit card, you can use that credit card to get 'free' money and use that money to purchase things. The reason why banks are willing to lend you this money is because you have previously paid them back. If you were to (1) ask for a lot more money than you make (e.x. $100 million) or (2) ask for money after failing to pay the bank back at the end of the month, the bank would reject you. Investors, like the bank, require governments (and individuals like you) to meet some requirements before they are willing to lend you money.  "
Why is UK infrastructure so expensive?,"The following information comes from this report, which was published by the British Government.  I'm not going to summarise the whole thing, but go over the major points. However if you are interested in the topic I suggest you read the whole thing, or even take a look at the more technical version which can be found here.  The report breaks down the higher costs into three issues: Policy and Systemic Issues: Funder and Client Issues: Supply chain delivery issues tl;dr  ""There is no single overriding factor driving higher costs. However,
  the investigation has identified that higher costs are mainly
  generated in the early project formulation and pre-construction phase"""
Take-it-or-leave-it PBE,"After posting a bad solution yesterday I believe I got a better one: The strategy of the buyer consists of two functions, $(f_1(v,p_1),f_2(v,p_1,p_2))$ where both functions map to $\left\{A,R\right\}$ (where $A$ stands for Accept, $R$ for Reject).
The strategy of the seller is $(p_1,p_2(f_1(v,p_1)))$. You get the solution via backward induction. In PBE $f_2(v,p_1,p_2)$ maps to $A$ if and only if $v \geq p_2$. (There is inconsequential leeway at equality.) In PBE the seller believes that there is a set $H$ of types for which the buyer refused her offer $p_1$. Then
$$
p_2^* = \arg\max_{p_2} p_2 \cdot Prob(f_2(v,p_1,p_2) = A | f_1(v,p_1) = R).
$$
The buyer will accept offer $p_1$ if and only if
$$
v - p_1 \geq \delta \cdot (v - p_2). 
$$
From this you get
$$
v \cdot (1 - \delta) \geq p_1 - \delta \cdot p_2. 
$$
The left hand side of this equation is increasing in $v$, so types with high valuation will Accept. This means that in PBE the set $H$ is such that 
$$
H = [0, \bar{v}). 
$$
From this we get the optimal $p_2$ given $\bar{v}$:
$$
p_2^* = \arg\max_{p_2} p_2 \cdot Prob(v \geq p_2 | v \in [0, \bar{v})) = \frac{\bar{v}}{2}.
$$
In PBE $\bar{v}$ is a function of $p_1$: 
$$
\bar{v} \cdot (1 - \delta) = p_1 - \delta \cdot \frac{\bar{v}}{2}, 
$$
so
$$
\bar{v} = \frac{p_1}{1 - \frac{\delta}{2}}.
$$
We have determined all the PBE strategies but $p_1$. 
The expected payoff of the seller is
$$
p_1 \cdot \left( 1 - \frac{p_1 - \delta \cdot p_2(\bar{v}(p_1))}{1 - \delta} \right) + \frac{1}{2} \cdot p_2(\bar{v}(p_1)) \cdot \left( \frac{p_1 - \delta \cdot p_2(\bar{v}(p_1))}{1 - \delta} - p_2(\bar{v}(p_1)) \right),
$$
where
$$
p_2(\bar{v}(p_1)) = \frac{\bar{v}(p_1)}{2} = \frac{\frac{p_1}{1 - \frac{\delta}{2}}}{2} = \frac{p_1}{2 - \delta}.
$$
Substituting this we get
$$
p_1 \cdot \left( 1 - \frac{p_1 - \delta \cdot \frac{p_1}{2 - \delta}}{1 - \delta} \right) + \frac{1}{2} \cdot \frac{p_1}{2 - \delta} \cdot \left( \frac{p_1 - \delta \cdot \frac{p_1}{2 - \delta}}{1 - \delta} - \frac{p_1}{2 - \delta} \right),
$$ You have to maximize this w.r.t. $p_1$. With $\delta = 0.5$ I got
$$
p_1^* = \frac{9}{20}, \hskip 20pt \bar{v} = \frac{3}{5}, \hskip 20pt p_2^* = \frac{3}{10}.
$$"
Examples of Factors in the ICAPM,"People have chosen different ways to pick factors. Chen, Roll and Ross are a classic example of attempts to find reasonable ICAPM factors. Fama-French factors are often explained as correlated with underlying ICAPM factors. Other researchers have chosen to look for factors without assuming outwardly observable exposures by analyzing returns using factor analysis or principal components. I don't know if there are ""commonly accepted"" explanations for the Fama French factors. This is not to say that there aren't explanations, there are a lot of them. Just no one seems to agree on which ones are ""best"". Candidates include both consumption based explanations AND production based models, each of which relates Fama-French factors to underlying economic variables. Cochrane has a nice summary of the performance of both in pricing Fama-French portfolios here. Fama and French are now talking about a five-factor asset pricing model which also includes profitability and investment, similar to Novy-Marx's paper and work by Chen, Novy-Marx and Zhang which more directly relates to production asset pricing. Momentum is also a consistent candidate, although it's a real pain to put any kind of explanation to. There are tons of others. This paper lists many of them, as well as calling the significance of some of them into question."
Competitive equilibrium in Leontief economies,"Strict convexity of preferences is not needed in existence results for competitive equilibria. Leontief preferences are quite well-behaved. They are continuous, convex, and strongly monotonic.   If all endowments are strictly positive, the existence of a competitive equilibrium in an exchange economy (or a production economy satisfying standard conditions) exists by the first result of the original Arrow-Debreu paper. Arrow-Debreu actually do not just require convexity, they make, as pointed out by denesp in a comment, the convexity assumption (III.c) on utility functions that $u(x)>u(x')$ and $0<t<1$ implies $u(tx+(1-t)x')>u(x')$. Plain convexity suffices for existence, but Leontief preferences do also satisfy condition (III.c).:  Assume $\min\{\alpha_i x_i\}>\min\{\alpha_i x_i'\}$. Then
$$\min\big\{\alpha_i (tx_i+(1-t)x_i')\big\}>\min\big\{\alpha_i tx_i\big\}+\min\big\{\alpha_i(1-t) x_i'\big\}$$ $$=t\min\{\alpha_i x_i\}+(1-t)\min\{\alpha_i x_i'\}>\min\{\alpha_i x_i'\}.$$ "
What are some different methods for pricing electricity?,"In most of Europa the physical price for electricity is determined in day-ahead spot auctions. Essentially it is a free market, where anyone can buy and sell power, given you that you can fulfill the regulation set forth by the different national power exchanges and system operators.  This means that every day before 10.00 - 12.00 AM all producers and consumers must have told how much power they wish to sell and buy. From 12.00 and to around 12.42 a number of welfare optimization algorithms are run. After this, the different power area prices are published, along with the supply and demand curves for almost all countries.  In essence it is a where supply meets demand. This is complicated a lot by different bid-types, power connections across borders, physical constraints on cables and so on, but you can download the newest curves from Germany and France each day from epex.com if you want.  Because there is a lot of competition, you will mostly see power producers selling their power at their marginal costs. Because of subsidies this means that renewable energy sells as low as -300 euro pr. MWh - yes, they actually in some cases pay people to take their product. Nuclear and lignite power is around 10-18 EUR, the comes cheap coal and hydro (depending on season), the more expensive coal, gas and finally oil. From the price you can more or less tell which kind of production is running in any given hour. "
Piketty's Return on Capital,"Although I'm not sure that Piketty ever directly discusses the exact definition of $r$, he does make it clear indirectly. On page 52 of the hardcover English-language edition of his book, Piketty declares his ""first fundamental law of capitalism"":  Piketty obtains the share of income $\alpha$ from capital in national income from the income side of the national accounts, dividing ""capital income"" by national income. The capital share $\alpha$ is just the inverse of the aggregate labor share of income, on which there is an extensive literature. (Of course, there are several methodological choices that must be made when defining these concepts, and some of Piketty's choices differ from choices elsewhere.) When finding $\beta$, Piketty appears to take the aggregate value of capital from respective countries' national accounts as well (though alternative sources are needed for older values). There is lots of information on the relevant choices in the technical appendix of Piketty and Zucman (2014), which is the basis of most discussion of $\alpha$ and $\beta$ in the book. Since Piketty makes clear that $r=\alpha/\beta$, his definition of $r$ for a given year is essentially
$$r=\frac{\alpha}{\beta}=\frac{\text{aggregate capital income taken from national accounts}}{\text{aggregate value of capital taken from national accounts}}$$
Note that this actually does not (directly) involve tax data. Tax data is the basis of much of Piketty's other work, but not this."
Why did the Swiss National Bank abandon its currency peg so suddenly?,"Just to synthesize the other two answers together a bit and ""join-the-dots"". Setting the Scene Since 2011 SNB had decided that it wanted to prevent the CHF from strengthening past 1.2 CHF per EUR.  In order to do this it had to do either or both of: Keeping rates low at the short end is easy enough - that's the base rate.  But that was scuppered by the low EUR short rates.  Keeping rates low at the long end is trickier, but can be done by not issuing too many bonds. Selling CHF is fine, but that means buying other currencies.  By the end of December the SNB had bought 85% of GDP worth of foreign currencies. At this point the SNB decided that it did not want to keep expanding the balance sheet.  Why it decided this is a different question (as the question points out).  The question is how to end a cap once you hold a huge Euro position. The Sudden Act Once the SNB had decided it could not defend the cap, it had three ways it could act. The third case would have almost certainly bankrupted the SNB.  If they had, as you suggest, let everyone know in advance what was going to happen, then everyone in the world would just arbitrage the situation into the ground.  Everyone would just buy CHF from the SNB at the cheap rate while they support the cap, and sell as soon as the cap is dropped. The first case is a little dishonest, and would almost certainly get found out (the worst kind of dishonesty!).  As soon as they stopped supporting the cap, the FX rate would drift slowly down from 1.2.  Once people realised that this was not ""supposed to happen"" and therefore the SNB must have stopped defending it, then it would probably snap anyway.  There are two dubious plus sides to this approach I see though As you can see this would be a rather dubious strategy, but I have often wondered if Thomas Jordan (chairman of SNB) could simply address the world with underpants on his head and his willy hanging out as a way to depress the CHF. The strategy they opted for was the ""ripping off a bandaid"" style.  The sudden shock is estimated to have cost the SNB about 13% of GDP due to all those foreign holdings.  Since the SNB made a massive profit in 2014 (see: http://www.bloomberg.com/news/2015-01-09/snb-sees-2014-profit-of-38-billion-francs-resumes-dividend.html) it probably saw this as an opportunity to get out before things got any worse.  Hold out longer and have 100+% GDP in foreign currency, and the bank might not have survived the jolt itself."
Bartik Instrument Intuition,"I think the ""first stage"" would be $L_{it}$ on $\tilde{L_{it}}$. In the Peri paper above, the Bartik instrument is actually just included directly as $\tilde{L_{it}}$ as a control variable because it is an exogenous regressor in that form. If you are running labor supply elasticity regressions (and thus want to see the effect of $L_{it}$ itself on labor supply), if you can argue that the Bartik instrument is in fact exogenous, you can use it as an instrument for $L_{it}$. But, putting it directly in, as you suggested, would amount to something very similar (i.e., the Reduced Form rather than the Structural Eq.). "
Which economic research articles or books use anthropological methods?,"In economics, people usually use mathematics as an intermediary to understand some economic mechanisms that would be difficult to understand without maths. 
Economists usually focus more on mechanisms in an analytic way than some historical facts in these mechanisms. That's a little bit what it is said ""homo economicus"" Youtube Lecture Link This link could be interesting for you but I don't know really (and don't think) some mainstream orthodox economists interested in the link between anthropology and economics. "
For what demand function is a monopoly most harmful?,"An arbitrarily large ratio should occur with demand curve  $P=\begin{cases}
\frac{1}{Q} & \text{if } Q>1 \\
2-Q & \text{if } Q\leq 1 \\
\end{cases}$. The monopolist prices at $P=1$, but the consumers' surplus if $P=0$ is infinite, because the area under the demand curve contains $\int_1^\infty \frac{1}{Q}dQ=\infty$."
What effect would removing the lowest denomination currency have?,"The practice of rounding to the smallest currency unit is called Swedish rounding. I know of a few papers that discuss likely economic concequences.  Eliminating the Penny from the U. S. Coinage System: An Economic Analysis Lombra (2001)  Time to Eliminate the Penny from the U.S. Coinage System: New Evidence Whaples (2007) Pennies, Pricing, and Rounding: Is All the Relevant Analysis in? Lombra (2007) "
Rationality and Common Belief in Rationality in Brandenburger & Dekel (1987),"The concept that Brandenburger and Dekel (1987) call an ""a posteriori equilibrium"" is roughly the same as what Dekel and Siniscalchi call an ""epistemic type structure for a complete information game"" in which all types are rational and there is common belief in rationality. Therefore, Brandenburger and Dekel's Proposition 2.1, together with the remark that immediately follows the proof of Propoistion 2.1, is roughly the same as Theorem 1 in Dekel and Siniscalchi.  "
Financial Smart Contract on Ethereum: can an Ether Futures contract be priced in Ether?,"Futures and Options are all about exchanging something for something else. These kind of derivatives all involve exchanging different 'kinds' of goods so it doesn't make sense to have both sides denominated in the same currency: When you have X Ethereum which you are willing to give up for Y Ethereum later, what you have is a loan. Loans aren't going to do much in terms of managing volatility, which is essentially, how much is Ethereum worth in terms of other stuff.  So the futures contracts needs to be exchanging Ethereum for that other stuff that happens to be volatile relative to Ethereum or vice versa. (could be dollars, could be Bitcoins). Of course since we can't predict the future there will be a winner and a loser. It's the nature of this sort of thing, however supposedly both parties are gaining a loss of volatility. If you want to manage the downside, you can structure it as an option. An example of this would be, You pay \$1 now to have the right to exchange \$300 for 1 Ethereum in 90 days. (or vice versa). If the Ethereum ends up being worth more than \$301 then you end up winning on that deal, and if Ethereum ends up being worth less than \$301, you don't buy it. This benefits both parties: The person selling the option gets \$1 either way, and if Ethereum drops the purchaser of the option doesnt't lose more than \$1. This type of option is called a 'call option'. If structured the opposite way (you pay \$1 for the right to exchange 1 Ethereum for \$300 in 90 days) then the opposite happens: you 'win' if Ethereum drops below \$299, and you don't exercise the option if Ethereum rises in value, in which case you're simply out \$1. This is called a 'put option'. No matter what, there will be a winner and a loser come the time to exercise the option. This is inevitable. However, the big idea is that the buyer of the option has a certain expectation on the market trend but wants to protect themselves against the opposite trend (in the case of call, fixed loss and unlimited upside, in the case of a put, guaranteed gain if there is downward movement vs. fixed loss if there is upward movement). The opposite side, the side selling the option, makes money off of the fee to purchase the option and is hoping that the volatility works out to less than the cost of the fee. You are right about fixed futures (where the price and quantity are fixed and there is no option to back out): they will definitely have a winner and a loser. However, both parties get a guaranteed price, and that may be more valuable than any potential gains or losses. For example, say I am a multinational corporation where most of my profits come in USD but have just built a factory in Kenya and have taken out a loan to do so from a local bank. To pay off the loan, I need to guarantee I can get a fixed amount of Kenyan shilling every month, and it may be worth it to set price in USD so I don't have to worry about exchange fluctuations causing the loan to cost more USD than I expect one month, leaving me short; in that case I won't care too much if I lose out on the potential for the USD/Kenyan Shilling rate to move in my favour. Maybe the same thing could apply for someone who has contracts denominated in Ethereum, who knows. On the other hand, any kind of investment based on expected future trends is just a smarter form of gambling, when you find someone to take the other side of the bet of course someone will win and someone will lose. Enforceability of such contracts is a separate measure, it would be the same problem for any futures market and ideally that is what contracts, reputation and the rule of law is for."
How did the Bail-In in Cyprus Work?,"Apparently it's deposit accounts, which I think means both savings and checking accounts.  No, brokerage accounts are definitely not it, as they do not represent a liability for the bank. The issue is that the 'bail-in"" is a reduction in the debt the bank has towards its depositors. When you own 100 of deposits, it's the bank that owes you, but when you owe a brokerage account that owns 100 of stocks, the bank owes you nothing. So, you received a statement from the bank saying you had 200000 euros in your account, then they impose a 40% levy on the uninsured part (100000). So next month you receive a bank statement that says you have now have 160000 in your account... "
Local and Central Wage Bargaining: What Is the Difference?,"If I understand well, the objective here is to illustrate the mathematical difference between centralized and decentralized bargaining (with a clear illustration of the model). I'll to my best to provide an answer. I'll base my modeling on Wallerstein (1990), and I greatly encourage you to read it (the references will be left after the text). Decentralized wage bargaining Wallerstein model of decentralized wage bargaining is basically based on the behavior of investors, and less on the behavior of firms. I appreciate it because it facilitate the link with macroeconomic considerations. So, the fundamental equation of the model is the following: \begin{equation*}
S\ =\ \frac{1}{y}\left( 1-\frac{p/v}{1-m_{t}}\right)
\end{equation*} Where: \begin{gather*}
S\ =\ rate\ of\ investment\\
y\ =\ risk\ aversion\ constant\ ( sensibility\ to\ risk,\ is\ superior\ to\ 1)\\
v\ =\ productivity\ of\ capital\\
p\ =\ discount\ rate\\
m_{t} \ =\ aggregate\ wage\ share\ of\ worker
\end{gather*} This equation tries to summarize the behavior of investors in the economy. According to Wallerstein investors will increase their investments with technological and managerial improvements, and will decrease it when risk or wages increase. There's then an implicit class conflict between financial capital holders and workers.
From this first formula, we can derive the optimal wage obtained from decentralized negotiations. \begin{equation*}
m^{*} \ =\ \frac{( p/v) +y-1}{1+n( y-1)}
\end{equation*} Where \begin{gather*}
m^{*} \ =\ optimal\ wage\ share\ of\ workers\\
n\ =\ number\ of\ unions
\end{gather*} According to this model, (1) wage decrease with improvement in technology and management, (2) wage decrease with the degree of wage decentralisation Competitive centralization Wallerstein will also modelize competitive centralization, where unions regroup in centrals, who in turn are in competition. In this scenario, the optimal solution for each central is: \begin{equation*}
m_{a} \ =\ \frac{M}{1+M}\left(\frac{( p/v) +y-1}{y}\right)
\end{equation*} Where \begin{equation*}
\frac{M}{1+M} \ =\ wage\ proportion\ of\ central\ A
\end{equation*} With a bit of algebraic manipulation, the author will show that this solution is not optimal. Cooperative centralisation This is not covered by Wallerstein, but it's worth exploring. What happen if each union central decide to cooperate?
In that case, there share of wage will be equal in the economy: \begin{equation*}
\frac{Ma}{1+Ma} =\ \frac{Mn}{1+Mn} =\ ....\ 1/n
\end{equation*} In that case, we can easily derive the optimal wage share (for every individual firms/unions), which is equal to: \begin{equation*}
m^{*} \ =\ \frac{( p/v) +y-1}{ny}
\end{equation*} Since
\begin{equation*}
ny\ \leqq \ 1+n( y-1)
\end{equation*} We can assume (if every value other than n is constant across models), that cooperative central bargain is inferior to decentralized negotiation. However, the difference between the two models will be reduced with increased in population (at full employment). Critiques and reflexions Wallerstein model is simple to grasp and can be modified with further assumptions. It's nice and usefull, but not enough. Here's some limit that are transcended by contemporary models: (1) There's a direct link between wages and labor productivity. Usually, when you increase wage, you increase productivity. This is lacking from the model. (see Kim et al. 2020) (2) Theres other model of wage negotiation. For exemple, you could have a model where a third institutions (ex: the State) mediate and influence negotiation and tries to balance powers. (3) This model assume that the capitalist firm - a model centered around the division of labor and capital - is the only thing in existence. Our economy is way more diversed. How can we modelise wage negociation in self-management? (the result may be way better, as shown by Vanek 1975) (4) Finally, there's an omission between wages, investments, and savings/consumer's wealth. References Hyuncheol Bryant Kim, Seonghoon Kim, Thomas T. Kim; The Role of Career and Wage Incentives in Labor Productivity: Evidence from a Two-Stage Field Experiment in Malawi. The Review of Economics and Statistics 2020; 102 (5): 839–851. doi: https://doi.org/10.1162/rest_a_00854 Wallerstein, Michael. “Centralized Bargaining and Wage Restraint.” American Journal of Political Science 34, no. 4 (1990): 982–1004. https://doi.org/10.2307/2111468. Vanek, Jaroslav. 1975. Self-Management: Economic Liberation of Man. New York : Penguin Book"
Why is it possible to calibrate your subjective probabilities?,"Empirically yes, it seems confidence has a positive correlation with correctness. One reference is Bordalo et al. 2018 Beliefs about gender, see Fig.1 for example. The actual probability of a correct answer is different from own or other people's estimate of the probability of a correct answer, but if the estimated probability is higher, then the actual probability is higher.  As to the axioms or theoretical assumptions - you can just assume that people's estimates of their correctness are increasing in the correctness. Or assume Bayesian updating about own correctness, in which case past correct guesses would increase the guesser's confidence in own correctness. The number of past correct guesses in turn is positively correlated with the guesser's actual correctness (by the Law of large numbers)."
eBay economics - total revenue impact from expanding ecosystem by 1 seller,"eBay seems to think that additional sellers increase its profit (not just revenue), because it advertises the option to sell on its website and tries to recruit more sellers. That's the empirical answer.  Theoretically, eBay could refuse to list the additional seller's product and not charge the seller any fees, so by revealed preference, eBay cannot be worse off with an additional seller. If eBay was forced to list every seller who comes along, then the revealed preference reasoning would fail, so then eBay may or may not be better off with an extra seller."
Matching Theory: Search Time,"The paper is a couple of years old by now, but I believe the first people to look seriously at the issues you're thinking of are Akbarpour, et al.  It might be useful to look at that paper and the handful of others that cite it. The paper was originally called 'Dynamic Matching Market Design', so it may have more citations under that name."
Are there any datasets for protectionism?,"Looked a bit further, I think I possibly found what you're looking for: The Overall Trade Restrictiveness Index (OTRI) summarizes the trade policy stance of a country by calculating the uniform tariff that will keep its overall imports at the current level when the country in fact has different tariffs for different goods. The data:
http://siteresources.worldbank.org/INTRES/Resources/469232-1107449512766/OTRI2009.xlsx Background: http://econ.worldbank.org/WBSITE/EXTERNAL/EXTDEC/EXTRESEARCH/0,,contentMDK:22574446~pagePK:64214825~piPK:64214943~theSitePK:469382,00.html hope this helps,
best regards"
Are low interest rates dangerous?,"If by dangerous it is meant they add possible downwards volatility of assets, that could easily be argued for as: $ PV = D / (1+r)^t$ Where D denotes any future income. This is especially the case if the reason for the low rates is very easy monetary policy, as such policy cannot be maintained and thus the rate is not equilibrium rate and must rise, causing a crash in asset values when that happens. Central banks determinate the interest rates in the short run, their policies are targeted to lower rates, thus a low rate. The other reason is that of supply and demand. There is not much demand for credit (no profitable ideas, no more room for consumer credit) and on the other hand the supply of credit is significant due to central bank action. High supply and low demand equals a low rate."
"In the eyes of a normal citizen, what difference does it make if my country has debt or no debt?","1)  Debt matters because it smoothes (or unsmoothes) taxes over time, which matters because the deadweight loss from taxation is (roughly) proportional not to the tax rate, but to the square of the tax rate. 2)  Because family sizes are not homogeneous, government debt (which transfers the tax burden to future generations) can redistribute the tax burden across families.  (Think, for example, about a taxpayer with no kids, or a family that has just immigrated.) 3)  Government debt makes it easier to borrow --- it's better to pay 3% to government bondholders than 18% to your credit card company.  This is possible because the penalty for not paying your taxes is considerably more severe than the penalty for not paying your credit card bill.  (In effect, government debt resurrects the institution of the debtor's prison.) 4)  There are of course issues with misperceptions, where debt can make people feel either richer or poorer than they really are, depending on whether they underestimate or overestimate their future tax burdens, and so can affect consumption paths in either direction."
What effect does changing the retirement age have on employment?,"""(...) increasing the retirement age (...) possibly takes jobs from
  young people and stifles their career."" Since this is often surfaces as ""the conventional wisdom"" (not necessarily in economics circles), let's present a simple argument, that shows that there are bound to be distinct effects for people already in the labor force, compared to the people that have not yet started their working period in life.   To bring in the surface the core effect, assume a constant population, and a constant demand for workers (that does not mean necessarily constant output of course, since productivity and efficiency may rise).   Assume that in this society, people start working as they enter their 21st year, and retire when the enter their 61th year (40 years of work). An old person should retire today -but he doesn't: he will stay on for another year (say, because the government raised the mandatory retirement age).   Then, the young person that was about to enter the labor force in order to replace him, will have to stay out of it for one more year. He will now enter the labor force when entering his 22nd year and he will exit when he will enter his 62nd year. But this means that this person will work again for 40 years: Raising the retirement age, does not mean raising the numbers of years worked for the new/future workers. What happens is that their working life shifts as regards its beginning and end on their personal age-time, but not as regards its lifetime duration.   Does this ""stifle their careers""? One could reasonably argue that starting one year later gives you one more year to accumulate human capital through education and training -and so when you enter the labor force, you will enter it better prepared than if the change in retirement age had not happened.   Casually observing western societies, this is what appears to be happening: people tend to acquire more and more education, and so also start working later in life. At the individual (or mass media) level, this may be negatively experienced/pictured as ""high unemployment for young people"" and ""difficulty to get a job that makes the young people accumulating more education in their attempt to find a job"". I presented above an alternative perspective on the matter. But as regards those that were already workers when the change happened, and so have entered on the labor force in the beginning of their 21st year, will now work in total 41 years. If we envision a gradual rise in professional ranks and income, the change in retirement age will mean that they may have to stay one more year ""where they were"", and in that sense, increasing the retirement age may appear to ""stifle their careers"". But there is also Arrow's ""Learning by doing"": human capital increase due to on-the-job experience (a verified phenomenon even in a perfectly static environments). So this one more year will not be a total waste in terms of increases in income earning capacity -and it may pay off more for the younger workers, than for the older ones (since the former will gain it earlier in their working life). So the ""career-stifling"" argument gets a little weaker, even for the class of workers-already.   Obviously this is a very abstract approach to the matter... for example, what happens to societies where population is rising? How many people can a society afford to invest in -create human capital for young non-workers- at any given time?
...but I believe it reveals one aspect of the matter that we tend to overlook."
Are there examples of state banks in history?,"Yes. An important and currently highly relevant example is India. In 1969, the Government of India issued the Banking Companies (Acquisition and Transfer of Undertakings) Ordinance, 1969 which nationalised the 14 largest private-sector banks of India, which contained about 85% of the bank deposits in the country. In 1980, 6 more banks were nationalised. After that, several mergers have taken place among these state-owned banks (see this for reference). See this for a list of public sector banks of different kinds in India. Several reasons were listed for this move: expanding the reach of banking to regions where the private sector would not want to go, removing regional imbalance; prioritising certain sectors deemed crucial for the country's economic growth and the welfare of the people; controlling private monopolies etc. Before the move, the then Prime Minister of India, Mrs. Indira Gandhi had released the paper, ""Stray thoughts on Bank Nationalisation"", expressing her views (see this and this). Financial inclusion of the poor was seen as an important goal, as they were (and to an extent still are) un-banked/ under-banked and have to pay extraordinarily high interest to traditional moneylenders. Abhijeet Banerjee and Esther Duflo provide a nice description of the poor in India borrowing in their book, Poor Economics. In 2015, public banks had 74.28% of the market share in loans in India, which reduced to 59.8% in 2020 (see here). Thus, the public sector banks still largely dominate the banking sector in India. But over time, several issues with public sector banks have come to surface: they face losses in rural India (which is perhaps the reason private banks didn't go to those places, which prompted Mrs. Gandhi to nationalise), large over-dues due to lending to certain sectors and demographics, worrying shares of non-performing assets etc. (see here). Furthermore, there is a lack of competition and exposure to large corporates more than the private banks have. This is not to say that private banks aren't facing problems: large private banks like ICICI, Yes Bank and HDFC have seen problems very recently. In the last few years, the Indian government has expressed its desire to privatise many of the banks, due to the stated problems. The government intends to introduce the Banking Laws (Amendment) Bill, 2021 in the ongoing winter session of the parliament (see here). As I write this answer, 900,000 employees of public-sector banks are on strike nationwide (for 16 and 17 December, 2021), opposing the move to privatise banks (see here and here). Among many other reasons, they fear that privatisation will lead to a loss of banking jobs, as private entities would try to cut costs (see here). The government has not yet introduced the bill, but is expected to do it soon in the ongoing session of the parliament."
Should selling price depend on product quality or on work to produce the product if both not in positive correlation?,"No one is ""right"" here, there is rarely a ""should"" in economics, it is not about morals. When making a purchase, you are usually willing to pay an amount $x$ larger than the price $p$ which is larger than the cost $c$ it took to make the product/provide the service. You are arguing that price should only depend on cost. The store is arguing that if you are willing to pay more, they can charge more. Unless they have competition who undercuts them, they are right. In the theoretical model of perfect competition, where there is a single type of product and firms face very strong competition the price would equal the average cost in the long run. I would argue the model does not apply here, competition is not perfect (if I am wrong, go to the competition). In case of imperfect competition it is normal to charge people whose demand is less elastic w.r.t. price more. What you can do: pay them or find a better offer. If you are convinced that you are right, found a rival printing firm, undercut them and put them out of business."
Can someone explain why sterling-dollar parity is bad?,"There's nothing wrong with \$1=£1. Unless it was \$1.50=£1 last week - then it's a big problem - for the people who have £, at least! The people who have \$ will be glad they can buy things from the UK for less money. Consider that your bank account is measured in dollars, because things you buy overseas are usually priced in dollars. Your bank account now has 30% less dollars than it did last month. Your paycheque has 30% less dollars than it did last month. Are you happy about that? The only reason \$1=£1 is a bad sign, is that normally it's closer to \$1.50=£1. If it's \$1=£1 now, that means about 33% of the UK economy went up in smoke. In contrast, since the ""normal"" level of ¥ is about \$1=¥100, if it was suddenly \$1=¥1 that would be extremely good for people who had ¥, and people who had $ would be annoyed they couldn't buy things from Japan any more. There's nothing actually special about the number 1 - it's just an easy number to fixate on, and drastically different from ""normal"""
Citing non-economics studies in an economics research paper,"As long as the main results/conclusions of your paper don't rely solely on the non-economics literature you cite, you should be okay. In other words, it's perfectly fine to use non-economics literature to motivate or even as part of the support for your thesis, as long as you also include proper economic arguments, i.e. theoretical models or econometric analyses. For example, Bidner and Eswaran (2015) theorized about the origin of the Indian caste system and cited quite a few sociological, anthropological, and biological literature to motivate their model. Likewise, Caplan and Dean (2008), which provides axiomatic foundations for a neuroscientific hypothesis (that dopamine encodes the ""reward prediction error""), is full of references to the psychology and neuroscience literature. The paper even includes an extended review of the non-economic literature and explains how they are relevant for economic decision-making. Similar examples are plentiful, and they end up in high-rank journals because, notwithstanding their non-economics citations, those papers contain substantive economic contributions."
Intuition - Why does elasticity vary along a curve?,"Think of it this way. A yacht costs $\$0.01$. How much do you demand? Then double the price. Now how many would you demand? Then consider something closer to a market price. Something in 6 or 7 figures I guess. If you've got some money, you might buy one at the market price. But then if that price doubles, wouldn't that affect your demand a lot more than the 1 to 2 pennies change? Simply put, as you go up the demand curve, the same percentage change in price is a much bigger absolute movement in price."
Has the world become poorer?,"The short answer is No.  Every single year, except 2009, for the past 55 years of continuously recorded economic history, the world has been getting richer. The -2.1% global recession in 2009 was made up in 2010 with 4.1% growth. I was just working with the World Bank's World Development Indicators, which track global GDP growth, and I double checked. We measure prosperity and growth by the quantities of goods produced. Money is irrelevant. Every year we, humans, produce more and more goods and services and that makes us richer. Not every country grows steadily though. Some countries are stagnant, like Greece, and some grow remarkably rapidly over very long periods of time, like China. Moreover, there is increasing inequality withing many countries (measured by GINI coefficient). So it is very natural for many to feel like everything around them either stands still or degrades. But on average, the world keeps getting richer. If you are in Switzerland, you may have felt the shock from recent currency appreciation. Sharp currency appreciations hammer exporters and industries reliant on tourism, which make up a large portion of Swiss economy."
Why couldn't the Karush-Kuhn-Tucker optimization find the solution?,"As @user32416 pointed out the first order stationarity conditions are not enough. Specifically it seems that you violate Slater's condition, which states that ""the feasible region must have an interior point"". There are no $x,y$ for which 
$$(x+y-2)^2 < 0.$$ If you rephrase the problem to
$$\max (xy)$$
$$x+y-2 = 0$$
$$x,y \geq 0$$
Slater's condition is met (for linear conditions no interior points are necessary) and you can apply Karush-Kuhn-Tucker."
Can someone explain graphically why MRS is invariant under monotonic transformation?,"You're right that it's a bit counterintuitive that the shape of the indifference curves shouldn't change when you transform the utility function. The reason is that you are transforming along an axis that is perpendicular to the plane where the indifference curve lives. Let's imagine we have two goods, x and y, and let's say that the original utility function is given by $U(x,y)=x^\frac{1}{3}y^\frac{2}{3}$. We can create a 3D plot of the utility function where $z = U(x,y)$. Any transformation $f(z)$ will work on the z dimension. In the following example, the transformation $f(\cdot)$ is multiplying the utility function by a positive scalar  If you look down on the $x-y$ plane from above, you will notice that you can plot an indifference curve through a point on that plane and it will look identical before and after the transformation. Note that it will not be the same indifference curve because the utility level will change under the transformation. Going back to the multiplication example, let's look at the indifference curve (red line) that goes through the bundle $(x,y) = (10,10)$ (red dot). Regardless of the multiplier, the indifference curve maintains its shape but the associated utility level increases from 10 to 20.  I have also included other level curves, for utility levels 4, 8, 12, ... 40. Note that as the factor with which we multiply the original utility function gets larger, these level curves get squeezed into the lower left corner. You can see the same thing happening with other monotonic transformations, for example translation (adding a constant) and power transformation.  "
Why hasn't massive derivatives exposures at banks already led to disaster?,"While gross notional exposures are huge, net exposures at the banks are much smaller, on the order of 0.1 percent of gross exposures. Since most financial risk (but perhaps not operational risk) is proportional to net exposures this is a more sensible measure of total derivative risk at the banks. Since net exposures at the 6 banks with the largest derivative exposures circa 2014 were on the order of \$300 billion while their total assets were more than \$10 trillion, we may not have seen a disaster because the actual risk is appropriate to the scale of their broader operations.  Here is an example of how this works. At time 0 a bank does derivative trade with notional of 100 with A and collects the offer (or ask) price. That is, the bank sells the derivative to someone who wants to buy it. At time 1 a bank does derivative trade with B with notional of 100 and collects the bid price. That is, they buy the derivative from someone who wants to sell it. Total (gross) derivative exposure is 200, two derivative contracts each with an exposure of 100. However, net exposure is zero, and the bank keeps the spread as profit for its trouble. There is likely some remaining risk in the form of operations and credit risk, but this is what the spread and initial and variation margin are for. "
Nash equilibrium of a Bertrand game with different marginal costs,"Yes, there is no equilibrium in pure strategies. For any price charged by firm 2 above $c_1$, firm one could only best respond by charging the largest price that is strictly smaller. which is impossible. If both firms charge at most $c_1$, one of these firms must make a loss, which cannot be a best response. So there is no Nash equilibrium in pure strategies. There are, however, equilibria in mixed strategies. Consider an equilibrium in which firm 1 chooses a price of $c_2$, while firm two randomizes uniformly over the interval $[c_2,c_2+\epsilon]$ for some $\epsilon>0$. For $\epsilon<c_2-c_1$, this is a Nash equilibrium and, moreover, it uses no weakly dominated strategies. Firm 1 is making a profit of $c_2-c_1$ and firm 2 a profit of $0$. Clearly, firm 2 has no profitable deviation. To see that firm $1$ has no profitable deviation, recall that there can be no profitable deviation in mixed strategies if there is no profitable deviation in pure strategies (a very general game-theoretic fact), and observe that firm $1$ cannot profit from deviating to a price strictly below $c_2$ or weakly above $c_2+\epsilon$. So take any $\delta$ satisfying $0<\delta<\epsilon$ and assume that firm 1 charges $c_2+\delta$ (note that $\delta=0$ would be no deviation.) The probability that firm 2 charges a lower price is $\delta/\epsilon$, so the expected profit of firm 1 is
$$(1-\delta/\epsilon)(c_2-c_1+\delta)+\delta/\epsilon~ 0.$$
Write $K$ for $c_2-c_1$. We have a profitable deviation if
$$(1-\delta/\epsilon)(K+\delta)>K,$$
which we can rearrange to get
$$\delta(1-K/\epsilon-\delta/\epsilon)>0,$$
which is equivalent to
$$K/\epsilon+\delta/\epsilon<1.$$ If $\epsilon< c_2-c_1=K$, this can never be the case, so we obtain a Nash equilibrium if $\epsilon>c_2-c_1$.  One can actually use the same construction to construct equilibria in which firm 1 charges less than $c_2$, but in the resulting equilibrium, firm 2 must play a weakly dominated strategy.  The construction in this answer is taken from the following short paper: Blume, Andreas. ""Bertrand without fudge."" Economics Letters 78.2
  (2003): 167-168."
On the relationship between income distribution and GDP,"You assume that higher spending causes higher GDP. This is not necessarily true.  Saving income will increase GDP through investments (unless you're in a Keynesian trap). Think about the most standard growth model, where the future (and steady state) GDP increases strictly in the savings rate. Hence, whatever increases the savings rate (in your toy model, higher inequality), potentially increases future GDP. This was an argument brought forward in Barro (2000). Version with linear savings rate are in Bertola et al (2006)  Of course, this is the neoclassical answer. There are many reasons why high inequality decreases growth/GDP which work through political economy models or similar. I'll list some of them despite you not asking explicitly for them, perhaps it's of use for future visitors:"
Why don't Burgers cost 5 cents?,"This question really forces one to think about the role that quantity plays in the competitive equilibrium. The two main points that, I think, explain the way this works are: I think the thing that is perhaps causing confusion here is that, recalling that it is a true statement that ""P = MC"" in competitive equilibrium is not sufficient enough to understand the way in which markets function. It is imperative to recall why this is true: because so long as burger sellers maximize profit and burger eaters maximize utility, then quantity will adjust to make it true.  In other words, ""P = MC"" is not a transcendental tautology that simply must be true under all conceivable circumstances; it is the end result of the rational actions of buyers and sellers interacting within the framework of a market mechanism. The original question only appears to be a puzzle if you attempt to abstract away from quantity, and allow yourself to imagine that it's not important how those burgers came to be sitting under the heat lamp in the first place. A fully proper answer to this question would require being explicit about the objective functions of both the suppliers and consumers in this market, but I think that the following shorthand might suffice to illustrate the point: In the original question, there are really two distinct notions of ""marginal cost."" The first is that of the marginal cost to produce the burgers. The second is the somewhat different concept of the marginal cost of delivering the completed burgers to the customer (ie, taking them out from under the heat lamp and handing them to the customer). Being sloppy in our use of language, and unintentionally blurring the line between these two distinct costs is, I think, another way to describe the ultimate source of confusion in this example. Let's just be clear, using clear notation. Call ""MC1"" the marginal cost of producing each burger. Let's say for the purposes of illustration that each burger costs $2 to make. Call ""MC2"" the marginal cost of handing a completed burger to the customer. As in the example, let's assume that this is equal to 5 cents per burger. Hopefully it does not require too much convincing to establish that, in competitive equilibrium, burger sellers will end up collectively supplying exactly the amount of burgers, Q, for which it is true that the prevailing price of a hamburger is exactly equal to MC1.  It's also true that, in this equilibrium, each burger seller can sell all the burgers that they have chosen to produce at a price of P = MC! = $2/burger, since the market clears. Now, at this point, each burger seller has already chosen a quantity of burgers to produce. So even though it's true that, once the burgers have been made, their production cost is a sunk cost, and from that point, the marginal cost of delivering the completed burgers to a customer is only equal to MC2 = $0.05, it will still be the case that no seller has any incentive to charge any less than P = MC1. Again, this is true because, in the competitive equilibrium characterized by P = MC1 and quantity Q, the market clears. This means that each and every seller of burgers can sell 100% of their stock of completed burgers at a price of MC1 ($2/burger). No seller has anything to gain by offering an even slightly lower price to the market, let alone offering a price as low as MC2. EDIT: To expound on the above a little... Perhaps it's helpful to reinforce the role of the (endogenous) equilibrium quantity Q by looking at a graph. It is certainly true that, for the quantity of burgers that the restaurant has chosen to produce (aka, for the number of burgers that are already sitting under the heat lamp), the marginal cost of delivering those already-made burgers to the customer is MC2 = 5 cents/burger.  But the paragraph above does not fully characterize the full marginal cost function, whose domain extends beyond the equilibrium quantity ("" Q* "" below). For any burgers beyond Q*, in order to deliver an additional burger to a customer, an additional burger must be produced first. So the marginal cost of any burgers beyond Q* is NOT 5 cents per burger, its $2/burger (strictly speaking, you would have to allow that it costs USD 1.95 to cook the burger and then 5 cents to hand it to the customer). Recognizing this discontinuity in marginal cost, we can see that the actual marginal cost function looks something like this:  And furthermore, the location of that discontinuity is endogenous as well, since it will always coincide with the quantity chosen by a rational seller (ie, the quantity where the marginal cost of production crosses the demand curve). So even if you wish to take the position that the cost of producing the first Q* burgers is sunk, and should be ignored, it is still impossible to separate the marginal cost of production from the strategic analysis of the problem. And, of course, to finalize the characterization of the competitive equilibrium, we need to include the demand curve. As you can see, this situation reflects the strategic incentives of the burger seller, where the quantity chosen by the seller is exactly the (only possible) quantity for which P = MC and quantity demanded equals quantity supplied (ie, the market clears).  As described above, the competitive equilibrium is characterized by the intersection of the demand and MC curves, at a quantity Q*, and a price of MC1 = $2.00/burger. As above, the seller sells all Q* of their burgers at this price, and so has absolutely zero incentive to charge a lower price of MC2 = 5 cents/burger."
Autonomous or non-autonomous optimal control system?,"The OP's answer is correct in its conclusion, but he applies a strange argument at the end to arrive there.   Applying brute-force differentiation, the present value Hamiltonian is $$\mathcal{H}=e^{-\triangle} U\left( c\right)
+\lambda _{1}^{}\left[ f(k)-c\right] +\lambda _{2}\left[ \rho
+h(k)\right] $$ and so  $$\frac {d\mathcal{H}}{dt} = -\dot \triangle e^{-\triangle}U\left( c\right)+
e^{-\triangle} U'(c)\dot c + \dot \lambda_1\left[ f(k)-c\right]  + \lambda_1\left[ f'(k)\dot k-\dot c\right] + \dot \lambda _{2}\left[ \rho
+h(k)\right]+\lambda _{2}h'(k) \dot k$$ Re-arranging, and using $\rho +h(k) = \dot \triangle$ we get $$\frac {d\mathcal{H}}{dt} = -\dot \triangle e^{-\triangle}U\left( c\right)+ [e^{-\triangle}U'(c)-\lambda_1]\dot c   + [\lambda_1f'(k)+\lambda _{2}h'(k)]\dot k + \dot \lambda _{2}\dot \triangle + \dot \lambda_1\left[ f(k)-c\right] $$ Along the optimal path, we have $e^{-\triangle}U'(c)=\lambda_1$ so the second term above vanishes. Also optimally we have $\dot{\lambda}_{1}=- \frac{\partial \mathcal{H}}{\partial k}$ and observe that $\frac{\partial \mathcal{H}}{\partial k} = [\lambda_1f'(k)+\lambda _{2}h'(k)]$. Substituting we get $$\frac {d\mathcal{H}}{dt} = -\dot \triangle e^{-\triangle}U\left( c\right)+ \frac{\partial \mathcal{H}}{\partial k}\dot k + \dot \lambda _{2}\dot \triangle  - \frac{\partial \mathcal{H}}{\partial k}\left[ f(k)-c\right] $$  $$=\frac{\partial \mathcal{H}}{\partial k}[\dot k - f(k)+c] + \dot \triangle \cdot\big[\dot \lambda _{2} - e^{-\triangle}U\left( c\right)\big] $$ The first term is zero, from the law of motion of capital. Moreover, another necessary condition for the optimal path, given how the problem has been formulated,  is $\dot{\lambda}_{2}=- \frac{\partial \mathcal{H}}{\partial \triangle}$.  so we arrive at $$\frac {d\mathcal{H}}{dt} =-\dot \triangle \cdot\left[ \frac{\partial \mathcal{H}}{\partial \triangle} + e^{-\triangle}U\left( c\right)\right]$$ Now
$$\frac{\partial \mathcal{H}}{\partial \triangle} = -e^{-\triangle} U\left( c\right)$$
so we obtain  $$\frac {d\mathcal{H}}{dt} =0$$ This directly proves that the problem is autonomous. More generally, irrespective of whether the problem is autonomous or not, we have that along the optimal path $$\frac {d\mathcal{H}}{dt} =\frac {\partial \mathcal{H}}{\partial t}$$ So if it is autonomous, then we get the zero-derivative result."
Book Recommendation for Microeconometrics of Discrete Data,"Personally, for choice analysis, I like (1) is a fairly short, readable classic. (2) is much more introductory, with lots of intuition, though it devotes a lot of space to the nlogit software. (3) has the same features as (2), but for a more focused set of topics. (4) is also pretty introductory and focuses on using Stata. It is perhaps the least ""economisty"" of the four, but still quite good. For count data, I like Cameron and Trivedi's count data book, followed by Winkelmann's. The books you have listed are good, particularly Wooldridge, but they cover much more ground than just discrete data at the expense of depth. "
What are different ways of specifying utility and decision making?,"I'm somewhat surprised that no one has linked to this paper: Backus, Routledge, and Zin (2004) Exotic Preferences for Macroeconomists (this version has some fixed typos, vs the NBER print). Their abstract is concise and extremely descriptive: We provide a user's guide to 'exotic' preferences: nonlinear time aggregators, departures from expected utility, preferences over time with known and unknown probabilities, risk-sensitive and robust control, 'hyperbolic' discounting, and preferences over sets ('temptations'). We apply each to a number of classic problems in macroeconomics and finance, including consumption and saving, portfolio choice, asset pricing, and Pareto optimal allocations. The paper itself does a great job of overviewing many options for ""non-standard"" preferences and utility. They introduce a preference, outline key features, and then apply them in a number of classic settings to give you a feel for what is going on.  If you are interested in using non-traditional preferences, you should certainly read this. I can't give extensive insight further than that -- I currently do nearly all my work with more traditional CRRA preferences. "
What is virtual valuation?,"Suppose you face a single buyer whose willingness to pay, $v$, is distributed according to $F(v)$. If you charge a price $p$, he will buy if and only if $v>p$, leaving you with expected revenue of $$r(p)=\Pr(v>p)p=[1-F(p)]p.$$ Let's maximise revenue by computing an FOC: $$r'(p)=1-F(p)-F'(p)p=0.$$ We can rearrange this as $$\phi(p)\equiv p-\frac{1-F(p)}{F'(p)}=0$$
i.e., the 'virtual valuation' should be zero. If we return to $r'(p)$ and think about what the individal terms mean, we can see where the ""cost"" you ask about comes from: a unit increase in price causes an extra unit of revenue in the fraciton $1-F$ of the time that the buyer is willing to buy, but reduces the liklihood of him buying by $F'$. This is the fundamental trade-off that a seller who doesn't know the buyer's willingness to pay must make. If you know anything about standard monopoly theory then this setup should be quite familiar. Usually, when we look at a profit-maximising monopolist with demand $D(p)$ and zero marginal cost we solve
$$\max_p D(p)p\iff \underbrace{D'(p)p-D(p)}_{\text{marginal revenue}}=0.$$
In an auction-like setting, the 'demand' $1-F(p)$ is simply the probability of the buyer being willing to pay $p$."
What does it mean to make an identification assumption?,"Identifying assumption: assumptions made about the DGP that allows you to draw causal inference. E.g. exogeneity assumption for IV, parallel trends assumption in diff-in-diff. Identifying assumptions (lack of endogeneity in general) can never be
statistically confirmed (a non-reject is good, but it's not confirmation). So assessment of plausibility consists of empirical arguments based on what you know about the DGP."
"What is the difference between risk, uncertainty and ambiguity","Here is a decision-theoretic formalization of your definitions. The usual framework to talk about objective risk is the situation where a decision-maker expresses preferences over objective lotteries. Formally, if $X$ is a prize space, objective lotteries are defined as elements of the space $\Delta(X)$ of probability distributions (usually with finite support) over $X$. For instance, the decision-maker might be asked to form preferences between the lottery that offers her/him an apple with probability 0.3 and an orange with probability 0.7, and the lottery that offers her/him an apple with probability 0.5 and an orange with probability 0.5. The standard result in that area (von Neumann-Morgenstern theorem) delivers a representation that identifies the agent's attitude towards objective risk (her/his utility function), while the probabilities are given as a primitive of the model. The usual framework to talk about ambiguity is the situation where a decision-maker expresses preferences over uncertain acts. Formally, if $X$ is a prize space and $S$ is a state space, acts are mappings $f:S \rightarrow X$ from $S$ to $X$. For instance, the decision-maker might be asked to form preferences between the act that offers her/him an apple if Novak Djokovic wins the 2017 Australian Open and an orange otherwise, and the lottery that offers her/him an apple if Andy Murray wins the 2017 Australian Open and an orange otherwise. The standard result in that area (von Neumann-Morgenstern theorem) delivers a representation that identifies both the agent's probabilistic beliefs regarding the states and her/his attitude towards risk (her/his utility function). There is a third widely used concept, usually called Anscombe-Aumann acts or horse races, which associates both objective lotteries and uncertain acts. Formally, given a prize space $X$, an Anscombe-Aumann act is a mapping $f:S \rightarrow \Delta(X)$ that associates an objective lottery to any state in $S$. Notice that the definitions of objective risk and ambiguity are to some extent subjective. The fact that risk is called ""objective"" relies very much on the assumption that the decision-maker agrees with the underlying probability model. For instance, if you observe the outcome of a coin toss, you might believe that heads happen with an objective probability of 0.5. It is implicitly imposed in the theory that the decision-maker agrees with this statement. Regarding ambiguity, you might yourself believe that the act ""receive an apple if Novak Djokovic wins the 2017 Australian Open"" is very ambiguous because you have no idea on how to compute a subjective probability for this event. That said, another decision-maker might very confidently believe that Djokovic has a 74% chance of winning the tournament, in which case she/he does not perceive this act as ambiguous at all. Ambiguity is a subjective notion, which is given by people's preferences and behavior and not by the choice situation itself."
Why should I get a bond with negative interest instead of having a bank deposit account either zero interest or positive interest,"It makes little sense to me either, but here are some possible reasons for buying a bond with negative interest rates rather than depositing the same amount in a bank:"
Does the Marshallian demand function always include prices and income?,"These are proper Marshallian demand functions, even though Income does not appear in them. This is due to specific form of the utility function (and the candidate solution of all goods being purchased at strictly positive quantities). It emerges that there is no income effect for goods $x_1$ and $x_2$ - optimal uncompensated demand does not depend after all on the level of income, but only on the relative prices.   The reason why Marshallian demand is defined as it is, is to make clear that it does not include any kind of ""income compensation"" as Hicksian demand does. But this does not preclude a case like yours, which again, depends on the form of the utility function.   Thinking economically, what goods can you think of whose demand may realistically not depend on the level of income but only on relative prices? Answering this question would be useful in order to map the mathematical expression of the utility function to real-world economic phenomena. Also, note that the specific utility function points to ""quasi-linear"" frameworks, where the good that enters additively, essentially functions as Income itself."
Deriving the Euler Equation,"The full problem is
$$\max_{\{C_t,K_{t+1}\}_0^{\infty}} \sum\limits_{t=0}^{T}  \beta^{t}U(C_t)$$ $$s.t. \;\;C_t+K_{t+1} \leq f(K_t) , t=0,1,2,...,T-1,\;\;\;\; -K_{T+1} \leq 0$$ So we maximize also with respect to consumption. The lagrangean is $$\mathcal L = \sum\limits_{t=0}^{T} \Big(\beta^{t}\big[U(C_t) + \lambda_t\big(f(K_t) - C_t-K_{t+1}\big)\big]\Big) $$ Note that the discount factor discounts also the constraint.
Then $$\frac{d\mathcal{L}}{dC_t}= \beta^{t}\big(U'(C_t)- \lambda_t\big) = 0 \implies U'(C_t) = \lambda_t$$ and so also $U'(C_{t+1}) = \lambda_{t+1}$ Moreover, $$\frac{d\mathcal{L}}{dK_{t+1}}=-\beta^t\lambda_t+\beta^{t+1}\lambda_{t+1}f'(k_{t+1}) = 0 \implies -\lambda_t+\beta\lambda_{t+1}f'(k_{t+1}) = 0$$ Combining and re-arranging, one gets the Euler equation."
"Why does Slutsky compensation ""overcompensate"" the consumer?","Here's a figure to explain:  Starting from the old price line, where the optimal consumption bundle is point $A$, we increase the price of $y$ to get the new price line. The Slutsky compensation says that we have to give the consumer enough extra income so that he can afford to old bundle ($A$) at the new price. Thus, we shift the new budget constraint out to the dashed line. The reason this ""overcompensates"" is that, taking the dashed line as the budget constraint, the consumer could afford bundle $B$, which gives him higher utility than he started with."
Are state owned enterprises really inefficient?,"Because of the large number of roughly comparably sized private and public firms, the petroleum industry provides a laboratory for exploring differences between private and state owned enterprises in related businesses. Without passing judgement on if it has to be that way,  it appears as though the private firms are vastly more efficient: Efficiency differences between private and state-owned enterprises in the international petroleum industry (1992) Technical (managerial), scale and allocative efficiency differences
  between private and state owned firms in the international petroleum
  industry are estimated. The estimation of Aigner-Chu deterministic
  frontiers, maximum likelihood stochastic frontiers, and maximum
  likelihood Gamma frontiers make this analysis the most complete and
  sophisticated testing of property rights theory available. The
  empirical findings suggest ceteris paribus, that state firms could
  satisfy the demand for their output with something less than half of
  their current resource inputs simply by being converted to private,
  for profit enterprises. Of course, that last claim depends on the costs of privatization and asserting those efficiency gains can actually be realized in the sorts of places where oil firms are state owned.    The Financial and Operating Performance of Newly Privatized Firms: An International Empirical Analysis (1994) This study compares the pre- and postprivatization financial and
  operating performance of 61 companies from 18 countries and 32
  industries that experience full or partial privatization through
  public share offerings during the period 1961 to 1990. Our results
  document strong performance improvements, achieved surprisingly
  without sacrificing employment security. Specifically, after being
  privatized, firms increase real sales, become more profitable,
  increase their capital investment spending, improve their operating
  efficiency, and increase their work forces. Furthermore, these
  companies significantly lower their debt levels and increase dividend
  payout. Finally, we document significant changes in the size and
  composition of corporate boards of directors after privatization. Some evidence from a broader array of industries: Ownership and Performance in Competitive Environments: A Comparison of the Performance
of Private, Mixed, and State-Owned Enterprises (1989) The coefficients for MEs [Mixed Ownership Enterprises] and SOEs [State Owned Enterprises] are negative and statistically
  significant at the .05 level for a one-sided alternative in all
  equations, which indicate that, on average, MEs and SOEs are
  significantly less profitable and less efficient than PCs [Private Corporations] after
  controlling for the factors described above. On average, SOEs have a
  return on equity of almost 12 percent less than PCs ; they have a
  return on assets and a return on sales that are about 2 percent less
  than PCs, and their net incomes are \$66 million less than PCs. MEs
  perform worse: their return on equity is more than 12 percent less
  than PCs, their return on assets and return on sales are about 3.5
  percent less than PCs, and their net income is \$165 million less than
  that of PCs. In terms of sales per asset, both SOEs and MEs fare
  equally poorly relative to PCs; but in terms of sales per employee,
  SOEs are less efficient than MEs. A more recent result: State-Owned and Privately Owned Firms: An Empirical Analysis of Profitability, Leverage, and Labor Intensity (2001) The results of the comparison strongly support the proposition that
  government firms display inferior profitability. ... We test an implication
  of the Boycko et al. (1996) model that government firms will tend to
  use more labor than their private counterparts. Cross-sectional
  comparisons support the model"
How to derive firm's cost function from production function?,"""I am not given wealth $w$ although I suppose I could assume any firm who
  is purchasing has some budget."" No. This is exactly where the fundamental microeconomic theory of the firm differs from the microeconomic Consumer Theory: the firm is not constrained by a budget. The reason is that this fundamental theory deals most and foremost with the ""long-term"" view, or even better, with the ""planning view"". So we assume that the amount necessary to cover expenses, will come from sales, since the firm won't enter production at a loss (remember also, this is a deterministic set-up, there is no uncertainty). Working capital considerations (the fact that usually first you have to actually pay expenses and then to actually collect revenues), does not enter the long-term view, justifiably, it is a short-phenomenon. Also, in the long-term or planning approach, there are no fixed costs, all factors are variable. Now, the ""cost-minimization"" approach to solve the firm's optimization problem, is an alternative behavioral assumption to the profit-maximizing setup, and it is very relevant in many real-world cases: public utilities that exist mainly to satisfy demand, and their motive is not to maximize profits -rather they want to minimize cost for the given level output, as determined by demand, in the context of efficient use of the always scarce resources.   But also, the case of a price-taking firm that is too small compared to its market, is closer to a cost-minimizing behavior rather than profit maximizing, since the firm has not really control over its production (except downward by direct decision).  In both of the above cases, an exogenous variable appears: the level of output itself. So we solve the problem by treating the level of output as a ""constant"" or better, we solve it for any given level of output, and the solution we obtain has the level of output as one of its components. So $$\min_{K,L} C\equiv rK + wL \\
s.t. F(K,L) = \bar Q$$ with the Lagrangean $$\Lambda = rK + wL +\lambda[\bar Q - F(K,L)]$$ The first order conditions are $$r = \lambda F_K,\;\;\; w=\lambda F_L  \tag{1}$$ which gives, at the optimum, $$rK + wL = C = \lambda\big(F_KK + F_L L) \tag{2}$$ Now assume that the production function is homogeneous of some degree $h$ (not necessarily homogeneous of degree one, i.e. exhibiting ""constant returns to scale"", but homogeneous -and yours is, of degree $h=1/2$.). From Euler's theorem for homogeneous functions of degree $h$ we have that $$F_KK + F_L L = hF(K,L) = h\bar Q \tag{3}$$ the last equality holding given the constraint of the initial problem. Inserting $(3)$ into $(2)$ we obtain $$C = \lambda h \bar Q$$ The multiplier $\lambda$ is optimal marginal Cost, denote it $C'(\bar Q)$, so we arrive at $$C = C'(\bar Q)\cdot (h\bar Q) \implies C'(\bar Q) + [(-1/h\bar Q)]\cdot C =0$$ This is a simple homogeneous differential equation with solution $$C = A\cdot \exp\left\{-\int(-1/h\bar Q) {\rm d}\bar Q \right\} = A\cdot \exp\left\{(1/h)\ln \bar Q\right\}$$ $$\implies C^* = A\cdot (\bar Q)^{1/h} \tag{4}$$ for some constant $A >0$. To complete the solution, we need to express the object of interest, $C^*$, in terms of the exogenous entities: $r,w,\bar Q$.
To do that derive the optimal marginal cost (which is equal to the multiplier) $$(4) \implies \lambda^* = (1/h)A(\bar Q)^{1/h-1} \tag{5}$$ Inserting $(5)$ into the first-order conditions we have $$r = (1/h)A(\bar Q)^{1/h-1} F_K,\;\;\; w=(1/h)A(\bar Q)^{1/h-1} F_L  \tag{6}$$ It is time to use the specific functional form of the production function $$F(K,L) = K^{1/2} + L^{1/2} \implies, F_K = \frac 12 K^{-1/2},\;\; F_L = \frac 12 L^{-1/2} \tag{7}$$ Inerting $(7)$ into $(6)$ together with $h=1/2$ we obtain, after manipulation, $$rK = \frac {A^2}{r}(\bar Q)^2,\;\; wL = \frac {A^2}{w}(\bar Q)^2 \tag{8}$$ Sum the two to obtain an alternative expression for the Cost function $$rK+wL = C^* = A(\bar Q)^2\cdot \left[\frac Ar + \frac Aw\right] \tag{9}$$ But inserting $h=1/2$ into $(4)$, we also have that  $$C^* = A(\bar Q)^2 \tag{10}$$
So $$ (9),(10) \implies   A(\bar Q)^2\cdot \left[\frac Ar + \frac Aw\right] = A(\bar Q)^2$$ $$\implies \frac Ar + \frac Aw = 1 \implies A = \frac {wr}{w+r} \tag {11}$$ Inserting $(11)$ into $(4)$ we conclude obtaining $$C^* = \frac {wr}{w+r}\cdot (\bar Q)^2 \tag{12}$$ Three things:
A) Verify that the second-order-conditions hold for all this to indeed lead to the optimal cost-function.   B) Solve the unconstrained profit-maximization problem with the same production function, normalizing the price of output to $p=1$ (i.e. treating the exogenous prices, $w,r$ as expressed in real terms), to verify that it will lead to a cost level that it is consistent with $(12)$.   C) If you are interested in the theory of the firm under a budget constraint, a related paper is
Lee, H., & Chambers, R. G. (1986). Expenditure constraints and profit maximization in US agriculture. American Journal of Agricultural Economics, 68(4), 857-865."
Alternatives to Pigouvian tax,"The most obvious answer is Coasian bargaining. What Coase showed in his famous ""The Problem of Social Cost"" is that if there are no transaction costs and if utility is transferable then it suffices to allocate property rights—i.e. to give one party the right either to engage in the externality-causing activity or to prohibit it. The two parties will then engage in bargaining with the result that the socially efficient level of the activity is undertaken. The idea is that if an activity has private value $v$, but imposes external social cost $c$ on others then if the private individual has the right to participate in the activity then others would collectively be willing to pay up to $c$ to persuade him not to. This offer will be accepted only if $c>v$ so the activity takes place only if it is optimal. if others have the right to prohibit the activity then the private individual would pay up to $v$ for them not to do so. This offer will be accepted only if $v>C$ so again the activity takes place only when it is optimal. This example assumes a negative externality, but the same approach works in the case of a positive externality. For example, if the private benefit is $v$ (which may be negative if the activity is very costly) but there is an external benefit of $u$ then third parties would collectively be willing to pay up to $u$ to encourage the private individual to engage in the activity. Thus, the activity takes place only if $v+u>0$---i.e. exactly when it is efficient. This solution is an important element of carbon trading schemes, which are one of the main ways that countries are attempting to tackle the problem of anthropogenic global warming. The Coasian solution has the attractive feature that it is relatively decentralised (there is no need for a central planner to accurately determine the size of the externality). Although this solution appears to work very well, it has a couple of important drawbacks: The zero transaction cost assumption is strong. This is particularly true when an activity imposes a small externality on a large number of people so that there is potentially the need for a large number of bilateral payments. If the externality falls on a large number of individuals then paying a subsity creates a public good problem: each individual could try to free ride and hope that a large enough subsidy is paid by others. The Coasian solution therefore works best when either or "
What is the mechanism that triggers a stock price change?,"I won’t discuss the fundamental reasons why stock prices change (discussed in another answer), but the mechanics (roughly) work like this. (Real world is more complex, since there are multiple exchanges, and high frequency trading.) An exchange matches orders from buyers and sellers. The sensible way of making an order is to put a limit price on it. So you either make a bid up to a maximum price, or sell at a minimum. Other orders are “market orders,” where you buy/sell at the best offer/bid. In the era of high frequency trading - where the prices move extremely fast - this is surprisingly risky. A market order can be considered to be a bid with a limit of infinity (!) or a sell at 0 (which explains the risk, if orders can jump extremely rapidly). In a market where most orders are market orders, they will account for most of the transactions - limits are set not to trigger a transaction, rather they wait for a market order. One thing that is often not appreciated is that professional traders will continuously monitor their open orders. They will remove them and add them back at new prices in response to news. This means that the prices can jump without any buying or selling: people can adjust prices without there being any transactions. This effect means that it is safest to think about prices as being set based on traders’ views, and not some mechanical supply and demand effect based on buying and selling flow numbers."
Are there any examples of tariffs working?,"I interpret your question more broadly as one about whether protectionism has ever ""worked"". Two economists that think it has are Chang Ha-Joon and Dani Rodrik. You can therefore look up their work. Two arguments they use are: In a 2007 article, Chang gives various examples of protectionism ""working"", including the US in the 19th century: the US shifted to protectionism after the Anglo-American War of 1812. By the 1830s, its industrial tariff rate, at 40-50 per cent, was the highest in the world, and remained so until the Second World War. The other oft-cited examples are East Asian economies like Japan, Korea, and Taiwan in the postwar era. The world is a second-best place and so movements towards seemingly-freer trade may not always be a good thing. Classic example as told by Rodrik (2015): imagine that beef is supplied by the United States to Germany at a price of \$100. Assume that Germany imposes a tariff of 20 percent, raising the consumer price of US beef in the German market to \$120. France, meanwhile, can supply beef of equivalent quality only at a price of \$119. Prior to the preferential agreement between France and Germany, French suppliers, facing the same tariff rate as US producers, were outcompeted. Now consider what happens when Germany eliminates its tariffs on imports from France but keeps in place those on the United States. French-supplied beef suddenly becomes cheaper in Germany (\$119 versus \$120), and imports from the United States collapse. German consumers are better off by \$1, but the German government forfeits \$20 of tariff revenue previously collected on US beef (which could have been handed back to consumers or used to reduce other taxes in Germany). On balance, Germany gets a raw deal. Two footnotes. First we have to decide what the goals of the policies were and what would count as ""working"". Then we have to do the empirical analysis and evaluate quantitatively the degree to which the policies ""worked"". So while Chang and Rodrik might give examples of protectionism ""working"" (19th-century US, Japan, Korea, Taiwan), other economists might disagree that protectionism really ""worked"" in these cases. Indeed, I am not aware that Chang or Rodrik has actually conducted any empirical analyses on this matter. Most (all?) economists, including Chang and Rodrik, would agree with the following statements: In contrast, a significant portion of the general public would disagree."
Understanding the shape of a Marginal Cost Curve,"I'll offer a less algebraic alternative to Alecos's answer. In short, yes and no. Normally the MC and AC curves would look like the following, with MC intersecting AC from below AC's minimum point. Suppose price $P_0$ were below this point. Then the firm would sell at a quantity below $Q_1$. But what does this imply for the firm's profit? On average, the firm would make $P_0$ per unit sold, but the cost per unit must be higher than $P_0$, i.e. at $AC_0$ if the firm is selling at $Q_0$. This means the firm would be making a loss (negative profit) and no profit maximizing firm would try to operate at this point; they'd be better off shutting down and get zero profit instead. Therefore, a firm's supply curve should be the fraction of its MC curve that's above the AC curve, which is always upward sloping. The quantity $Q_1$ where a firm would start producing is sometimes referred to as the minimum efficient scale of production. In your demand-supply diagram, the supply curve starts from the origin. Imagine this is just as having the red-dash line in the following diagram ""approximating"" the part of the MC curve that is below AC --- filling the gap, so to speak. "
Why do falling oil prices take stocks with it?,"First of all, the fall in prices does not only concern oil but most commodities — in particular raw materials such as iron ore, copper, nickel and many other — though it is indeed most noticeable for gas and oil. To see the full extent of this drop in price, check out the IMF Primary Commodity Prices and their monthly reports, from which I extracted the following chart:  What are the causes of lower commodity prices? There is no secret: the demand is falling and the supply increasing. The demand is falling mainly because of the slowing growth in China. As China imports less commodities to fuel its industry, a large chunk of the demand for commodities has disappeared, leading to lower prices. But simultaneously, the supply keeps steady or increasing on most of the commodity markets. On the oil market for example, 2015 has been a great year of surge or expected surge: OPEC countries are committed to keep flooding the market — mainly because the Gulf countries can afford lower prices thanks to low production costs, while their competitor might not sustain low prices very long. Furthermore, the ban on Iranian exports and the ban on US exports are going to be lifted. The market anticipates already this formidable future new supply, which drives prices down. The US are quasi independent as far as Oil is concerned, as the Shale gas industry has been rising in this country. What are the consequences of lower commodity prices? The first consequence is not caused by lower commodity prices, but rather by the root of this price drop: China's growth is slowing. Therefore there is more uncertainty than before about this market. As a consequence, exporters to China begin to worry as well as firms who have factories in China, as slowing growth and reduced imports might be the sign of a stabilization and potential lower inner demand. China being one of the world's biggest market, and one with a still relatively low penetration for technologies, it is indeed worrying. Thus, the stock of a lot of companies have been going down all over the world because of the situation in China, reflected in Oil prices. Another crucial factor comes into action: most firms in the commodity sector have huge debt. Indeed, this sector requires big investments to begin the exploitation. Among others, Glencore's $30bn debt has been discussed a lot, but it is only an example among many others. In particular, there are two areas that are under threat: shale gas and renewable. These two sectors require higher investment than others. For shale gas, because the exploitation is costly and technically complex. For the renewable, because the R&D is costly and the production as a whole is costly as it uses high technologies. Lately, the partial default of Abengoa gave us a taste of what might follow. The debt accumulated by this industry is by no mean sustainable with such low commodity prices."
Is the economy a zero-sum game?,"Yes, it's possible, as long as the money is invested in ways that increase production by 4%. The economy is certainly not a zero sum game -- if it was, then it couldn't grow."
Why does increasing the money supply decrease the interest rate in layman's terms?,"On most fundamental level it is because interest rate is price for money. In the same way as price of milk goes down when supply of milk increases (ceteris paribus) price of money goes down when supply of money increases. For example, consider the following diagram from Blanchard et al. Macroeconomics below. You have supply of money (by central bank) and then you have demand for money by people. Interest rate ensures that demand for money = supply of money. If supply increases (shift to the right) interest rate has to decrease otherwise people would not be willing to get and hold that additional money. "
Why is it good to create jobs?,"In general, it is not: Despite the focus on ‘jobism’, jobs are a cost of production and
  consumption, not a benefit, and should be minimized Mark J. Perry @ AEI  Public discourse tends to be carried out in terms of jobs, as if a great objective was to create jobs. Now that’s not our objective at
  all. There’s no problem about creating jobs. We can create any number
  of jobs in having people dig holes and fill them up again. Do we want
  jobs like that? No. Jobs are a price and we have to work to live.
  Whereas if you listen to the terminology you would think that we live
  to work. Now some of us do. There are workaholics just like there are
  alcoholics and some of us do live to work. But in the main, what we
  want is not jobs, but productive jobs. We want jobs that will be able
  to produce the goods and services that we consume at a minimum
  expenditure of effort. In a way, the appropriate national objective is
  to have the fewest possible jobs. That is to say, the least amount of
  work for the greatest amount of products. Milton Friedman in a 1980 lecture  Under most circumstances, wages paid to people employed on a project
  should not be counted as benefits, although this practice is
  disturbingly widespread. As mentioned above, the opportunity cost of
  labor used on a project is the output that that labor would have
  generated elsewhere, either in paid work or in household production.
  Including wages paid to labor on a project as a benefit implies that
  that labor would have produced nothing otherwise and that the agency
  making the wage payments does not have standing. Zerbe and Bellas (page 117-118): I say in general, because there are some evidence that ""jobs are good for you"", and if increasing jobs diminishes the number of long term unemployed, that might be considered a benefit, even though it diminishes the quantity of leisure and home production they consume. "
Why do currencies cover orders of magnitude?,"No matter where you live in the world, there is sort of a minimum value that your currency needs to practically be able to represent somehow.  If your standard unit of currency is more than that minimum value, you need to subdivide your currency to be able to represent smaller values.  In the U.S., we subdivide our U.S. Dollar into 100 cents, meaning that the smallest value that you can spend is $0.01.  (My parents talk about being able to go to the store when they were kids and buy candy for 1 cent, but that is no longer the case.) If your standard unit of currency is small enough, you don't need to subdivide it any further.  This is the case with Japanese yen; 1 yen is roughly equivalent in value to 1 U.S. cent, and you won't find anything for sale in Japan for a fraction of a yen. I visited the Czech Republic last summer.  One Czech koruna is roughly equal to 4 U.S. cents, and I didn't see anything for sale there for fractions of a koruna. There really isn't anything more to it than that.  If we, in the U.S., decided that we didn't like the decimal point anymore and valued everything in cents, nothing would change, except that what we know today as the one-dollar bill would have the number 100 on it, and a price tag that had $4.95 on it today would look like 495¢ instead. Currencies change in value with respect to each other from time to time.  You can read about some of the events in the past that affected the value of the Japanese yen on the yen Wikipedia article, but none of that history is really relevant if you are concerned about the value today.  If a currency goes up in value, it will just have to be subdivided more so that the smallest values can still be represented.  If on the other hand, the value of the currency goes down (much more common), some of the smallest subdivisions will be eliminated.  Canada has already stopped production of their penny, and there is discussion in the U.S. about doing the same here. When the Euro was created (about 20 years ago), they had to pick some value to begin with, and they chose to make the Euro equal in value to the U.S. dollar at the time.  It has since changed in value with respect to the dollar, and today 1 euro = $1.11 USD."
"What institutions are examples of ""shadow banking""?","While whomever told you about ""shadow banking"" in China is correct that in an international context, the term can often refer to informal banking arrangements (the earliest use of the term); however, these days, it is usually used in the sense first coined by Paul McCulley in a speech he delivered (""Teton Reflections"") at the 2007 Jackson Hole conference. He described them as: [T]he whole alphabet soup of levered up non-bank investment conduits, vehicles, and structures... Unlike regulated real banks, who fund themselves with insured deposits, backstopped by access to the Fed’s discount window, unregulated shadow banks fund themselves with un-insured commercial paper, which may or may not be backstopped by liquidity lines from real banks. Structured investment vehicles, as noted by Kitsune, are certainly one type of shadow bank, but nonbank broker-dealers, certain real estate investment trusts, and particular hedge funds can be viewed as types of shadow bank. The question to ask in determining whether an entity is a ""shadow bank"" is twofold— do they: As a direct answer to your question, Pozsar, Adrian, Ashcraft, and Boesky (2010) say: Examples of shadow banks include finance companies, asset-backed commercial paper (ABCP) conduits, structured investment
vehicles (SIVs), credit hedge funds, money market mutual funds, securities lenders, limited-purpose finance companies (LPFCs), and the government-sponsored enterprises (GSEs). I strongly recommend clicking on that link and taking your time to browse the map on page 2. It's worth noting both that many of these entities have existed for long periods of time without incident, and that due to choices in how banking is regulated, shadow banks are often the result of a desire to both limit the scope of regulation and to limit the risk of failure within the scope of regulation— which has the effect of pushing intermediation activities outside the regulated sphere."
In simple terms: what are the implications of homothetic and nonhomothetic consumer preferences?,"From a mathematical point of view, if the function $f(x,y)$ is homogeneous (of any degree), and $g()$ is a function whose first derivative is everywhere non-zero, then the function $$H(x, y) = g[f(x,y)]$$ is homothetic. In economics, we usually impose something more restrictive, namely that $g' >0$. But this makes a homothetic function a monotonic transformation of a homogeneous function. Now, homogeneous functions are a strict subset of homothetic functions: not all homothetic functions are homogeneous.  Therefore, not all monotonic transformations preserve the homogeneity property of a utility function. The simplest example is Cobb-Douglas utility. It is homogeneous of degree one. In an ordinal utility framework, we are ok with monotonic transformations, so we can consider the natural logarithm of it. Fine, but the natural logarithm will not preserve homogeneity. Nevertheless it will be homothetic.  The fundamental property of a homothetic function is that its expansion path is linear (this is a property also of homogeneous functions, and thankfully it proves to be a property of the more general class of homothetic functions). In consumption theory, this means that, keeping the prices or the price ratio constant, if we vary the income of the consumer, in the $(x,y)$ plane the  tangency point of the income constraint with the highest feasible indifference curve will always reflect a fixed ratio $x/y$. This in turn implies that expenditures for each good grow all at the same rate as income, and so expenditure shares remain constant for the whole income range (always for a given price ratio).
While this may sound restrictive, in fact, it has been shown that homothetic preferences do not impose any special restrictions on aggregate demand (essentially due to the endowment vector being arbitrary and independent of preferences, which messes, or frees, things up)."
Is it possible to eliminate the U.S. national debt?,"Yes, I imagine there's reason to contest that claim. Consider the GDP to debt ratio. While it has risen over time and with the recent recession, it was not too far away from Germany which is considered, I think, the absolute picture of fiscal health. Consider also the critical point that the US government does not have a chance of death like a normal citizen. For example, if you take 30 year loan from JP Morgan, they consider the possibility that you might die. It is critical that they acquire this money before that happens- or they will lose their investment! The government does not have such concerns, and as a result face supernaturally low interest rates. Furthermore, they can pay back debt at a leisurely pace, extend their debt frequently, and if the interest rate is below the rate of inflation (Which it has been!), then they actually earn money by waiting to pay it back. One of the main reasons for the increase in the debt-to-GDP ratio has been not an increase in spending, but a noticeable decline in government income.  The US tax rates are at historic lows. Even worse the US GDP has been smashed by the recession. The debt could be paid back eventually if it were politically feasible to raise taxes while keeping expenditures constant. The banks of the world have bet billions, trillions if you count derivative and second-stage investments, on this feasibility. Finally, despite the fact that it is fiat money, it would be very unwise to act as if the debt didn't exist.  Note the panic over congress's threat to default. The consequences of even contemplating a default included: higher future interest rates, lower GDP, and  typically, a devalued currency. Conversely, if money is printed to cover the debt, the consequence is hyperinflation, leading to widespread drops in the standard of living. In my estimation,  if the United States did go into a spontaneous and total default, or hyperinflated to pay the debt- this would likely be the greatest economic disaster of all time, given the size of the US GDP currently. This is not hyperbole- I do not think there is a serious macroeconomist who would disagree."
Translog Preferences,"The translog function can be used not only in preferences but also in production and cost functions. I am not very familiar with its implications in consumer theory, but from the production point of view, i've seen it widely used. The Translog Function doesn't impose additivity and homogeneity, and hence Constant Elasticity of Substitution. This is interesting because it doesn't require a ""smooth"" substitution among the inputs (in production analysis). I guess that in consumer theory the interpretation would be similar. So basically, the translog function is less restrictive than a cobb-douglas. If you impose some restrictions while calculating the translog function parameters, you get a cobb-douglas function. That is why it is a ""generalization"". In other words, the cobb-douglas is a specific case of the Translog function imposing additivity and homogeneity (i.e. imposing constant elasticity of substitution). I think that the other reply is more complete than mine. But i'm just going to add something i consider useful for you to have a broader understanding. I assume that you're familiar with indifference curves. I refer you to this site (from where i took the graphs), in case you're not. An indifference curve is just a mapping of all the combinations of two (or more) goods that give you the same utility, or ""make you happy at the same level"". First, see this indifference curve: 
Fig 1: source This setting is known as ""complements"". Because as you can see, adding one thousand units of good x (that is moving to the right), without adding good y (that is not move upwards) does not make you happier: you move along the indifference curve. Think of this as the left shoe and the right shoe. It is useless to have a thousand additional left shoes without adding a right shoe because they are perfect complements. Now, look at this one:

Fig 2: source This one is called ""substitutes"". It's the opposite case to the complements. You can think of this as beef and chicken. You can cook using only beef, or you can substitute and cook using only chicken. But you can cook with certain combination too, say 150 grams of beef and 100 grams of chicken beccause they are perfect substitutes (Sorry, i couldn't come up with a better example but this one makes the point). Now, this extreme cases make easier to imagine all the settings that are ""in the middle"". That is, two types of good that are not perfect complements neither perfect substitutes. Think of food and drinks. They can't be perfect substitutes because you can't have lots of food without drinks. The're not perfect complements either because the mix of food and drinks is not fixed. For this setting the cobb-douglas could be a nice approximation as can be seen in the next figure:  Fig 3: source Now, the Cobb-Douglas utility function doesn't solve everything, as it imposes certain constraints by construction. For example, the line that goes from the origin through all the curves (the expansion path) is 45° and straight by construction: it cannot be changed. This means that as you get richer (even infintely rich), your preferences over this goods remains constant. The formal name is homotheticity or homothetic preferences. This is empirically false, as it has been shown that the richer you are, you use a smaller share of your income for food. With Cobb-Douglas preferences, this cannot happen. Translog preferences relaxes this assumption. In the next figure, you have a utility map relaxing the homotheticity assumption:  Fig 4: source Think of this graph as good y being food and good x being entertainment. As you get richer (or farther from the origin), you will destin more of your income to entertainment. Finally, i will talk about the Elasticity of Substitution which is known as $\sigma$ (sigma) which can be imagined as being the curvature of the indifference curve. In the Fig 1, the perfect complements $\sigma = 0$: No curvature. In the perfect substitutes, $\sigma = infinity$: straigt line. In Cobb-Douglas, $\sigma = 1$: a slight curvature. Nonetheless, as you get richer (distant to the origin), this Elasticity of substitution remain constant in the three settings. Even in the non homothetic preferences seen in Fig 4, the elasticity of substitution remains constant. These are the **Constant Elasticity of Substitution (CES)**preferences. But what if you allow the curve to have different shapes as you get richer? Look at Fig 5:  source In this example, the indifference curves get less elastic every time. Hence, they are not CES preferences. The advantage of the Translog preferences is that, since you don't impose neither CES neither homotheticity, you can test this hypothesis with observed data. You can see that the Translog utility function is much less restrictive than the Cobb-Douglas preferences. As a final remark, i shall say that it can be the case that you do not reject the hypothesis of homotheticity, CES and $\sigma = 1$ in a dataset of observed behavior. This would leave you in a Cobb-Douglas preferences setting. So, by using Translog you're not necessarily ruling out Cobb-Douglas."
What are the benefits of a first price sealed bid auction?,"A good article to read about this is by Susan Athey, Jonathan Levin and Enrique Seira. It has the title: ""Comparing Open and Sealed Bid Auctions: Theory and Evidence from Timber Auctions,"" was published in the Quarterly Journal of Economics, vol. 126(1), 2011, 207-257, and is available online at:  http://faculty-gsb.stanford.edu/athey/documents/comparingformats.pdf.  The abstract includes this: ""We study entry and bidding patterns in sealed bid and open auctions. Using data from U.S. Forest Service timber auctions, we document a set of systematic effects: sealed bid auctions attract more small bidders, shift the allocation towards these bidders, and can also generate higher revenue. A private value auction model with endogenous participation can account for these
qualitative effects of auction format."""
Extensions of Nash equilibria to games with infinite strategies,"Yes, there is such a setting. The result is that If each player's strategy space is convex compact and if payoffs are continuous then there exists at least one Nash equilibrium (possibly in mixed strategies). This holds even when the set of possible actions is uncountably infinite. If one additionally assumes that payoffs are quasiconcave then the best-response correspondence will be convex even when we restrict attention to pure strategies so that we are then guaranteed to have at least one equilibrium in pure strategies in such a game. I believe the original reference here is  The treatment in Glicksberg's paper, though, does not  seem very accessible. A good starting reference is more likely to be section 1.3 of Fudenberg & Tirole's book ""Game Theory""."
Is the mainstream against agent based modelling?,"I'm an outsider, so I can't speak of what the boards of prestigious economic journals think of this, but... I suspect it has something to do with the opacity of computer simulations using agent-based models (ABM) compared to math proofs or even the statistical models that are the staple of applied econometrics... although simpler computer simulations like Monte Carlo methods are well accepted.  E.g. Arnold Kling, who has some academic and some Federal Reserve credentials, writes My concern with ABM is getting a result and not really knowing why you got it. Alternatively, if you really understand how the ABM is getting its result, you should be able to show how to get that results in a simple model, perhaps with a numerical example. He doesn't seem to be on the board of any journals, but I doubt his opinion is uncommon. BoE economist A. Turrell has written a (lengthier) paper containing a fairly balanced (IMO) list of pros and cons of ABM. ""Calibration and interpretation"" takes most space there when it comes weaknesses. He also mentions the Lucas critique as relevant, and notes that the most fertile cases for ABM being behavioral-economics oriented, e.g. using bounded rationality, also makes them most difficult to proof against the Lucas critique. At least currently, the staple of macro-economics models is DSGE, which nearly always include a representative-agent: All the different views mainstream macroeconomists have about the state of their field and about possible areas of improvement should not diminish the degree to which they converged methodologically in studying fluctuations. They all analyse such phenomena usually through a dynamic stochastic general equilibrium model with a representative agent, firmly grounded on microeconomic principles. Moreover, several of them agree with Chari (2010: 2) that “any interesting model must be a dynamic stochastic general equilibrium model. From this perspective, there is no other game in town.” Therefore, he continues, “a useful aphorism in macroeconomics is: ‘If you have an interesting and coherent story to tell, you can tell it in a DSGE model. If you cannot, your story is incoherent.’” Generally DSGE models assume a representative agent (in part because they need to side-step Sonnenschein–Mantel–Debreu type results). The theorems of Hugo Sonnenschein, Rolf Mantel and Gerard Debreu in the early 1970s established that the restrictions that generate well-behaved individual demand functions do not constrain aggregate demand functions to exhibit the same properties [...]. The new classicals sidestepped the problem of aggregation either by imagining an economy composed of identical individuals or by assuming that there is one individual who represents the whole economy, so that the solution to the optimization problem of this representative agent gives the aggregate relationships in that economy. There were more steps before the current new synthesis DSGE approach, but I don't want to get into that here. My point with this is that if you have an optimizing representative-agent (RA), you don't need much computer/behavioral simulation... Of course, RA models are criticized (Fagiolo and Roventini summarize RA/DSGE criticism from much more famous economists, Stiglitz etc.) exactly because of this, e.g. their inability to predict (or supposedly even explain) the Great Recession etc. But this criticism seems to not be enough to dislodge DSGE from the current mainstream macro, presently. (There is a bit more subtlety to RA than what I've covered here; RA can include some sources of parametric heterogeneity but assumes a sort of ""structural"" homogeneity, if I understand that correctly.) Fagiolo and Roventini have a detailed comparison of DGSE with ACE/ABM  approaches, and while they find that the two approaches share some criticism, they point out the following difference: The last issue worth mentioning is specific to ACE and it concerns the comparability of
  different agent-based models. DSGE models are all built using a commonly-shared set of behavioral
  rules (e.g., representative agents solving a stochastic dynamic optimization problems)
  and their empirical performance is assessed with common techniques (i.e., VAR models). This
  allows to develop a common protocol about ""how to do macroeconomics with DSGE models""
  and it eases the comparison of the results produced by competing models. Given the relatively
  infancy of the ACE paradigm, the lack of such a widespread agreement among the ACE community
  hinders the dialogue among different ABMs, reducing the comparability of their results,
  and possibly slowing down new developments. In that respect, the development of common
  documentation guidelines (Wolf et al., 2013a), dedicated languages and platforms can surely
  improve the situation, increase the exchanges among ACE scholars, and reduce the entry cost
  to agent-based modeling. In other words, because of its infancy, there's a lack of standardization in ACE/ABM (unlike DSGE), which presently also hinders wider adoption for the former. (As minor point of terminology, they use ACE/ABM interchangeably: ""ACE [agent-based computational economics] models (often called agent-based models,
ABMs)"".)"
Can $u(x) = \sqrt{x_1 x_2} + \sqrt{x_3 x_4}$ be solved by Kuhn–Tucker conditions?,"No, you can't. The Karush-Kuhn-Tucker theorem is for functions defined on $\mathbb{R}^n$ or at least an open subset thereof and requires the function to be maximized to be differentiable on the domain. That the solution has to lie in the nonnegative orthant is not something that can be represented by inequality constraints here, since the utility function is not defined on a larger open domain on which it is differentiable. Indeed, the function $(x_1,x_2)\mapsto \sqrt{x_1x_2}$ is not differentiable at points where $x_1=0$ or $x_2=0$. To see this, note that if the function is differentiable at, for example, the point $(x_1,x_2)=(0,1)$, then there would exists a vector $v=(v_1,v_2)$ such that
$$\lim_{h\to 0}\frac{\|\sqrt{(x_1+h_1)(x_2+h_2)}-\sqrt{x_1x_2}-v_1x_1-v_2x_2 \|}{\|h\|}=0$$
$$\lim_{h\to 0}\frac{\|\sqrt{h_1+h_1h_2}-v_2 \|}{\|h\|}=0.$$
But, for example, taking for $h=(0,1/n)$
$$\lim_{n\to \infty}\frac{\|\sqrt{0+0\cdot 1/n}-v_2 \|}{1/n}=\lim_{n\to\infty }n\|-v_2\|,$$
which is only $0$ if $v_2=0$. So assume $v_2=0$ and take $h=(1/n,1/n)$ instead,
you get
$$\liminf_{n\to \infty}\frac{\|\sqrt{1/n+1/n^2}\|}{\sqrt 2~ 1/n}\geq 1/\sqrt 2.$$
No derivative exists at $(0,1)$. Misuse of the Karush-Kuhn-Tucker theorem is very prevalent, even in otherwise very rigorous textbooks. The appendix of the following paper lists several examples of textbooks that apply the KKT-theorem to problems where it can't: Erik J. Balder, ""Exact and useful optimization methods for
microeconomics"" in: New Insights into the Theory of Giffen Goods
(W. Heijman and P. Mouche, eds.), Lecture Notes in Economics and
Mathematical Systems 655, Springer, 2012, pp. 21-38."
"How is Singapore able to provide first-world public services, given its low income tax rate?","I'm not an expert on this, but as a Singaporean, here are some factors off the top of my head (plus some Googling), explaining why Singapore is different from other first-world countries (in terms of revenue and expenditures).  These COEs (""Vehicle Quota Premiums"") make up 6.0% of government revenue.  Besides this very expensive piece of paper, a car-owner still has to pay all the other usual taxes (GST, road taxes) and insurance, plus a lot of unusual fees (e.g. Electronic Road Pricing - an idea which London borrowed). A lot of these flow to government coffers. Together, these make Singapore easily the most expensive place in the world to own cars. It is true that over 80% of Singapore residents live in government housing (HDB flats). But these are not the sort of heavily-subsidized public housing you have in some Western countries. Indeed, it is a frequent gripe amongst Singaporeans that these are incredibly expensive and there are often conspiracy theories that the government sometimes actively works to push up prices.  Given the lack of transparency and also the fact that Singaporeans are actively encouraged to use their CPF (retirement) savings to pay for these homes, these conspiracy theories are not without merit. Government housing is generally priced at market rates. For example, recently a 1,001 square foot flat went for S\$900,000 (≈ US\$632,000) - this was in the news because it broke a local (Clementi) record, but it doesn't get much cheaper than that. There are some subsidies for first-time Singaporean home-buyers: If both you and your spouse are first-time buyers, then you get a total subsidy of S\$30,000. But this is not a large subsidy. Some exceptional hardship cases (which involves an elaborate application process and onerous requirements) get to rent a small one-room flat for extremely cheap (as little as S\$26 a month), but these cases are so few in number as to be negligible. Unlike in say some US cities, where public transport is always losing money, public transport in Singapore ALWAYS makes very healthy profits. So these are not drains on but rather contributors to the government coffers. If you've only been in US cities where there's often only a few people on a bus, you may find it hard to imagine. But come to any Asian city where public transport is always jam-packed and you can see why public transport is so profitable. This is a forced retirement savings scheme. In Sep 2015, the CPF had S\$293.9b in total members' balances (source). That's about 75% of GDP.  This is another frequent gripe of Singaporeans because these savings are all locked up (with very draconian rules on how you can gradually withdraw small amounts after you're 55).  Moreover, these funds are ""managed"" by the various Singapore sovereign wealth funds (GIC, Temasek). GIC in particular is not well-ranked in terms of transparency (source). So it is unclear as to how and how much exactly these help to contribute to Singapore government coffers.  Healthcare spending is 4.6% of GDP (World Bank).  I have heard some of my friends who work at the Ministry of Health (Singapore) complaining about how inefficient the Singapore health care system is.  But as someone who has lived in both Singapore and the US for a good number of years, I would say (based purely on my own anecdotal experience) that the Singapore healthcare system is extremely efficient, especially if compared to the US. But the US is perhaps the worst-case scenario. I'm not sure how Singapore would compare to other first-world countries."
What is the difference between Stochastic game and Bayesian game ?,"In a Bayesian game, information is incomplete. To cope, players have beliefs about the state of the game. In a sense, each player strategizes as if the game was as he or she believes. So each player operates in his or her own world. And if every player plays a Nash equilibrium in one's own world, that's a Bayesian Nash equilibrium. In a stochastic game, the information about the current state of the game may indeed be public. At a given time step, the next state is determined by the current state, the strategy profile played at that time step, and some stochastic process (like a Markov chain, for example).  We can have a stochastic Bayesian game where information is incomplete and there is a stochastic switching device like a Markov chain."
Constant Elasticity of Substitution: Special Cases,"We know that if $u$ represents $\succeq$ on $X$, then for any strictly increasing function $f: \mathbb{R} \rightarrow \mathbb{R}$, then $v(x) = f(u(x))$ represents $\succeq$ on $X$ ($X$ in this case is $\mathbb{R^n}$) Consider $v(x, \rho) = \ln(u(x, \rho)) - \frac{\ln\left[\sum^n_{i=1}\alpha_i \right]}{\rho}$, which is strictly increasing. $$v(x, \rho) = \frac{\ln\left[\sum^n_{i=1} \alpha_i x^\rho_i \right]}{\rho} - \frac{\ln\left[\sum^n_{i=1}\alpha_i \right]}{\rho} = 
\frac{\ln\left[\sum^n_{i=1} \alpha_i x^\rho_i \right] - \left [\ln\sum^n_{i=1}\alpha_i \right]}{\rho}$$ The limit of this as $\rho \rightarrow 0$ is indeterminate, $\frac{0}{0}$. So we can use L'Hopital's Rule and take the derivative with respect to $\rho$ of the numerator and denominator. $$\lim_{\rho \rightarrow 0} \frac{\ln\left[\sum^n_{i=1} \alpha_i x^\rho_i \right] - \left [\ln\sum^n_{i=1}\alpha_i \right]}{\rho} = 
\lim_{\rho \rightarrow 0} \frac{1}{\sum^n_{i=1} \alpha_i x_i^\rho} \cdot \left(\sum^n_{i=1} \alpha_i x_i^\rho \ln x_i\right)$$ by the Chain Rule. $$= \lim_{\rho \rightarrow 0} \frac{\sum^n_{i=1} \alpha_i x_i^\rho \ln x_i}{\sum^n_{i=1} \alpha_i x_i^\rho} = \frac{\sum^n_{i=1} \alpha_i \ln x_i}{\sum^n_{i=1} \alpha_i} = \frac{1}{\sum^n_{i=1} \alpha_i} \cdot \ln\left(\prod^n_{i=1} x_i^{\alpha_i}\right)$$ Consider $w(x, \rho) = \mathrm{e}^{(\sum^n_{i=1} \alpha_i) \cdot v(x, \rho)}$, which is another monotonic transformation, strictly increasing. So $w$ still represents the same preference as $u$. $$\lim_{\rho \rightarrow 0} w(x, \rho) = \mathrm{e}^{(\sum^n_{i=1} \alpha_i) \cdot \lim_{\rho \rightarrow 0} v(x, \rho)} = \prod^n_{i=1} x_i^{\alpha_i}$$ which is a Cobb-Douglas function.  $\square$ To show the second point, it is sufficient to show that $$\lim_{\rho \rightarrow -\infty} u(x) = \left\{x_k \ \forall j \neq k \mid x_j \geq x_k \right\}$$ $$u(x) = \left[\sum^n_{i=1} \alpha_i x^\rho_i \right]^\frac{1}{\rho} = x_k \left[(\sum^n_{i=1, i \neq k} \alpha_i x^\rho_i) + \alpha_k \right]^\frac{1}{\rho}$$ $(\frac{x_j}{x_k})^\rho \rightarrow 0$ as $\rho \rightarrow -\infty$ if $x_j > x_k$ $(\frac{x_j}{x_k})^\rho \rightarrow 1$ as $\rho \rightarrow -\infty$ if $x_j = x_k$ So  $$\lim_{\rho \rightarrow -\infty} x_k \left[(\sum^n_{i=1, i \neq k} \alpha_i x^\rho_i) + \alpha_k \right]^\frac{1}{\rho} = x_k$$ since $1/\rho \rightarrow 0$ and a constant to the zeroth power is 1. Construct a similar argument for any $k$. Thus $\lim_{\rho \rightarrow -\infty} u(x) = \min \left\{x_1,...,x_n \right\} $ $\square$"
"Do perfect complements have to be normal goods? If so, why?","A good is normal if its demand is increasing in income. So let $p_x$ and $p_y$ be the price of the goods with quantities $x$ and $y$ and let $m$ be income. Suppose $ax>by$. Then $\min\{ax,by\}=by$. By slightly reducing $x$ by and spending the saved money on $y$, one gets a better bundle. For an optimal bundle, this cannot be.   Similarly, it cannot be optimal that $by>ax$. So in the optimal consumption bundle, it must be the case that $ax=by$. It is also not that hard to see that the consumer will spend all her income. So rewrite the condition as 
$$y=\frac{a}{b}x$$ and plug it into the budget equation $$p_x x+p_y y=m$$ to get
$$p_x x+ p_y\frac{a}{b}x=m=x\Big(p_x+p_y\frac{a}{b}\Big).$$
Therefore, we get the demand function given by $$x(p_x,p_y,m)=\frac{m}{p_x+p_y\frac{a}{b}},$$ which is clearly increasing in $m$. Similarly, one shows that the other good is normal too. Pedantic remark: A differentiable function can be increasing at every point without the derivative being strictly positve everywhere. The function given by $x\mapsto x^3$ has derivative $0$ at $0$ but is everywhere increasing."
Departure point for research in economics,"Always use smaller models when you can. Typically, you want to show that Have the smallest model that you need to make your point (that contains A,B). Additional features are irrelevant to make your point and will only distract you and your audience.  There are many models which are famous for being able to model A, B, or similar. If you need A and C, take a model famous for being good at modeling A, and add feature C. If there is nothing clean (any model that has A, also has A2, A3, A4 - which you really don't need), you might be better off writing your own model / simplifying an existing one). For example,  If you combine A,B (and a few other basic elements), you can interestingly show an environment with room for monetary policy, the standard NK-model.  Say you want to look at the impact of sticky prices onto unemployment. You could take A,B and add sticky wages and monopolistic competition on the labor supply side. Or, you could start with the Diamonds-Mortensen-Pissarides (DMP) model and extend it with B.  The choice of model depends particularly on  If you believe that workers really are setting the wage and there is some stickiness in their ability to do so, (i) is the way to go. If you believe it is important that employment comes after some matching-period, and wages are set through bargaining from both employers and employees, extending DMP would be a better way.  Fundamentally, if you believe that you could use DMP, but it has some clutter that you don't need, get rid of that first, and then add your price-stickiness."
"How does a reduction in consumer spending in favour of consumer saving, affect economic activity?","Such a ""planned"" and sought-after re-allocation of given income from consumption to saving, is justified only if the savings in an economy are sub-optimal (or we think so), in the sense of hurting the investment rate, which in turns hurts the (human and physical) capital infrastructure.   Think about the extremes: consume all that you produce, save nothing (as a society). Your capital base will erode, leaving in the end only the other factor of production, labor, an orphan, without the necessary capital to produce, leaving perhaps only land for the society to return to a non-industrial agricultural state.   So it is a matter of intertemporal (re)allocation of resources in order to guarantee that the economy survives, and then that it does a bit better than just surviving. The conundrum: if businesses see consumption declining, why would they rush to make new investments even though now the ""price"" of these investments is lower, due to increased savings and so to increased availability of funds?   I would invoke here ""Say's Law"": Supply finds its own demand. (see the controversies over it).
For me, this law is better interpreted in the context of heterogeneity: no matter what overall consumer spending does, there is always some fields where consumer desire goes unfulfilled. Entrepreneurs try to identify these fields, and invest there (this has a Schumpeterian flavor). The above discussion does not mean that we will not observe phenomena like increased unemployment. After all, investing in new fields requires re-allocation of productive resources, and this is not easily nor quickly done. And there is always the case that the ""savings drive"" may overshoot, and the economy will be led to a lengthy recession, Japan some 20 years ago being the classic modern example (in fact in Japan, the government tried actively to persuade the citizens to raise their consumption, but it failed)."
Economic theory journals for a refinement theorem about utility function representation,"For theory you have in order of prestige... (I know subjective) There are of course other journals that publish theory, like: Feel free to add to the list."
"All else being equal, What is the value of job creation?","You are correct that creating jobs in themselves, generally speaking does not have any economic value (save some exceptional situations - see below). In fact this is not novel observation, and you will find this stated in any 101 economic textbook for example see Mankiw Principles of Economics 8ed pp 33. As written in the textbook (in an excerpt from Economics article): Little in the literature seems more relevant to
contemporary economic debates than what usually is called the broken window fallacy. Whenever a government program is justified not on its merits but by the jobs it will create, remember the broken window: Some teenagers, being the little
beasts that they are, toss a brick through a bakery window. A crowd gathers and laments, “What a shame.” But before you know it, someone suggests a silver lining to the situation: Now the baker will have to spend money to have the window repaired. This will add to the income of the
repairman, who will spend his additional income,
which will add to another seller’s income, and so
on. You know the drill. The chain of spending will
multiply and generate higher income and employment. If the broken window is large enough, it
might produce an economic boom! . . . Most voters fall for the broken window fallacy,
but not economics majors. They will say, “Hey,
wait a minute!” If the baker hadn’t spent his
money on window repair, he would have spent it
on the new suit he was saving to buy. Then the
tailor would have the new income to spend, and
so on. The broken window didn’t create net new
spending; it just diverted spending from somewhere else. The broken window does not create
new activity, just different activity. People see the
activity that takes place. They don’t see the activity that would have taken place.
The broken window fallacy is perpetuated in
many forms. Whenever job creation or retention
is the primary objective I call it the job-counting fallacy. Economics majors understand the
non-intuitive reality that real progress comes
from job destruction. It once took 90 percent of
our population to grow our food. Now it takes 3
percent. Pardon me, Willie, but are we worse off
because of the job losses in agriculture? The
would-have-been farmers are now college professors and computer gurus. . . .
So instead of counting jobs, we should make
every job count The above being said there is some value to creating/preserving jobs in some situations. For example, during recessions some public works program might be part of a mix that is intended to stimulate the economy, or during short but severe unpredictable crises such as the current Covid-19 situation. For example, the Furlough/Kurzarbeit schemes  are generally considered by conventional economists  to be valuable as they can help boost aggregate demand and prevent even more severe recession (Giupponi & Landais; 2020),however such support is exceptional."
The Printing of Money for Paying Debt,"Printing money causes inflation because someone (or some institution) will get the money and will spend it somehow. This increases demand for goods, but the number of goods, the supply was not affected by printing money. Hence demand at the old price level is larger than supply at the old price level. Prices will rise until a new equilibrium is reached. The problem is that you seem to think that the N dollars you give to China would be gone. But it wouldn't be, the people who got it in China can now spend it in the US. They can import things, come as tourists, make investment purchases etc. This increases demand and results in inflation again."
Does a strong US Dollar hurt American Manufacturing?,"Most manufactured products are sold domestically, but even so a strong dollar will (usually) cause a decline in sales and profitability of US manufacturers. The reason for this is that a strong dollar makes foreign products relatively cheaper, not just manufactured products, but all products. Therefore it becomes cheaper for person to buy, say a foreign-made washing machine compared to an American-made washing machine. What news articles like this don't mention is that a weak dollar hurts Americans because it makes everything they buy, like gasoline, more expensive, not just manufactured goods. The exception to the rule is during wartime. For example, as a result of World War II the United States dollar became stronger AND manufacturing profitability increased at the same time. But this was because competitors in Europe had all their factories in ruins, which is not the case today obviously."
Why would a cash-rich company borrow money?,"You are right that it seems strange why a cash-rich company is borrowing. In the case of Apple, the money that they are borrowing is being used to pay dividends to shareholders. The reason why they aren't using their \$200 billion is because doing so would cost them tens of billions of dollars in taxes. The current US tax code taxes corporations at 35% when they bring money they made overseas back to the US (this is called repatriation). So while, Apple has \$200 billion on the surface, \$180 billion of that is sitting overseas and can't be used to pay shareholders. It is cheaper for them to borrow money to pay dividends, than incur the huge tax costs associated with repatriation. In general, however, borrowing is also attractive to firms today since interest rates are so low. If you can borrow at 4%, and expect to earn a return of 8% on that capital, taking on debt seems like a good deal. Therefore firms flush with cash still may borrow if they feel the return they can earn on the borrowed money is greater than the cost of interest.  Check out this article from Bloomberg for more information on the Apple case: Bloomberg Article"
Why is the percentage of Australian businesses that don't have any employees so high?,"Employment excludes non-salaried directors, volunteers, persons paid by commission only, and self employed persons such as consultants and contractors. The actively trading businesses with zero employees are therefore those businesses where the staff members are drawn exclusively from that group. That may cover most one-person outfits, perhaps some family firms, groups of self-employed people, and so on. These businesses are typically formed in that way as a response to particular regulations, usually tax rules. The answer is in the explanatory notes attached to that dataset, section 56, from where the above quote is taken.  The explanatory notes are attached to that dataset to answer questions such as this. When you have questions about a particular dataset, the explanatory notes (which might also be called metadata, data descriptors, or similar), will usually have the answer, as they do in this case, and should be any enquirer's first place to look."
What are the differences between the older gold standard and the current fiat money standard?,"The gold standard in the US was actually kind of complicated. You had the government issuing notes off of gold, and private banks issuing bank notes off of gold...but also banks issuing notes/deposits off governments notes as well.  A pyramid on a pyramid. Neither was 100% backed and both were vulnerable to runs.   In 1933, FDR no longer let the public redeem gold for dollars.  A default of sorts...but we were never on a true gold standard!  We always had more promises than gold.  The same with the banks.  Given that gold used to be equivalent to dollars (as the Fed used to exchange the two), banks would use dollars as reserves for convenience...but soon they became the monetary base. Now the US still maintained an international gold standard.  But this too was fractional and had more promises than assets.  Countries (most notably) the French did not like how ""overbooked"" the dollar was to gold and started redeeming their gold in mass.  There was a run by international central banks on the dollar and Nixon had no choice but to take us off the international gold standard as well. So it is confusing, but we went off the gold standard twice.  Once to the public (FDR) and once to international central banks (Nixon). So how did this work before 1972?  Gold was the international reserve currency, but on top of this the dollar was the means by which gold would be commonly traded between countries.  It was much more difficult to clear transactions in gold than paper (or accounting) dollars, which led to its popularity as a reserve currency.  On occasion of course individuals would demand gold for dollars and you would have physical transfers take place (as in load the gold onto the ship and send it over the high seas). If you are interested in the history of the American gold standard, I highly recommend the book A History of Money and Banking in the United States:
The Colonial Era to World War II by Murray N. Rothbard: mises.org So what is the difference between the old system and the current system?  Not much as we were never on a true gold standard.  But in a nutshell, banks used to be able to redeem dollar bills or central bank deposits for gold.  Now dollars are not redeemdable for anything.  You can see on the Fed balance sheet a holdover for this as gold is still considered an asset and dollars (paper and electronic) as liabilities. en.wikipedia.org So how would trade occur pre-Bretton Woods?  Mostly through banks.  The receiver bank would obtain credit for gold, or a national currency that was convertible to gold (dollars or say pounds).  If one party wanted to convert currencies they could but it mostly took place as a transfer from say ""pound deposits"" to ""dollar deposits"".  If the new party demanded their payment in gold they could obtain this but it rarely happened. After Bretton Woods, international trade was mostly facilitated by dollar deposits with the promise of gold redemption of a foreign central bank demanded it (which Nixon ended). Now when gold reserves were physically demanded, did this cause problems?  Yes!  Remember, all countries and banks issued more notes/deposits for dollars than they had (somewhat dishonest but that is another matter).  Say the pound (during the standard) lost a bunch of reserves...so they go from a ratio of say 1000 central bank deposits to 200 gold to 900 central bank deposits and 100 gold.  A serious blow to credibility which could lead to a run on the currency. Do they have to print or destroy more notes/deposits...no because they do not 100% back gold. So why do countries hold reserve currencies today?  They kind of don't need to, but there is a reason.  Basically private banks operate by maturity mismatching (balancing short term debt against long term assets).  When a currency is not stable (like from a small nation) short term interest rates are too volatile for banking and in fact bank runs can be initiated from clever predator foreign inverters (like George Soros).  Politicians like banks...and for some reason they think they are important to the economy, so go to great ends to prop them up.  For a small nation this means their currency must be somewhat stable for their banking system to survive...and the nation achieves this by maintaining a peg.  You can only maintain a peg if you can manipulate the market, so countries hoard foreign assets like dollars so they can buy and sell their home currency (like say the Peso) so it achieve its peg rate. As for clearing of international transactions this is mostly done with bank money, but dollars are a common facilitating base for these transactions (although certainly many other currencies are being used now)."
